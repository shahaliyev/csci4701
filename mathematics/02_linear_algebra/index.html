
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://shahaliyev.org/csci4701/mathematics/02_linear_algebra/">
      
      
        <link rel="prev" href="../01_calculus/">
      
      
        <link rel="next" href="../03_probability/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Linear Algebra - CSCI 4701 Deep Learning</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
      <link rel="stylesheet" href="../../assets/styles/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-S1QRRQG9BM"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-S1QRRQG9BM",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-S1QRRQG9BM",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#linear-algebra" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="CSCI 4701 Deep Learning" class="md-header__button md-logo" aria-label="CSCI 4701 Deep Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CSCI 4701 Deep Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Linear Algebra
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/shahaliyev/csci4701" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../course/spring-2026/01_syllabus/" class="md-tabs__link">
          
  
  
  Course

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../introduction/01_deep_learning/" class="md-tabs__link">
          
  
  
  Introduction

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../notebooks/" class="md-tabs__link">
          
  
  
  Notebooks

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
  
  Mathematics

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../supplementary/" class="md-tabs__link">
          
  
  
  Supplementary

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../advanced/" class="md-tabs__link">
          
  
  
  Advanced

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="CSCI 4701 Deep Learning" class="md-nav__button md-logo" aria-label="CSCI 4701 Deep Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    CSCI 4701 Deep Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/shahaliyev/csci4701" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Course
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Course
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_1" >
        
          
          <label class="md-nav__link" for="__nav_1_1" id="__nav_1_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Spring 2026
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Spring 2026
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course/spring-2026/01_syllabus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Syllabus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course/spring-2026/02_assesments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Assessments
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course/spring-2026/03_project/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Introduction
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Introduction
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/01_deep_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Deep Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/02_machine_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Machine Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/03_resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Notebooks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Notebooks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture Notebooks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/01_backprop/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01. From Derivatives to Backpropagation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/02_neural_network/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02. From Neuron to Neural Network
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/03_cnn_torch/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03. From Kernel to Convolutional Neural Network
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/04_regul_optim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    04. Regularization and Optimization
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/05_batchnorm_resnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    05. Batch Normalization and Residual Blocks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/06_nn_ngram/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    06. Neural Network N-Gram Model
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/07_vae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    07. Variational Autoencoders (VAE)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Mathematics
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Mathematics
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Mathematics of Deep Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Calculus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Linear Algebra
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Linear Algebra
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#scalars-and-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scalars and Vectors
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#matrices-and-tensors" class="md-nav__link">
    <span class="md-ellipsis">
      
        Matrices and Tensors
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transpose-identity-inversion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transpose, Identity, Inversion
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vector-and-matrix-multiplications" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vector and Matrix Multiplications
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-dependence-and-span" class="md-nav__link">
    <span class="md-ellipsis">
      
        Linear Dependence and Span
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Linear Systems
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basis-and-rank" class="md-nav__link">
    <span class="md-ellipsis">
      
        Basis and Rank
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#norms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Norms
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#diagonal-symmetric-orthogonal-matrices" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diagonal, Symmetric, Orthogonal Matrices
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#eigenvalues-and-eigenvectors" class="md-nav__link">
    <span class="md-ellipsis">
      
        Eigenvalues and Eigenvectors
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#determinant" class="md-nav__link">
    <span class="md-ellipsis">
      
        Determinant
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#eigendecomposition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Eigendecomposition
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#singular-value-decomposition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Singular Value Decomposition
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#moorepenrose-pseudoinverse" class="md-nav__link">
    <span class="md-ellipsis">
      
        Moore–Penrose Pseudoinverse
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#trace-operator" class="md-nav__link">
    <span class="md-ellipsis">
      
        Trace Operator
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Probability Theory
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04_information/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Information Theory
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05_prob_modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Probabilistic Modeling
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Supplementary
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Supplementary
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supplementary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Suppementary Material
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supplementary/pca/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Principal Component Analysis
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supplementary/svd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Singular Value Decomposition
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Advanced
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Advanced
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Advanced Material
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#scalars-and-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scalars and Vectors
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#matrices-and-tensors" class="md-nav__link">
    <span class="md-ellipsis">
      
        Matrices and Tensors
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transpose-identity-inversion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transpose, Identity, Inversion
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vector-and-matrix-multiplications" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vector and Matrix Multiplications
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-dependence-and-span" class="md-nav__link">
    <span class="md-ellipsis">
      
        Linear Dependence and Span
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Linear Systems
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basis-and-rank" class="md-nav__link">
    <span class="md-ellipsis">
      
        Basis and Rank
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#norms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Norms
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#diagonal-symmetric-orthogonal-matrices" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diagonal, Symmetric, Orthogonal Matrices
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#eigenvalues-and-eigenvectors" class="md-nav__link">
    <span class="md-ellipsis">
      
        Eigenvalues and Eigenvectors
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#determinant" class="md-nav__link">
    <span class="md-ellipsis">
      
        Determinant
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#eigendecomposition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Eigendecomposition
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#singular-value-decomposition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Singular Value Decomposition
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#moorepenrose-pseudoinverse" class="md-nav__link">
    <span class="md-ellipsis">
      
        Moore–Penrose Pseudoinverse
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#trace-operator" class="md-nav__link">
    <span class="md-ellipsis">
      
        Trace Operator
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="linear-algebra">Linear Algebra</h1>
<div style="margin:.3rem 0 1rem;font-size:.9em;color:#555;display:flex;align-items:center;gap:.35rem;font-family:monospace">
  <time datetime="2026-01-26">26 Jan 2026</time>
</div>

<p>Linear algebra is the branch of mathematics that studies vector spaces and the linear mappings between them. In deep learning, almost all computation is formulated in the language of linear algebra: data, model parameters, activations, gradients are represented as vectors or matrices. A clear understanding of what these objects represent — and how they behave under linear operations — is necessary not only for correct implementation, but for reasoning about model structure, learning dynamics, and numerical behavior.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>The following sources were consulted in preparing this material:</p>
<ul>
<li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep Learning</em>. MIT Press. <a href="https://www.deeplearningbook.org/contents/linear_algebra.html">Chapter 2: Linear Algebra</a>.</li>
<li>Sanderson, G. <em>Essence of Linear Algebra</em>. 3Blue1Brown. <a href="https://www.3blue1brown.com/topics/linear-algebra">https://www.3blue1brown.com/topics/linear-algebra</a></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>Please note that some concepts in this material are simplified for pedagogical purposes. These simplifications slightly reduce precision but preserve the core ideas relevant to deep learning.</p>
</div>
<h2 id="scalars-and-vectors">Scalars and Vectors</h2>
<p>A <a href="https://en.wikipedia.org/wiki/Scalar_(mathematics)">scalar</a> is a single number (often real-valued). It is more challenging to define a <strong>vector</strong>. From a mathematician's point of view, a vector is an element of a <a href="https://en.wikipedia.org/wiki/Vector_space">vector space</a>: something you can <strong>add and scale while satisfying certain axioms</strong> (closure, associativity, distributivity, etc.). The axioms exist to guarantee that linear combinations behave predictably. From these requirements, any <a href="https://en.wikipedia.org/wiki/Linear_map">linear map</a> <span class="arithmatex">\(f\)</span> between vector spaces satisfies the following combined property of additivity and homogeneity (scaling):
$$
f(\alpha x + \beta y) = \alpha f(x) + \beta f(y).
$$
This equation does not define vectors themselves, but rather characterizes <a href="https://en.wikipedia.org/wiki/Linearity">linear</a> transformations acting on vectors. Vectors are defined by the operations of addition and scalar multiplication and linear maps are functions that preserve this structure.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Raw audio signals satisfy the linearity properties to a good approximation. If two sounds are played at the same time, the resulting waveform is (approximately) the sum of the individual waveforms. If the volume of a sound is increased or decreased, its waveform is scaled by a constant factor. Because audio combines by superposition and scales linearly with amplitude, it can be naturally represented as a vector and manipulated using linear algebra.</p>
</div>
<p>From a physicist's point of view, a vector represents a <strong>quantity with direction and magnitude</strong> (e.g. velocity, force). You add forces, scale forces, decompose into components. The vector predicts physical behavior. Lastly, from a computer scientist's point of view, a vector is simply an <strong>array of numbers</strong>. It can represent pixel values of an image, coordinates of a point, words in a document, etc.</p>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>In many ways, machine/deep learning borrows terminology from mathematics and uses it rather freely. Terms like <strong>vector</strong>, <strong>dimension</strong>, <strong>space</strong>, <strong>metric</strong>, <strong>manifold</strong>, and even <strong>linear</strong> are frequently misused. For example, by "dimension" one could assume "vector size". This is convenient shorthand, but it can break intuition if you don't keep in mind the underlying differences between deep learning and mathematics which can use the same tools for different purposes.</p>
</div>
<p>A vector is often written explicitly as a column of numbers. For example, a vector with <span class="arithmatex">\(n\)</span> real-valued components can be written as
$$
\mathbf{v} =
\begin{bmatrix}
v_1 \\
v_2 \\
\vdots \\
v_n
\end{bmatrix}
\in \mathbb{R}^n
$$</p>
<p>In deep learning, such a vector is typically understood operationally: it is stored as a contiguous array of <span class="arithmatex">\(n\)</span> real numbers and is mathematically an element of <span class="arithmatex">\(\mathbb{R}^n\)</span>, the <a href="https://en.wikipedia.org/wiki/Cartesian_product">Cartesian product</a> of <span class="arithmatex">\(\mathbb{R}\)</span> with itself <span class="arithmatex">\(n\)</span> times. In this context, its "dimension" refers simply to its length <span class="arithmatex">\(n\)</span>. When <span class="arithmatex">\(n = 2\)</span> or <span class="arithmatex">\(n = 3\)</span>, the vector can be visualized geometrically as a point or an arrow. When <span class="arithmatex">\(n\)</span> is large, direct visualization is no longer possible, but the same algebraic operations — addition, scalar multiplication, dot products, and linear transformations — still apply. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Depending on context, vectors can be visualized in different ways. In geometry and physics, they are often drawn as arrows representing magnitude and direction. In other settings, a vector can be viewed as a function that assigns a value to each index or coordinate. These visualizations are useful for building intuition, especially in low dimensions, but they do not alter the underlying algebraic definition of a vector. Linear algebra itself does not rely on geometric interpretation. It is fundamentally an algebraic theory of vector spaces and linear maps, and all definitions and results are independent of visualization. Geometry serves only as an intuitive aid not as a prerequisite. Beyond three dimensions, geometry in the visual sense becomes unusable. Since most representations in deep learning live in very high-dimensional spaces, geometric visualization is generally not available and plays no direct role in practice. What remains meaningful are algebraic and analytical notions — such as inner products, norms, projections, and linear maps — rather than pictures or spatial intuition.</p>
</div>
<h2 id="matrices-and-tensors">Matrices and Tensors</h2>
<p>A <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)">matrix</a> is a rectangular array of numbers arranged in rows and columns. Formally, a real-valued matrix with <span class="arithmatex">\(m\)</span> rows and <span class="arithmatex">\(n\)</span> columns is an element of <span class="arithmatex">\(\mathbb{R}^{m \times n}\)</span>:
$$
\mathbf{A} =
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix}
$$</p>
<p>From a mathematical point of view, a matrix represents a linear map between vector spaces. Given a vector <span class="arithmatex">\(\mathbf{x} \in \mathbb{R}^n\)</span>, multiplication by a matrix <span class="arithmatex">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> produces a new vector <span class="arithmatex">\(\mathbf{y} \in \mathbb{R}^m\)</span>: $$
\mathbf{A}\mathbf{x} = \mathbf{y}.$$ </p>
<p>This operation encodes all <strong>linear transformations</strong>: rotations, scalings, projections, and combinations of these. The key idea is that matrices do not just store numbers; they describe how vectors are transformed. In deep learning, matrices appear everywhere. For example, in</p>
<ul>
<li>Model parameters (weights of fully connected layers)</li>
<li>Batches of input data</li>
<li>Linear layers of the form <span class="arithmatex">\( \mathbf{y} = \mathbf{W}\mathbf{x} + \mathbf{b} \)</span></li>
<li><a href="../01_calculus">Jacobians and Hessians</a> (implicitly, through <a href="../../notebooks/01_backprop">automatic differentiation</a>)</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>In deep learning, layers of the form <span class="arithmatex">\(\mathbf{W}\mathbf{x}+\mathbf{b}\)</span> are often informally called "linear". This shorthand is convenient in practice, but it is useful to remember that the bias term shifts the output and allows models to represent functions that a purely linear map could not. Strictly speaking, the map <span class="arithmatex">\(\mathbf{x}\mapsto \mathbf{W}\mathbf{x}\)</span> is linear: it preserves addition, scaling, and maps the zero vector to the zero vector. Adding a bias term <span class="arithmatex">\(\mathbf{b}\)</span> produces an <a href="https://en.wikipedia.org/wiki/Affine_transformation">affine map</a>, which is a linear transformation followed by a translation. Because of this translation, affine maps do not preserve the origin.</p>
</div>
<p>A matrix is stored as a 2D array in memory, but it should be understood as a single object representing a linear operation. Confusing these two viewpoints — matrix as data vs. matrix as transformation — is a common source of misunderstanding.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When training <a href="../../notebooks/02_neural_network">neural networks</a>, we rarely reason about individual entries of a matrix. Instead, we reason about the effect of the matrix as a whole: how it mixes input features, how it changes dimensionality, and how it interacts with nonlinearities. Frameworks exploit this by implementing matrix multiplication using highly optimized numerical kernels.</p>
</div>
<p>A <a href="https://en.wikipedia.org/wiki/Tensor_(machine_learning)">tensor</a> is a generalization of scalars, vectors, and matrices to higher dimensions. Informally: a scalar is a 0-order tensor, a vector is a 1-order tensor, a matrix is a 2-order tensor, etc. In deep learning practice, a tensor is best understood as a multidimensional array of numbers with a fixed shape.</p>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>In pure mathematics and physics, tensors have a precise coordinate-independent definition involving multilinear maps. In deep learning, the word <em>tensor</em> is used more loosely to mean <em>n</em>-dimensional array. This is a practical simplification, but it is important not to confuse it with the full mathematical theory of tensors.</p>
</div>
<p>Tensor operations in deep learning are designed to preserve linear structure wherever possible. Linear operations (e.g. matrix multiplication, <a href="../../notebooks/03_cnn_torch">convolution</a>) remain linear even when expressed in tensor form. Most neural network layers can be viewed as linear maps acting on tensors, followed by nonlinear functions applied elementwise. Understanding which parts of a computation are linear and which are not is essential for reasoning about optimization and numerical stability.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>High-dimensional tensors cannot be visualized geometrically. Their meaning comes from structure and indexing, not from geometrical intuition. What matters is how dimensions correspond to data and how linear operations act along specific axes (e.g. columns, rows).</p>
</div>
<h2 id="transpose-identity-inversion">Transpose, Identity, Inversion</h2>
<p>The <strong>transpose</strong> of a matrix swaps rows and columns. For a matrix <span class="arithmatex">\(\mathbf{A}\in\mathbb{R}^{m\times n}\)</span>, its transpose <span class="arithmatex">\(\mathbf{A}^\top\in\mathbb{R}^{n\times m}\)</span> is defined by <span class="arithmatex">\(\mathbf{A}^\top_{ij} = a_{ji}.\)</span> A column vector becomes a row vector, and vice versa. Basically, transpose reflects (like a mirror) a matrix across its main diagonal. Elements on the diagonal remain fixed, off-diagonal elements are mirrored:
$$
\mathbf{A} =
\left[
\begin{array}{ccc}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23}
\end{array}
\right]
\quad\Rightarrow\quad
\mathbf{A}^\top =
\left[
\begin{array}{cc}
a_{11} &amp; a_{21} \\
a_{12} &amp; a_{22} \\
a_{13} &amp; a_{23}
\end{array}
\right]
$$</p>
<p>The <strong>identity</strong>  matrix <span class="arithmatex">\(\mathbf{I}\in\mathbb{R}^{n\times n}\)</span> is defined by a matrix whose diagonal are ones with all other elements being zeros:
$$
\mathbf{I} =
\left[
\begin{array}{cccc}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{array}
\right].
$$</p>
<p>The identity matrix represents the linear map that leaves every vector unchanged and acts as the same way that <span class="arithmatex">\(1\)</span> does in the rational numbers:
$$
\mathbf{I}\mathbf{x} = \mathbf{x}, \qquad
\mathbf{A}\mathbf{I} = \mathbf{I}\mathbf{A} = \mathbf{A}.
$$</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Identity matrices appear implicitly in <a href="../../notebooks/05_batchnorm_resnet">residual connections</a> and linear solvers. Adding <span class="arithmatex">\(\mathbf{I}\)</span> to a matrix corresponds to biasing a transformation toward preserving information.</p>
</div>
<p>The matrix <strong>inversion</strong> provides a formal way to solve linear systems of the form <span class="arithmatex">\(\mathbf{y} = \mathbf{A}\mathbf{x}\)</span>. If the matrix <span class="arithmatex">\(\mathbf{A}\)</span> is square (<span class="arithmatex">\(n \times n\)</span>) and <a href="https://en.wikipedia.org/wiki/Invertible_matrix">invertible</a>, there exists a matrix <span class="arithmatex">\(\mathbf{A}^{-1}\)</span> such that
<span class="arithmatex">\(\mathbf{A}^{-1}\mathbf{A} = \mathbf{I}.\)</span>
Multiplying both sides of the equation by <span class="arithmatex">\(\mathbf{A}^{-1}\)</span> yields
<span class="arithmatex">\(\mathbf{x}=\mathbf{A}^{-1}\mathbf{y}.\)</span></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Matrix inversion corresponds to undoing a linear transformation: applying <span class="arithmatex">\(\mathbf{A}^{-1}\)</span> reverses the effect of <span class="arithmatex">\(\mathbf{A}\)</span>. In practice, however, explicit matrix inversion is rarely used in numerical computation or deep learning. It is primarily a theoretical tool. Solving linear systems is typically done using more stable and efficient methods that avoid forming <span class="arithmatex">\(\mathbf{A}^{-1}\)</span> directly, especially when matrices are large or ill-conditioned.</p>
</div>
<h2 id="vector-and-matrix-multiplications">Vector and Matrix Multiplications</h2>
<p>Linear algebra uses small set of multiplication rules which make sure that the initial axioms are followed. In deep learning, nearly every forward and backward computation reduces to combinations of the operations described here.</p>
<p><strong>Dot (inner) product.</strong> For <span class="arithmatex">\(\mathbf{x},\mathbf{y}\in\mathbb{R}^n\)</span>,
$
\mathbf{x}\cdot\mathbf{y}=\sum_{i=1}^n x_i y_i .
$
The result is a scalar. Algebraically, the dot product is <a href="https://en.wikipedia.org/wiki/Bilinear_map">bilinear</a>: linear in each argument when the other is held fixed<sup id="fnref:bilinear"><a class="footnote-ref" href="#fn:bilinear">1</a></sup>. This property is essential for gradient-based optimization. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The dot product has several complementary interpretations. It measures how strongly <span class="arithmatex">\(\mathbf{x}\)</span> aligns with weights <span class="arithmatex">\(\mathbf{y}\)</span> by summing componentwise contributions. Geometrically (when visualization is possible), it measures alignment between vectors: large positive values indicate similar directions, values near zero indicate near-orthogonality, and negative values indicate opposing directions. In practice, it appears as neuron pre-activations, similarity scores, attention mechanisms (queries-keys), and projections.</p>
</div>
<p>The dot product between vectors can also be written in matrix form as
<span class="arithmatex">\(\mathbf{x}\cdot\mathbf{y} = \mathbf{x}^\top \mathbf{y}\)</span>, and is commutative: <span class="arithmatex">\(\mathbf{x}^\top \mathbf{y} = \mathbf{y}^\top \mathbf{x}.\)</span> </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Orientation matters. <span class="arithmatex">\(\mathbf{x}\mathbf{y}^\top\)</span> and <span class="arithmatex">\(\mathbf{x}^\top\mathbf{y}\)</span> are different objects with different meanings. Many shape errors in neural network implementations come from ignoring this distinction.</p>
</div>
<p><strong>Hadamard (elementwise) product.</strong> The Hadamard product multiplies vectors componentwise:
$
(\mathbf{x}\odot\mathbf{y})_i = x_i y_i .
$
The result is a vector in <span class="arithmatex">\(\mathbb{R}^n\)</span>. This operation does not mix coordinates: each output component depends only on the corresponding input components. For a fixed vector, it acts as a simple coordinate-wise scaling. In deep learning, the Hadamard product is used when features are masked, gated, or rescaled individually, such as in attention masks.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <strong>cross product</strong> known from physics curriculum is defined only in <span class="arithmatex">\(\mathbb{R}^3\)</span> (and, with <a href="https://en.wikipedia.org/wiki/Seven-dimensional_cross_product">special constructions</a>, <span class="arithmatex">\(\mathbb{R}^7\)</span>). It produces a vector orthogonal to its inputs and relies on three-dimensional geometry. Since deep learning representations typically live in high-dimensional spaces with no notion of "orthogonal direction in space", the cross product has no general role in deep learning and is not used in standard models.</p>
</div>
<p><strong>Matrix-vector multiplication.</strong> Let <span class="arithmatex">\(\mathbf{A}\in\mathbb{R}^{m\times n}\)</span> and <span class="arithmatex">\(\mathbf{x}\in\mathbb{R}^n\)</span>. Then
$$
\mathbf{y}=\mathbf{A}\mathbf{x}\in\mathbb{R}^m,\qquad
y_i=\sum_{j=1}^n a_{ij}x_j .
$$
Each output component is a <em>dot product</em> between one row of <span class="arithmatex">\(\mathbf{A}\)</span> and <span class="arithmatex">\(\mathbf{x}\)</span>. This is the fundamental linear operation in deep learning: rows act as learned feature detectors and dimensionality may change (<span class="arithmatex">\(n\to m\)</span>). </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A <a href="../../notebooks/02_neural_network">fully connected  layer</a> has the form <span class="arithmatex">\(\mathbf{y}=\mathbf{W}\mathbf{x}+\mathbf{b}\)</span>. The nonlinearity that follows does not alter the linearity of this step.</p>
</div>
<p><strong>Matrix-matrix multiplication.</strong> For <span class="arithmatex">\(\mathbf{A}\in\mathbb{R}^{m\times n}\)</span> and <span class="arithmatex">\(\mathbf{B}\in\mathbb{R}^{n\times p}\)</span>,
$$
\mathbf{C}=\mathbf{A}\mathbf{B}\in\mathbb{R}^{m\times p},\qquad
c_{ij}=\sum_{k=1}^n a_{ik}b_{kj}.
$$
This represents composition of linear maps: applying <span class="arithmatex">\(\mathbf{B}\)</span> then <span class="arithmatex">\(\mathbf{A}\)</span> equals applying <span class="arithmatex">\(\mathbf{A}\mathbf{B}\)</span>. Stacked linear layers, gradient propagation via transposes, and backpropagation all rely on this structure.</p>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>Matrix multiplication satisfies <em>distributivity</em> and <em>associativity</em>, but it is <strong>not</strong> <em>commutative</em> <span class="arithmatex">\(\mathbf{A}\mathbf{B}\neq\mathbf{B}\mathbf{A}\)</span>. This reflects the fact that matrix multiplication represents the composition of linear transformations. Changing the order changes which transformation is applied first, and therefore changes the result. Only in special cases—when two transformations are compatible in a specific way—does commutativity hold.</p>
</div>
<p>Note that, for any compatible matrices:
<span class="arithmatex">\((\mathbf{A}\mathbf{B})^\top = \mathbf{B}^\top \mathbf{A}^\top .\)</span>
The order reverses because transposition swaps rows and columns, effectively reversing the sequence of linear transformations. This property is used constantly in backpropagation, where gradients are propagated through layers via transposed weight matrices.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since a scalar is equal to its own transpose, this identity also explains why the dot product is commutative. Written in matrix form:
<span class="arithmatex">\(\mathbf{x}^\top \mathbf{y} = (\mathbf{x}^\top \mathbf{y})^\top = \mathbf{y}^\top \mathbf{x}.\)</span> What appears as a symmetry of vectors is therefore a direct consequence of more general properties of matrix transpose.</p>
</div>
<h2 id="linear-dependence-and-span">Linear Dependence and Span</h2>
<p>A collection of vectors is <strong>linearly dependent</strong> if at least one vector in the set can be written as a linear combination of the others. Formally, vectors <span class="arithmatex">\(\mathbf{v}_1,\dots,\mathbf{v}_k\)</span> are linearly dependent if there exist scalars <span class="arithmatex">\(\alpha_1,\dots,\alpha_k\)</span>, not all zero, such that
$$
\alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \cdots + \alpha_k \mathbf{v}_k = \mathbf{0}.
$$
Linear dependence means redundancy: some vectors do not add new directions or information. If no such non-trivial combination exists, the vectors are <strong>linearly independent</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In deep learning and applied linear algebra, linear dependence indicates unnecessary or duplicated features. Independent vectors represent genuinely distinct directions in a space.</p>
</div>
<p>The <strong>span</strong> of a set of vectors is the collection of all vectors that can be formed by taking linear combinations of them. Given vectors <span class="arithmatex">\(\mathbf{v}_1,\dots,\mathbf{v}_k\)</span>, their span consists of all vectors that can be written as
$$
\mathbf{s} = [\,\mathbf{v}_1\ \mathbf{v}_2\ \cdots\ \mathbf{v}_k\,]\boldsymbol{\alpha},
\qquad \boldsymbol{\alpha}\in\mathbb{R}^k.
$$</p>
<p>The span describes all vectors that are reachable using those directions. If the vectors are linearly dependent, their span does not grow when all vectors are included. Dependent vectors do not expand the space.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In practice, the span corresponds to the set of outputs a linear layer can produce. Linear dependence among columns of a weight matrix limits expressiveness, while linear independence maximizes the range of representable transformations.</p>
</div>
<p>Consider the linear system <span class="arithmatex">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span>
with <span class="arithmatex">\(\mathbf{A}\in\mathbb{R}^{m\times n}\)</span> and <span class="arithmatex">\(\mathbf{b}\in\mathbb{R}^m\)</span>. The system has a solution iff <span class="arithmatex">\(\mathbf{b}\)</span> lies in the span of the columns of <span class="arithmatex">\(\mathbf{A}\)</span>. This is called the <strong>column space</strong> (or range) of <span class="arithmatex">\(\mathbf{A}\)</span>. For the system to have a solution for all <span class="arithmatex">\(\mathbf{b}\in\mathbb{R}^m\)</span>, the column space of <span class="arithmatex">\(\mathbf{A}\)</span> must be all of <span class="arithmatex">\(\mathbb{R}^m\)</span>. This immediately requires <span class="arithmatex">\(n\ge m\)</span>. Otherwise, the column space has dimension at most <span class="arithmatex">\(n&lt;m\)</span> and cannot fill <span class="arithmatex">\(\mathbb{R}^m\)</span>. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For example, a <span class="arithmatex">\(3\times2\)</span> matrix can only produce a 2-dimensional plane inside <span class="arithmatex">\(\mathbb{R}^3\)</span>. The equation has a solution only when <span class="arithmatex">\(\mathbf{b}\)</span> lies on that plane.</p>
</div>
<p>The condition <span class="arithmatex">\(n\ge m\)</span> is necessary but not sufficient. Columns may be redundant. A matrix whose columns are linearly dependent does not expand its column space. Therefore, for the column space to equal <span class="arithmatex">\(\mathbb{R}^m\)</span>, the matrix must have rank <span class="arithmatex">\(m\)</span>, meaning that it contains <span class="arithmatex">\(m\)</span> linearly independent columns. This condition is necessary and sufficient for <span class="arithmatex">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span> to have a solution for every <span class="arithmatex">\(\mathbf{b}\in\mathbb{R}^m\)</span>.</p>
<div class="admonition success">
<p class="admonition-title">Exercise</p>
<p>Which properties must a matrix have in order to ensure <strong>uniqueness</strong> of the solution?</p>
</div>
<p>A square matrix with linearly dependent columns is called a <strong>singular</strong> matrix. If <span class="arithmatex">\(\mathbf{A}\)</span> is invertible, the unique solution is
<span class="arithmatex">\(\mathbf{x}=\mathbf{A}^{-1}\mathbf{b}.\)</span><sup id="fnref:inverse"><a class="footnote-ref" href="#fn:inverse">2</a></sup> If <span class="arithmatex">\(\mathbf{A}\)</span> is not square or is singular, solutions may still exist, but matrix inversion cannot be used.</p>
<p>Closely related to the column space is the <strong>null space</strong> of a matrix. The null space of <span class="arithmatex">\(\mathbf{A}\)</span> is the set of all vectors <span class="arithmatex">\(\mathbf{x}\)</span> such that
$
\mathbf{A}\mathbf{x} = \mathbf{0}.
$
Vectors in the null space are mapped to zero and therefore cannot be recovered from the output. A matrix has linearly dependent columns iff its null space contains nonzero vectors.</p>
<h2 id="linear-systems">Linear Systems</h2>
<p>Consider the <a href="https://en.wikipedia.org/wiki/Linear_system">linear system</a>
<span class="arithmatex">\(\mathbf{A}\mathbf{x}=\mathbf{b},\)</span> where <span class="arithmatex">\(\mathbf{A}\in\mathbb{R}^{m\times n}\)</span>. A solution <em>exists</em> iff the vector <span class="arithmatex">\(\mathbf{b}\)</span> lies in the column space of <span class="arithmatex">\(\mathbf{A}\)</span>. If <span class="arithmatex">\(\mathbf{b}\)</span> cannot be expressed as a linear combination of the columns of <span class="arithmatex">\(\mathbf{A}\)</span>, then no vector <span class="arithmatex">\(\mathbf{x}\)</span> can satisfy the equation.</p>
<p>If a solution exists, its <em>uniqueness</em> depends on the null space of <span class="arithmatex">\(\mathbf{A}\)</span>. If the null space contains only the <a href="https://en.wikipedia.org/wiki/Null_vector">zero vector</a>, then the solution is unique. In this case, no nonzero direction can be added to a solution without changing the output.</p>
<p>If the null space contains nonzero vectors, then <em>infinitely many solutions</em> exist. Any solution can be modified by adding a null-space vector, producing a different input that yields the same output. In this situation, the linear map collapses information: different inputs are indistinguishable after applying <span class="arithmatex">\(\mathbf{A}\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In elementary linear algebra, these cases are typically analyzed using <a href="https://en.wikipedia.org/wiki/Gaussian_elimination">Gaussian elimination</a> and row-echelon form. While these procedures are essential for conceptual understanding and small problems, they are not used directly in deep learning or large-scale numerical computation. Modern frameworks instead rely on matrix factorizations and optimized solvers that achieve the same goals more efficiently and with better numerical stability.</p>
</div>
<h2 id="basis-and-rank">Basis and Rank</h2>
<p>A <strong>basis</strong> of a vector space is a set of vectors that is both linearly independent and spanning the space. Every vector in the space can be written <em>uniquely</em> as a linear combination of the basis vectors. Consider the standard basis of <span class="arithmatex">\(\mathbb{R}^2\)</span>, given by
<span class="arithmatex">\(\mathbf{e}_1=[\,1\;\;0\,]^\top\)</span> and <span class="arithmatex">\(\mathbf{e}_2=[\,0\;\;1\,]^\top\)</span>.
Any vector <span class="arithmatex">\(\mathbf{x}\in\mathbb{R}^2\)</span> can be written uniquely as
$$
\mathbf{x}=x_1\mathbf{e}_1+x_2\mathbf{e}_2.
$$
The pair <span class="arithmatex">\((x_1,x_2)\)</span> are the coordinates of <span class="arithmatex">\(\mathbf{x}\)</span> in the standard basis. Now consider a different basis,
<span class="arithmatex">\(\mathbf{v}_1=[\,1\;\;1\,]^\top\)</span> and <span class="arithmatex">\(\mathbf{v}_2=[\,1\;\;-1\,]^\top\)</span>.
This set is linearly independent and spans <span class="arithmatex">\(\mathbb{R}^2\)</span>.
The same vector <span class="arithmatex">\(\mathbf{x}\)</span> can be written as
$$
\mathbf{x}=c_1\mathbf{v}_1+c_2\mathbf{v}_2,
$$
but the coefficients <span class="arithmatex">\((c_1,c_2)\)</span> are different. The vector itself has not changed, only its coordinates have.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Let <span class="arithmatex">\(\mathbf{V}=[\,\mathbf{v}_1\ \mathbf{v}_2\,]\)</span>.
Then <span class="arithmatex">\(\mathbf{x}=\mathbf{V}\mathbf{c}\)</span> and <span class="arithmatex">\(\mathbf{c}=\mathbf{V}^{-1}\mathbf{x}\)</span>.
Changing basis corresponds to switching between coordinate systems using <span class="arithmatex">\(\mathbf{V}\)</span> and its inverse.</p>
</div>
<p>If a space has a basis consisting of <span class="arithmatex">\(k\)</span> vectors, we say the space has <strong>dimension <span class="arithmatex">\(k\)</span></strong>. In <span class="arithmatex">\(\mathbb{R}^n\)</span>, any basis contains exactly <span class="arithmatex">\(n\)</span> vectors. For matrices, the analogous notion to dimension is <strong>rank</strong>. The rank of a matrix is the dimension of the space spanned by its columns (equivalently, its rows). It measures how many linearly independent directions the matrix preserves. If a matrix has rank <span class="arithmatex">\(r\)</span>, then its columns form a basis for an <span class="arithmatex">\(r\)</span>-dimensional subspace. </p>
<p>Consider the matrix
<span class="arithmatex">\(\mathbf{A}=
\begin{bmatrix}
1 &amp; 1 \\
2 &amp; 2
\end{bmatrix}\)</span>.
The second row is a multiple of the first, so <span class="arithmatex">\(\mathbf{A}\)</span> has rank <span class="arithmatex">\(1\)</span>.</p>
<p>Now take two different vectors,
<span class="arithmatex">\(\mathbf{x}_1 = [\,1\;\;0\,]^\top\)</span>
and
<span class="arithmatex">\(\mathbf{x}_2 = [\,0\;\;1\,]^\top\)</span>.
They are clearly distinct. Multiplying by <span class="arithmatex">\(\mathbf{A}\)</span> gives
<span class="arithmatex">\(\mathbf{A}\mathbf{x}_1 = [\,1\;\;2\,]^\top\)</span>
and
<span class="arithmatex">\(\mathbf{A}\mathbf{x}_2 = [\,1\;\;2\,]^\top\)</span>. Although <span class="arithmatex">\(\mathbf{x}_1 \neq \mathbf{x}_2\)</span>, they are mapped to the same output. The matrix cannot distinguish between directions that differ only within its null space. Information is collapsed because the transformation preserves only one independent direction.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In deep learning, rank determines whether a linear layer preserves information or collapses it into a lower-dimensional representation. </p>
</div>
<h2 id="norms">Norms</h2>
<p>Sometimes we need to measure the size of a vector. In machine learning, this is usually done using a <strong>norm</strong>. Formally, the <span class="arithmatex">\(L_p\)</span> norm of a vector <span class="arithmatex">\(\mathbf{x}\in\mathbb{R}^n\)</span> is defined as
$$
|\mathbf{x}|_p=\left(\sum_i |x_i|^p\right)^{1/p},
\qquad p\ge 1.
$$</p>
<p>Intuitively, a norm measures the distance from the origin to the point <span class="arithmatex">\(\mathbf{x}\)</span>. More precisely, a norm is any function <span class="arithmatex">\(f\)</span> satisfying:</p>
<ul>
<li><span class="arithmatex">\(f(\mathbf{x})=0 \Rightarrow \mathbf{x}=\mathbf{0}\)</span></li>
<li><span class="arithmatex">\(f(\mathbf{x}+\mathbf{y})\le f(\mathbf{x})+f(\mathbf{y})\)</span> (triangle inequality)</li>
<li><span class="arithmatex">\(f(\alpha\mathbf{x})=|\alpha|f(\mathbf{x})\)</span> for all <span class="arithmatex">\(\alpha\in\mathbb{R}\)</span></li>
</ul>
<p>The <span class="arithmatex">\(L_2\)</span> norm (<span class="arithmatex">\(p=2\)</span>), called the <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean norm</a>, is used so frequently that it is often written simply as <span class="arithmatex">\(\|\mathbf{x}\|\)</span>. Its square can be written compactly as
$
|\mathbf{x}|_2^2=\mathbf{x}^\top\mathbf{x}.
$</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <strong>squared</strong> <span class="arithmatex">\(L_2\)</span> norm is often preferred in optimization because it is smooth and has simple derivatives. </p>
</div>
<p>However, the squared <span class="arithmatex">\(L_2\)</span> norm grows slowly near zero, which makes it less suitable when distinguishing exact zeros from small nonzero values matters. In such cases, the <span class="arithmatex">\(L_1\)</span> norm, also known as the <a href="https://en.wikipedia.org/wiki/Taxicab_geometry">Manhattan distance</a>, is commonly used:
$
|\mathbf{x}|_1=\sum_i |x_i|
$. Here, each component contributes linearly, so moving an element away from zero by <span class="arithmatex">\(\varepsilon\)</span> increases the norm by exactly <span class="arithmatex">\(\varepsilon\)</span>. This property makes the <span class="arithmatex">\(L_1\)</span> norm useful for encouraging sparsity.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sometimes one wants to count the number of nonzero entries in a vector. This quantity is often (incorrectly) called the <span class="arithmatex">\(L_0\)</span> norm. It is not a true norm, because it is invariant under scaling (doesn't meet the third property described previously). In practice, the <span class="arithmatex">\(L_1\)</span> norm is often used as a continuous <a href="https://en.wikipedia.org/wiki/Surrogate_model">surrogate</a> for this count.</p>
</div>
<p>Another common norm is the <span class="arithmatex">\(L_\infty\)</span> norm (<a href="https://en.wikipedia.org/wiki/Uniform_norm">maximum norm</a>)<sup id="fnref:maxnorm"><a class="footnote-ref" href="#fn:maxnorm">3</a></sup>, defined as
$
|\mathbf{x}|_\infty=\max_i |x_i|.
$
It measures the magnitude of the largest component of the vector.</p>
<p>Finally, the dot product of two vectors can be expressed in terms of norms:
$
\mathbf{x}^\top\mathbf{y}=|\mathbf{x}|_2\,|\mathbf{y}|_2\cos\theta,
$
where <span class="arithmatex">\(\theta\)</span> is the angle between <span class="arithmatex">\(\mathbf{x}\)</span> and <span class="arithmatex">\(\mathbf{y}\)</span>. This relation explains why the dot product measures both magnitude and alignment, and why normalized dot products are often used as similarity measures in machine/deep learning.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Norms can also be defined for matrices. In deep learning, the most common choice is the <a href="https://en.wikipedia.org/wiki/Matrix_norm">Frobenius norm</a>, which is directly analogous to the <span class="arithmatex">\(L_2\)</span> norm of a vector.</p>
</div>
<h2 id="diagonal-symmetric-orthogonal-matrices">Diagonal, Symmetric, Orthogonal Matrices</h2>
<p>A <strong>diagonal matrix</strong> consists of zeros everywhere except possibly on the main diagonal. Formally, a matrix <span class="arithmatex">\(\mathbf{D}\)</span> is diagonal if <span class="arithmatex">\(D_{ij}=0\)</span> for all <span class="arithmatex">\(i\neq j\)</span>. The identity matrix is a special case of a diagonal matrix with all diagonal entries equal to 1. We write <span class="arithmatex">\(\mathrm{diag}(\mathbf{v})\)</span> to denote a square diagonal matrix whose diagonal entries are given by the vector <span class="arithmatex">\(\mathbf{v}\)</span>.</p>
<p>Diagonal matrices are important because multiplication by them is computationally efficient. The product <span class="arithmatex">\(\mathrm{diag}(\mathbf{v})\mathbf{x}\)</span> simply scales each component of <span class="arithmatex">\(\mathbf{x}\)</span> by the corresponding entry of <span class="arithmatex">\(\mathbf{v}\)</span>: 
$$
\mathrm{diag}(\mathbf{v})\mathbf{x} = \mathbf{v}\odot\mathbf{x}.
$$</p>
<p>Inversion is also efficient. A square diagonal matrix is invertible iff all diagonal entries are nonzero, in which case
$$
\mathrm{diag}(\mathbf{v})^{-1} = \mathrm{diag}\bigl([1/v_1,\dots,1/v_n]^\top\bigr).
$$</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Many algorithms can be simplified and accelerated by restricting certain matrices to be diagonal.</p>
</div>
<p>A <strong>symmetric matrix</strong> is a matrix equal to its transpose:
$
\mathbf{A}=\mathbf{A}^\top.
$
Symmetric matrices often arise when entries depend on pairs of elements in an order-independent way, such as distance matrices where <span class="arithmatex">\(A_{ij}=A_{ji}\)</span>. A symmetric matrix <span class="arithmatex">\(\mathbf{A}\)</span> is called <em>positive semidefinite</em> if
<span class="arithmatex">\(\mathbf{x}^\top\mathbf{A}\mathbf{x}\ge 0\)</span> for all <span class="arithmatex">\(\mathbf{x}\)</span>, and <em>positive definite</em> if the inequality is strict for all <span class="arithmatex">\(\mathbf{x}\neq 0\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These notions describe how a matrix assigns a scalar value to every direction via the quadratic form <span class="arithmatex">\(\mathbf{x}^\top\mathbf{A}\mathbf{x}\)</span>. Matrices of the form <span class="arithmatex">\(\mathbf{A}^\top\mathbf{A}\)</span> and <a href="../03_probability">covariance</a> matrices are always positive semidefinite. When a matrix is positive definite, the expression <span class="arithmatex">\(\mathbf{x}^\top\mathbf{A}\mathbf{x}\)</span> behaves like a squared norm.</p>
</div>
<p>Two vectors <span class="arithmatex">\(\mathbf{x}\)</span> and <span class="arithmatex">\(\mathbf{y}\)</span> are orthogonal if <span class="arithmatex">\(\mathbf{x}^\top\mathbf{y}=0\)</span>. In <span class="arithmatex">\(\mathbb{R}^n\)</span>, at most <span class="arithmatex">\(n\)</span> nonzero vectors can be mutually orthogonal. If the vectors are both orthogonal and have unit norm<sup id="fnref:unitvector"><a class="footnote-ref" href="#fn:unitvector">4</a></sup>, they are called <strong>orthonormal</strong>. An <strong>orthogonal matrix</strong> is a square matrix whose rows and columns are orthonormal:
$$
\mathbf{A}^\top\mathbf{A}=\mathbf{A}\mathbf{A}^\top=\mathbf{I}.
$$
This implies
$
\mathbf{A}^{-1}=\mathbf{A}^\top,
$
so orthogonal matrices are especially convenient computationally. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Pay attention that "orthogonal" here means "orthonormal"; there is no standard term for matrices whose rows or columns are orthogonal but not normalized.</p>
</div>
<h2 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h2>
<p>Let <span class="arithmatex">\(\mathbf{A}\in\mathbb{R}^{n\times n}\)</span>. A nonzero vector <span class="arithmatex">\(\mathbf{v}\)</span> is called an <strong>eigenvector</strong> of <span class="arithmatex">\(\mathbf{A}\)</span> if there exists a scalar <span class="arithmatex">\(\lambda\)</span> such that
$
\mathbf{A}\mathbf{v} = \lambda \mathbf{v}.
$
The scalar <span class="arithmatex">\(\lambda\)</span> is the corresponding <strong>eigenvalue</strong>. </p>
<p>Illustrated below is the impact of the transformation matrix <span class="arithmatex">\(\mathbf{A}=\begin{bmatrix}2&amp;1\\1&amp;2\end{bmatrix}\)</span> on different vectors.</p>
<p><figure>
  <img src="../../assets/images/linear_algebra/eigenvectors.gif" alt="Eigenvectors and eigenvlues" style="max-width: 100%; height: auto;">
<br />
<figcaption style="margin-top: 0.5em; font-size: 0.9em; opacity: 0.85;">
    The transformation matrix preserves the directions of the magenta vectors parallel to $\mathbf{v}_{\lambda=1}=[\,1\;\;-1\,]^\top$ and the blue vectors parallel to $\mathbf{v}_{\lambda=3}=[\,1\;\;1\,]^\top$. Red vectors are not parallel to either eigenvector, so their directions change under the transformation. Magenta vectors keep the same length (eigenvalue $1$), while blue vectors become three times longer (eigenvalue $3$). By <a href="//commons.wikimedia.org/wiki/User:LucasVB" title="User:LucasVB">Lucas Vieira</a> - <span class="int-own-work" lang="en">Own work</span>, Public Domain, <a href="https://commons.wikimedia.org/w/index.php?curid=19449791">Link</a>
  </figcaption></p>
</figure>
<p>Eigenvectors identify directions that are preserved by the linear transformation <span class="arithmatex">\(\mathbf{A}\)</span>. Applying <span class="arithmatex">\(\mathbf{A}\)</span> to an eigenvector does not change its direction; it only scales it by the factor <span class="arithmatex">\(\lambda\)</span>. If <span class="arithmatex">\(|\lambda|&gt;1\)</span>, vectors in that direction are stretched, if <span class="arithmatex">\(|\lambda|&lt;1\)</span>, they are compressed, if <span class="arithmatex">\(\lambda=0\)</span>, the direction is collapsed.</p>
<h2 id="determinant">Determinant</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Determinant">determinant</a> of a square matrix <span class="arithmatex">\(\mathbf{A}\)</span>, denoted <span class="arithmatex">\(\det(\mathbf{A})\)</span>, is a scalar that summarizes how the linear transformation defined by <span class="arithmatex">\(\mathbf{A}\)</span> scales space. The determinant is equal to the product of the eigenvalues of <span class="arithmatex">\(\mathbf{A}\)</span>:
$$
\det(\mathbf{A})=\prod_{i=1}^n \lambda_i.
$$
This interpretation becomes especially clear when <span class="arithmatex">\(\mathbf{A}\)</span> is diagonal or diagonalizable. For a diagonal matrix
$
\boldsymbol{\Lambda}=\mathrm{diag}(\lambda_1,\dots,\lambda_n),
$
the determinant is simply
$$
\det(\boldsymbol{\Lambda})=\lambda_1\lambda_2\cdots\lambda_n.
$$</p>
<p>Each diagonal entry scales space along one coordinate direction, and the determinant multiplies these scalings together. The following transformation scales area by a factor of 6.
$$
\boldsymbol{\Lambda}
=
\begin{bmatrix}
2 &amp; 0 \\
0 &amp; 3
\end{bmatrix}
\;\Rightarrow\;
\det(\boldsymbol{\Lambda}) = 2\cdot 3 = 6.
$$</p>
<div class="admonition success">
<p class="admonition-title">Exercise</p>
<p>Let <span class="arithmatex">\(\mathbf{A}=\mathbf{Q}\boldsymbol{\Lambda}\mathbf{Q}^\top\)</span> be the eigendecomposition of a real symmetric matrix, where <span class="arithmatex">\(\mathbf{Q}\)</span> is orthogonal and <span class="arithmatex">\(\boldsymbol{\Lambda}=\mathrm{diag}(\lambda_1,\dots,\lambda_n)\)</span>. Show that
$$
\det(\mathbf{A})=\det(\boldsymbol{\Lambda})=\prod_i \lambda_i,
$$
and explain why the orthogonality of <span class="arithmatex">\(\mathbf{Q}\)</span> implies that the change of basis does not affect volume.</p>
</div>
<p>Geometrically, the absolute value <span class="arithmatex">\(|\det(\mathbf{A})|\)</span> measures how much the transformation expands or contracts volume. If <span class="arithmatex">\(|\det(\mathbf{A})|&gt;1\)</span>, volume is expanded; if <span class="arithmatex">\(|\det(\mathbf{A})|&lt;1\)</span>, volume is contracted. If <span class="arithmatex">\(\det(\mathbf{A})=1\)</span>, volume is preserved. If <span class="arithmatex">\(\det(\mathbf{A})=0\)</span>, space is collapsed along at least one direction, causing the transformation to lose volume entirely. In this case, the matrix is <strong>singular</strong> and <strong>not invertible</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The determinant can also be defined directly in terms of matrix entries, without reference to eigenvalues. For example, for a <span class="arithmatex">\(2\times2\)</span> matrix,
$$
\mathbf{A}
=
\begin{bmatrix}
a &amp; b \\
c &amp; d
\end{bmatrix},
\qquad
\det(\mathbf{A}) = ad - bc.
$$
For larger matrices, the determinant is defined recursively via <a href="https://en.wikipedia.org/wiki/Laplace_expansion">cofactor expansion</a> or computed using row operations. While these formulas are often used for computation, the eigenvalue interpretation provides the clearest conceptual understanding of what the determinant represents for deep learning.</p>
</div>
<h2 id="eigendecomposition">Eigendecomposition</h2>
<p>If a matrix <span class="arithmatex">\(\mathbf{A}\)</span> has a full set of linearly independent eigenvectors, these can be arranged as columns of a matrix <span class="arithmatex">\(\mathbf{V}\)</span>, with the corresponding eigenvalues placed on the diagonal of a matrix <span class="arithmatex">\(\boldsymbol{\Lambda}\)</span>. Such a factorization is called the <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">eigendecomposition</a> and the matrix can then be written as
$$
\mathbf{A}=\mathbf{V}\boldsymbol{\Lambda}\mathbf{V}^{-1}.
$$</p>
<p>Eigendecomposition represents a linear transformation as:</p>
<ol>
<li>a change of basis into the eigenvector basis (<span class="arithmatex">\(\mathbf{V}^{-1}\)</span>),</li>
<li>independent scalings along each eigenvector direction (<span class="arithmatex">\(\boldsymbol{\Lambda}\)</span>),</li>
<li>a change back to the original basis (<span class="arithmatex">\(\mathbf{V}\)</span>).</li>
</ol>
<p>Eigendecomposition is also the cleanest example of a general computational principle: decompose a matrix into parts with simple structure. Suppose <span class="arithmatex">\(\mathbf{A}=\mathbf{V}\boldsymbol{\Lambda}\mathbf{V}^{-1}\)</span>. Then:
$$
\begin{aligned}
\mathbf{A}^2
&amp;= (\mathbf{V}\boldsymbol{\Lambda}\mathbf{V}^{-1})
   (\mathbf{V}\boldsymbol{\Lambda}\mathbf{V}^{-1}) \\
&amp;= \mathbf{V}\boldsymbol{\Lambda}
   (\mathbf{V}^{-1}\mathbf{V})
   \boldsymbol{\Lambda}\mathbf{V}^{-1} \\
&amp;= \mathbf{V}\boldsymbol{\Lambda}^2\mathbf{V}^{-1}.
\end{aligned}
$$</p>
<p>The middle factors <span class="arithmatex">\(\mathbf{V}^{-1}\mathbf{V}\)</span> cancel to the identity. Repeating this argument gives
$$
\mathbf{A}^k=\mathbf{V}\boldsymbol{\Lambda}^k\mathbf{V}^{-1}.
$$</p>
<p>Thus, repeated matrix multiplication reduces to raising the diagonal entries <span class="arithmatex">\(\lambda_i\)</span> to the power <span class="arithmatex">\(k\)</span>:
$$
\boldsymbol{\Lambda}^k
=\mathrm{diag}(\lambda_1,\dots,\lambda_n)^k
=\mathrm{diag}(\lambda_1^k,\dots,\lambda_n^k).
$$
Decomposition replaces repeated dense matrix multiplication with exponentiating scalars <span class="arithmatex">\(\lambda_i\)</span>. The expensive part—the interaction between coordinates—disappears: in the eigenvector basis, each component is scaled independently by <span class="arithmatex">\(\lambda_i^k\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Diagonal matrices are cheap to multiply, invert, and exponentiate, which is why eigendecomposition is so effective.</p>
</div>
<p>Not every matrix admits eigendecomposition with real eigenvalues and eigenvectors. However, every <em>real symmetric matrix</em> does:
$$
\mathbf{A}=\mathbf{Q}\boldsymbol{\Lambda}\mathbf{Q}^\top,
$$
where <span class="arithmatex">\(\mathbf{Q}\)</span> is orthogonal and <span class="arithmatex">\(\boldsymbol{\Lambda}\)</span> is real and diagonal. In this case, the inverse is especially simple: <span class="arithmatex">\(\mathbf{Q}^{-1}=\mathbf{Q}^\top\)</span>.</p>
<p>The eigendecomposition of a real symmetric matrix is not necessarily unique. When eigenvalues are repeated, any orthonormal basis of the corresponding eigenspace yields a valid decomposition. By convention, eigenvalues are usually ordered from largest to smallest.</p>
<p>Eigenvalues immediately reveal important properties. A matrix is <strong>singular</strong> iff at least one eigenvalue is zero. For symmetric matrices, eigenvalues also characterize quadratic forms:
$$
\mathbf{x}^\top\mathbf{A}\mathbf{x}, \qquad |\mathbf{x}|_2=1.
$$
The maximum and minimum values are the largest and smallest eigenvalues, attained at the corresponding eigenvectors. </p>
<h2 id="singular-value-decomposition">Singular Value Decomposition</h2>
<p>Eigendecomposition is defined only for square matrices. For a general matrix <span class="arithmatex">\(\mathbf{A}\in\mathbb{R}^{m\times n}\)</span>, the analogous tool is the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a>:
$$
\mathbf{A}=\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top,
$$
where <span class="arithmatex">\(\mathbf{U}\in\mathbb{R}^{m\times m}\)</span> and <span class="arithmatex">\(\mathbf{V}\in\mathbb{R}^{n\times n}\)</span> are orthogonal matrices, and <span class="arithmatex">\(\boldsymbol{\Sigma}\in\mathbb{R}^{m\times n}\)</span> is diagonal (not necessarily square). The diagonal entries of <span class="arithmatex">\(\boldsymbol{\Sigma}\)</span> are called the <strong>singular values</strong> of <span class="arithmatex">\(\mathbf{A}\)</span>. The columns of <span class="arithmatex">\(\mathbf{U}\)</span> are the <strong>left singular vectors</strong>, and the columns of <span class="arithmatex">\(\mathbf{V}\)</span> are the <strong>right singular vectors</strong>.</p>
<figure>
  <img src="../../assets/images/linear_algebra/svd.svg" alt="Singular Value Decomposition (SVD)" style="max-width: 100%; height: auto;">
  <figcaption style="margin-top: 0.5em; font-size: 0.9em; opacity: 0.85;">
    Illustration of the singular value decomposition $U\Sigma V^{*}$ of a real $2 \times 2$ matrix $M$. Top: The action of $M$, indicated by its effect on the unit disc $D$ and the two canonical unit vectors $e_1$ and $e_2$. Left: The action of $V^{*}$, a rotation, on $D$, $e_1$, and $e_2$. Bottom: The action of $\Sigma$, a scaling by the singular values $\sigma_1$ horizontally and $\sigma_2$ vertically. Right: The action of $U$, another rotation. By <a href="//commons.wikimedia.org/wiki/User:Georg-Johann" title="User:Georg-Johann">Georg-Johann</a> - <span class="int-own-work" lang="en">Own work</span>, <a href="https://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=11342212">Link</a>
  </figcaption>
</figure>

<p>Singular value decomposition is closely related to eigendecomposition. The left singular vectors of <span class="arithmatex">\(\mathbf{A}\)</span> are the eigenvectors of <span class="arithmatex">\(\mathbf{A}\mathbf{A}^\top\)</span>, and the right singular vectors are the eigenvectors of <span class="arithmatex">\(\mathbf{A}^\top\mathbf{A}\)</span>. The nonzero singular values are the square roots of the nonzero eigenvalues of <span class="arithmatex">\(\mathbf{A}^\top\mathbf{A}\)</span> (and equivalently of <span class="arithmatex">\(\mathbf{A}\mathbf{A}^\top\)</span>).</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For a more detailed explanation on how decompositions emerge, see the supplementary material on the <a href="../../supplementary/svd">singular value decomposition</a>. </p>
</div>
<p>One important use of singular value decomposition is that it provides a principled way to extend matrix inversion to non-square or singular matrices via the pseudoinverse.</p>
<h2 id="moorepenrose-pseudoinverse">Moore–Penrose Pseudoinverse</h2>
<p>Matrix inversion is not defined for non-square matrices. Suppose we want a matrix <span class="arithmatex">\(\mathbf{B}\)</span> that acts like a left-inverse so that we can solve
<span class="arithmatex">\(\mathbf{A}\mathbf{x}=\mathbf{y}\)</span>
by writing <span class="arithmatex">\(\mathbf{x}=\mathbf{B}\mathbf{y}\)</span>. Depending on the shape of <span class="arithmatex">\(\mathbf{A}\)</span>, an exact solution may not exist (tall matrices) or may not be unique (wide matrices).</p>
<p>The <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore–Penrose pseudoinverse</a> <span class="arithmatex">\(\mathbf{A}^+\)</span> provides a standard choice. It can be defined as
$$
\mathbf{A}^+ = \lim_{\alpha\to 0}(\mathbf{A}^\top\mathbf{A}+\alpha\mathbf{I})^{-1}\mathbf{A}^\top,
$$
but in practice it is computed using the SVD. If <span class="arithmatex">\(\mathbf{A}=\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top\)</span>, then
$$
\mathbf{A}^+ = \mathbf{V}\boldsymbol{\Sigma}^+\mathbf{U}^\top,
$$
where <span class="arithmatex">\(\boldsymbol{\Sigma}^+\)</span> is obtained by taking the reciprocal of each nonzero diagonal entry of <span class="arithmatex">\(\boldsymbol{\Sigma}\)</span> and then transposing the result.</p>
<p>If <span class="arithmatex">\(\mathbf{A}\)</span> has more columns than rows (<span class="arithmatex">\(n&gt;m\)</span>), the system may have infinitely many solutions. The pseudoinverse returns the solution with minimal Euclidean norm <span class="arithmatex">\(\|\mathbf{x}\|_2\)</span> among all solutions. If <span class="arithmatex">\(\mathbf{A}\)</span> has more rows than columns (<span class="arithmatex">\(m&gt;n\)</span>), the system may have no exact solution. In that case, <span class="arithmatex">\(\mathbf{x}=\mathbf{A}^+\mathbf{y}\)</span> minimizes the least-squares error <span class="arithmatex">\(\|\mathbf{A}\mathbf{x}-\mathbf{y}\|_2\)</span>. </p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To get a nice and clear geometric intuition on the matter, see the chapter on the <strong>method of least squares</strong> from David C. Lay's <em><a href="https://www.google.com/books/edition/_/bM6gBwAAQBAJ?hl=en&amp;newbks=1">Linear Algebra and its Applications</a></em>.  </p>
</div>
<h2 id="trace-operator">Trace Operator</h2>
<p>The <strong>trace</strong> of a square matrix is the sum of its diagonal entries:
<span class="arithmatex">\(\mathrm{Tr}(\mathbf{A}) = \sum_i A_{ii}.\)</span>
The trace is useful because it allows scalar quantities involving matrices to be written compactly using matrix products rather than explicit summations. For example, the Frobenius norm of a matrix can be written as
<span class="arithmatex">\(\|\mathbf{A}\|_F = \sqrt{\mathrm{Tr}(\mathbf{A}\mathbf{A}^\top)}.\)</span></p>
<p>Several properties of the trace are especially important in deep learning and optimization. The trace is invariant under transposition,
<span class="arithmatex">\(\mathrm{Tr}(\mathbf{A}) = \mathrm{Tr}(\mathbf{A}^\top),\)</span>
and invariant under <a href="https://en.wikipedia.org/wiki/Cyclic_permutation">cyclic permutation</a> of matrix products when dimensions are compatible:
$$
\mathrm{Tr}(\mathbf{A}\mathbf{B}\mathbf{C})
= \mathrm{Tr}(\mathbf{B}\mathbf{C}\mathbf{A})
= \mathrm{Tr}(\mathbf{C}\mathbf{A}\mathbf{B}).
$$
This cyclic property is essential in matrix calculus, as it allows expressions to be rearranged so that derivatives with respect to a given variable can be taken systematically.</p>
<p>In deep learning, the trace operator is used mainly "behind the scenes". It appears in derivations of gradients, Jacobians, and Hessians, and in the formulation of losses and regularization terms involving matrix products. Modern deep learning frameworks rely on these trace identities internally when implementing automatic differentiation and optimized linear algebra algorithms, even though the trace operator itself rarely appears in user code.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:bilinear">
<p>A function <span class="arithmatex">\(f(\mathbf{x},\mathbf{y})\)</span> is called <strong>bilinear</strong> if it is linear in each argument separately when the other argument is held fixed. For the dot product <span class="arithmatex">\(f(\mathbf{x},\mathbf{y})=\mathbf{x}\cdot\mathbf{y}\)</span>, this means:
Holding <span class="arithmatex">\(\mathbf{y}\)</span> fixed, the map <span class="arithmatex">\(\mathbf{x}\mapsto \mathbf{x}\cdot\mathbf{y}\)</span> is linear:
$$
(\alpha \mathbf{x}_1+\beta \mathbf{x}_2)\cdot\mathbf{y}
= \alpha(\mathbf{x}_1\cdot\mathbf{y})+\beta(\mathbf{x}_2\cdot\mathbf{y}).
$$
Holding <span class="arithmatex">\(\mathbf{x}\)</span> fixed, the map <span class="arithmatex">\(\mathbf{y}\mapsto \mathbf{x}\cdot\mathbf{y}\)</span> is also linear:
$$
\mathbf{x}\cdot(\alpha \mathbf{y}_1+\beta \mathbf{y}_2)
= \alpha(\mathbf{x}\cdot\mathbf{y}_1)+\beta(\mathbf{x}\cdot\mathbf{y}_2).
$$
Bilinearity does not mean the function is linear in both arguments at once. It means that if one vector is treated as constant, the dot product behaves exactly like a linear function of the other. This property is what allows dot products to distribute over sums and pull out scalar factors, and it is why gradients propagate cleanly through linear layers in deep learning.&#160;<a class="footnote-backref" href="#fnref:bilinear" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:inverse">
<p>So far, inverses were defined by left multiplication:
<span class="arithmatex">\(\mathbf{A}^{-1}\mathbf{A}=\mathbf{I}.\)</span>
A right inverse satisfies
<span class="arithmatex">\(\mathbf{A}\mathbf{A}^{-1}=\mathbf{I}.\)</span>
For square matrices, left and right inverses coincide.&#160;<a class="footnote-backref" href="#fnref:inverse" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:maxnorm">
<p>The <span class="arithmatex">\(L_\infty\)</span> norm is also known as the <strong>uniform norm</strong>. This name comes from functional analysis: a sequence of functions <span class="arithmatex">\(\{f_n\}\)</span> converges to a function <span class="arithmatex">\(f\)</span> under the metric induced by the uniform norm iff <span class="arithmatex">\(f_n\)</span> converges to <span class="arithmatex">\(f\)</span> <em>uniformly</em>, meaning the maximum deviation <span class="arithmatex">\(\sup_x |f_n(x)-f(x)|\)</span> goes to zero. In finite-dimensional vector spaces, this reduces to taking the maximum absolute component.&#160;<a class="footnote-backref" href="#fnref:maxnorm" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:unitvector">
<p>A <strong>unit vector</strong> is a vector with unit Euclidean norm:
$
|\mathbf{x}|_2=1.
$&#160;<a class="footnote-backref" href="#fnref:unitvector" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../01_calculus/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Calculus">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Calculus
              </div>
            </div>
          </a>
        
        
          
          <a href="../03_probability/" class="md-footer__link md-footer__link--next" aria-label="Next: Probability Theory">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Probability Theory
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://shahaliyev.org/" target="_blank" rel="noopener" title="shahaliyev.org" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M399 384.2c-22.1-38.4-63.6-64.2-111-64.2h-64c-47.4 0-88.9 25.8-111 64.2 35.2 39.2 86.2 63.8 143 63.8s107.8-24.7 143-63.8M0 256a256 256 0 1 1 512 0 256 256 0 1 1-512 0m256 16a72 72 0 1 0 0-144 72 72 0 1 0 0 144"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/shahaliyev/csci4701" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://ada.edu.az/en/" target="_blank" rel="noopener" title="ada.edu.az" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M271.9 20.2c-9.8-5.6-21.9-5.6-31.8 0l-224 128c-12.6 7.2-18.8 22-15.1 36S17.5 208 32 208h32v208l-51.2 38.4C4.7 460.4 0 469.9 0 480c0 17.7 14.3 32 32 32h448c17.7 0 32-14.3 32-32 0-10.1-4.7-19.6-12.8-25.6L448 416V208h32c14.5 0 27.2-9.8 30.9-23.8s-2.5-28.8-15.1-36l-224-128zM400 208v208h-64V208zm-112 0v208h-64V208zm-112 0v208h-64V208zm80-112a32 32 0 1 1 0 64 32 32 0 1 1 0-64"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.sections", "navigation.expand", "navigation.top", "navigation.footer", "navigation.tabs", "content.code.copy", "content.code.annotate", "search.highlight", "search.suggest", "content.footnote.tooltips"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../assets/app/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>