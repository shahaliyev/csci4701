
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Probabilistic modeling is the practice of describing how data is generated using probability distributions. This page introduces likelihood, maximum likelihood estimation (MLE), negative log-likelihood, maximum a posteriori estimation (MAP), Bayesian inference, and structured probabilistic models used in machine learning and deep learning.
">
      
      
      
        <link rel="canonical" href="https://shahaliyev.org/csci4701/mathematics/05_prob_modeling/">
      
      
        <link rel="prev" href="../04_information/">
      
      
        <link rel="next" href="../../supplementary/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Probabilistic Modeling - CSCI 4701 Deep Learning</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
      <link rel="stylesheet" href="../../assets/styles/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-S1QRRQG9BM"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-S1QRRQG9BM",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-S1QRRQG9BM",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#probabilistic-modeling" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="CSCI 4701 Deep Learning" class="md-header__button md-logo" aria-label="CSCI 4701 Deep Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CSCI 4701 Deep Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Probabilistic Modeling
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/shahaliyev/csci4701" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../course/spring-2026/01_syllabus/" class="md-tabs__link">
          
  
  
  Course

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../introduction/01_deep_learning/" class="md-tabs__link">
          
  
  
  Introduction

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../notebooks/" class="md-tabs__link">
          
  
  
  Notebooks

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
  
  Mathematics

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../supplementary/" class="md-tabs__link">
          
  
  
  Supplementary

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../advanced/" class="md-tabs__link">
          
  
  
  Advanced

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="CSCI 4701 Deep Learning" class="md-nav__button md-logo" aria-label="CSCI 4701 Deep Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    CSCI 4701 Deep Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/shahaliyev/csci4701" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Course
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Course
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_1" >
        
          
          <label class="md-nav__link" for="__nav_1_1" id="__nav_1_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Spring 2026
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Spring 2026
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course/spring-2026/01_syllabus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Syllabus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course/spring-2026/02_assesments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Assessments
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course/spring-2026/03_project/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Introduction
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Introduction
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/01_deep_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Deep Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/02_machine_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Machine Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/03_resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Notebooks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Notebooks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture Notebooks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/01_backprop/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01. From Derivatives to Backpropagation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/02_neural_network/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02. From Neuron to Neural Network
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/03_cnn_torch/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03. From Kernel to Convolutional Neural Network
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/04_regul_optim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    04. Regularization and Optimization
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/05_batchnorm_resnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    05. Batch Normalization and Residual Blocks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/06_nn_ngram/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    06. Neural Network N-Gram Model
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/07_vae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    07. Variational Autoencoders (VAE)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Mathematics
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Mathematics
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Mathematics of Deep Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Calculus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02_linear_algebra/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Linear Algebra
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03_probability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Probability Theory
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04_information/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Information Theory
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Probabilistic Modeling
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Probabilistic Modeling
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#likelihood" class="md-nav__link">
    <span class="md-ellipsis">
      
        Likelihood
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#maximum-likelihood-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Maximum Likelihood Estimation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#negative-log-likelihood" class="md-nav__link">
    <span class="md-ellipsis">
      
        Negative Log-Likelihood
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Negative Log-Likelihood">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#binary-cross-entropy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Binary Cross-Entropy
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#categorical-cross-entropy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Categorical Cross-Entropy
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bayesian-inference" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bayesian Inference
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#maximum-a-posteriori-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Maximum A Posteriori Estimation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#latent-variable-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Latent Variable Models
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mixture-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixture Models
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#expectation-maximization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Expectation-Maximization
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#structured-probabilistic-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Structured Probabilistic Models
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generative-vs-discriminative-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Generative vs Discriminative Modeling
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Supplementary
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Supplementary
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supplementary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Suppementary Material
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supplementary/pca/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Principal Component Analysis
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supplementary/svd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Singular Value Decomposition
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supplementary/tsne/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    t-Distributed Stochastic Neighbor Embedding
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Advanced
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Advanced
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Advanced Material
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#likelihood" class="md-nav__link">
    <span class="md-ellipsis">
      
        Likelihood
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#maximum-likelihood-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Maximum Likelihood Estimation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#negative-log-likelihood" class="md-nav__link">
    <span class="md-ellipsis">
      
        Negative Log-Likelihood
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Negative Log-Likelihood">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#binary-cross-entropy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Binary Cross-Entropy
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#categorical-cross-entropy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Categorical Cross-Entropy
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bayesian-inference" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bayesian Inference
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#maximum-a-posteriori-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Maximum A Posteriori Estimation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#latent-variable-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Latent Variable Models
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mixture-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixture Models
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#expectation-maximization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Expectation-Maximization
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#structured-probabilistic-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Structured Probabilistic Models
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generative-vs-discriminative-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Generative vs Discriminative Modeling
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="probabilistic-modeling">Probabilistic Modeling</h1>
<div style="margin:.3rem 0 1rem;font-size:.9em;color:#555;display:flex;align-items:center;gap:.35rem;font-family:monospace">
  <time datetime="2026-02-09">9 Feb 2026</time>
</div>

<p><a href="https://en.wikipedia.org/wiki/Statistical_model">Statistical modeling</a> is the practice of describing real-world data using mathematical models with unknown parameters. In machine learning, statistical models are often expressed in probabilistic form, meaning we assume data is generated from a probability distribution. <strong>Probabilistic modeling</strong> is therefore the practice of describing how data is generated using probability distributions. Instead of treating observations as deterministic, we assume they are generated by an underlying random process, often with noise and uncertainty.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>The following sources were consulted in preparing this material: </p>
<ul>
<li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep Learning</em>. MIT Press. <a href="https://www.deeplearningbook.org/contents/prob.html">Chapter 3: Probability and Information Theory</a>.</li>
<li>Grosse, R. (2020). <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc311_f20/readings/L07%20Probabilistic%20Models.pdf">Lecture 7: Probabilistic Models</a>. CSC 311: Introduction to Machine Learning, University of Toronto.</li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>Some concepts in this material are simplified for pedagogical purposes. These simplifications slightly reduce precision but preserve the core ideas relevant to deep learning.</p>
</div>
<h2 id="likelihood">Likelihood</h2>
<p>In <a href="../03_probability">probability</a>, we often write expressions like <span class="arithmatex">\(p(y \mid \theta)\)</span>, where <span class="arithmatex">\(\theta\)</span> is a parameter of a model, and <span class="arithmatex">\(y\)</span> is a possible outcome. The same expression can be interpreted in two different ways: as a probability or as a likelihood. The likelihood is not a different formula â€” it is the same function, interpreted differently.</p>
<ul>
<li><strong>Probability</strong> treats <span class="arithmatex">\(\theta\)</span> as fixed and <span class="arithmatex">\(y\)</span> as uncertain. It answers: <em>If the model parameter is <span class="arithmatex">\(\theta\)</span>, how likely is outcome <span class="arithmatex">\(y\)</span>?</em></li>
<li><strong>Likelihood</strong> treats <span class="arithmatex">\(y\)</span> as fixed (because we already observed it) and views the same expression as a function of <span class="arithmatex">\(\theta\)</span>. It answers: <em>Given the observed outcome <span class="arithmatex">\(y\)</span>, which values of <span class="arithmatex">\(\theta\)</span> make it most plausible?</em></li>
</ul>
<p>For continuous variables, <span class="arithmatex">\(p(y\mid\theta)\)</span> is a probability density rather than a probability. The <a href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood function</a> is defined as:
$$
L(\theta \mid y) = p(y \mid \theta).
$$</p>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>Likelihood is not a probability distribution over <span class="arithmatex">\(\theta\)</span>. In general:
$$
\int L(\theta \mid y)\,d\theta \ne 1.
$$
Likelihood values only measure relative support for different parameter values of <span class="arithmatex">\(\theta\)</span>.</p>
</div>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>A coin toss can be modeled as <span class="arithmatex">\(Y \sim \mathrm{Bernoulli}(\theta),\)</span> where <span class="arithmatex">\(Y \in \{0,1\}\)</span>, and <span class="arithmatex">\(\theta\)</span> is the probability of observing <span class="arithmatex">\(Y=1\)</span> (e.g., heads). If the coin is weighted, then <span class="arithmatex">\(\theta \ne 0.5\)</span>. The probability of observing outcome <span class="arithmatex">\(y\)</span> is:
$$
p(y \mid \theta) = \theta^y(1-\theta)^{1-y}.
$$</p>
<p>If <span class="arithmatex">\(\theta\)</span> is fixed, this is a probability statement about the random outcome <span class="arithmatex">\(Y\)</span>. But after observing <span class="arithmatex">\(y\)</span>, the same expression becomes a likelihood function of <span class="arithmatex">\(\theta\)</span>:
$$
L(\theta \mid y) = \theta^y(1-\theta)^{1-y}.
$$</p>
<p>For example, if we observe <span class="arithmatex">\(y=1\)</span> (heads), then:
$$
L(\theta \mid y=1) = \theta,
$$
which is maximized near <span class="arithmatex">\(\theta=1\)</span>. If we observe <span class="arithmatex">\(y=0\)</span> (tails), then:
$$
L(\theta \mid y=0) = 1-\theta,
$$
which is maximized near <span class="arithmatex">\(\theta=0\)</span>.</p>
</div>
<p>In practice, likelihood values can become extremely small, because they often involve multiplying many probabilities. For this reason, we usually work with the <strong>log-likelihood</strong>:
$$
\log L(\theta \mid y) = \log p(y \mid \theta).
$$</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Log-likelihood is used because it turns products into sums. If we assume i.i.d. samples <span class="arithmatex">\(y^{(1)},\dots,y^{(m)}\)</span>, then:
$$
p(y^{(1)},\dots,y^{(m)} \mid \theta)
=
\prod_{i=1}^{m} p(y^{(i)} \mid \theta),
$$
so the log-likelihood becomes:
$$
\log p(y^{(1)},\dots,y^{(m)} \mid \theta)
=
\sum_{i=1}^{m} \log p(y^{(i)} \mid \theta).
$$</p>
</div>
<h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2>
<p>Once we choose a probabilistic model <span class="arithmatex">\(p(x \mid \theta)\)</span>, the main question becomes: <em>which parameter values <span class="arithmatex">\(\theta\)</span> best explain the observed dataset?</em> Given a dataset of <span class="arithmatex">\(m\)</span> samples:
$$
D = {x^{(1)},x^{(2)},\dots,x^{(m)}},
$$
the likelihood of the dataset is:
$$
p(D \mid \theta)
=
\prod_{i=1}^{m} p(x^{(i)} \mid \theta),
$$
assuming the samples are i.i.d. <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximum likelihood estimation (MLE)</a> chooses the parameter values that maximize this likelihood:</p>
<div style="overflow-x:auto; max-width:100%; margin:-0.4rem 0;">
$$
\hat{\theta}_{\mathrm{MLE}}
=
\arg\max_{\theta} p(D \mid \theta)
$$
</div>

<p>With log-likelihood, the formula becomes:</p>
<div style="overflow-x:auto; max-width:100%; margin:-0.4rem 0;">
$$
\hat{\theta}_{\mathrm{MLE}}
=
\arg\max_{\theta} \log p(D \mid \theta)
=
\arg\max_{\theta} \sum_{i=1}^{m} \log p(x^{(i)} \mid \theta).
$$
</div>

<p>Because the logarithm is <a href="../03_probability#logarithm">monotonic</a>, maximizing likelihood and maximizing log-likelihood produce the same solution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>MLE is the standard statistical justification behind many deep learning loss functions. In practice, training often means choosing parameters <span class="arithmatex">\(\theta\)</span> so that the observed dataset becomes as likely as possible under the model.</p>
</div>
<h2 id="negative-log-likelihood">Negative Log-Likelihood</h2>
<p>In deep learning, we usually convert MLE into a minimization problem. This leads to the <strong>negative log-likelihood</strong> loss:
$$
\mathcal{L}(\theta)
=
-\log p(D\mid \theta).
$$</p>
<p>Minimizing negative log-likelihood is equivalent to maximizing likelihood, so negative log-likelihood is the most common probabilistic form of a training objective.</p>
<h3 id="binary-cross-entropy">Binary Cross-Entropy</h3>
<p>In binary classification, the label is <span class="arithmatex">\(y \in \{0,1\}\)</span> and the model predicts the probability of the positive class:
$$
\hat{p} = p_\theta(Y=1 \mid x),
$$
where <span class="arithmatex">\(\theta\)</span> represents the model parameters (weights).</p>
<p>Under a Bernoulli model, the likelihood of observing <span class="arithmatex">\(y\)</span> is:
$$
p_\theta(y\mid x)
=
\hat{p}^{\,y}(1-\hat{p})^{1-y}.
$$</p>
<p>Taking the negative logarithm gives the binary <a href="../04_information">cross-entropy</a> loss:</p>
<div style="overflow-x:auto; max-width:100%;">
$$
\mathcal{L}_{\mathrm{BCE}}(y,\hat{p})
=
-y\log(\hat{p})-(1-y)\log(1-\hat{p}).
$$
</div>

<p>So binary cross-entropy is exactly the negative log-likelihood of a Bernoulli model.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Suppose the true label is <span class="arithmatex">\(y=1\)</span> (positive class). If the model predicts <span class="arithmatex">\(\hat{p}=0.9\)</span>, then:
$$
\mathcal{L}_{\mathrm{BCE}}
=
-\log(0.9)
\approx
0.105.
$$</p>
<p>If the model predicts <span class="arithmatex">\(\hat{p}=0.1\)</span>, then:
$$
\mathcal{L}_{\mathrm{BCE}}
=
-\log(0.1)
\approx
2.303.
$$</p>
<p>The loss is small when the model assigns high probability to the correct label, and large when it assigns low probability.</p>
</div>
<h3 id="categorical-cross-entropy">Categorical Cross-Entropy</h3>
<p>In <a href="https://en.wikipedia.org/wiki/Multiclass_classification">multiclass classification</a> with <span class="arithmatex">\(k\)</span> classes, the label is:
$$
y \in {1,2,\dots,k}.
$$</p>
<p>The model outputs a probability vector <span class="arithmatex">\(\hat{p}\in\mathbb{R}^k\)</span>. The likelihood of observing class <span class="arithmatex">\(y\)</span> is the probability assigned to that class:
$$
p(y\mid x) = \hat{p}_y.
$$</p>
<p>Therefore, the negative log-likelihood becomes:
$$
\mathcal{L}(y,\hat{p})
=
-\log(\hat{p}_y).
$$</p>
<p>If we represent the label as a one-hot vector <span class="arithmatex">\(e_y\)</span>, the same loss can be written as:
$$
\mathcal{L}(y,\hat{p})
=
-\sum_{i=1}^k e_{y,i}\log(\hat{p}_i).
$$</p>
<p>So categorical cross-entropy is exactly the negative log-likelihood of a categorical distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Cross-entropy loss is widely used because minimizing negative log-likelihood is equivalent to maximizing the likelihood of the observed labels under the model. In practice, neural networks usually output logits (unnormalized scores) rather than probabilities. Cross-entropy is computed using numerically stable implementations that combine softmax and log into a single operation:
$$
\mathcal{L}(y,z)
=
-\log\left(\mathrm{softmax}(z)_y\right),
$$
where <span class="arithmatex">\(z \in \mathbb{R}^k\)</span> is the logits vector.</p>
</div>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Suppose we have <span class="arithmatex">\(k=3\)</span> classes and the true class is <span class="arithmatex">\(y=2\)</span>. If the model predicts <span class="arithmatex">\(\hat{p}=(0.1,0.8,0.1)\)</span>, then:
$$
\mathcal{L}
=
-\log(0.8)
\approx
0.223.
$$</p>
<p>If the model predicts <span class="arithmatex">\(\hat{p}=(0.4,0.2,0.4)\)</span>, then:
$$
\mathcal{L}
=
-\log(0.2)
\approx
1.609.
$$</p>
<p>The loss increases sharply when the model assigns low probability to the correct class.</p>
</div>
<h2 id="bayesian-inference">Bayesian Inference</h2>
<p>MLE treats the model parameters <span class="arithmatex">\(\theta\)</span> as fixed but unknown. In <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a>, we instead treat <span class="arithmatex">\(\theta\)</span> as a random variable and represent uncertainty about its value using probability distributions. Before observing data, we have a certain <em>belief</em> <span class="arithmatex">\(p(\theta).\)</span> about the data distribution. After observing a dataset <span class="arithmatex">\(D\)</span>, we update this belief using <a href="../03_probability#bayes-rule">Bayes' rule</a>:
$$
p(\theta \mid D)
=
\frac{p(D \mid \theta)\,p(\theta)}{p(D)}.
$$</p>
<p>Here:</p>
<ul>
<li><span class="arithmatex">\(p(\theta)\)</span> is the <strong>prior</strong>, representing our belief about <span class="arithmatex">\(\theta\)</span> before seeing data.</li>
<li><span class="arithmatex">\(p(D\mid\theta)\)</span> is the <strong>likelihood</strong>, measuring how well <span class="arithmatex">\(\theta\)</span> explains the observed data.</li>
<li><span class="arithmatex">\(p(\theta\mid D)\)</span> is the <strong>posterior</strong>, representing our updated belief after seeing data.</li>
<li><span class="arithmatex">\(p(D)\)</span> is the <strong>marginal likelihood</strong> (or <strong>evidence</strong>), which normalizes the posterior:
$$
p(D)
=
\int p(D \mid \theta)\,p(\theta)\,d\theta.
$$</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Bayesian inference provides a way to combine prior assumptions with observed data. Instead of producing a single best estimate of parameters, it produces a full probability distribution over plausible parameter values.  When the dataset is small, the posterior remains broad. As more data is observed, the posterior typically becomes more concentrated around parameter values that explain the data well.</p>
</div>
<h2 id="maximum-a-posteriori-estimation">Maximum A Posteriori Estimation</h2>
<p>MLE chooses parameters <span class="arithmatex">\(\theta\)</span> that maximize the likelihood of the observed dataset:</p>
<div style="overflow-x:auto; max-width:100%; margin:-0.4rem 0;">
$$
\hat{\theta}_{\mathrm{MLE}}
=
\arg\max_{\theta} p(D \mid \theta).
$$
</div>

<p>In Bayesian inference, we instead compute the posterior distribution:
$$
p(\theta \mid D)
=
\frac{p(D \mid \theta)\,p(\theta)}{p(D)}.
$$</p>
<p><a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">Maximum a posteriori estimation (MAP)</a> chooses the parameter value that maximizes the posterior:</p>
<div style="overflow-x:auto; max-width:100%; margin:-0.4rem 0;">
$$
\hat{\theta}_{\mathrm{MAP}}
=
\arg\max_{\theta} p(\theta \mid D).
$$
</div>

<p>Since <span class="arithmatex">\(p(D)\)</span> does not depend on <span class="arithmatex">\(\theta\)</span>, maximizing the posterior is equivalent to:</p>
<div style="overflow-x:auto; max-width:100%; margin:-0.4rem 0;">
$$
\hat{\theta}_{\mathrm{MAP}}
=
\arg\max_{\theta} p(D \mid \theta)\,p(\theta).
$$
</div>

<p>Taking the logarithm gives the common optimization form:</p>
<div style="overflow-x:auto; max-width:100%; margin:-0.4rem 0;">
$$
\hat{\theta}_{\mathrm{MAP}}
=
\arg\max_{\theta}
\Big(
\log p(D \mid \theta) + \log p(\theta)
\Big).
$$
</div>

<p>MAP can be seen as MLE with an additional term <span class="arithmatex">\(\log p(\theta)\)</span> that encourages parameter values that are consistent with the prior.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Consider linear regression with Gaussian noise:
$$
y = w^\top x + \epsilon,
\qquad
\epsilon \sim \mathcal{N}(0,\sigma^2).
$$</p>
<p>This implies the likelihood:
$$
p(D \mid w)
\propto
\exp\left(
-\frac{1}{2\sigma^2}\sum_{i=1}^m (y^{(i)} - w^\top x^{(i)})^2
\right).
$$</p>
<p>Maximizing this likelihood (MLE) is equivalent to minimizing the mean squared error. Now assume a Gaussian prior over weights:
$$
w \sim \mathcal{N}(0,\tau^2 I).
$$</p>
<p>The MAP objective becomes:
$$
\hat{w}_{\mathrm{MAP}}
=
\arg\max_w
\Big(
\log p(D \mid w) + \log p(w)
\Big).
$$</p>
<p>Since the Gaussian prior contributes a penalty term proportional to <span class="arithmatex">\(\|w\|^2\)</span>, MAP becomes equivalent to minimizing:
$$
\sum_{i=1}^m (y^{(i)} - w^\top x^{(i)})^2
+
\lambda |w|^2.
$$</p>
<p>This is exactly <span class="arithmatex">\(L_2\)</span> regularization (ridge regression). Therefore, MAP estimation provides a probabilistic justification for <a href="../../notebooks/04_regul_optim">weight decay</a> in deep learning.</p>
</div>
<h2 id="latent-variable-models">Latent Variable Models</h2>
<div class="admonition quote">
<p class="admonition-title">Quote</p>
<p>But the latent process of which we speak, is far from being obvious to menâ€™s minds, beset as they now are. For we mean not the measures, symptoms, or degrees of any process which can be exhibited in the bodies themselves, but simply a continued process, which, for the most part, escapes the observation of the senses. ~â€Š<a href="https://en.wikipedia.org/wiki/Francis_Bacon">Francis Bacon</a> (<a href="https://en.wikipedia.org/wiki/Novum_Organum">Novum Organum</a>) </p>
</div>
<p>In many real-world problems, the observed data <span class="arithmatex">\(x\)</span> is influenced by hidden (latent) factors that we do not directly measure. A <a href="https://en.wikipedia.org/wiki/Latent_variable_model">latent variable model</a> introduces an <a href="https://en.wikipedia.org/wiki/Latent_and_observable_variables">unobserved random variable</a> <span class="arithmatex">\(z\)</span> to represent this hidden structure. The model assumes that data is generated in two steps:</p>
<ol>
<li>Latent variable was <em>sampled</em> <span class="arithmatex">\(z \sim p(z).\)</span></li>
<li>Observation <em>conditioned</em> on <span class="arithmatex">\(z\)</span> was generated <span class="arithmatex">\(x \sim p(x \mid z).\)</span></li>
</ol>
<p>Together, this defines the joint distribution:
$$
p(x,z) = p(x \mid z)\,p(z).
$$</p>
<p>Since <span class="arithmatex">\(z\)</span> is not observed, the probability of an observation <span class="arithmatex">\(x\)</span> is obtained by marginalizing over all possible latent values:
$$
p(x) = \int p(x,z)\,dz
=
\int p(x \mid z)\,p(z)\,dz.
$$</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In practice, latent variable models are powerful because they can represent complex data-generating processes, such as clustering, hidden states, or abstract representations. However, they are also more difficult to train, because computing the marginal likelihood often requires intractable integration (or summation) over latent variables. Many important machine learning models can be viewed as latent variable models, including mixture models, <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model">hidden Markov models (HMMs)</a>, and <a href="../../notebooks/07_vae">variational autoencoders (VAEs)</a>.</p>
</div>
<h2 id="mixture-models">Mixture Models</h2>
<p>In the probability section, we introduced <a href="../03_probability#mixture-distributions">mixture distributions</a> as weighted combinations of simpler distributions:</p>
<div style="overflow-x:auto; max-width:100%; margin:-0.4rem 0;">
$$
p(x) = \sum_{k=1}^{K} \pi_k\,p_k(x),
\qquad
\pi_k \ge 0,
\qquad
\sum_{k=1}^{K}\pi_k = 1.
$$
</div>

<p>In probabilistic modeling, the same idea is interpreted as a latent variable model. We assume that each data point was generated by one of <span class="arithmatex">\(K\)</span> hidden components. This is modeled by introducing a latent variable:
$$
Z \in {1,2,\dots,K},
$$
which represents the unknown component assignment. The generative process is:</p>
<ol>
<li>Sample a component index: <span class="arithmatex">\(Z \sim \mathrm{Categorical}(\pi_1,\dots,\pi_K).\)</span></li>
<li>Sample the observation from the chosen component: <span class="arithmatex">\(X \sim p(x \mid Z=k).\)</span></li>
</ol>
<p>The marginal distribution of <span class="arithmatex">\(X\)</span> is therefore:
$$
p(x) = \sum_{k=1}^{K} \pi_k\,p(x \mid Z=k).
$$</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The key modeling idea is that <span class="arithmatex">\(Z\)</span> is not observed.   Learning a mixture model means learning both the component distributions and the hidden assignments of data points.</p>
</div>
<h2 id="expectation-maximization">Expectation-Maximization</h2>
<p>Many probabilistic models contain latent variables, such as mixture models. In these models, the likelihood involves marginalizing over hidden variables. If the latent variable <span class="arithmatex">\(z\)</span> is discrete, the likelihood contains a sum:
$$
p(D \mid \theta)
=
\prod_{i=1}^{m}
\sum_{z^{(i)}} p(x^{(i)}, z^{(i)} \mid \theta).
$$</p>
<p>If the latent variable <span class="arithmatex">\(z\)</span> is continuous, the likelihood contains an integral:
$$
p(D \mid \theta)
=
\prod_{i=1}^{m}
\int p(x^{(i)}, z^{(i)} \mid \theta)\,dz^{(i)}.
$$</p>
<p>Directly maximizing this likelihood is often difficult, because of the intractable sum (or integral) over latent variables. The <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation-Maximization (EM) algorithm</a> is an iterative method for maximum likelihood estimation in latent variable models. It alternates between estimating the latent variables (softly) and updating the parameters.</p>
<p>In the <em>E-step (Expectation)</em>, we compute the posterior distribution of the latent variable given the current parameters:
$$
p(z^{(i)} \mid x^{(i)}, \theta^{(t)}).
$$</p>
<p>This gives a <em>soft assignment</em> of each data point to latent states or mixture components.</p>
<p>In the <em>M-step (Maximization)</em>, we update the parameters by maximizing the expected log-likelihood under these soft assignments:</p>
<div style="overflow-x:auto; max-width:100%; margin:-0.4rem 0;">
$$
\theta^{(t+1)}
=
\arg\max_{\theta}
\mathbb{E}_{z \sim p(z \mid x,\theta^{(t)})}
\left[
\log p(x,z \mid \theta)
\right].
$$
</div>

<p>This step improves the likelihood by fitting the model parameters to the expected latent structure. EM repeats these two steps until convergence.  Each iteration is guaranteed not to decrease the data likelihood.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>EM is most commonly associated with <a href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">Gaussian Mixture Models (GMMs)</a>. Consider a GMM with <span class="arithmatex">\(K\)</span> Gaussian components:
$$
p(x)
=
\sum_{k=1}^{K} \pi_k\,\mathcal{N}(x;\mu_k,\Sigma_k).
$$</p>
<p>Each data point <span class="arithmatex">\(x^{(i)}\)</span> is assumed to be generated by an unknown component <span class="arithmatex">\(z^{(i)} \in \{1,\dots,K\}\)</span>.</p>
<p><strong>E-step.</strong> Compute the posterior probability that point <span class="arithmatex">\(x^{(i)}\)</span> belongs to component <span class="arithmatex">\(k\)</span>:
$$
\gamma_{ik}
=
p(z^{(i)}=k \mid x^{(i)},\theta)
=
\frac{
\pi_k\,\mathcal{N}(x^{(i)};\mu_k,\Sigma_k)
}{
\sum_{j=1}^{K} \pi_j\,\mathcal{N}(x^{(i)};\mu_j,\Sigma_j)
}.
$$</p>
<p>The values <span class="arithmatex">\(\gamma_{ik}\)</span> are called <em>responsibilities</em>, and they act as soft cluster assignments.</p>
<p><strong>M-step.</strong> Update the parameters using these responsibilities:
$$
N_k = \sum_{i=1}^{m} \gamma_{ik},
\qquad
\pi_k = \frac{N_k}{m},
$$
$$
\mu_k
=
\frac{1}{N_k}\sum_{i=1}^{m} \gamma_{ik} x^{(i)},
$$
$$
\Sigma_k
=
\frac{1}{N_k}\sum_{i=1}^{m} \gamma_{ik}
(x^{(i)}-\mu_k)(x^{(i)}-\mu_k)^\top.
$$</p>
<p>EM repeats these steps until the parameters converge.
Intuitively, the E-step estimates soft cluster memberships, and the M-step recomputes the cluster parameters based on those memberships.</p>
</div>
<h2 id="structured-probabilistic-models">Structured Probabilistic Models</h2>
<p>In many deep learning problems, we work with multiple random variables. Modeling the full joint distribution <span class="arithmatex">\(p(x_1,\dots,x_n)\)</span> directly is often impractical, because the number of possible interactions grows rapidly with <span class="arithmatex">\(n\)</span>. Instead, we exploit the fact that most variables interact only with a small subset of others.</p>
<p>A <a href="https://en.wikipedia.org/wiki/Graphical_model">structured probabilistic model</a> (also called a <strong>graphical model</strong>) represents a joint probability distribution using a graph.   Each node represents a random variable, and edges represent direct probabilistic dependencies.</p>
<p>In a <strong>directed</strong> graphical model (also called a <strong>Bayesian network</strong>), edges are arrows that indicate conditional dependence. A Bayesian network must be a <strong>directed acyclic graph (DAG)</strong> (it cannot contain directed cycles). The joint distribution factorizes into conditional probabilities:
$$
p(x_1,\dots,x_n)
=
\prod_{i=1}^{n} p(x_i \mid \mathrm{Pa}(x_i)),
$$
where <span class="arithmatex">\(\mathrm{Pa}(x_i)\)</span> denotes the parents of <span class="arithmatex">\(x_i\)</span> in the graph.</p>
<figure>
  <img src="../../assets/images/prob_modeling/directed.svg" alt="Example of a directed graphical model (Bayesian network)" style="max-width: 100%; height: auto;">
  <figcaption style="margin-top: 0.5em; font-size: 0.9em; opacity: 0.85;">
    An example of a directed acyclic graphical model (Bayesian network). In this example, $A$ is a parent of $B$, $C$, and $D$, and $C$ is also a parent of $D$, so the factorization is $p(A,B,C,D)=p(A)\,p(B\mid A)\,p(C\mid A)\,p(D\mid A,C)$. ~ <a href="//commons.wikimedia.org/w/index.php?title=User:RJE42&amp;action=edit&amp;redlink=1" class="new" title="User:RJE42 (page does not exist)">RJE42</a> - <span class="int-own-work" lang="en">Own work</span>, <a href="https://creativecommons.org/licenses/by-sa/4.0" title="Creative Commons Attribution-Share Alike 4.0">CC BY-SA 4.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=105416431">Link</a>
  </figcaption>
</figure>

<p>In an <strong>undirected</strong> graphical model (Markov Random Field), edges do not have direction. Instead of conditional probabilities, the distribution is represented using non-negative functions called <em>potential functions</em>. The joint distribution is written as:
$$
p(x)
=
\frac{1}{Z}
\prod_{i=1}^{m} \phi^{(i)}(C^{(i)}),
$$
where each <span class="arithmatex">\(\phi^{(i)}\)</span> is a potential function over a clique <span class="arithmatex">\(C^{(i)}\)</span>, and <span class="arithmatex">\(Z\)</span> is a normalizing constant called the <em>partition function</em>. Unlike conditional probabilities, potential functions are not required to sum to <span class="arithmatex">\(1\)</span>. They only need to be non-negative.</p>
<figure>
  <img src="../../assets/images/prob_modeling/undirected.svg" alt="Example of an undirected graphical model (Markov random field)" style="max-width: 100%; height: auto;">
  <figcaption style="margin-top: 0.5em; font-size: 0.9em; opacity: 0.85;">
    Example of an undirected graphical model. For example, the edge $A-B$ indicates that $A$ and $B$ are not independent. Since there are no edges between $B$, $C$, and $D$, the model assumes they are conditionally independent given $A$. ~ <a href="//commons.wikimedia.org/w/index.php?title=User:RJE42&amp;action=edit&amp;redlink=1" class="new" title="User:RJE42 (page does not exist)">RJE42</a> - <span class="int-own-work" lang="en">Own work</span>, <a href="https://creativecommons.org/licenses/by-sa/4.0" title="Creative Commons Attribution-Share Alike 4.0">CC BY-SA 4.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=105416381">Link</a>
  </figcaption>
</figure>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In an undirected graph, edges do not represent causal direction. Instead, an edge between two nodes indicates that the variables directly interact.</p>
</div>
<h2 id="generative-vs-discriminative-modeling">Generative vs Discriminative Modeling</h2>
<p>Probabilistic models are often divided into two categories. A <strong>generative model</strong> describes how the data is generated by modeling the joint distribution <span class="arithmatex">\(p(x,y).\)</span> Using the joint distribution, we can answer different types of questions, such as generating new samples <span class="arithmatex">\(x\)</span> or perform classification using Bayes' rule. Examples of generative models include Naive Bayes, GMMs, HMMs, VAEs, etc.</p>
<p>A <strong>discriminative model</strong> focuses directly on predicting the target variable from the input by modeling: <span class="arithmatex">\(p(y \mid x),\)</span> or by learning a direct decision function. Discriminative models do not attempt to model the full data distribution <span class="arithmatex">\(p(x)\)</span>, and are often preferred when the main goal is prediction. Examples include logistic regression, support vector machines (SVMs), and neural networks trained with cross-entropy.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Generative models are typically more flexible and can be used for sampling and missing-data problems, but they can be harder to train.
Discriminative models often achieve better performance in supervised prediction tasks when large labeled datasets are available.</p>
</div>
<p>In this page, we introduced probabilistic modeling as a way to describe how data can be generated from probability distributions. We saw how the likelihood function turns observed data into a tool for choosing good model parameters. This leads to maximum likelihood estimation, and to negative log-likelihood, which is the basis of many loss functions used in deep learning. We also introduced Bayesian inference, where model parameters are treated as uncertain and described using probability distributions. MAP estimation was presented as a practical way to combine the likelihood with a prior. Finally, we discussed latent variable models, mixture models, and the Expectation-Maximization algorithm, which are useful when data is generated by hidden factors. We also introduced structured probabilistic models, which use graphs to represent dependencies between random variables, and explained the difference between generative and discriminative modeling.</p>







  
  



  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../04_information/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Information Theory">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Information Theory
              </div>
            </div>
          </a>
        
        
          
          <a href="../../supplementary/" class="md-footer__link md-footer__link--next" aria-label="Next: Suppementary Material">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Suppementary Material
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://shahaliyev.org/" target="_blank" rel="noopener" title="shahaliyev.org" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M399 384.2c-22.1-38.4-63.6-64.2-111-64.2h-64c-47.4 0-88.9 25.8-111 64.2 35.2 39.2 86.2 63.8 143 63.8s107.8-24.7 143-63.8M0 256a256 256 0 1 1 512 0 256 256 0 1 1-512 0m256 16a72 72 0 1 0 0-144 72 72 0 1 0 0 144"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/shahaliyev/csci4701" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://ada.edu.az/en/" target="_blank" rel="noopener" title="ada.edu.az" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M271.9 20.2c-9.8-5.6-21.9-5.6-31.8 0l-224 128c-12.6 7.2-18.8 22-15.1 36S17.5 208 32 208h32v208l-51.2 38.4C4.7 460.4 0 469.9 0 480c0 17.7 14.3 32 32 32h448c17.7 0 32-14.3 32-32 0-10.1-4.7-19.6-12.8-25.6L448 416V208h32c14.5 0 27.2-9.8 30.9-23.8s-2.5-28.8-15.1-36l-224-128zM400 208v208h-64V208zm-112 0v208h-64V208zm-112 0v208h-64V208zm80-112a32 32 0 1 1 0 64 32 32 0 1 1 0-64"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.sections", "navigation.expand", "navigation.top", "navigation.footer", "navigation.tabs", "content.code.copy", "content.code.annotate", "search.highlight", "search.suggest", "content.footnote.tooltips"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../assets/app/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>