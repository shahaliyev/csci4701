
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://shahaliyev.org/csci4701/mathematics/03_probability/">
      
      
        <link rel="prev" href="../02_linear_algebra/">
      
      
        <link rel="next" href="../04_information/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Probability Theory - CSCI 4701 Deep Learning</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
      <link rel="stylesheet" href="../../assets/styles/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-S1QRRQG9BM"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-S1QRRQG9BM",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-S1QRRQG9BM",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#probability-theory" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="CSCI 4701 Deep Learning" class="md-header__button md-logo" aria-label="CSCI 4701 Deep Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CSCI 4701 Deep Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Probability Theory
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/shahaliyev/csci4701" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../course/spring-2026/01_syllabus/" class="md-tabs__link">
          
  
  
  Course

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../introduction/01_deep_learning/" class="md-tabs__link">
          
  
  
  Introduction

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../notebooks/" class="md-tabs__link">
          
  
  
  Notebooks

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
  
  Mathematics

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../supplementary/" class="md-tabs__link">
          
  
  
  Supplementary

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../advanced/" class="md-tabs__link">
          
  
  
  Advanced

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="CSCI 4701 Deep Learning" class="md-nav__button md-logo" aria-label="CSCI 4701 Deep Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    CSCI 4701 Deep Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/shahaliyev/csci4701" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Course
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Course
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_1" >
        
          
          <label class="md-nav__link" for="__nav_1_1" id="__nav_1_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Spring 2026
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Spring 2026
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course/spring-2026/01_syllabus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Syllabus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course/spring-2026/02_assesments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Assessments
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course/spring-2026/03_project/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Introduction
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Introduction
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/01_deep_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Deep Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/02_machine_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Machine Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/03_resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Notebooks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Notebooks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture Notebooks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/01_backprop/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01. From Derivatives to Backpropagation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/02_neural_network/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02. From Neuron to Neural Network
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/03_cnn_torch/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03. From Kernel to Convolutional Neural Network
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/04_regul_optim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    04. Regularization and Optimization
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/05_nn_ngram/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    05. Neural Network N-Gram Model
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/06_batchnorm_resnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    06. Batch Normalization and Residual Blocks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/07_vae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    07. Variational Autoencoders (VAE)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Mathematics
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Mathematics
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Mathematics of Deep Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01_calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Calculus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02_linear_algebra/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Linear Algebra
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Probability Theory
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Probability Theory
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#kolmogorov-axioms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kolmogorov Axioms
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#two-views-of-probability" class="md-nav__link">
    <span class="md-ellipsis">
      
        Two Views of Probability
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-variables" class="md-nav__link">
    <span class="md-ellipsis">
      
        Random Variables
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#probability-distributions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Probability Distributions
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#joint-and-marginal-distributions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Joint and Marginal Distributions
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conditional-probability" class="md-nav__link">
    <span class="md-ellipsis">
      
        Conditional Probability
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#independence" class="md-nav__link">
    <span class="md-ellipsis">
      
        Independence
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chain-rule" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chain Rule
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#independent-and-identically-distributed" class="md-nav__link">
    <span class="md-ellipsis">
      
        Independent and Identically Distributed
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bayes-rule" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bayes' Rule
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04_information/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Information Theory
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Supplementary
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Supplementary
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supplementary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Suppementary Material
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supplementary/pca/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Principal Component Analysis
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../supplementary/svd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Singular Value Decomposition
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Advanced
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Advanced
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Advanced Material
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#kolmogorov-axioms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kolmogorov Axioms
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#two-views-of-probability" class="md-nav__link">
    <span class="md-ellipsis">
      
        Two Views of Probability
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-variables" class="md-nav__link">
    <span class="md-ellipsis">
      
        Random Variables
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#probability-distributions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Probability Distributions
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#joint-and-marginal-distributions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Joint and Marginal Distributions
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conditional-probability" class="md-nav__link">
    <span class="md-ellipsis">
      
        Conditional Probability
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#independence" class="md-nav__link">
    <span class="md-ellipsis">
      
        Independence
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chain-rule" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chain Rule
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#independent-and-identically-distributed" class="md-nav__link">
    <span class="md-ellipsis">
      
        Independent and Identically Distributed
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bayes-rule" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bayes' Rule
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="probability-theory">Probability Theory</h1>
<div style="margin:.3rem 0 1rem;font-size:.9em;color:#555;display:flex;align-items:center;gap:.35rem;font-family:monospace">
  <time datetime="2026-02-07">7 Feb 2026</time>
</div>

<div class="admonition warning">
  <p class="admonition-title">Important</p>
  <p style="margin: 1em 0;">
    The page is currently under development.
  </p>
</div>

<p>Probability theory is the mathematical framework for reasoning under uncertainty. In artificial intelligence, probability is used in two main ways: (i) as a guide for how an intelligent system should reason under uncertainty, and (ii) as a tool for analyzing and understanding the behavior of learning algorithms. <a href="../../introduction/01_deep_learning">Deep learning</a> relies on probability because real-world data is noisy, incomplete, and never fully deterministic, so uncertainty is unavoidable.  </p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>The following source was consulted in preparing this material: Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep Learning</em>. MIT Press. <a href="https://www.deeplearningbook.org/contents/prob.html">Chapter 3: Probability and Information Theory</a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>Some concepts in this material are simplified for pedagogical purposes. These simplifications slightly reduce precision but preserve the core ideas relevant to deep learning.</p>
</div>
<h2 id="kolmogorov-axioms">Kolmogorov Axioms</h2>
<p>Probability is a consistent system governed by a small set of rules (axioms) that prevent contradictions. In modern mathematics, these rules are usually given by the axioms introduced by Kolmogorov<sup id="fnref:kolmogorov"><a class="footnote-ref" href="#fn:kolmogorov">1</a></sup>. The three axioms define probability as a function <span class="arithmatex">\(P(\cdot)\)</span> that assigns a number to each event in a set of valid events. For any event <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(B\)</span>, a probability function must satisfy the following basic rules:</p>
<ul>
<li>It can never assign a negative value: <span class="arithmatex">\(P(A) \ge 0\)</span></li>
<li>It must assign probability <span class="arithmatex">\(1\)</span> to a certain event: <span class="arithmatex">\(P(\text{certain event}) = 1\)</span></li>
<li>If two events cannot happen at the same time (<span class="arithmatex">\(A \cap B = \emptyset\)</span>), their probabilities add up: <span class="arithmatex">\(P(A \cup B)=P(A)+P(B)\)</span>.</li>
</ul>
<p>These axioms guarantee that probability remains logically consistent.<sup id="fnref:dutchbook"><a class="footnote-ref" href="#fn:dutchbook">2</a></sup> Probability assigns numbers between <span class="arithmatex">\(0\)</span> and <span class="arithmatex">\(1\)</span> to events in order to represent how plausible those events are, given some assumptions or information. A value of <span class="arithmatex">\(0\)</span> means the event is impossible under the assumed model, a value of <span class="arithmatex">\(1\)</span> means it is certain, and intermediate values represent partial uncertainty.</p>
<h2 id="two-views-of-probability">Two Views of Probability</h2>
<p>Historically, probability was first developed to describe repeatable experiments, such as rolling dice, drawing cards, or observing outcomes in games of chance. Under this interpretation, called <strong>frequentist probability</strong>, <span class="arithmatex">\(P(A)\)</span> represents the long-run fraction of times event <span class="arithmatex">\(A\)</span> occurs if the experiment were repeated infinitely many times. For example, <span class="arithmatex">\(P(\text{heads})=0.5\)</span> means that if we toss a fair coin a very large number of times, about half of the tosses will result in heads.<sup id="fnref:frequentist"><a class="footnote-ref" href="#fn:frequentist">3</a></sup></p>
<p>Later, probability began to be used in a broader sense: not only for repeatable experiments, but also for reasoning about unique situations where repetition is impossible. For example, a doctor may assign a probability that a patient has a disease, even though we cannot create infinitely many identical copies of the patient. In this interpretation, called <strong>Bayesian probability</strong>, probability measures a <em>degree of belief</em> given incomplete information.</p>
<p>Although the interpretations differ, the same probability formulas apply to both. The axioms and rules of probability provide a consistent framework for reasoning under uncertainty, regardless of whether probability is interpreted as frequency or degree of belief.</p>
<h2 id="random-variables">Random Variables</h2>
<p>In probability theory, we rarely assign probabilities directly to raw outcomes. Instead, we define a <strong>random variable</strong>, which is a variable whose value depends on the outcome of an uncertain process. A random variable does not necessarily mean the process is truly random. It simply means that, from our perspective, the value is unknown until the outcome is observed. Random variables can be:</p>
<ul>
<li><strong>Discrete</strong>, meaning they can take only a finite or <a href="https://en.wikipedia.org/wiki/Countable_set">countably infinite</a> set of values (e.g. <span class="arithmatex">\(0,1,2,\dots\)</span>).</li>
<li><strong>Continuous</strong>, meaning they can take any real value in an interval (e.g. any number in <span class="arithmatex">\([0,1]\)</span>).</li>
</ul>
<p>For example, the result of a coin toss can be modeled as a discrete random variable <span class="arithmatex">\(X \in \{0,1\}\)</span>, while the temperature measured by a sensor is naturally modeled as a continuous random variable. In probability notation, we usually write a random variable using a capital letter such as <span class="arithmatex">\(X\)</span>, and a specific realized value using a lowercase letter such as <span class="arithmatex">\(x\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In deep learning, we often model data as random variables. For example, an image can be treated as a random variable <span class="arithmatex">\(X\)</span>, and its label (such as "cat" or "dog") as another random variable <span class="arithmatex">\(Y\)</span>. The goal of learning is then to discover patterns in how these random variables relate to each other.</p>
</div>
<h2 id="probability-distributions">Probability Distributions</h2>
<p>A random variable by itself only describes what values are possible. To reason quantitatively, we must also specify a <strong>probability distribution</strong>, which assigns probabilities to the different values the random variable can take.</p>
<p>Once the outcome is observed, the random variable takes a specific value. If a random variable is discrete, its distribution is described by a <strong>probability mass function (PMF)</strong>, denoted by <span class="arithmatex">\(P(X=x)\)</span>. It assigns a probability to each possible value, such that:</p>
<ul>
<li><span class="arithmatex">\(0 \le P(X=x) \le 1\)</span></li>
<li><span class="arithmatex">\(\sum_x P(X=x) = 1\)</span></li>
</ul>
<p>If a random variable is continuous, its distribution is described by a <strong>probability density function (PDF)</strong>, denoted by <span class="arithmatex">\(p(x)\)</span>. The PDF must satisfy:</p>
<ul>
<li><span class="arithmatex">\(p(x) \ge 0\)</span></li>
<li><span class="arithmatex">\(\int_{-\infty}^{\infty} p(x)\,dx = 1\)</span></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>For continuous variables, <span class="arithmatex">\(p(x)\)</span> is a <em>density</em>, not a probability. The value <span class="arithmatex">\(p(x)\)</span> can be greater than <span class="arithmatex">\(1\)</span>. Probabilities are obtained only by integrating over an interval:
$$
P(a \le X \le b) = \int_a^b p(x)\,dx
$$
The probability of observing any exact value <span class="arithmatex">\(X=x\)</span> is always <span class="arithmatex">\(0\)</span>. For example, if <span class="arithmatex">\(X\)</span> represents a real-valued measurement such as temperature, the probability of observing exactly <span class="arithmatex">\(20.000^\circ\)</span> is essentially zero, because the measurement could always end up being <span class="arithmatex">\(19.999\)</span> or <span class="arithmatex">\(20.001\)</span> instead. Only intervals have non-zero probability, such as <span class="arithmatex">\(P(19.9 \le X \le 20.1)\)</span>.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For integration and related topics, see the page dedicated to deep learning <a href="../01_calculus">Calculus</a>.</p>
</div>
<h2 id="joint-and-marginal-distributions">Joint and Marginal Distributions</h2>
<p>So far, we have described probability distributions over a single random variable. In many real problems, we must model multiple random variables at the same time. The probability distribution over two variables <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> together is called the <strong>joint distribution</strong>. If <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> are discrete, the joint distribution is written as:
<span class="arithmatex">\(P(X=x, Y=y).\)</span>
If <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> are continuous, the joint distribution is written as a joint density:
<span class="arithmatex">\(p(x,y).\)</span></p>
<p>Often, we are only interested in the distribution of one variable by itself. This is called the <strong>marginal distribution</strong>, and it can be obtained by summing (discrete case) or integrating (continuous case) over the other variable. For discrete variables:
$$
P(X=x) = \sum_y P(X=x, Y=y).
$$
For continuous variables:
$$
p(x) = \int p(x,y)\,dy.
$$</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For example, suppose <span class="arithmatex">\(X\)</span> represents the outcome of a coin toss (tails or heads), and <span class="arithmatex">\(Y\)</span> represents the number shown on a fair die (<span class="arithmatex">\(1\)</span> to <span class="arithmatex">\(6\)</span>). If we assume the coin toss does not affect the die roll (and vice versa), then each of the <span class="arithmatex">\(2 \times 6 = 12\)</span> outcomes is equally likely, so the joint distribution assigns probability <span class="arithmatex">\(1/12\)</span> to every possible pair:</p>
<table>
<thead>
<tr>
<th><span class="arithmatex">\(X \backslash Y\)</span></th>
<th><span class="arithmatex">\(1\)</span></th>
<th><span class="arithmatex">\(2\)</span></th>
<th><span class="arithmatex">\(3\)</span></th>
<th><span class="arithmatex">\(4\)</span></th>
<th><span class="arithmatex">\(5\)</span></th>
<th><span class="arithmatex">\(6\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>tails</td>
<td>1/12</td>
<td>1/12</td>
<td>1/12</td>
<td>1/12</td>
<td>1/12</td>
<td>1/12</td>
</tr>
<tr>
<td>heads</td>
<td>1/12</td>
<td>1/12</td>
<td>1/12</td>
<td>1/12</td>
<td>1/12</td>
<td>1/12</td>
</tr>
</tbody>
</table>
<p>To compute the marginal distribution of <span class="arithmatex">\(X\)</span>, we sum across each row:<sup id="fnref:marginal"><a class="footnote-ref" href="#fn:marginal">4</a></sup>
$$
P(X=\text{tails}) = \sum_{y=1}^{6} P(X=\text{tails},Y=y) = \frac{1}{2}.
$$</p>
<div class="admonition success">
<p class="admonition-title">Exercise</p>
<p>Compute the marginal distributions <span class="arithmatex">\(P(X=\text{heads})\)</span> and <span class="arithmatex">\(P(Y=1)\)</span>.</p>
</div>
</div>
<h2 id="conditional-probability">Conditional Probability</h2>
<p>In many situations, we are interested in the probability of an event given that another event has already occurred. This is called <strong>conditional probability</strong>. The conditional probability of <span class="arithmatex">\(A\)</span> given <span class="arithmatex">\(B\)</span> is denoted by <span class="arithmatex">\(P(A \mid B)\)</span>.</p>
<p>For discrete random variables <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span>, the conditional distribution of <span class="arithmatex">\(Y\)</span> given <span class="arithmatex">\(X\)</span> is defined as:
$$
P(Y=y \mid X=x) = \frac{P(X=x, Y=y)}{P(X=x)}.
$$</p>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>Many textbooks use shorthand notation such as <span class="arithmatex">\(P(y \mid x)\)</span> instead of <span class="arithmatex">\(P(Y=y \mid X=x)\)</span>. We will mostly use explicit notation for clarity.</p>
</div>
<p>For continuous random variables, we use probability densities instead:
$$
p(y \mid x) = \frac{p(x,y)}{p(x)}.
$$</p>
<p>These formulas are only defined when <span class="arithmatex">\(P(X=x)&gt;0\)</span> or <span class="arithmatex">\(p(x)&gt;0\)</span>, since we cannot condition on an event that never occurs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Suppose <span class="arithmatex">\(X\)</span> is an image and <span class="arithmatex">\(Y\)</span> is its label. The joint distribution <span class="arithmatex">\(P(X,Y)\)</span> describes how often we encounter a specific image together with its correct label. The marginal distribution <span class="arithmatex">\(P(X)\)</span> describes what kinds of images appear in the world or in our dataset, regardless of their labels. The conditional distribution <span class="arithmatex">\(P(Y \mid X)\)</span> describes the probability of each label given a particular image. When we train a classifier, we are essentially training a model that takes an image <span class="arithmatex">\(x\)</span> and outputs estimates of probabilities like <span class="arithmatex">\(P(Y=\text{cat} \mid X=x)\)</span> and <span class="arithmatex">\(P(Y=\text{dog} \mid X=x)\)</span>, where <span class="arithmatex">\(x\)</span> could be a particular input image provided to our model.</p>
</div>
<h2 id="independence">Independence</h2>
<p>In many problems, we work with multiple random variables. The relationship between these variables determines how complicated the joint distribution is. Two random variables <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> are called <strong>independent</strong> if knowing the value of one gives no information about the other. Formally, <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> are independent if their joint distribution factorizes into a product of marginals:
$$
P(X=x, Y=y) = P(X=x)\,P(Y=y).
$$</p>
<p>Equivalently, independence can be expressed using conditional probability:
$$
P(Y=y \mid X=x) = P(Y=y),
$$
meaning that observing <span class="arithmatex">\(X\)</span> does not change the probability of <span class="arithmatex">\(Y\)</span>. For continuous variables, the same definition applies using probability densities:
$$
p(x,y) = p(x)\,p(y).
$$</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Independence is a very strong assumption. In real-world data, variables are often correlated. However, independence assumptions are extremely useful because they allow us to build computationally efficient models.</p>
</div>
<p>Sometimes variables are not independent in general, but become independent once we condition on a third variable. We say that <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> are <strong>conditionally independent</strong> given <span class="arithmatex">\(Z\)</span> if:
$$
P(X=x, Y=y \mid Z=z) = \\ 
P(X=x \mid Z=z)\,P(Y=y \mid Z=z).
$$</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Suppose <span class="arithmatex">\(Z\)</span> represents whether it is raining. Let <span class="arithmatex">\(X\)</span> be whether the street is wet, and <span class="arithmatex">\(Y\)</span> be whether people are carrying umbrellas.  </p>
<p>In general, <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> are strongly correlated: if the street is wet, umbrellas are more likely. However, once we condition on <span class="arithmatex">\(Z\)</span> (rain), the relationship mostly disappears: given that it is raining, the street being wet does not provide much additional information about umbrellas. This illustrates conditional independence: <span class="arithmatex">\(X \perp Y \mid Z\)</span>.</p>
</div>
<h2 id="chain-rule">Chain Rule</h2>
<p>Probability has its own chain rule. Even when random variables are not independent, we can still represent any joint distribution by repeatedly applying the definition of conditional probability. For two variables, the joint distribution can be rewritten as:
$$
P(X=x, Y=y) = P(Y=y \mid X=x)\,P(X=x).
$$</p>
<p>In general, for <span class="arithmatex">\(n\)</span> random variables <span class="arithmatex">\((X_1, X_2, \dots, X_n)\)</span>, we can expand the joint distribution as:
$$
P(X_1, X_2, \dots, X_n)
=
\prod_{i=1}^{n} P(X_i \mid X_1, \dots, X_{i-1}).
$$</p>
<p>This identity is called the <strong>chain rule</strong> of probability. It is simply a consequence of how conditional probability is defined.</p>
<p>The chain rule gives a valid factorization, but it is often too complex because each conditional distribution depends on many variables. Independence assumptions simplify the factorization. For example, if we assume <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> are independent, then:
$$
P(X,Y) = P(X)\,P(Y).
$$</p>
<p>If we assume <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span> are conditionally independent given <span class="arithmatex">\(Z\)</span>, then:
$$
P(X,Y,Z) = P(X \mid Z)\,P(Y \mid Z)\,P(Z).
$$</p>
<p>This type of factorization is the foundation of many probabilistic models, including Bayesian networks and graphical models.</p>
<h2 id="independent-and-identically-distributed">Independent and Identically Distributed</h2>
<p>In deep learning, the most common simplifying assumption is that training examples are <strong>independent and identically distributed (i.i.d.)</strong>. Suppose we have a dataset of samples:
$$
{x^{(1)}, x^{(2)}, \dots, x^{(m)}}.
$$</p>
<p>The i.i.d. assumption means that each sample is generated independently of the others, and all samples come from the same underlying distribution. Formally, if each sample is drawn from the same distribution <span class="arithmatex">\(P(X)\)</span> and samples are independent, then the joint probability of the dataset factorizes as:
$$
P(x^{(1)}, x^{(2)}, \dots, x^{(m)})
=
\prod_{i=1}^{m} P(x^{(i)}).
$$</p>
<p>This assumption is extremely important because it makes learning feasible: it allows the likelihood of a dataset to be written as a product, and the log-likelihood as a sum.</p>
<div class="admonition warning">
<p class="admonition-title">Important</p>
<p>The i.i.d. assumption is often violated in practice. Examples include time series, video frames, financial data, and datasets affected by distribution shift.   However, i.i.d. is still widely used because it provides a simple baseline model of how data is generated.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a href="../../notebooks/04_regul_optim">Stochastic gradient descent (SGD)</a> implicitly relies on the i.i.d. assumption: each mini-batch is treated as a random sample from the same distribution, so its gradient is assumed to approximate the full dataset gradient.</p>
</div>
<h2 id="bayes-rule">Bayes' Rule</h2>
<p>We have reached a crucial point in probability theory. Terminology can be dense and seem complicated, so I suggest spending some time here to clearly understand the concepts. You will frequently see the described terminology in deep learning literature. </p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For a visual intuition of Bayes' rule, see the video below on Bayes' theorem.</p>
</div>
<div style="display:flex;justify-content:center;margin:1rem 0;">
  <div style="width:80%;max-width:900px;position:relative;padding-bottom:56.25%;height:0;overflow:hidden;">
    <iframe
      src="https://www.youtube.com/embed/HZGCoVF3YvM"
      style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"
      allowfullscreen>
    </iframe>
  </div>
</div>

<p><a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes rule</a> allows us to reverse conditional probabilities. It provides a way to compute the probability of a hypothesis after observing <strong>evidence</strong>. For discrete random variables, Bayes rule is:
$$
P(X=x \mid Y=y) = \frac{P(Y=y \mid X=x)\,P(X=x)}{P(Y=y)}.
$$</p>
<p>Here, <span class="arithmatex">\(P(X=x)\)</span> is called the <strong>prior</strong>. It describes how likely <span class="arithmatex">\(x\)</span> was before observing <span class="arithmatex">\(y\)</span>. The term <span class="arithmatex">\(P(Y=y \mid X=x)\)</span> is the <strong>likelihood</strong>. It measures how compatible the observation <span class="arithmatex">\(y\)</span> is with the hypothesis <span class="arithmatex">\(x\)</span>. The result <span class="arithmatex">\(P(X=x \mid Y=y)\)</span> is called the <strong>posterior</strong>.</p>
<div class="admonition notes">
<p class="admonition-title">Notes</p>
<p>The denominator <span class="arithmatex">\(P(Y=y)\)</span> acts as a normalization constant. It ensures that the posterior distribution <span class="arithmatex">\(P(X \mid Y=y)\)</span> sums to <span class="arithmatex">\(1\)</span> over all possible values of <span class="arithmatex">\(X\)</span>.  </p>
</div>
<p>For continuous random variables, we use probability densities instead:
$$
p(x \mid y) = \frac{p(y \mid x)\,p(x)}{p(y)}.
$$</p>
<p>The denominator <span class="arithmatex">\(p(y)\)</span> is the marginal probability density of observing <span class="arithmatex">\(y\)</span>. It is obtained by summing over all possible values of <span class="arithmatex">\(x\)</span> that could have produced <span class="arithmatex">\(y\)</span> (in the continuous case, summation becomes integration):
$$
p(y) = \int p(y \mid x)\,p(x)\,dx.
$$</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This continuous form of Bayes' rule will become important for us later when discussing <a href="../../notebooks/07_vae">variational autoencoders</a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Consider a spam detection example. Let <span class="arithmatex">\(X\)</span> represent whether an email is spam (<span class="arithmatex">\(X=\text{spam}\)</span> or <span class="arithmatex">\(X=\text{not spam}\)</span>), and let <span class="arithmatex">\(Y\)</span> represent whether the email contains the phrase "win money" (<span class="arithmatex">\(Y=\text{yes}\)</span> or <span class="arithmatex">\(Y=\text{no}\)</span>). </p>
<p>Suppose only <span class="arithmatex">\(2\%\)</span> of all emails are spam, so the prior probability is <span class="arithmatex">\(P(\text{spam})=0.02\)</span> and <span class="arithmatex">\(P(\text{not spam})=0.98\)</span>. Now assume spam emails contain the phrase "win money" <span class="arithmatex">\(60\%\)</span> of the time, so <span class="arithmatex">\(P(\text{yes} \mid \text{spam})=0.60\)</span>, while normal emails contain it only <span class="arithmatex">\(1\%\)</span> of the time, so <span class="arithmatex">\(P(\text{yes} \mid \text{not spam})=0.01\)</span>. If we observe an email containing "win money", Bayes rule gives:
$$
P(\text{spam} \mid \text{yes})
=
\frac{P(\text{yes} \mid \text{spam})P(\text{spam})}
{P(\text{yes})}.
$$
The numerator is <span class="arithmatex">\(0.60 \cdot 0.02 = 0.012\)</span>. The denominator is computed by marginalization:
$$
P(\text{yes})
=
P(\text{yes} \mid \text{spam})P(\text{spam})
+ \\
P(\text{yes} \mid \text{not spam})P(\text{not spam})
= \\
0.60\cdot 0.02 + 0.01\cdot 0.98
=
0.0218.
$$
Therefore,
$$
P(\text{spam} \mid \text{yes}) = \frac{0.012}{0.0218} \approx 0.55.
$$
After observing the phrase, the probability that the email is spam jumps from <span class="arithmatex">\(2\%\)</span> to about <span class="arithmatex">\(55\%\)</span>. This illustrates Bayes rule as a mechanism for updating beliefs: the likelihood tells us how strongly the evidence points toward spam, while the prior reflects how common spam is overall.</p>
</div>
<div class="footnote">
<hr />
<ol>
<li id="fn:kolmogorov">
<p>Kolmogorov, A.N. (1933, 1950). <a href="https://archive.org/details/foundationsofthe00kolm">Foundations of the theory of probability</a>. New York, US: Chelsea Publishing Company.&#160;<a class="footnote-backref" href="#fnref:kolmogorov" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:dutchbook">
<p>A <a href="https://en.wikipedia.org/wiki/Dutch_book_theorems">Dutch book argument</a> says that if your probability assignments are inconsistent, someone can design a set of bets that guarantees you lose money no matter what happens. For example, if you assign <span class="arithmatex">\(P(A)=0.6\)</span> and <span class="arithmatex">\(P(\neg A)=0.6\)</span>, then you are claiming both an event and its opposite are more likely than not. A bettor could sell you a bet on <span class="arithmatex">\(A\)</span> and also sell you a bet on <span class="arithmatex">\(\neg A\)</span>, and you would overpay in total, while only one of them can ever pay out. This guarantees a loss. The probability axioms prevent such contradictions.&#160;<a class="footnote-backref" href="#fnref:dutchbook" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:frequentist">
<p>A real coin is not infinitely thin, so in principle it could land on its edge. In practice this outcome is extremely rare, so it is usually ignored and the sample space is approximated as having only two outcomes.&#160;<a class="footnote-backref" href="#fnref:frequentist" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:marginal">
<p>The term <em>marginal</em> is said to come from the traditional way of computing these sums on paper: one writes the joint distribution in a table and records the row and column totals in the margins of the page.&#160;<a class="footnote-backref" href="#fnref:marginal" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../02_linear_algebra/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Linear Algebra">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Linear Algebra
              </div>
            </div>
          </a>
        
        
          
          <a href="../04_information/" class="md-footer__link md-footer__link--next" aria-label="Next: Information Theory">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Information Theory
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://shahaliyev.org/" target="_blank" rel="noopener" title="shahaliyev.org" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M399 384.2c-22.1-38.4-63.6-64.2-111-64.2h-64c-47.4 0-88.9 25.8-111 64.2 35.2 39.2 86.2 63.8 143 63.8s107.8-24.7 143-63.8M0 256a256 256 0 1 1 512 0 256 256 0 1 1-512 0m256 16a72 72 0 1 0 0-144 72 72 0 1 0 0 144"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/shahaliyev/csci4701" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://ada.edu.az/en/" target="_blank" rel="noopener" title="ada.edu.az" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M271.9 20.2c-9.8-5.6-21.9-5.6-31.8 0l-224 128c-12.6 7.2-18.8 22-15.1 36S17.5 208 32 208h32v208l-51.2 38.4C4.7 460.4 0 469.9 0 480c0 17.7 14.3 32 32 32h448c17.7 0 32-14.3 32-32 0-10.1-4.7-19.6-12.8-25.6L448 416V208h32c14.5 0 27.2-9.8 30.9-23.8s-2.5-28.8-15.1-36l-224-128zM400 208v208h-64V208zm-112 0v208h-64V208zm-112 0v208h-64V208zm80-112a32 32 0 1 1 0 64 32 32 0 1 1 0-64"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.sections", "navigation.expand", "navigation.top", "navigation.footer", "navigation.tabs", "content.code.copy", "content.code.annotate", "search.highlight", "search.suggest", "content.footnote.tooltips"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../assets/app/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>