{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01. From Derivatives to Backpropagation\n",
        "\n",
        "<div style=\"margin:.3rem 0 1rem;font-size:.9em;color:#555;display:flex;align-items:center;gap:.35rem;font-family:monospace\">\n",
        "  <time datetime=\"2025-01-25\">25 Jan 2025</time> ·\n",
        "  <time datetime=\"2026-01-19\">19 Jan 2026</time> ·\n",
        "  <time datetime=\"PT18M\">18 min</time>\n",
        "</div>\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/shahaliyev/csci4701/blob/main/docs/notebooks/01_backprop.ipynb\"\n",
        "   target=\"_blank\" rel=\"noopener\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open in Colab\"\n",
        "  />\n",
        "</a>\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "  <p class=\"admonition-title\">Tip</p>\n",
        "  <p style=\"margin: 1em 0;\">\n",
        "     It will be helpful to revise the necessary <a href='../../mathematics/01_calculus'>Calculus</a> needed for DL for understanding the material that follows.\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIfPGtWcyURZ"
      },
      "source": [
        "We will go from illustrating differentiation and finding derivatives in Python, all the way down till the implementation of the backpropagation algorithm. Even if mathematically the idea of derivatives is very familiar, it still makes sense to see its various visualization plots via code.\n",
        "\n",
        "## Differentiation\n",
        "\n",
        "The code below shows the function $f(x) = x^2$ in action. Our goal is to see how much the output changes as we modify the input by some value h. You can modify and see behavior for more values on other functions as well. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPyKg5HfJcOu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nsi6rh__HkDn"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "  return x**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI6Lp_iOJyym",
        "outputId": "fb52cfb0-48f9-4230-c5a1-e82e887a0f44"
      },
      "outputs": [],
      "source": [
        "x = 3.0\n",
        "for h in [10, 1, 0.1, 0]:\n",
        "  print(f\"If we shift input by {h}, output becomes {f(x+h)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The well-known equation for the change $ \\Delta y = f(x + \\Delta x) - f(x)$ we can code and  visualize for various cases as follows. Again, feel free to modify the values and functions to observe the behavior of change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXi_sgndLqnU",
        "outputId": "280093e8-db11-49fe-fc1a-d4650df1f372"
      },
      "outputs": [],
      "source": [
        "h = 1.0\n",
        "\n",
        "dx = h\n",
        "dy = f(x+h) - f(x)\n",
        "\n",
        "print(f\"Δx: {dx}\")\n",
        "print(f\"Δy: {dy}\")\n",
        "print(f\"When you change x by {dx} unit, y changes by {dy} units.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVOCfNshsLYg"
      },
      "outputs": [],
      "source": [
        "def plot_delta(x, h, start=-4, stop=4, num=30):\n",
        "  # `np.linspace` returns an array of num inputs within a range.\n",
        "  x_all = np.linspace(start, stop, num)\n",
        "  y_all = f(x_all)\n",
        "\n",
        "  plt.figure(figsize=(4, 4))\n",
        "  plt.plot(x_all, y_all)\n",
        "\n",
        "  # dx & dy\n",
        "  plt.plot([x, x + h], [f(x), f(x)], color='r')\n",
        "  plt.plot([x + h, x + h], [f(x), f(x + h)], color='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "vEe98C4KoBmA",
        "outputId": "f9f23591-b1d4-4df9-985c-3f2fee714b3a"
      },
      "outputs": [],
      "source": [
        "plot_delta(x=2, h=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTNGlB6wweiZ"
      },
      "source": [
        "How to find if the ouput changes significantly when we change the input by some amount $h$? We should be familiar with the rate of change ratio when we choose a small step size $h$:\n",
        "\n",
        "$$ \\dfrac{\\Delta y}{\\Delta x} = \\dfrac{f(x + h) - f(x)}{h}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot0VqoH2t_Iz"
      },
      "outputs": [],
      "source": [
        "def plot_roc(x, h):\n",
        "  dx = h\n",
        "  dy = f(x + h) - f(x)\n",
        "\n",
        "  plot_delta(x, h)\n",
        "  print(f\"Rate of change is {dy / dx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "5dp4YMQIuMnS",
        "outputId": "278d5403-a7ad-44f4-cf43-840979b10ae6"
      },
      "outputs": [],
      "source": [
        "plot_roc(3, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "uR8Z5ZkMzJ-D",
        "outputId": "1aca1298-96b7-4a40-c15b-dde731343cce"
      },
      "outputs": [],
      "source": [
        "plot_roc(3, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "rzIBx0eTxZFm",
        "outputId": "41bf815c-fb2b-4805-f968-2e70dbdc44b5"
      },
      "outputs": [],
      "source": [
        "plot_roc(1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "GmMFlzUxxusG",
        "outputId": "081ca915-c1db-4070-f915-ba5e199c3bdd"
      },
      "outputs": [],
      "source": [
        "plot_roc(-2, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOxK9imjytwx"
      },
      "source": [
        "The rate of change for different values of $h$ are different at the same point $x$. We would like to come up with a single value that would tell how significantly $y$ changes at a given point $x$ within the function:\n",
        "\n",
        "$$ L = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} $$\n",
        "\n",
        "\n",
        "Simply, this limit tells us how much the value of the output will change when we change the input by just a very small amount. \n",
        "\n",
        "<div class=\"admonition note\">\n",
        "    <p class=\"admonition-title\">Note</p>\n",
        "    <p style=\"margin: 1em 0;\">\n",
        "        Essentially, <a href='https://math.stackexchange.com/questions/4837307/is-the-derivative-of-a-function-a-value-or-is-it-the-derivative-function-of-that#:~:text=The%20derivative%20of%20a%20function%20is%20a%20function%20(say%20d,xex%3Dex'>derivative is a function</a>: it assigns to each input $ x $ the rate at which the original function changes at that point. Meaning, for each input, the limit above produces a value, and as $ x $ changes, this value changes as well. \n",
        "    </p>\n",
        "    </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsxGLW4fxzea",
        "outputId": "cdb7a364-a1bd-4822-f1cc-f8420d2f2a76"
      },
      "outputs": [],
      "source": [
        "x = 3\n",
        "h = 0.000001 # The limit of h approaching 0\n",
        "d = (f(x + h) - f(x)) / h\n",
        "f\"The value of derivative function is {d}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKNEz3FbLViS"
      },
      "source": [
        "## Partial Derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLEPBXhiSf1A"
      },
      "source": [
        "When we extend this idea to <a href='https://en.wikipedia.org/wiki/Multivariable_calculus'>multivariable calculus</a>, we use partial derivatives. A partial derivative <em>with respect to (wrt)</em> a given variable measures how much the output changes when we nudge that variable alone by a very small amount, while keeping all other variables fixed. Below we will see how it behaves during addition $f(x, y)=x+y$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69jBpL8yVxQM"
      },
      "outputs": [],
      "source": [
        "f = lambda x, y: x + y\n",
        "\n",
        "x = 2\n",
        "y = 3\n",
        "\n",
        "f(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpxISXaLOKGn"
      },
      "source": [
        "Partial derivatives w.r.t $x$ an $y$ will be as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW6zQ4jhYora",
        "outputId": "91d50f04-7f44-4aff-e51b-8f3db7ee09c6"
      },
      "outputs": [],
      "source": [
        "h = 0.000001\n",
        "(f(x + h, y) - f(x, y)) / h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHA3IkFrY6An",
        "outputId": "3e28d3de-151f-4c02-8317-b262744435c1"
      },
      "outputs": [],
      "source": [
        "h = 0.000001\n",
        "(f(x, y+h) - f(x, y)) / h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmDVzivkOOOg"
      },
      "source": [
        "No matter what the input values are, the expression will always approach $1.0$ for addition, because adding a constant increases the output by the same amount as the input change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wANkr8wZFIw",
        "outputId": "7834d30e-d7b7-4abe-db8a-5e41e0880dab"
      },
      "outputs": [],
      "source": [
        "for x, y in zip([-20, 2, 3], [300, 75, 10]):\n",
        "  print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91eZs7FUZZJi"
      },
      "source": [
        "Indeed, if we have a simple addition $x + y$, then increasing $x$ or $y$ by some amount will increase the result by the exact same amount. Assertion will work for any number $h$ gets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q0h4w8BaAc1"
      },
      "outputs": [],
      "source": [
        "h = 10\n",
        "assert f(x+h, y) - f(x, y) == h\n",
        "assert f(x, y+h) - f(x, y) == h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VkaEOP0bC_6"
      },
      "source": [
        "Let's now see the case for multiplication $f(x, y)=x * y$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab5AicsVbH1E"
      },
      "outputs": [],
      "source": [
        "f = lambda x, y: x * y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4iow7oQb1qp",
        "outputId": "a5e802a8-bab2-4a57-a2a3-a252104b6f24"
      },
      "outputs": [],
      "source": [
        "x = 2\n",
        "y = 3\n",
        "h = 1e-5 # scientific notation for 0.00001\n",
        "(f(x + h, y) - f(x, y)) / h # wrt x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOCh4FXsczZ4",
        "outputId": "497546e8-6d4b-4d60-ec36-f68c31e8bc42"
      },
      "outputs": [],
      "source": [
        "for x in [-20, 2, 3]:\n",
        "  print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NId3z2dQdbrA",
        "outputId": "cb4c7569-272d-4aa7-aa15-a0f3dc1874a8"
      },
      "outputs": [],
      "source": [
        "x = 10\n",
        "h = 5\n",
        "pdx = (f(x+h, y) - f(x, y)) / h\n",
        "print(pdx, y)\n",
        "assert round(pdx, 2) == round(y, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we will consider a complex function with three variables $f(x, y, z)=x^2+y^3-z$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bYBDlIkLXKz"
      },
      "outputs": [],
      "source": [
        "def f(x, y, z):\n",
        "  return x**2 + y**3 - z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgncoWhbSFeU",
        "outputId": "9d459d79-a853-4339-bdd0-37a89b4aafdc"
      },
      "outputs": [],
      "source": [
        "x = 2\n",
        "y = 3\n",
        "z = 4\n",
        "\n",
        "f(x, y, z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71zFUcF-SMHr",
        "outputId": "814ce206-d88c-4757-9a06-486f6e9c5f3b"
      },
      "outputs": [],
      "source": [
        "h = 1\n",
        "\n",
        "f(x + h, y, z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cowkpjobSrRV",
        "outputId": "ef2e846b-01ee-4a23-dd96-ed6f0c39a9ab"
      },
      "outputs": [],
      "source": [
        "f(x + h, y, z) - f(x, y, z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fA5lI49TAyw",
        "outputId": "475c6ea5-6261-4bd1-ba9c-7be53c09b778"
      },
      "outputs": [],
      "source": [
        "(f(x + h, y, z) - f(x, y, z)) / h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNY89h-iTQr5",
        "outputId": "2f2d4419-57ec-4959-83eb-917c79d969b4"
      },
      "outputs": [],
      "source": [
        "h = 0.00001 # change in limit\n",
        "pdx = (f(x + h, y, z) - f(x, y, z)) / h\n",
        "pdx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKk-pI3pUmy1"
      },
      "outputs": [],
      "source": [
        "assert 2*x == round(pdx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACw2ZZmAT8zv"
      },
      "source": [
        "Partial derivative w.r.t $x$ is $2x$ (by the power rule), and for $x=2$ indeed we get $4$.\n",
        "\n",
        "\n",
        "<div class=\"admonition success\">\n",
        "  <p class=\"admonition-title\">Execise</p>\n",
        "  <p style=\"margin: 1em 0;\">\n",
        "    Code the partial derivative w.r.t $y$ and $z$ and verify if the result correct.\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPzEU3Nm6ki-"
      },
      "source": [
        "## Computation Graph\n",
        "\n",
        "<div class=\"admonition info\">\n",
        "  <p class=\"admonition-title\">Info</p>\n",
        "  <p style=\"margin: 1em 0;\">The following source was used in preparing this material: <a href='https://www.youtube.com/watch?v=VMj-3S1tku0&t=6309s'>Andrej Karpathy's lecture</a> on <a href='https://github.com/karpathy/micrograd'>Micrograd</a>.</p>\n",
        "</div>\n",
        " \n",
        " [Micrograd](https://github.com/karpathy/micrograd) is an educational library that demonstrates the core ideas behind _automatic differentiation_. The idea behind it is the same one used in major DL frameworks. `Micrograd` closely mirrors the logic of the PyTorch [autograd engine](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html). In this and the [following](../02_neural_network.ipynb) notebooks, we will modify `micrograd` further to improve correspondence with PyTorch and make the explanations clearer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"admonition tip\">\n",
        "  <p class=\"admonition-title\">Tip</p>\n",
        "  <p style=\"margin: 1em 0;\">\n",
        "    It is recommended to run this notebook on <a href=\"https://colab.research.google.com/\">Google Colab</a>, where <a href='https://graphviz.org/'>Graphviz</a> is available by default. For local development, graph visualization requires both the Python wrapper (<code>pip install graphviz</code>) and the Graphviz system executable to be installed and available on the system PATH.\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBdVCLBmDQgY"
      },
      "outputs": [],
      "source": [
        "# This is a graph visualization code from micrograd, no need to understand the details\n",
        "# https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb\n",
        "from graphviz import Digraph\n",
        "\n",
        "def trace(root):\n",
        "    nodes, edges = set(), set()\n",
        "    def build(v):\n",
        "        if v not in nodes:\n",
        "            nodes.add(v)\n",
        "            for child in v._prev:\n",
        "                edges.add((child, v))\n",
        "                build(child)\n",
        "    build(root)\n",
        "    return nodes, edges\n",
        "\n",
        "def draw_dot(root, format='svg', rankdir='LR'):\n",
        "    \"\"\"\n",
        "    format: png | svg | ...\n",
        "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
        "    \"\"\"\n",
        "    assert rankdir in ['LR', 'TB']\n",
        "    nodes, edges = trace(root)\n",
        "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
        "\n",
        "    for n in nodes:\n",
        "        dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')\n",
        "        if n._op:\n",
        "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
        "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
        "\n",
        "    for n1, n2 in edges:\n",
        "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
        "\n",
        "    return dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The purpose of the `Value` class below is to store both numerical values and the **history of operations** that produced them. Each instance represents a node in a [computation graph]([computation graph](https://colah.github.io/posts/2015-08-Backprop/)). When a `Value` object is created, it stores:\n",
        "- the numerical result of a computation (`data`),\n",
        "- references to the input values that produced it (`_prev`),\n",
        "- the operation that combined those inputs (`_op`),\n",
        "- label provided by us in order to see it on the computation graph (`label`),\n",
        "- gradient of the final output w.r.t this value (`grad`).\n",
        "\n",
        "Arithmetic operations such as addition and multiplication are overloaded so that, instead of returning plain numbers, they return new `Value` objects. These new objects remember *which values were combined* and *how* they were combined. As computations are chained together, this process builds a directed graph, later used to propagate gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXLQbqRz3gEx"
      },
      "outputs": [],
      "source": [
        "# Value class stores a number and \"remembers\" information about its origins\n",
        "class Value:\n",
        "  def __init__(self, data, _prev=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self._prev = _prev\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "    self.grad = 0\n",
        "\n",
        "  def __add__(self, other):\n",
        "    data = self.data + other.data\n",
        "    out = Value(data, (self, other), '+')\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    data = self.data * other.data\n",
        "    out = Value(data, (self, other), \"*\")\n",
        "    return out\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data}, grad={self.grad})\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is the example of a simple expression built with the help of `Value` class: $L=c*d$ where $c=a+b$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "398NAgL-AXsy"
      },
      "outputs": [],
      "source": [
        "a = Value(5, label='a')\n",
        "b = Value(3, label='b')\n",
        "c = a + b; c.label = 'c'\n",
        "d = Value(10, label='d')\n",
        "L = c * d; L.label = 'L'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaWe-szwj8n5",
        "outputId": "ff5ba04a-0000-47b1-f704-3c8e61d135f8"
      },
      "outputs": [],
      "source": [
        "print(a, a._prev)\n",
        "print(L, L._prev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the helper function built with `graphviz` above to plot our computation graph as a nice visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "5wgL8nz1Fokv",
        "outputId": "8ca410fc-79db-42f9-85cd-d09070a49679"
      },
      "outputs": [],
      "source": [
        "draw_dot(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "z7CCdkxcJgAo",
        "outputId": "c5857cc5-ad54-48f1-b66f-a1f8b85a6c16"
      },
      "outputs": [],
      "source": [
        "draw_dot(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW-7N9P3wEkZ"
      },
      "source": [
        "## Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDR7yaPjjeMM"
      },
      "source": [
        "Gradient is [vector](../../mathematics/01_calculus) of partial derivatives. We want to know how much changing each variable will affect the output of `L` (loss). We will store those partial derivatives inside each `grad` variable of each `Value` object. We start by noting that the derivative of a variable with respect to itself is $1$ (you get the same $dx/dy$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBsFY8kBpk7m"
      },
      "outputs": [],
      "source": [
        "L.grad = 1.0\n",
        "\n",
        "f = lambda x: x\n",
        "h = 1e-5\n",
        "pdx = (f(x + h) - f(x)) / h\n",
        "assert round(pdx) == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVHFyPpTre1Q"
      },
      "source": [
        "Now let's see how changing other variables will affect the eventual result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6x337Yoml22"
      },
      "outputs": [],
      "source": [
        "# extended version of the function we saw previously\n",
        "def f(ha=0, hb=0, hc=0, hd=0):\n",
        "  a = Value(5 + ha, label='a')\n",
        "  b = Value(3 + hb, label='b')\n",
        "  c = a + b + Value(hc); c.label = 'c'\n",
        "  d = Value(10 + hd, label='d')\n",
        "  L = c * d; L.label = 'L'\n",
        "  return L.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD7ANr_ytAY9",
        "outputId": "1a175289-5b28-40c4-dcfd-3e998e7ee28f"
      },
      "outputs": [],
      "source": [
        "h = 1e-5\n",
        "(f(hd=h) - f()) / h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU3oPuQrtSSz"
      },
      "source": [
        "From the computational graph we can also know that $L=c*d$. When we change the value of $d$ just a little bit (derivative of $L$ wrt $d$) the value of $L$ will change by the amount of $c$, which is $8.0$. We saw it above in the partial derivative of a multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHZDEGaxuN11"
      },
      "outputs": [],
      "source": [
        "d.grad = c.data\n",
        "c.grad = d.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIjVrxFjuwXa"
      },
      "source": [
        "With the same logic, the derivative of $L$ wrt $c$ will be the value of $d$, which is $10.0$ in our specific case. We can verify it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVhDFMDls_ln",
        "outputId": "aa732c95-aa05-47eb-9fe7-c0db727ca6c1"
      },
      "outputs": [],
      "source": [
        "(f(hc=h) - f()) / h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JjzURHxvaSK"
      },
      "source": [
        "## Chain Rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQfXOH9Ewxlc"
      },
      "source": [
        "To determine how much changing earlier variables in the computation graph will affect the loss, we can apply the [chain rule](../../mathematics/01_calculus). Simply, the derivative of $L$ wrt $a$ is the derivative of $c$ wrt $a$ multiplied by the derivative of $L$ wrt $c$:\n",
        "\n",
        "$$\n",
        "\\frac{dL}{da} = \\frac{dL}{dc} \\cdot \\frac{dc}{da}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWP73t_gz8Xj"
      },
      "source": [
        "The derivate of $c$ both wrt $a$ and $b$ is $1.0$ due to the property of addition we had seen previously. From here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w0KmRCnz6ZP",
        "outputId": "00f4113e-72f3-421f-88f2-54eadcff4d32"
      },
      "outputs": [],
      "source": [
        "a.grad = 1.0 * c.grad\n",
        "b.grad = 1.0 * c.grad\n",
        "\n",
        "a.grad, b.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaSszQxr1IOT"
      },
      "source": [
        "We can verify it as well. Let's see how much $L$ gets affected, when we shift $a$ or $b$ by a very small amount."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBrH1MM1JgaW",
        "outputId": "e130cf0d-c5e9-4896-c62d-b8323f22f854"
      },
      "outputs": [],
      "source": [
        "(f(ha=h) - f()) / h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGczB2u6rkP7",
        "outputId": "400b4d35-db9d-41a5-f432-a944d64703a8"
      },
      "outputs": [],
      "source": [
        "(f(hb=h) - f()) / h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fL02zWq1ly7"
      },
      "source": [
        "We will finally redraw the manually updated computation graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "8O6AnZtctBVB",
        "outputId": "aaf0276c-4acb-49f8-edd7-71a059522ebf"
      },
      "outputs": [],
      "source": [
        "draw_dot(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-3iqGfM2p4X"
      },
      "source": [
        "It basically implies that, for example, changing the value of $a$ by $1.0$ unit (from $5$ to $6$) will increase the value of $L$ by $10$ units (from $80$ to $90$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6lNYfQQ2gPw",
        "outputId": "bbbb8858-2f09-40ac-f2e3-f25c68d6c42f"
      },
      "outputs": [],
      "source": [
        "f(ha=1), f(hb=1), f(hc=1), f(hd=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PY6RsCg5uPp"
      },
      "source": [
        "## Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgLGJtem51qr"
      },
      "source": [
        "What we saw above was one backward pass done manually. We are mainly interested in the **signs** of partial derivatives to know if they are positively or negatively influencing the eventual loss $L$ of our model. In our case, all the derivatives are positive and influence loss positively.  We have to simply nudge the values in the opposite direction of the gradient to bring the loss down. This is known as [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent).\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\, \\nabla_\\theta L\n",
        "$$\n",
        "\n",
        "Here, $ \\theta $ denotes a model **parameter**, $ \\nabla_\\theta L $ is the gradient of the loss wrt that parameter, and $ \\eta $ is the **learning rate**. The [learning rate](https://en.wikipedia.org/wiki/Learning_rate) controls how large each update step is during gradient descent. If the learning rate is too small, the parameters change very slowly and training takes a long time. If the learning rate is too large, the updates can overshoot the minimum and cause the loss to increase or oscillate. We will discuss learning rate in a greater detail in the future when discussing [optimization](../04_regul_optim)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COpW_4NU585t"
      },
      "outputs": [],
      "source": [
        "lr = 0.01 \n",
        "\n",
        "a.data -= lr * a.grad\n",
        "b.data -= lr * b.grad\n",
        "d.data -= lr * d.grad\n",
        "\n",
        "# we skip c which is controlled by the values of a and b\n",
        "# pay attention that the rest are leaf nodes in the computation graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYdzTg4X-6Gm"
      },
      "source": [
        "<div class=\"admonition note\">\n",
        "  <p class=\"admonition-title\">Note</p>\n",
        "  <p style=\"margin: 1em 0;\">\n",
        "    In case the loss is a negative value (not common), we will need to <em>gradient ascend</em> the loss upwards towards zero and change the sign to <code>+=</code> from <code>-=</code>. Note that the values of <em>parameters</em> (a, b, d) can decrease or increase depending on the sign of <code>grad</code>.\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsNNuRG7-Akb"
      },
      "source": [
        "We will now do a single *forward pass* to see if loss has been decreased. Recall that the previous loss was `80`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqrjXw2F9pi9",
        "outputId": "629bf7de-3788-4f6e-9899-6d50c89fab7b"
      },
      "outputs": [],
      "source": [
        "c = a + b\n",
        "L = c * d\n",
        "\n",
        "L.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfmW6OwG-ZZ1"
      },
      "source": [
        "It seems like we _optimized_ our values and brought down the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExJ9H0YOAR35"
      },
      "source": [
        "## Backward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW6mjwRQAbLl"
      },
      "source": [
        "Manually calculating gradient is good only for educational purposes. We should implement automatic _backward pass_ which will calculate gradients. To support this, we will rewrite our `Value` class to store a `_backward` function. This function will enforce the **local gradient** associated with the operation that produced the value. Initially, `_backward` does nothing. \n",
        "\n",
        "For addition, the local derivatives are: $ \\frac{\\partial c}{\\partial a} = 1 $ and $ \\frac{\\partial c}{\\partial b} = 1 $. Hence, the gradient flowing into $c$ (stored in `out.grad`) is passed unchanged to both inputs:\n",
        "\n",
        "- `a.grad += out.grad`\n",
        "- `b.grad += out.grad`\n",
        "\n",
        "This is why, in the `_backward` function for addition, both operands receive the same gradient contribution.\n",
        "\n",
        "For multiplication, the local derivatives are: $ \\frac{\\partial c}{\\partial a} = b $ and $ \\frac{\\partial c}{\\partial b} = a $. During backpropagation, the incoming gradient is scaled by the opposite operand:\n",
        "\n",
        "- `a.grad += b.data * out.grad`\n",
        "- `b.grad += a.data * out.grad`\n",
        "\n",
        "This reflects the chain rule: each variable’s gradient depends on how the output changes with respect to that variable. By storing a `_backward` function at every node and calling these functions in reverse topological order, the full gradient of the loss with respect to all intermediate values can be computed automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzCw1a9NAUPj"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "  def __init__(self, data, _prev=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self._prev = _prev\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "    self.grad = 0.0\n",
        "    self._backward = lambda: None # initially it is a function which does nothing\n",
        "    \n",
        "  def __add__(self, other):\n",
        "    data = self.data + other.data\n",
        "    out = Value(data, (self, other), '+')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = 1.0 * out.grad\n",
        "      other.grad = 1.0 * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    data = self.data * other.data\n",
        "    out = Value(data, (self, other), \"*\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = other.data * out.grad\n",
        "      other.grad = self.data * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data}, grad={self.grad})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j5_G3poD2zF"
      },
      "outputs": [],
      "source": [
        "# Recreating the same function\n",
        "a = Value(5, label='a')\n",
        "b = Value(3, label='b')\n",
        "c = a + b; c.label = 'c'\n",
        "d = Value(10, label='d')\n",
        "L = c * d; L.label = 'L'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "CB1Nk5QrEBV2",
        "outputId": "2b591b7d-637f-4b64-eabe-e873034a7f27"
      },
      "outputs": [],
      "source": [
        "draw_dot(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq_kO6XPELu0"
      },
      "source": [
        "We should not forget to initialize the gradient of the loss to be $1.0$ and then call `backward` function. With correct implementation, we will get the same results which we manually calculated previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjSWZ6I-EDyL"
      },
      "outputs": [],
      "source": [
        "L.grad = 1.0\n",
        "L._backward()\n",
        "c._backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "mVRYtmO4Eiu-",
        "outputId": "41fcddd4-0431-46fa-e2af-84dcc018e810"
      },
      "outputs": [],
      "source": [
        "draw_dot(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_FkqaRIHObi"
      },
      "source": [
        "<div class=\"admonition success\">\n",
        "  <p class=\"admonition-title\">Exercise</p>\n",
        "  <p style=\"margin: 1em 0;\">\n",
        "    Make sure that all operations and their partial derivatives can be calculated (e.g. division, power).\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNoliBPNKPs3"
      },
      "source": [
        "## Model Training with Backpropagation\n",
        "\n",
        "We can now **train the model** by repeatedly performing a _forward pass_, a _backward pass_, and an _optimization_ (gradient descent) step to reduce the loss. The backward pass, which computes gradients by propagating them through the whole computation graph, is called [backpropagation](https://en.wikipedia.org/wiki/Backpropagation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csvUYTVbElCT",
        "outputId": "90a4fb70-6b35-451e-b07f-7466826bb7ca"
      },
      "outputs": [],
      "source": [
        "# optimization\n",
        "lr = 0.05\n",
        "a.data -= lr * a.grad\n",
        "b.data -= lr * b.grad\n",
        "d.data -= lr * d.grad\n",
        "\n",
        "# forward pass\n",
        "c = a + b\n",
        "L = c * d\n",
        "\n",
        "# backward pass\n",
        "L.grad = 1.0\n",
        "L._backward()\n",
        "c._backward()\n",
        "\n",
        "L.data # loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IduNeP0cKEVd"
      },
      "source": [
        "We have now trained the model for a single **epoch**. Even though what we do is oversimplistic and not precise, the main intuition and concepts behind training a [neural network](../../02_neural_network) will be the same. We will now train the model for multiple epochs until we reduce the loss down to zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFpCd2qQJrkr",
        "outputId": "91f2242c-b95e-4570-fffa-482b6edd7d8c"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "  # optimization\n",
        "  a.data -= lr * a.grad\n",
        "  b.data -= lr * b.grad\n",
        "  d.data -= lr * d.grad\n",
        "\n",
        "  # forward pass\n",
        "  c = a + b\n",
        "  L = c * d\n",
        "\n",
        "  # backward pass\n",
        "  L.grad = 1.0\n",
        "  L._backward()\n",
        "  c._backward()\n",
        "\n",
        "  if L.data < 0:\n",
        "    break\n",
        "\n",
        "  print(f'Loss: {round(L.data,2)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqlXzzH3ObX7"
      },
      "source": [
        "## PyTorch Implementation\n",
        "\n",
        "<div class=\"admonition tip\">\n",
        "  <p class=\"admonition-title\">Tip</p>\n",
        "  <p style=\"margin: 1em 0;\">\n",
        "    <a href=\"https://pytorch.org/\">PyTorch</a> is preinstalled and configured for <a href=\"https://colab.research.google.com/\">Google Colab</a>. For local development, PyTorch can be installed via <code>pip install torch</code>. If you plan to use a GPU, make sure the installed PyTorch version matches your <a href='https://pytorch.org/get-started/locally/'>CUDA</a> setup.\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "\n",
        "As mentioned in the beginning, our manual implementation is built into PyTorch. We will do a forward and backward pass with the help of [autograd](https://docs.pytorch.org/docs/stable/autograd.html) engine and check if the gradients are what we had previosuly calculated. As gradients need not to be calculated for, say, leaf nodes, to optimizate calculation further, `requires_grad` is set to `False` by default, which we need to update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtGu--l6NPsE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor(5.0);    a.requires_grad = True\n",
        "b = torch.tensor(3.0);    b.requires_grad = True\n",
        "c = a + b\n",
        "d = torch.tensor(10.0);   d.requires_grad = True\n",
        "L = c * d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCfASLIxPPW8"
      },
      "outputs": [],
      "source": [
        "L.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OTlgIQaQeRJ",
        "outputId": "db42dc7a-83d3-4b59-cd91-5d1398ede014"
      },
      "outputs": [],
      "source": [
        "a.grad, b.grad, d.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "majsJxI0PBPY"
      },
      "source": [
        "This notebook introduced the core ideas behind gradient-based learning. It began with differentiation and extended the concept to partial derivatives for multivariable functions. Computation graphs were then introduced to represent how values depend on one another and to make gradient flow explicit. Using this structure, the gradient and the chain rule were used to explain how gradients were propagated backward from the loss to earlier variables. Gradient descent was presented as the mechanism for updating parameters in the direction that reduces the loss. The roles of the forward pass and backward pass were clarified and combined to describe model training with backpropagation. Finally, these ideas were demonstrated through a PyTorch implementation using automatic differentiation. In the next notebook, a neural network will be trained using a more advanced training engine.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "b4U6EydyWp64"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
