{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to CSCI 4701!","text":"<p>This is the website of the CSCI 4701: Deep Learning course taught at ADA University. </p> <p>Deep Learning focuses on artificial neural networks and how they are trained and optimized. This course covers core concepts starting with backpropagation, including regularization and optimization techniques. Students will gain practical skills using the PyTorch framework and learn neural architectures used in both Computer Vision (CV) and Natural Language Processing (NLP), including Convolutional Neural Networks (CNNs) and Transformer-based models. The course introduces generative modeling with Variational Autoencoders (VAEs) and briefly covers current developments in deep learning, such as Latent Diffusion Models (LDMs) and Large Language Models (LLMs). </p> <p>You can navigate through the course starting from the introductory overview or directly check the practical notebooks via the navigation bar.</p>"},{"location":"course/spring-2026/syllabus/","title":"Syllabus","text":"<p>Important</p> <p>     The content of this syllabus is subject to change. Please consistently check the course page on Blackboard and the ADA University Academic Calendar for modifications. The last day of the add/drop period, holidays, and other academic deadlines are noted in the calendar.   </p> <p>Info</p> <p>    Square brackets in the Assessment / Notes column indicate the range of classes whose material is covered by the assessment. For example, Quiz 1 [1\u20133] means that the quiz assesses material covered in classes 1 through 3.   </p> Week Topic Learning Outcomes Assessment / Notes 1 Deep Learning (DL) Overview / Course Structure Describe the scope of DL and the course syllabus. Fulfill technological requirements. 2 Mathematics of DL: Linear Algebra / Calculus Work with vectors, matrices, and tensors; apply norms and inner products. Compute partial derivatives and apply the chain rule. Optional: intuition behind eigenvectors and SVD. 3 Gradient Descent / Backpropagation I Compute gradients on computational graphs. Perform forward and backward passes. Understand gradient descent updates and automatic differentiation (PyTorch autograd, micrograd). 4 Gradient Descent / Backpropagation II Implement full backpropagation. Feb 3: Quiz 1 [1\u20133]Last day to submit team member details 5 Activation Functions / Neuron Implement activation functions and understand non-linearity. Backpropagate over an N-dimensional neuron. 6 Multilayer Perceptron (MLP) Construct an MLP from stacked neurons. Train a simple MLP classifier on a small dataset. Feb 10: Project proposal deadline 7 Images as Tensors / MLP on MNIST / Batching &amp; Cross-Entropy Understand image representations, tensor shapes, and batching. Use torchvision datasets and dataloaders. Train an MLP on MNIST with SGD + cross-entropy. Feb 12: Quiz 2 [5\u20136] 8 Convolutional Neural Networks (CNN) Define and implement 2D cross-correlation (convolution) and pooling with kernels, including padding and stride. Train a LeNet-style CNN on MNIST. Compare MLP with CNN. 9 Mathematics of DL: Probability Theory Describe random variables; distinguish discrete and continuous distributions; work with PMF/PDF. Compute expectation, variance, and covariance. Use conditional probability, independence, and Bayes\u2019 rule. Recognize common distributions. Feb 19: Quiz 3 [7\u20138] 10 Regularization Apply weight decay and dropout. Handle exploding and vanishing gradients. Use Xavier and He initialization. Distinguish local minima from saddle points in training dynamics. 11 Optimization Adjust learning rate and apply schedules. Use SGD with momentum. Apply RMSProp and Adam. Compare optimizers based on convergence behavior and practical performance. 12 Regularization / Optimization Train a regularized CNN on CIFAR-10 using optimizers. Apply hyperparameter tuning. Mar 3: Quiz 4 [10\u201311] 13 Paper: AlexNet Discuss AlexNet, its key ideas, what is outdated, and the paper structure. 14 Bigram Model / Negative Log-Likelihood / Softmax Build a character-level bigram model and sample from it. Distinguish probability vs likelihood. Compute average negative log-likelihood as a loss. Explain the purpose of softmax. 15 Neural Network N-gram Model / Mini-Batch Training Construct a neural N-gram model. Train the model with mini-batch updates. Mar 12: Quiz 5 [14]Project milestone 1 deadline 16 Midterm Exam \u2014 Tuesday, Mar 17: Midterm Exam [1\u201315] 17 Midterm Exam Review Half-semester overview. \u2014 Holidays \u2014 Mar 20\u201330 18 Batch Normalization / Layer Normalization Explain why normalization helps training deep networks. Implement batch normalization and understand training vs evaluation behavior. Understand batch-size effects and when to prefer layer normalization. 19 Residual Blocks / Residual Network for NLP Understand residual (skip) connections. Add a residual block to a feed-forward N-gram model with correct dimensions. Connect residuals to vanishing gradients and regularization. 20 Sequence Modeling: Autoregressive Models and RNN/LSTM Explain autoregressive modeling beyond fixed context windows. Describe how RNNs maintain state. Identify limitations of RNN/LSTM/GRU. Apr 7: Quiz 6 [18\u201319] 21 Attention Mechanism Understand attention as weighted information selection. Derive queries, keys, and values at the tensor level. Implement attention with matrix operations and verify shapes and normalization. 22 Transformer Architecture / Self-Attention Explain self-attention and Transformer blocks. Explain how Transformers scale. Apr 14: Quiz 7 [20\u201321] 23 Transformer Blocks Assemble a Transformer block from self-attention and feed-forward sublayers. Trace signal flow. Analyze training stability and sensitivity to initialization and learning rate. 24 Paper Reading: Transformer, Vision Transformer, Swin Transformer Extract core architectural ideas and compare attention for sequences vs images. Discuss scalability and efficiency constraints. Apr 21: Quiz 8 [22\u201323] 25 Mathematics of DL: Information Theory and Probabilistic Modeling Compute entropy, cross-entropy, and KL divergence. Derive cross-entropy loss from maximum likelihood. Interpret common losses as probabilistic objectives. 26 Variational Autoencoders I Introduce latent-variable generative models. Explain latent representations and probabilistic encoders/decoders. Explain approximate inference and why variational methods are needed. Apr 28: Project milestone 2 deadline 27 Variational Autoencoders II Understand the VAE objective (ELBO). Implement a VAE. Interpret reconstruction and regularization terms and their trade-off. 28 Diffusion Models / Score Matching Formulate diffusion via forward noising and learned reverse denoising. Interpret the training objective as denoising score matching. Explain sampling as iterative probabilistic inference. May 5: Quiz 9 [25\u201327] 29 Foundation Models and Modern Trends Explain large-scale pretraining and transfer learning. Examine GPT, BERT, CLIP, and latent diffusion models (LDMs). Discuss scaling behavior and limitations. \u2014 Final Exam \u2014 Tuesday, May 12: Final Exam [1\u201329]"},{"location":"introduction/01_overview/","title":"Deep Learning Overview","text":"18 Jan 2026 \u00b7   9 min <p>Artificial Intelligence (AI) is the broad field concerned with building systems that perform tasks requiring intelligence. Machine Learning (ML) is a subfield of AI that enables systems to learn patterns and make decisions from data rather than explicit rules. Deep Learning (DL) is a subfield of ML that uses multi-layer neural networks to learn complex representations from large datasets.</p> <p>Note</p> <p>     The following sources were used in preparing this text:   </p> <ul> <li>       Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016).       Deep Learning. MIT Press.        </li> <li>       Schmidhuber, J. (2015).       Deep Learning in Neural Networks: An Overview.       Neural Networks.     </li> <li>       Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J.      Dive into Deep Learning. d2l.ai."},{"location":"introduction/01_overview/#ai-ml-dl","title":"AI / ML / DL","text":"<p>AI initially focused on what is often called the knowledge-based approach, where intelligence was treated as something that could be explicitly written down. Researchers attempted to encode reasoning as rules, symbols, and logical statements. If a human expert knew how to solve a problem, the reasoning steps would be formalized and executed by a machine.</p> <p>This approach failed when faced with the ambiguity and variability of the real world. Tasks that humans perform effortlessly, such as recognizing faces or understanding speech, are precisely the tasks that are hardest to describe step by step. Human expertise in these domains is largely implicit rather than explicit. Rule-based systems therefore became brittle, difficult to scale, and expensive to maintain. Small changes in the environment often required rewriting large portions of the system, making progress slow and fragile.</p>      Deep Learning and AI ~ Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.     <p>ML offered a different perspective. Instead of programming intelligence directly, machines were allowed to learn patterns from data. Classical ML algorithms such as linear models, logistic regression, na\u00efve Bayes, and decision trees achieved real success in applications like medical decision support, spam filtering, and credit scoring. However, these methods relied heavily on hand-crafted features. Human designers had to decide in advance which properties of the data were relevant, and performance depended more on feature design than on the learning algorithm itself.</p> <p>This reliance on features became a serious limitation as data grew more complex. Images, audio signals, and language live in very high-dimensional spaces. In such spaces, intuition breaks down, a phenomenon often referred to as the curse of dimensionality. As dimensionality increases, data becomes sparse, distances lose their meaning, and small modeling assumptions can cause large failures. Feature engineering becomes brittle and does not scale to the richness of real-world data.</p> <p>The natural response to this problem was representation learning. Instead of manually defining features, the model learns useful representations directly from raw data. Early methods such as Principal Component Analysis (PCA), kernel methods, sparse coding, and shallow neural networks pursued this idea. They demonstrated that learning intermediate representations could significantly improve performance and reduce reliance on handcrafted features. However, these approaches were typically shallow, consisting of only one or two layers of transformation. As a result, they struggled to capture the hierarchical structure present in real-world data.</p> <p>Many perceptual tasks are inherently compositional. Images are composed of edges, edges form textures and parts, parts form objects, and objects form scenes. Speech and language exhibit similar hierarchies. Shallow models can learn simple transformations, but they cannot efficiently represent such multi-level abstractions. Attempting to do so requires an exponential number of features or parameters, making learning unstable and data-inefficient. In practice, representation learning without depth hit a ceiling: it reduced feature engineering, but it could not scale to the complexity of vision, speech, and language.</p> <p>DL extends representation learning by stacking many layers of nonlinear transformations. Each layer learns to represent the data at a higher level of abstraction, allowing complex structures to be built incrementally. </p> <p>At a fundamental level, both classical ML and DL do the same thing: they learn a function from data. The difference is not in what is learned, but in how much of the function is learned automatically. In all cases, learning amounts to selecting parameters so that a function best approximates the desired input\u2013output relationship under a given objective.</p> <p>Interestingly, DL did not introduce fundamentally new mathematical ideas. Many concepts, such as multi-layer neural networks, backpropagation, gradient-based optimization, and even convolutional architectures were known decades earlier. </p>"},{"location":"introduction/01_overview/#biological-and-artificial-neurons","title":"Biological and Artificial Neurons","text":"<p>DL is not an attempt to simulate the brain. Artificial neural networks are inspired by biological neurons, but the resemblance is conceptual rather than literal. </p>        Structure of a typical neuron with Schwann cells in the peripheral nervous system ~ \"Anatomy and Physiology\" by the US National Cancer Institute's Surveillance | CC BY-SA 3.0 | Wikimedia Commons <p>A biological neuron is a living cell designed for communication in a noisy, energy-constrained environment. It receives signals through dendrites, integrates them in the soma (cell body), and, if a threshold is reached, sends an electrical pulse along the axon to other neurons through synapses. Learning occurs locally by strengthening or weakening synaptic connections through repeated interaction with the environment.</p>        Artificial Neuron ~ Funcs, Own work | CC0 | Wikimedia Commons <p>An artificial neuron is a mathematical function that combines numerical inputs and produces a numerical output. Much like how airplanes were inspired by birds but rely on entirely different aerodynamic mechanisms, the success of DL does not come from biological realism. Biological systems served primarily as inspiration.</p>"},{"location":"introduction/01_overview/#evolution-of-deep-learning","title":"Evolution of Deep Learning","text":"<p>Learning from data predates computers. The mathematical backbone of modern deep learning is the chain rule, formalized by Gottfried Wilhelm Leibniz and later exploited by backpropagation algorithms. Carl Friedrich Gauss and Adrien-Marie Legendre used linear regression in the early nineteenth century, a method mathematically equivalent to a shallow neural network. In the mid-twentieth century, researchers such as Warren McCulloch and Walter Pitts, Frank Rosenblatt, and Bernard Widrow explored learning machines inspired by biological neurons. These early systems were limited\u2014often linear or single-layer\u2014and constrained by the theory and hardware of their time.</p> <p>Multi-layer learning systems already existed by the 1960s and 1970s. Alexey Ivakhnenko and Valentin Lapa trained models with adaptive hidden layers, while Kunihiko Fukushima introduced the Neocognitron, a hierarchical, convolution-like architecture that anticipated modern convolutional networks.</p> <p>But why did DL become popular only after the 2010s? The obstacle was never the lack of a correct algorithm. It was the lack of data and the cost of computation. DL worked because three forces aligned. Data became abundant because digital life produces it automatically. Computation became affordable because parallel hardware matured. And (less critically) software matured enough to make experimentation fast and scalable.</p>"},{"location":"introduction/01_overview/#data","title":"Data","text":"<p>The modern era began when data stopped being rare. This shift was driven by broader technological changes. Digital sensors replaced analog ones, smartphones placed cameras and microphones in billions of pockets, and the internet enabled continuous sharing of images, text, audio, and video. Companies began logging user interactions by default, storage became cheap, and bandwidth increased dramatically. Data was no longer collected deliberately, it was generated automatically as a byproduct of everyday life.</p> <p>Before large-scale datasets became feasible, progress relied on small, carefully curated benchmarks. The famous MNIST dataset was collected by the National Institute of Standards and Technology (NIST), and later was modified (hence the M before NIST) for simpler usage of ML algorithms<sup>2</sup>. MNIST is a simple dataset of handwritten digits that allowed researchers to isolate questions about optimization, architectures, and learning dynamics without the confounding effects of scale and noise. </p>      MNIST inputs ~ Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.     <p>A symbolic moment was the creation of ImageNet (Deng et al.). ImageNet contained roughly 14 million labeled images, with about 1.2 million training images across 1,000 categories used in its main benchmark. This scale exposed the limitations of hand-crafted features. Models that performed well on small datasets failed to generalize, while systems capable of learning representations directly from data improved reliably.</p> <p>In 2012, AlexNet (Krizhevsky et al.) won the ImageNet competition by a large margin. The model was unusually large and computationally demanding, and training it required GPUs rather than CPUs. This detail is crucial. DL did not succeed merely because sufficient data became available, it succeeded because the models finally fit within the limits of available hardware.<sup>1</sup></p> <p>As of 2016, a rough rule of thumb is that a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category and will match or exceed human performance when trained with a dataset containing at least 10 million labeled examples.  </p> <p>Deep Learning  (Chapter I) ~ Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). MIT Press.</p>"},{"location":"introduction/01_overview/#hardware","title":"Hardware","text":"<p>Training neural networks is dominated by large-scale numerical operations repeated many times. CPUs are optimized for general-purpose tasks and complex control flow, but they are inefficient for massive parallel arithmetic. GPUs, originally designed for rendering images, apply the same operation to many data points simultaneously. This made them a natural fit for neural network training.</p> <p>NVIDIA became central to DL because it invested early in programmable GPUs and the software needed to exploit them. Although originally developed for video games, GPUs are fundamentally optimized for massively parallel linear algebra, especially matrix and tensor operations. The introduction of CUDA exposed this capability to researchers, making large-scale matrix multiplications\u2014the core computational workload of neural networks\u2014efficient and accessible. As a result, models that once took weeks to train on CPUs could be trained in days or hours. Later accelerators such as Tensor Processing Unit (TPU) followed the same principle: DL scales when hardware is designed around dense linear algebra, high memory bandwidth, and parallel computation.</p>"},{"location":"introduction/01_overview/#software","title":"Software","text":"<p>The relevant software emerged in parallel with hardware. Python became the dominant language for ML because it allowed researchers to write clear, concise code while delegating computationally intensive operations to highly optimized numerical libraries implemented in C, C++, and CUDA. This separation between high-level model logic and low-level performance-critical kernels proved decisive. Researchers could focus on ideas rather than infrastructure, iterating rapidly while still benefiting from efficient linear algebra routines running on GPUs.</p> <p>Modern DL frameworks such as PyTorch and TensorFlow made it possible to automate differentiation, memory management, and efficient parallel execution. As a result, experiments that once required weeks of careful implementation could be expressed in hundreds of lines of code and tested within days.</p> <p>PyTorch is primarily a tool for research and experimentation. It is designed to feel like ordinary Python code, which makes models easy to write, modify, and debug. Tools such as PyTorch Lightning build on this flexibility by handling routine tasks like training loops and logging, allowing users to keep their focus on the model itself.</p> <p>TensorFlow, on the other hand, is more strongly oriented toward engineering and deployment. It was built to support large systems that need to run reliably across different machines and environments. With the addition of Keras, TensorFlow offers a high-level interface that makes it easy to define standard models and training pipelines in a consistent way. This structure is well suited to production settings, where models must be maintained, scaled, and deployed efficiently over long periods of time.</p>"},{"location":"introduction/01_overview/#transformers-and-beyond","title":"Transformers and Beyond","text":"<p>Computer Vision (CV) and Natural Language Processing (NLP) are the two main perception-oriented branches of modern DL. Both aim to convert raw, high-dimensional signals into structured representations that machines can reason over, but they operate on different data modalities and evolved under different constraints.</p> <p>CV focuses on visual data such as images and videos. Early progress was driven by convolutional neural networks (CNN). NLP deals with sequential, symbolic data such as text and speech. While early neural NLP relied on recurrent models (RNN), a major conceptual shift occurred with the introduction of the Transformer architecture (Vaswani et al., 2017), which replaced sequential recurrence with attention-based information routing. This change enabled massive parallelism, better long-range dependency modeling, and effective scaling with data and compute. The same architecture was later adapted to images via Vision Transformers (Dosovitskiy et al., 2020), revealing that vision and language could share a common computational backbone despite their different input structures.</p> <p>DL also extended beyond perception into decision-making. The combination of deep learning and reinforcement learning became widely visible through AlphaGo and later AlphaZero (Silver et al., 2016; 2018), which learned complex games through self-play without human examples.</p> <p>Building on the Transformer architecture, Large Language Models (LLM) such as Generative Pre-trained Transformer (GPT) marked a shift from task-specific NLP systems to general-purpose foundation models. By training a model on massive text corpora, GPT-style models learn broad linguistic, semantic, and world-level regularities that can be reused across tasks. Their success demonstrated that scale\u2014data, parameters, and compute\u2014can replace handcrafted linguistic structure, and that a single architecture can support a wide range of capabilities, including translation, summarization, reasoning, and code generation, without explicit task-specific design.</p> <ol> <li> <p>Even then, Alex Krizhevsky had to distribute training across two NVIDIA GeForce GTX 580 GPUs, each with 3 GB of memory (best at the moment), because the network did not fit on a single GPU.\u00a0\u21a9</p> </li> <li> <p>Geoffrey Hinton called this dataset \"the drosophila of ML\", a fruit fly extensively used in genetic research labs.\u00a0\u21a9</p> </li> </ol>"},{"location":"introduction/02_materials/","title":"Study Materials","text":""},{"location":"mathematics/la/","title":"Linear Algebra","text":""},{"location":"notebooks/01_backprop/","title":"01. From Derivatives to Backpropagation","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[2]: Copied! <pre>def f(x):\n  return x**2\n</pre> def f(x):   return x**2 In\u00a0[3]: Copied! <pre>x = 3.0\nfor h in [10, 1, 0.1, 0]:\n  print(f\"If we shift input by {h}, output becomes {f(x+h)}\")\n</pre> x = 3.0 for h in [10, 1, 0.1, 0]:   print(f\"If we shift input by {h}, output becomes {f(x+h)}\") <pre>If we shift input by 10, output becomes 169.0\nIf we shift input by 1, output becomes 16.0\nIf we shift input by 0.1, output becomes 9.610000000000001\nIf we shift input by 0, output becomes 9.0\n</pre> In\u00a0[4]: Copied! <pre>h = 1.0\n\ndx = h\ndy = f(x+h) - f(x)\n\nprint(f\"\u0394x: {dx}\")\nprint(f\"\u0394y: {dy}\")\nprint(f\"When you change x by {dx} unit, y changes by {dy} units.\")\n</pre> h = 1.0  dx = h dy = f(x+h) - f(x)  print(f\"\u0394x: {dx}\") print(f\"\u0394y: {dy}\") print(f\"When you change x by {dx} unit, y changes by {dy} units.\") <pre>\u0394x: 1.0\n\u0394y: 7.0\nWhen you change x by 1.0 unit, y changes by 7.0 units.\n</pre> In\u00a0[5]: Copied! <pre>def plot_delta(x, h, start=-4, stop=4, num=30):\n  # `np.linspace` returns an array of num inputs within a range.\n  x_all = np.linspace(start, stop, num)\n  y_all = f(x_all)\n\n  plt.figure(figsize=(4, 4))\n  plt.plot(x_all, y_all)\n\n  # dx &amp; dy\n  plt.plot([x, x + h], [f(x), f(x)], color='r')\n  plt.plot([x + h, x + h], [f(x), f(x + h)], color='r')\n</pre> def plot_delta(x, h, start=-4, stop=4, num=30):   # `np.linspace` returns an array of num inputs within a range.   x_all = np.linspace(start, stop, num)   y_all = f(x_all)    plt.figure(figsize=(4, 4))   plt.plot(x_all, y_all)    # dx &amp; dy   plt.plot([x, x + h], [f(x), f(x)], color='r')   plt.plot([x + h, x + h], [f(x), f(x + h)], color='r') In\u00a0[6]: Copied! <pre>plot_delta(x=2, h=1)\n</pre> plot_delta(x=2, h=1) <p>How to find if the ouput changes significantly when we change the input by some amount h?</p> In\u00a0[7]: Copied! <pre>def plot_roc(x, h):\n  dx = h\n  dy = f(x + h) - f(x)\n\n  plot_delta(x, h)\n  print(f\"Rate of change is {dy / dx}\")\n</pre> def plot_roc(x, h):   dx = h   dy = f(x + h) - f(x)    plot_delta(x, h)   print(f\"Rate of change is {dy / dx}\") In\u00a0[8]: Copied! <pre>plot_roc(3, 1)\n</pre> plot_roc(3, 1) <pre>Rate of change is 7.0\n</pre> In\u00a0[9]: Copied! <pre>plot_roc(3, 0.5)\n</pre> plot_roc(3, 0.5) <pre>Rate of change is 6.5\n</pre> In\u00a0[10]: Copied! <pre>plot_roc(1, 1)\n</pre> plot_roc(1, 1) <pre>Rate of change is 3.0\n</pre> In\u00a0[11]: Copied! <pre>plot_roc(-2, 0.5)\n</pre> plot_roc(-2, 0.5) <pre>Rate of change is -3.5\n</pre> <p>The rate of change for different values of h are different at the same point x. We would like to come up with a single value that would tell how significantly y changes at a given point x within the function (<code>a</code> in the formula corresponds to <code>x</code> in the code).</p> <p></p> <p>Simply, this limit tells us how much the value of y will change when we change x by just a very small amount. Note: Essentially, derivative is a function.</p> In\u00a0[12]: Copied! <pre>x = 3\nh = 0.000001 # limit of h approaches 0\nd = (f(x + h) - f(x)) / h\nprint(f\"The value of derivative function is {d}\")\n</pre> x = 3 h = 0.000001 # limit of h approaches 0 d = (f(x + h) - f(x)) / h print(f\"The value of derivative function is {d}\") <pre>The value of derivative function is 6.000001000927568\n</pre> <p>Partial derivative with respect to some variable basically means how much the output will change when we nudge that variable by a very small amount.</p> In\u00a0[13]: Copied! <pre>f = lambda x, y: x + y\n</pre> f = lambda x, y: x + y In\u00a0[14]: Copied! <pre>x = 2\ny = 3\n\nf(x, y)\n</pre> x = 2 y = 3  f(x, y) Out[14]: <pre>5</pre> In\u00a0[15]: Copied! <pre>h = 0.000001\nf(x + h, y)\n</pre> h = 0.000001 f(x + h, y) Out[15]: <pre>5.000001</pre> <p>Let\u2019s see partial derivatives with respect to x an y.</p> In\u00a0[16]: Copied! <pre># wrt x\n(f(x + h, y) - f(x, y)) / h\n</pre> # wrt x (f(x + h, y) - f(x, y)) / h Out[16]: <pre>1.000000000139778</pre> In\u00a0[17]: Copied! <pre># wrt y\n(f(x, y+h) - f(x, y)) / h\n</pre> # wrt y (f(x, y+h) - f(x, y)) / h Out[17]: <pre>1.000000000139778</pre> <p>It will always approach 1 for addition, no matter what are the input values.</p> In\u00a0[18]: Copied! <pre>for x, y in zip([-20, 2, 3], [300, 75, 10]):\n  print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}')\n</pre> for x, y in zip([-20, 2, 3], [300, 75, 10]):   print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}') <pre>x=-20, y=300: 0.9999999974752427\nx=2, y=75: 0.9999999974752427\nx=3, y=10: 1.0000000010279564\n</pre> <p>Indeed, if we have simple addition x + y, then increasing x or y by some amount will increase the result by the exact same amount. Assertion will work for any number h gets.</p> In\u00a0[19]: Copied! <pre>h = 10\nassert f(x+h, y) - f(x, y) == h\nassert f(x, y+h) - f(x, y) == h\n</pre> h = 10 assert f(x+h, y) - f(x, y) == h assert f(x, y+h) - f(x, y) == h In\u00a0[20]: Copied! <pre>f = lambda x, y: x * y\n</pre> f = lambda x, y: x * y In\u00a0[21]: Copied! <pre>x = 2\ny = 3\nh = 1e-5 # same as 0.00001\n(f(x + h, y) - f(x, y)) / h # wrt x\n</pre> x = 2 y = 3 h = 1e-5 # same as 0.00001 (f(x + h, y) - f(x, y)) / h # wrt x Out[21]: <pre>3.000000000064062</pre> In\u00a0[22]: Copied! <pre>for x in [-20, 2, 3]:\n  print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}')\n</pre> for x in [-20, 2, 3]:   print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}') <pre>x=-20, y=3: 2.999999999531155\nx=2, y=3: 3.000000000064062\nx=3, y=3: 3.000000000064062\n</pre> In\u00a0[23]: Copied! <pre>x = 10\nh = 5\npdx = (f(x+h, y) - f(x, y)) / h\nprint(pdx, y)\nassert round(pdx, 2) == round(y, 2)\n</pre> x = 10 h = 5 pdx = (f(x+h, y) - f(x, y)) / h print(pdx, y) assert round(pdx, 2) == round(y, 2) <pre>3.0 3\n</pre> In\u00a0[24]: Copied! <pre>def f(a, b, c):\n  return a**2 + b**3 - c\n</pre> def f(a, b, c):   return a**2 + b**3 - c In\u00a0[25]: Copied! <pre>a = 2\nb = 3\nc = 4\n\nf(a, b, c)\n</pre> a = 2 b = 3 c = 4  f(a, b, c) Out[25]: <pre>27</pre> In\u00a0[26]: Copied! <pre>h = 1\n\nf(a + h, b, c)\n</pre> h = 1  f(a + h, b, c) Out[26]: <pre>32</pre> In\u00a0[27]: Copied! <pre>f(a + h, b, c) - f(a, b, c)\n</pre> f(a + h, b, c) - f(a, b, c) Out[27]: <pre>5</pre> In\u00a0[28]: Copied! <pre>(f(a + h, b, c) - f(a, b, c)) / h\n</pre> (f(a + h, b, c) - f(a, b, c)) / h Out[28]: <pre>5.0</pre> In\u00a0[29]: Copied! <pre>h = 0.00001 # when the change is approaching zero\npda = (f(a + h, b, c) - f(a, b, c)) / h\npda\n</pre> h = 0.00001 # when the change is approaching zero pda = (f(a + h, b, c) - f(a, b, c)) / h pda Out[29]: <pre>4.000010000027032</pre> In\u00a0[30]: Copied! <pre>assert 2*a == round(pda)\n</pre> assert 2*a == round(pda) <p>Our function was <code>f(a,b,c) = a<sup>2</sup>+b<sup>3</sup>-c</code>. Partial derivative with respect to a is <code>2a</code> (by the power rule), and when <code>a=2</code> indeed we get 4.</p> <p>Exercise: Code the partial derivative with respect to b and c and verify if the result correct.</p> In\u00a0[31]: Copied! <pre># This is a graph visualization code from micrograd, no need to understand the details\n# https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb\nfrom graphviz import Digraph\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n\n    for n in nodes:\n        dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n\n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> # This is a graph visualization code from micrograd, no need to understand the details # https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb from graphviz import Digraph  def trace(root):     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root, format='svg', rankdir='LR'):     \"\"\"     format: png | svg | ...     rankdir: TB (top to bottom graph) | LR (left to right)     \"\"\"     assert rankdir in ['LR', 'TB']     nodes, edges = trace(root)     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})      for n in nodes:         dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')         if n._op:             dot.node(name=str(id(n)) + n._op, label=n._op)             dot.edge(str(id(n)) + n._op, str(id(n)))      for n1, n2 in edges:         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[32]: Copied! <pre># Value class stores a number and \"remembers\" information about its origins\nclass Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self.grad = 0\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+')\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), \"*\")\n    return out\n\n  def __repr__(self):\n    return f\"Value(data={self.data}, grad={self.grad})\"\n</pre> # Value class stores a number and \"remembers\" information about its origins class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self.grad = 0    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+')     return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), \"*\")     return out    def __repr__(self):     return f\"Value(data={self.data}, grad={self.grad})\" In\u00a0[33]: Copied! <pre>a = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b; c.label = 'c'\nd = Value(10, label='d')\nL = c * d; L.label = 'L'\n</pre> a = Value(5, label='a') b = Value(3, label='b') c = a + b; c.label = 'c' d = Value(10, label='d') L = c * d; L.label = 'L' In\u00a0[34]: Copied! <pre>print(a, a._prev)\nprint(L, L._prev)\n</pre> print(a, a._prev) print(L, L._prev) <pre>Value(data=5, grad=0) ()\nValue(data=80, grad=0) (Value(data=8, grad=0), Value(data=10, grad=0))\n</pre> In\u00a0[35]: Copied! <pre>draw_dot(c)\n</pre> draw_dot(c) Out[35]: In\u00a0[36]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[36]: <p>Gradient is vector of partial derivatives.</p> <p>We want to know how much changing each variable will affect the output of <code>L</code>. We will store those partial derivatives inside each <code>grad</code> variable of each <code>Value</code> object.</p> In\u00a0[37]: Copied! <pre>L.grad = 1.0\n</pre> L.grad = 1.0 <p>The derivative of a variable with respect to itself is 1 (you get the same dx/dy).</p> In\u00a0[38]: Copied! <pre>f = lambda x: x\nh = 1e-5\npdx = (f(x + h) - f(x)) / h\nassert round(pdx) == 1\n</pre> f = lambda x: x h = 1e-5 pdx = (f(x + h) - f(x)) / h assert round(pdx) == 1 <p>Now let's see how changing other variables will affect the eventual result.</p> In\u00a0[39]: Copied! <pre>def f(ha=0, hb=0, hc=0, hd=0):\n  # same function as before\n  a = Value(5 + ha, label='a')\n  b = Value(3 + hb, label='b')\n  c = a + b + Value(hc); c.label = 'c'\n  d = Value(10 + hd, label='d')\n  L = c * d; L.label = 'L'\n  return L.data\n</pre> def f(ha=0, hb=0, hc=0, hd=0):   # same function as before   a = Value(5 + ha, label='a')   b = Value(3 + hb, label='b')   c = a + b + Value(hc); c.label = 'c'   d = Value(10 + hd, label='d')   L = c * d; L.label = 'L'   return L.data In\u00a0[40]: Copied! <pre>h = 1e-5\n(f(hd=h) - f()) / h\n</pre> h = 1e-5 (f(hd=h) - f()) / h Out[40]: <pre>7.999999999697137</pre> <p>From the computational graph we can also see that <code>L=c*d</code>. When we change the value of d just a little bit (derivative of <code>L</code> with respect to <code>d</code>) the value of <code>L</code> will change by the amount of <code>c</code>, which is <code>8.0</code>. We saw it above in the partial derivative of a multiplication.</p> In\u00a0[41]: Copied! <pre>d.grad = c.data\nc.grad = d.data\n</pre> d.grad = c.data c.grad = d.data <p>With the same logic, the derivative of <code>L</code> wrt <code>c</code> will be the value of <code>d</code>, which is <code>10.0</code>. We can verify it.</p> In\u00a0[42]: Copied! <pre>(f(hc=h) - f()) / h\n</pre> (f(hc=h) - f()) / h Out[42]: <pre>10.000000000331966</pre> <p>To determine how much changing earlier variables in the computation graph will affect the <code>L</code> variable, we can apply the chain rule. Simply, the derivative of <code>L</code> with respect to <code>a</code> is the derivative of <code>c</code> with respect to <code>a</code> multiplied by the derivative of <code>L</code> with respect to <code>c</code>. God bless Leibniz.</p> <p></p> <p>The derivate of <code>c</code> both wrt <code>a</code> and 'b' is <code>1</code> due to the property of addition shown before (<code>c=a+b</code>). From here:</p> In\u00a0[43]: Copied! <pre>a.grad = 1.0 * c.grad\nb.grad = 1.0 * c.grad\n\na.grad, b.grad\n</pre> a.grad = 1.0 * c.grad b.grad = 1.0 * c.grad  a.grad, b.grad Out[43]: <pre>(10.0, 10.0)</pre> <p>We can verify it as well. Let's see how much <code>L</code> gets affected, when we shift <code>a</code> or <code>b</code> by a small amount.</p> In\u00a0[44]: Copied! <pre>(f(ha=h) - f()) / h\n</pre> (f(ha=h) - f()) / h Out[44]: <pre>10.000000000331966</pre> In\u00a0[45]: Copied! <pre>(f(hb=h) - f()) / h\n</pre> (f(hb=h) - f()) / h Out[45]: <pre>10.000000000331966</pre> <p>We will finally redraw the manually updated computation graph.</p> In\u00a0[46]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[46]: <p>It basically implies that, for example, changing the value of <code>a</code> by <code>1</code> unit (from 5 to 6) will increase the value of <code>L</code> by <code>10</code> units (from 80 to 90).</p> In\u00a0[47]: Copied! <pre>f(ha=1)\n</pre> f(ha=1) Out[47]: <pre>90</pre> In\u00a0[48]: Copied! <pre>f(hb=1), f(hc=1), f(hd=1) # the rest of the cases\n</pre> f(hb=1), f(hc=1), f(hd=1) # the rest of the cases Out[48]: <pre>(90, 90, 88)</pre> <p>What we saw above was one backward pass done manually. We are mainly interested in the signs of partial derivatives to know if they are positively or negatively influencing the eventual loss <code>L</code> of our model. In our case, all the derivatives are positive and influence loss positively.</p> <p>We have to simply nudge the values in the opposite direction of the gradient to bring the loss down. This is known as gradient descent.</p> In\u00a0[49]: Copied! <pre>lr = 0.01 # we will discuss learning rate later on\n\na.data -= lr * a.grad\nb.data -= lr * b.grad\nd.data -= lr * d.grad\n\n# we skip c which is controlled by the values of a and b\n# pay attention that the rest are leaf nodes in the computation graph\n</pre> lr = 0.01 # we will discuss learning rate later on  a.data -= lr * a.grad b.data -= lr * b.grad d.data -= lr * d.grad  # we skip c which is controlled by the values of a and b # pay attention that the rest are leaf nodes in the computation graph <p>In case the loss is a negative value (not common), we will need to \"gradient ascend\" the loss upwards towards zero and change the sign to <code>+=</code> from <code>-=</code>. Note that the values of parameters (a, b, d) can decrease or increase depending on the sign of <code>grad</code>.</p> <p>We will now do a single forward pass to see if loss has been decreased. Previous loss was <code>80</code>.</p> In\u00a0[50]: Copied! <pre># We will now forward pass\nc = a + b\nL = c * d\n\nL.data\n</pre> # We will now forward pass c = a + b L = c * d  L.data Out[50]: <pre>77.376</pre> <p>We optimized our values and brought down the loss.</p> <p>Manually calculating gradient is good only for educational purposes. We should implement automatic backward pass which will calculate gradients. We will rewrite our <code>Value</code> class for <code>backward()</code> function.</p> In\u00a0[51]: Copied! <pre>class Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self.grad = 0.0\n    self._backward = lambda: None # initially it is a function which does nothing\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+')\n\n    def _backward():\n      self.grad = 1.0 * out.grad\n      other.grad = 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), \"*\")\n\n    def _backward():\n      self.grad = other.data * out.grad\n      other.grad = self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __repr__(self):\n    return f\"Value(data={self.data}, grad={self.grad})\"\n</pre> class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self.grad = 0.0     self._backward = lambda: None # initially it is a function which does nothing    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+')      def _backward():       self.grad = 1.0 * out.grad       other.grad = 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), \"*\")      def _backward():       self.grad = other.data * out.grad       other.grad = self.data * out.grad     out._backward = _backward      return out    def __repr__(self):     return f\"Value(data={self.data}, grad={self.grad})\" In\u00a0[52]: Copied! <pre># Recreating the same function\na = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b; c.label = 'c'\nd = Value(10, label='d')\nL = c * d; L.label = 'L'\n</pre> # Recreating the same function a = Value(5, label='a') b = Value(3, label='b') c = a + b; c.label = 'c' d = Value(10, label='d') L = c * d; L.label = 'L' In\u00a0[53]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[53]: <p>We will initialize the gradient of the loss to be 1.0 and then call backward function. We should get the same results which we manually calculated previously.</p> In\u00a0[54]: Copied! <pre>L.grad = 1.0\nL._backward()\nc._backward()\n</pre> L.grad = 1.0 L._backward() c._backward() In\u00a0[55]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[55]: <p>Exercise: Make sure that all operations and their partial derivatives can be calculated (e.g. division, power).</p> <p>We can now call the optimization process, as well as forward and backward passes to reduce loss. Training model with the help of backward pass, optimization, and forward pass is called backpropagation.</p> In\u00a0[56]: Copied! <pre># optimization\nlr = 0.01\na.data -= lr * a.grad\nb.data -= lr * b.grad\nd.data -= lr * d.grad\n\n# forward pass\nc = a + b\nL = c * d\n\n# backward pass\nL.grad = 1.0\nL._backward()\nc._backward()\n\nL.data # loss\n</pre> # optimization lr = 0.01 a.data -= lr * a.grad b.data -= lr * b.grad d.data -= lr * d.grad  # forward pass c = a + b L = c * d  # backward pass L.grad = 1.0 L._backward() c._backward()  L.data # loss Out[56]: <pre>77.376</pre> <p>We have now trained the model for a single <code>epoch</code>. Even though what we do is oversimplistic and not precise, the main intuition and concepts behind training a neural network is the same.</p> <p>We will train the model for multiple epochs until we reduce the loss down to zero.</p> In\u00a0[57]: Copied! <pre>while True:\n  # optimization\n  a.data -= lr * a.grad\n  b.data -= lr * b.grad\n  d.data -= lr * d.grad\n\n  # forward pass\n  c = a + b\n  L = c * d\n\n  # backward pass\n  L.grad = 1.0\n  L._backward()\n  c._backward()\n\n  if L.data &lt; 0:\n    break\n\n  print(f'Loss: {round(L.data,2)}')\n</pre> while True:   # optimization   a.data -= lr * a.grad   b.data -= lr * b.grad   d.data -= lr * d.grad    # forward pass   c = a + b   L = c * d    # backward pass   L.grad = 1.0   L._backward()   c._backward()    if L.data &lt; 0:     break    print(f'Loss: {round(L.data,2)}') <pre>Loss: 74.81\nLoss: 72.31\nLoss: 69.87\nLoss: 67.49\nLoss: 65.16\nLoss: 62.88\nLoss: 60.66\nLoss: 58.48\nLoss: 56.35\nLoss: 54.27\nLoss: 52.23\nLoss: 50.24\nLoss: 48.28\nLoss: 46.37\nLoss: 44.49\nLoss: 42.65\nLoss: 40.84\nLoss: 39.07\nLoss: 37.33\nLoss: 35.62\nLoss: 33.94\nLoss: 32.29\nLoss: 30.66\nLoss: 29.06\nLoss: 27.48\nLoss: 25.93\nLoss: 24.39\nLoss: 22.88\nLoss: 21.39\nLoss: 19.91\nLoss: 18.45\nLoss: 17.0\nLoss: 15.57\nLoss: 14.16\nLoss: 12.75\nLoss: 11.36\nLoss: 9.97\nLoss: 8.59\nLoss: 7.22\nLoss: 5.86\nLoss: 4.5\nLoss: 3.15\nLoss: 1.8\nLoss: 0.45\n</pre> <p>All we did manually is built in to PyTorch. We will do a forward and backward pass and check if the gradients are what we had previosuly calculated. As gradients are not always calculated, for optimization purposes <code>requires_grad</code> is set to False by default. We cannot also calculate gradient for leaf nodes.</p> In\u00a0[58]: Copied! <pre>import torch\n\na = torch.tensor(5.0);    a.requires_grad = True\nb = torch.tensor(3.0);    b.requires_grad = True\nc = a + b\nd = torch.tensor(10.0);   d.requires_grad = True\nL = c * d\n</pre> import torch  a = torch.tensor(5.0);    a.requires_grad = True b = torch.tensor(3.0);    b.requires_grad = True c = a + b d = torch.tensor(10.0);   d.requires_grad = True L = c * d In\u00a0[59]: Copied! <pre>L.backward()\n</pre> L.backward() In\u00a0[60]: Copied! <pre>a.grad, b.grad, d.grad\n</pre> a.grad, b.grad, d.grad Out[60]: <pre>(tensor(10.), tensor(10.), tensor(8.))</pre> <p>We got the expected result.</p>"},{"location":"notebooks/01_backprop/#01-from-derivatives-to-backpropagation","title":"01. From Derivatives to Backpropagation\u00b6","text":"<p>Important</p> <p>     The notebook is currently under revision.   </p> <p>Note</p> <p>The notebook is mainly based on Andrej Karpathy's lecture on Micrograd</p> <p>We will go from illustrating differentiation and finding derivatives in Python, all the way down till the implementation of the backpropagation algorithm.</p>"},{"location":"notebooks/01_backprop/#differentiation","title":"Differentiation\u00b6","text":""},{"location":"notebooks/01_backprop/#partial-derivatives","title":"Partial Derivatives\u00b6","text":""},{"location":"notebooks/01_backprop/#addition","title":"Addition\u00b6","text":""},{"location":"notebooks/01_backprop/#multiplication","title":"Multiplication\u00b6","text":""},{"location":"notebooks/01_backprop/#complex","title":"Complex\u00b6","text":""},{"location":"notebooks/01_backprop/#micrograd-and-computation-graph","title":"Micrograd and Computation Graph\u00b6","text":"<p>Based on Karpathy's https://github.com/karpathy/micrograd</p>"},{"location":"notebooks/01_backprop/#gradient","title":"Gradient\u00b6","text":""},{"location":"notebooks/01_backprop/#chain-rule","title":"Chain Rule\u00b6","text":""},{"location":"notebooks/01_backprop/#optimization-with-gradient-descent","title":"Optimization with Gradient Descent\u00b6","text":""},{"location":"notebooks/01_backprop/#forward-pass","title":"Forward Pass\u00b6","text":""},{"location":"notebooks/01_backprop/#backward-pass","title":"Backward Pass\u00b6","text":""},{"location":"notebooks/01_backprop/#training-model-with-backpropagation","title":"Training Model with Backpropagation\u00b6","text":""},{"location":"notebooks/01_backprop/#pytorch-implementation","title":"PyTorch Implementation\u00b6","text":""}]}