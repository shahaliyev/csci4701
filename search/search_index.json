{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to CSCI 4701!","text":"<p>This is the website of the CSCI 4701: Deep Learning course taught at ADA University. </p> <p>Deep Learning focuses on artificial neural networks and how they are trained and optimized. This course covers core concepts starting with backpropagation, including regularization and optimization techniques. Students will gain practical skills using the PyTorch framework and learn neural architectures used in both Computer Vision (CV) and Natural Language Processing (NLP), including Convolutional Neural Networks (CNNs) and Transformer-based models. The course introduces generative modeling with Variational Autoencoders (VAEs) and briefly covers current developments in deep learning, such as Latent Diffusion Models (LDMs) and Large Language Models (LLMs). </p> <p>You can navigate through the course starting from the introductory overview or directly check the practical notebooks via the navigation bar.</p>"},{"location":"course/spring-2026/syllabus/","title":"Syllabus","text":"<p>Important</p> <p>     The content of this syllabus is subject to change. Please consistently check the course page on Blackboard and the ADA University Academic Calendar for modifications. The last day of the add/drop period, holidays, and other academic deadlines are noted in the calendar.   </p> <p>Info</p> <p>    Square brackets in the Assessment / Notes column indicate the range of classes whose material is covered by the assessment. For example, Quiz 1 [1\u20133] means that the quiz assesses material covered in classes 1 through 3.   </p> Week Topic Learning Outcomes Assessment / Notes 1 Deep Learning (DL) Overview / Course Structure Describe the scope of DL and the course syllabus. Fulfill technological requirements. 2 Mathematics of DL: Linear Algebra / Calculus Work with vectors, matrices, and tensors; apply norms and inner products. Compute partial derivatives and apply the chain rule. Optional: intuition behind eigenvectors and SVD. 3 Gradient Descent / Backpropagation I Compute gradients on computational graphs. Perform forward and backward passes. Understand gradient descent updates and automatic differentiation (PyTorch autograd, micrograd). 4 Gradient Descent / Backpropagation II Implement full backpropagation. Feb 3: Quiz 1 [1\u20133]Last day to submit team member details 5 Activation Functions / Neuron Implement activation functions and understand non-linearity. Backpropagate over an N-dimensional neuron. 6 Multilayer Perceptron (MLP) Construct an MLP from stacked neurons. Train a simple MLP classifier on a small dataset. Feb 10: Project proposal deadline 7 Images as Tensors / MLP on MNIST / Batching &amp; Cross-Entropy Understand image representations, tensor shapes, and batching. Use torchvision datasets and dataloaders. Train an MLP on MNIST with SGD + cross-entropy. Feb 12: Quiz 2 [5\u20136] 8 Convolutional Neural Networks (CNN) Define and implement 2D cross-correlation (convolution) and pooling with kernels, including padding and stride. Train a LeNet-style CNN on MNIST. Compare MLP with CNN. 9 Mathematics of DL: Probability Theory Describe random variables; distinguish discrete and continuous distributions; work with PMF/PDF. Compute expectation, variance, and covariance. Use conditional probability, independence, and Bayes\u2019 rule. Recognize common distributions. Feb 19: Quiz 3 [7\u20138] 10 Regularization Apply weight decay and dropout. Handle exploding and vanishing gradients. Use Xavier and He initialization. Distinguish local minima from saddle points in training dynamics. 11 Optimization Adjust learning rate and apply schedules. Use SGD with momentum. Apply RMSProp and Adam. Compare optimizers based on convergence behavior and practical performance. 12 Regularization / Optimization Train a regularized CNN on CIFAR-10 using optimizers. Apply hyperparameter tuning. Mar 3: Quiz 4 [10\u201311] 13 Paper: AlexNet Discuss AlexNet, its key ideas, what is outdated, and the paper structure. 14 Bigram Model / Negative Log-Likelihood / Softmax Build a character-level bigram model and sample from it. Distinguish probability vs likelihood. Compute average negative log-likelihood as a loss. Explain the purpose of softmax. 15 Neural Network N-gram Model / Mini-Batch Training Construct a neural N-gram model. Train the model with mini-batch updates. Mar 12: Quiz 5 [14]Project milestone 1 deadline 16 Midterm Exam \u2014 Tuesday, Mar 17: Midterm Exam [1\u201315] 17 Midterm Exam Review Half-semester overview. \u2014 Holidays \u2014 Mar 20\u201330 18 Batch Normalization / Layer Normalization Explain why normalization helps training deep networks. Implement batch normalization and understand training vs evaluation behavior. Understand batch-size effects and when to prefer layer normalization. 19 Residual Blocks / Residual Network for NLP Understand residual (skip) connections. Add a residual block to a feed-forward N-gram model with correct dimensions. Connect residuals to vanishing gradients and regularization. 20 Sequence Modeling: Autoregressive Models and RNN/LSTM Explain autoregressive modeling beyond fixed context windows. Describe how RNNs maintain state. Identify limitations of RNN/LSTM/GRU. Apr 7: Quiz 6 [18\u201319] 21 Attention Mechanism Understand attention as weighted information selection. Derive queries, keys, and values at the tensor level. Implement attention with matrix operations and verify shapes and normalization. 22 Transformer Architecture / Self-Attention Explain self-attention and Transformer blocks. Explain how Transformers scale. Apr 14: Quiz 7 [20\u201321] 23 Transformer Blocks Assemble a Transformer block from self-attention and feed-forward sublayers. Trace signal flow. Analyze training stability and sensitivity to initialization and learning rate. 24 Paper Reading: Transformer, Vision Transformer, Swin Transformer Extract core architectural ideas and compare attention for sequences vs images. Discuss scalability and efficiency constraints. Apr 21: Quiz 8 [22\u201323] 25 Mathematics of DL: Information Theory and Probabilistic Modeling Compute entropy, cross-entropy, and KL divergence. Derive cross-entropy loss from maximum likelihood. Interpret common losses as probabilistic objectives. 26 Variational Autoencoders I Introduce latent-variable generative models. Explain latent representations and probabilistic encoders/decoders. Explain approximate inference and why variational methods are needed. Apr 28: Project milestone 2 deadline 27 Variational Autoencoders II Understand the VAE objective (ELBO). Implement a VAE. Interpret reconstruction and regularization terms and their trade-off. 28 Diffusion Models / Score Matching Formulate diffusion via forward noising and learned reverse denoising. Interpret the training objective as denoising score matching. Explain sampling as iterative probabilistic inference. May 5: Quiz 9 [25\u201327] 29 Foundation Models and Modern Trends Explain large-scale pretraining and transfer learning. Examine GPT, BERT, CLIP, and latent diffusion models (LDMs). Discuss scaling behavior and limitations. \u2014 Final Exam \u2014 Tuesday, May 12: Final Exam [1\u201329]"},{"location":"introduction/01_overview/","title":"Deep Learning Overview","text":"18 Jan 2026 \u00b7   9 min <p>Artificial Intelligence (AI) is the broad field concerned with building systems that perform tasks requiring intelligence. Machine Learning (ML) is a subfield of AI that enables systems to learn patterns and make decisions from data rather than explicit rules. Deep Learning (DL) is a subfield of ML that uses multi-layer neural networks to learn complex representations from large datasets.</p> <p>Info</p> <p>     The following sources were used in preparing this text:   </p> <ul> <li>       Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016).       Deep Learning. MIT Press.        </li> <li>       Schmidhuber, J. (2015).       Deep Learning in Neural Networks: An Overview.       Neural Networks.     </li> <li>       Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J.      Dive into Deep Learning. d2l.ai."},{"location":"introduction/01_overview/#ai-ml-dl","title":"AI / ML / DL","text":"<p>AI initially focused on what is often called the knowledge-based approach, where intelligence was treated as something that could be explicitly written down. Researchers attempted to encode reasoning as rules, symbols, and logical statements. If a human expert knew how to solve a problem, the reasoning steps would be formalized and executed by a machine.</p> <p>This approach failed when faced with the ambiguity and variability of the real world. Tasks that humans perform effortlessly, such as recognizing faces or understanding speech, are precisely the tasks that are hardest to describe step by step. Human expertise in these domains is largely implicit rather than explicit. Rule-based systems therefore became brittle, difficult to scale, and expensive to maintain. Small changes in the environment often required rewriting large portions of the system, making progress slow and fragile.</p>      Deep Learning and AI ~ Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.     <p>ML offered a different perspective. Instead of programming intelligence directly, machines were allowed to learn patterns from data. Classical ML algorithms such as linear models, logistic regression, na\u00efve Bayes, and decision trees achieved real success in applications like medical decision support, spam filtering, and credit scoring. However, these methods relied heavily on hand-crafted features. Human designers had to decide in advance which properties of the data were relevant, and performance depended more on feature design than on the learning algorithm itself.</p> <p>This reliance on features became a serious limitation as data grew more complex. Images, audio signals, and language live in very high-dimensional spaces. In such spaces, intuition breaks down, a phenomenon often referred to as the curse of dimensionality. As dimensionality increases, data becomes sparse, distances lose their meaning, and small modeling assumptions can cause large failures. Feature engineering becomes brittle and does not scale to the richness of real-world data.</p> <p>The natural response to this problem was representation learning. Instead of manually defining features, the model learns useful representations directly from raw data. Early methods such as Principal Component Analysis (PCA), kernel methods, sparse coding, and shallow neural networks pursued this idea. They demonstrated that learning intermediate representations could significantly improve performance and reduce reliance on handcrafted features. However, these approaches were typically shallow, consisting of only one or two layers of transformation. As a result, they struggled to capture the hierarchical structure present in real-world data.</p> <p>Many perceptual tasks are inherently compositional. Images are composed of edges, edges form textures and parts, parts form objects, and objects form scenes. Speech and language exhibit similar hierarchies. Shallow models can learn simple transformations, but they cannot efficiently represent such multi-level abstractions. Attempting to do so requires an exponential number of features or parameters, making learning unstable and data-inefficient. In practice, representation learning without depth hit a ceiling: it reduced feature engineering, but it could not scale to the complexity of vision, speech, and language.</p> <p>DL extends representation learning by stacking many layers of nonlinear transformations. Each layer learns to represent the data at a higher level of abstraction, allowing complex structures to be built incrementally. </p> <p>At a fundamental level, both classical ML and DL do the same thing: they learn a function from data. The difference is not in what is learned, but in how much of the function is learned automatically. In all cases, learning amounts to selecting parameters so that a function best approximates the desired input\u2013output relationship under a given objective.</p> <p>Interestingly, DL did not introduce fundamentally new mathematical ideas. Many concepts, such as multi-layer neural networks, backpropagation, gradient-based optimization, and even convolutional architectures were known decades earlier. </p>"},{"location":"introduction/01_overview/#biological-and-artificial-neurons","title":"Biological and Artificial Neurons","text":"<p>DL is not an attempt to simulate the brain. Artificial neural networks are inspired by biological neurons, but the resemblance is conceptual rather than literal. </p>        Structure of a typical neuron with Schwann cells in the peripheral nervous system ~ \"Anatomy and Physiology\" by the US National Cancer Institute's Surveillance | CC BY-SA 3.0 | Wikimedia Commons <p>A biological neuron is a living cell designed for communication in a noisy, energy-constrained environment. It receives signals through dendrites, integrates them in the soma (cell body), and, if a threshold is reached, sends an electrical pulse along the axon to other neurons through synapses. Learning occurs locally by strengthening or weakening synaptic connections through repeated interaction with the environment.</p>        Artificial Neuron ~ Funcs, Own work | CC0 | Wikimedia Commons <p>An artificial neuron is a mathematical function that combines numerical inputs and produces a numerical output. Much like how airplanes were inspired by birds but rely on entirely different aerodynamic mechanisms, the success of DL does not come from biological realism. Biological systems served primarily as inspiration.</p>"},{"location":"introduction/01_overview/#evolution-of-deep-learning","title":"Evolution of Deep Learning","text":"<p>Learning from data predates computers. The mathematical backbone of modern deep learning is the chain rule, formalized by Gottfried Wilhelm Leibniz and later exploited by backpropagation algorithms. Carl Friedrich Gauss and Adrien-Marie Legendre used linear regression in the early nineteenth century, a method mathematically equivalent to a shallow neural network. In the mid-twentieth century, researchers such as Warren McCulloch and Walter Pitts, Frank Rosenblatt, and Bernard Widrow explored learning machines inspired by biological neurons. These early systems were limited\u2014often linear or single-layer\u2014and constrained by the theory and hardware of their time.</p> <p>Multi-layer learning systems already existed by the 1960s and 1970s. Alexey Ivakhnenko and Valentin Lapa trained models with adaptive hidden layers, while Kunihiko Fukushima introduced the Neocognitron, a hierarchical, convolution-like architecture that anticipated modern convolutional networks.</p> <p>But why did DL become popular only after the 2010s? The obstacle was never the lack of a correct algorithm. It was the lack of data and the cost of computation. DL worked because three forces aligned. Data became abundant because digital life produces it automatically. Computation became affordable because parallel hardware matured. And (less critically) software matured enough to make experimentation fast and scalable.</p>"},{"location":"introduction/01_overview/#data","title":"Data","text":"<p>The modern era began when data stopped being rare. This shift was driven by broader technological changes. Digital sensors replaced analog ones, smartphones placed cameras and microphones in billions of pockets, and the internet enabled continuous sharing of images, text, audio, and video. Companies began logging user interactions by default, storage became cheap, and bandwidth increased dramatically. Data was no longer collected deliberately, it was generated automatically as a byproduct of everyday life.</p> <p>Before large-scale datasets became feasible, progress relied on small, carefully curated benchmarks. The famous MNIST dataset was collected by the National Institute of Standards and Technology (NIST), and later was modified (hence the M before NIST) for simpler usage of ML algorithms<sup>2</sup>. MNIST is a simple dataset of handwritten digits that allowed researchers to isolate questions about optimization, architectures, and learning dynamics without the confounding effects of scale and noise. </p>      MNIST inputs ~ Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.     <p>A symbolic moment was the creation of ImageNet (Deng et al.). ImageNet contained roughly 14 million labeled images, with about 1.2 million training images across 1,000 categories used in its main benchmark. This scale exposed the limitations of hand-crafted features. Models that performed well on small datasets failed to generalize, while systems capable of learning representations directly from data improved reliably.</p> <p>In 2012, AlexNet (Krizhevsky et al.) won the ImageNet competition by a large margin. The model was unusually large and computationally demanding, and training it required GPUs rather than CPUs. This detail is crucial. DL did not succeed merely because sufficient data became available, it succeeded because the models finally fit within the limits of available hardware.<sup>1</sup></p>     \"Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5).\" ~ Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems (NeurIPS)      <p>As of 2016, a rough rule of thumb is that a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category and will match or exceed human performance when trained with a dataset containing at least 10 million labeled examples.  </p> <p>Deep Learning  (Chapter I) ~ Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). MIT Press.</p>"},{"location":"introduction/01_overview/#hardware","title":"Hardware","text":"<p>Training neural networks is dominated by large-scale numerical operations repeated many times. CPUs are optimized for general-purpose tasks and complex control flow, but they are inefficient for massive parallel arithmetic. GPUs, originally designed for rendering images, apply the same operation to many data points simultaneously. This made them a natural fit for neural network training.</p> <p>NVIDIA became central to DL because it invested early in programmable GPUs and the software needed to exploit them. Although originally developed for video games, GPUs are fundamentally optimized for massively parallel linear algebra, especially matrix and tensor operations. The introduction of CUDA exposed this capability to researchers, making large-scale matrix multiplications\u2014the core computational workload of neural networks\u2014efficient and accessible. As a result, models that once took weeks to train on CPUs could be trained in days or hours. Later accelerators such as Tensor Processing Unit (TPU) followed the same principle: DL scales when hardware is designed around dense linear algebra, high memory bandwidth, and parallel computation.</p>"},{"location":"introduction/01_overview/#software","title":"Software","text":"<p>The relevant software emerged in parallel with hardware. Python became the dominant language for ML because it allowed researchers to write clear, concise code while delegating computationally intensive operations to highly optimized numerical libraries implemented in C, C++, and CUDA. This separation between high-level model logic and low-level performance-critical kernels proved decisive. Researchers could focus on ideas rather than infrastructure, iterating rapidly while still benefiting from efficient linear algebra routines running on GPUs.</p> <p>Modern DL frameworks such as PyTorch and TensorFlow made it possible to automate differentiation, memory management, and efficient parallel execution. As a result, experiments that once required weeks of careful implementation could be expressed in hundreds of lines of code and tested within days.</p> <p>PyTorch is primarily a tool for research and experimentation. It is designed to feel like ordinary Python code, which makes models easy to write, modify, and debug. Tools such as PyTorch Lightning build on this flexibility by handling routine tasks like training loops and logging, allowing users to keep their focus on the model itself.</p> <p>TensorFlow, on the other hand, is more strongly oriented toward engineering and deployment. It was built to support large systems that need to run reliably across different machines and environments. With the addition of Keras, TensorFlow offers a high-level interface that makes it easy to define standard models and training pipelines in a consistent way. This structure is well suited to production settings, where models must be maintained, scaled, and deployed efficiently over long periods of time.</p>"},{"location":"introduction/01_overview/#transformers-and-beyond","title":"Transformers and Beyond","text":"<p>Computer Vision (CV) and Natural Language Processing (NLP) are the two main perception-oriented branches of modern DL. Both aim to convert raw, high-dimensional signals into structured representations that machines can reason over, but they operate on different data modalities and evolved under different constraints.</p> <p>CV focuses on visual data such as images and videos. Early progress was driven by convolutional neural networks (CNN). NLP deals with sequential, symbolic data such as text and speech. While early neural NLP relied on recurrent models (RNN), a major conceptual shift occurred with the introduction of the Transformer architecture (Vaswani et al., 2017), which replaced sequential recurrence with attention-based information routing. This change enabled massive parallelism, better long-range dependency modeling, and effective scaling with data and compute. The same architecture was later adapted to images via Vision Transformers (Dosovitskiy et al., 2020), revealing that vision and language could share a common computational backbone despite their different input structures.</p> <p>DL also extended beyond perception into decision-making. The combination of deep learning and reinforcement learning became widely visible through AlphaGo and later AlphaZero (Silver et al., 2016; 2018), which learned complex games through self-play without human examples.</p> <p>Building on the Transformer architecture, Large Language Models (LLM) such as Generative Pre-trained Transformer (GPT) marked a shift from task-specific NLP systems to general-purpose foundation models. By training a model on massive text corpora, GPT-style models learn broad linguistic, semantic, and world-level regularities that can be reused across tasks. Their success demonstrated that scale\u2014data, parameters, and compute\u2014can replace handcrafted linguistic structure, and that a single architecture can support a wide range of capabilities, including translation, summarization, reasoning, and code generation, without explicit task-specific design.</p> <ol> <li> <p>Even then, Alex Krizhevsky had to distribute training across two NVIDIA GeForce GTX 580 GPUs, each with 3 GB of memory (best at the moment), because the network did not fit on a single GPU.\u00a0\u21a9</p> </li> <li> <p>Geoffrey Hinton called this dataset \"the drosophila of ML\", a fruit fly extensively used in genetic research labs.\u00a0\u21a9</p> </li> </ol>"},{"location":"introduction/02_materials/","title":"Study Materials","text":""},{"location":"mathematics/","title":"Mathematics of Deep Learning","text":"18 Jan 2026 \u00b7   6 min <p>Deep Learning (DL) relies on mathematics, but not on all of mathematics equally. Many topics that are common in standard mathematics curricula play little or no role in the practice of DL. The purpose of this section is to explain which parts of mathematics matter for DL and what role they play. </p> <p>In preparing this material, two widely used resources were consulted and found to be highly valuable: Deep Learning (Goodfellow et al., 2016) and Dive into Deep Learning (Zhang et al., online).</p> <p>Deep Learning presents the mathematics in a concise and rigorous form. Its strength lies in precision and breadth, but this compact style can make it difficult for readers to develop intuition, especially when encountering these ideas for the first time. Key concepts are often introduced quickly, with limited space for informal explanation or gradual buildup.</p> <p>Dive into Deep Learning takes a different approach, tightly integrating mathematical ideas with executable code. This makes experimentation accessible and practical, but it can also blur the boundary between mathematical concepts and their implementation. The mathematical knowledge is not always presented in a clearly systematized form.</p> <p>The goal of the present material is to combine the strengths of both approaches while addressing their limitations. Mathematical ideas are introduced carefully and explained in simple language, with implementation details separated whenever possible. Each concept is included because it plays a clear role in DL, not because it belongs to a traditional mathematics curriculum. The aim is to provide a conceptual foundation that supports both practical experimentation and deeper theoretical study.</p> <p>More detailed overviews can be found in the separate pages dedicated to Calculus, Linear Algebra, Probability Theory, and Information Theory. Below is a summary of the main mathematical concepts required for DL.</p>"},{"location":"mathematics/#calculus","title":"Calculus","text":"<p>Within calculus, the central idea for DL is the rate of change: if we change some model parameters slightly, how does the output change? In DL, the output of interest is usually a single number called the loss, which measures how bad the model's prediction is. A derivative tells us how much the loss changes when we slightly change one parameter. This makes derivatives a practical tool for learning, since they indicate the direction in which parameters should be adjusted to reduce the loss.</p> <p>Partial derivatives are essential because a model typically has many parameters. A partial derivative measures how sensitive the loss is to one parameter while all other parameters are kept fixed. The gradient simply collects all these sensitivities into a single array. The chain rule explains how sensitivities propagate through a model that is built from many smaller operations, and backpropagation is the algorithm that applies the chain rule efficiently to compute gradients.</p> <p>All of this relies on an important assumption: the loss changes smoothly with respect to the parameters. This means that small changes in parameters lead to small, predictable changes in the loss, making derivatives reliable guides for optimization.</p> <p>Integration and the Fundamental Theorem of Calculus appear more quietly in the background. They underlie concepts such as expectations and averages, which are central to training objectives. In DL, integrals are rarely computed by hand; understanding what integration represents is more important than learning how to calculate it.</p>"},{"location":"mathematics/#linear-algebra","title":"Linear Algebra","text":"<p>Linear algebra is the language in which DL models are written. Data points, parameters, and gradients are represented as vectors. Linear layers are represented as matrix\u2013vector or matrix\u2013matrix multiplications.</p> <p>What matters most is intuition. Vectors should be understood as ordered collections of numbers. Matrices should be understood as operations that transform vectors by scaling, rotating, or mixing their components. These ideas explain how information flows through a network and why many computations can be done efficiently in parallel.</p> <p>Linear algebra also explains why gradients have the same shape as parameters, why batching works, and why modern hardware such as GPUs is effective for DL. Model parameters are stored as vectors and matrices, and gradients are derivatives with respect to those parameters, so they naturally have the same structure. This one-to-one correspondence makes parameter updates straightforward: each parameter is adjusted using its matching gradient entry.</p> <p>Modern DL frameworks are built almost entirely on linear algebra operations. Matrix multiplication, matrix addition, and vectorized nonlinear functions form the core of both the forward and backward passes. Most performance optimizations are handled automatically by numerical libraries, allowing users to express models at a high level while relying on efficient low-level implementations. </p> <p>While matrix factorizations such as Singular Value Decomposition (SVD) are rarely invoked explicitly during training, they are used internally in optimized linear solvers, low-rank approximations, spectral normalization, and in estimating matrix norms or conditioning. Through these mechanisms, factorization-based ideas influence numerical stability, efficiency, and scaling behavior in DL systems without appearing directly in model code.</p> <p>Batching works because linear algebra operations naturally extend from single vectors to collections of vectors stacked into matrices or higher-dimensional tensors. Processing many data points at once is not a special trick, but a direct consequence of writing models in matrix form. GPUs are effective for DL for the same reason: linear algebra operations consist of many simple arithmetic operations that can be carried out in parallel. As a result, DL benefits directly from both the mathematical structure of linear algebra and the hardware designed to execute it efficiently.</p>"},{"location":"mathematics/#probability-theory","title":"Probability Theory","text":"<p>DL models are trained on data that is noisy, incomplete, and often ambiguous. Probability provides the language for describing this uncertainty and for turning learning into a well-defined mathematical problem. In DL, models are often best understood not as systems that produce a single \"correct\" output, but as systems that assign probabilities to possible outcomes.</p> <p>From this perspective, a model defines a probability distribution, either explicitly or implicitly. Training the model means adjusting its parameters so that the observed data becomes more probable under this distribution. Many commonly used loss functions arise directly from this idea. Minimizing such losses is equivalent to maximizing likelihood.</p> <p>Expectations play a central role because learning is not based on a single data point, but on averages over data drawn from an underlying distribution. Training objectives are typically expectations of a loss over the data distribution, which in practice are approximated using finite datasets and minibatches.</p> <p>DL does not require advanced probability theory, but it does require a clear understanding of what probabilistic models represent, how likelihood and expectation relate to loss functions, and why uncertainty is an essential part of learning from real data.</p>"},{"location":"mathematics/#information-theory","title":"Information Theory","text":"<p>Information theory enters DL when we want to measure how different two probability distributions are. Many DL models define a distribution over possible outputs rather than producing a single fixed prediction. Information-theoretic quantities provide a principled way to compare these predicted distributions to the true data distribution.</p> <p>Concepts such as entropy and cross-entropy arise naturally in this setting. Entropy measures uncertainty, while cross-entropy measures how well one distribution represents another. Minimizing cross-entropy encourages the model to assign high probability to the observed data.</p> <p>A closely related quantity is the Kullback\u2013Leibler (KL) divergence, which measures how much information is lost when one distribution is used to approximate another. Many common training objectives can be interpreted as minimizing a KL divergence, even when this connection is not stated explicitly.</p>"},{"location":"mathematics/#additional-mathematics","title":"Additional Mathematics","text":"<p>In addition to the core areas discussed above, several mathematical perspectives play an important role in DL. While they may not always appear as standalone topics or require extensive formal development, they shape how models are designed, trained, and evaluated. These ideas recur across many DL settings and deserve explicit attention, even when they are introduced briefly.</p> <p>Statistics enters DL through the fact that models are trained on finite samples rather than full data-generating processes. Concepts such as generalization, overfitting, and the bias\u2013variance tradeoff describe the structural limits of what can be learned from data and how model complexity interacts with sample size and noise. These considerations shape how results should be interpreted, how sensitive conclusions are to data variation, and how confidently performance can be expected to transfer beyond the observed sample.</p> <p>Optimization theory addresses a small set of practical questions that arise once a loss function is defined. Given a highly non-convex objective with millions of parameters, how can it be minimized efficiently, and why do simple gradient-based methods work at all? How do learning rates, momentum, adaptive updates, and noise from minibatching affect training behavior? Rather than providing exact convergence proofs, optimization theory offers guidance on training stability, speed, and failure modes.</p> <p>Geometry treats representations, parameters, and activations as points in high-dimensional spaces. Distances and angles define similarity, with measures such as cosine similarity capturing directional alignment between representations. Optimization itself is a geometric process, moving parameters across a loss surface whose local curvature influences learning speed and stability. Geometric intuition about distances, neighborhoods, and curvature helps explain why certain architectures, losses, and similarity measures are effective in practice.</p> <p>Graph theory becomes relevant whenever data is structured by relations rather than simple vectors. In graph neural networks (GNNs), data is represented as nodes and edges, and learning depends directly on graph connectivity. Related ideas also appear more broadly in message passing, relational reasoning, and attention mechanisms applied to structured inputs.</p> <p>Numerical computation and stability constrain how DL models are implemented. Because training relies on finite-precision arithmetic, issues such as overflow, underflow, and loss of precision directly influence model behavior. Many standard techniques in DL\u2014such as normalization layers, carefully designed loss functions, and specific activation choices\u2014exist primarily to ensure stable and reliable computation.</p> <p>Together, these perspectives complement the core mathematical foundations and connect them to practical modeling, training, and evaluation. They do not replace the core framework, but they shape how it is applied and understood in real-world deep learning systems.</p>"},{"location":"mathematics/01_calculus/","title":"Calculus","text":"18 Jan 2026 \u00b7   12 min <p>Calculus studies two closely related ideas: accumulation (integration) and change (differentiation). In DL, learning is defined by accumulating error across data, usually as an average loss. Training then proceeds by making small changes to model parameters in order to reduce this accumulated error. Calculus provides the language and structure for both. This section builds calculus concepts from fundamentals, with the goal of understanding how they support learning, optimization, and model behavior in deep learning.</p>"},{"location":"mathematics/01_calculus/#functions","title":"Functions","text":"<p>A function maps inputs to outputs. We write this as \\(y = f(x)\\). If the input \\(x\\) changes, the output \\(y\\) usually changes as well. Some functions change slowly, some change quickly, and some change differently depending on at which point of the function you are. Calculus begins by asking how these changes are related. </p> <p>Note</p> <p>For example, changing the brightness of an image slightly may barely affect a model's output in one case, but cause a large change in another.</p>"},{"location":"mathematics/01_calculus/#integration","title":"Integration","text":"<p>An integral such as \\(\\int_a^b f(x)\\,dx\\) represents the total accumulation of the values of \\(f(x)\\) as \\(x\\) moves from \\(a\\) to \\(b\\). It is simply the continuous analogue of a summation. You can think of an integral as summing many small contributions of \\(f(x)\\) over an interval. The exact techniques for computing integrals are less important in DL than the idea they represent.</p> <p>Conceptually, integration means breaking an interval into many small pieces. For each piece, we take the value of \\(f(x)\\) and multiply it by the width of the piece. Adding all these pieces together gives an approximation of the total accumulation. As the pieces become smaller and more numerous, this approximation approaches the integral. Mathematically, we represent this very small width as \\(dx\\).</p>      Riemann Integration and Darboux Lower Sums. By IkamusumeFan - Own work\u00a0This plot was created with Matplotlib., CC BY-SA 3.0, Link <p>Note</p> <p>In DL, training is never based on a single example. A model is evaluated by how it performs across many examples, so errors must be combined into one overall value. In practice, we only have access to a limited number of training examples. A common case in ML and DL is mean squared error (MSE), where for each training sample we compute a prediction error, square it (so negative and positive errors do not cancel, and larger mistakes are penalized), and then average these squared errors over the dataset. Conceptually, however, this dataset-level average is not the final goal. </p> <p>The dataset is usually treated as a small collection of examples drawn from a much larger source of data. Ideally, we would like to measure the model\u2019s average error over all possible data points it might encounter, not just the ones we happened to collect. The finite average used in training should therefore be understood as a practical approximation of a more general, ideal accumulated continuous quantity. For the values \\(g(x_1), g(x_2), \\dots, g(x_N)\\), we can write</p> \\[ \\mathbb{E}_{x \\sim p}[g(x)] \\approx \\frac{1}{N}\\sum_{i=1}^N g(x_i). \\] <p>Here, the right-hand side is what we compute from data, and the left-hand side represents the ideal quantity we are trying to approximate. This ideal accumulated quantity is written precisely using the concept of an expectation. When data is described by a probability distribution \\(p(x)\\), the average value of a quantity \\(g(x)\\) is written as</p> \\[ \\mathbb{E}_{x \\sim p}[g(x)] = \\int g(x)\\,p(x)\\,dx. \\] <p>You do not need a deep understanding of probability to read this expression. Conceptually, it means: consider all possible values of \\(x\\), weight each value by how common it is, and add everything up. Many DL loss functions can be understood this way, as average losses over all possible data. For discrete datasets, this expectation reduces to a finite sum, while for continuous variables it is written as an integral. The integral itself is not special\u2014it is simply the mathematical way to express an average over all possible inputs when the space of inputs is continuous.</p> <p>Tip</p> <p>If the idea of expectations or probability distributions feels unfamiliar, you may want to read the page dedicated to the Probability Theory alongside this section.</p>"},{"location":"mathematics/01_calculus/#differentiation","title":"Differentiation","text":"<p>Differentiation answers the question: if we change the input slightly, how much does the output change?  Suppose we start at \\(x\\) and then move a small step \\(h\\) to \\(x+h\\). The corresponding change in the output is \\(f(x+h) - f(x)\\). By itself, this number depends on how large \\(h\\) is. To describe change in a way that does not depend on the step size, we compare the output change to the input change by forming the ratio</p> \\[ \\frac{f(x+h) - f(x)}{h}. \\] <p>This ratio is called a difference quotient. It describes the average rate of change of the function over the small interval from \\(x\\) to \\(x+h\\). The derivative is defined as the limit of this ratio as the step size approaches zero:</p> \\[ f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}. \\] <p>This definition captures the idea of an \"instantaneous\" rate of change. Intuitively, the derivative tells us the slope of the function at the point \\(x\\): if \\(f'(x)\\) is large, a tiny change in \\(x\\) causes a large change in \\(f(x)\\), if \\(f'(x)\\) is close to zero, the function is locally flat. </p> <p>Note</p> <p>In DL, if increasing a model weight slightly increases the loss, then the derivative of the loss with respect to that weight is positive. That means decreasing the weight slightly should reduce the loss (at least locally).</p> <p>The derivative is not only a number; it also provides a practical approximation of how a function behaves near a given point. The key idea is that, over very small distances, a smooth function behaves almost like a straight line. If we start at a point \\(x\\) and move a small step \\(h\\), the derivative \\(f'(x)\\) tells us how steep the function is at \\(x\\). Using this slope, we can estimate how much the output will change. This leads to the approximation</p> \\[ f(x+h) \\approx f(x) + f'(x)\\,h. \\] <p>This formula should be read as a prediction: \"start from the current value \\(f(x)\\), then add the change suggested by the slope times the step size.\" The approximation becomes more accurate as the step \\(h\\) becomes smaller. Geometrically, this means that near the point \\(x\\), the function can be replaced by its tangent line. The tangent line touches the function at \\(x\\) and has the same slope there. Over a very small region, the curve and the tangent line are almost indistinguishable, which is why the linear approximation works.</p>      By Chorch - Own Work, Public Domain, Link <p>Note</p> <p>In DL, training works because, at each step, we treat the loss as locally almost linear in the parameters. The gradient (see below) gives the slope of this local linear approximation. By making small parameter updates in the direction opposite to the gradient, we can reliably reduce the loss step by step, even when the overall loss function is highly complex.</p>"},{"location":"mathematics/01_calculus/#partial-derivatives","title":"Partial derivatives","text":"<p>Deep learning models depend on many parameters at once. If the loss is written as</p> \\[ L = f(\\theta_1, \\theta_2, \\dots, \\theta_n), \\] <p>then each parameter has its own partial derivative \\(\\frac{\\partial L}{\\partial \\theta_i}.\\) A partial derivative measures how the loss changes when one parameter is varied while all others are held fixed.</p> <p>Note</p> <p>For example, changing a single weight in a neural network affects the loss while all other weights remain unchanged.</p>"},{"location":"mathematics/01_calculus/#gradients","title":"Gradients","text":"<p>The gradient collects all partial derivatives into a single vector:</p> \\[ \\nabla_{\\theta} L = \\left[ \\frac{\\partial L}{\\partial \\theta_1}, \\frac{\\partial L}{\\partial \\theta_2}, \\dots, \\frac{\\partial L}{\\partial \\theta_n} \\right]. \\] <p>The gradient points in the direction where the loss increases most rapidly. Moving in the opposite direction locally reduces the loss. Each component of the gradient corresponds to one parameter and tells us how that parameter influences the loss. Training typically consists of repeated updates of the form</p> \\[ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L, \\] <p>where \\(\\eta\\) is the learning rate. Each update makes a small change. Over many updates, these small changes accumulate and reduce the overall loss.</p> <p>Tip</p> <p>The learning rate update through backward pass is discussed in the notebook dedicated to backpropagation.</p>"},{"location":"mathematics/01_calculus/#jacobian","title":"Jacobian","text":"<p>The Jacobian is the general first-order derivative for functions with vector inputs and vector outputs. If a function maps an \\(n\\)-dimensional input vector to an \\(m\\)-dimensional output vector, \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m,\\) its Jacobian is an \\(m \\times n\\) matrix defined as</p> \\[ J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\dots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\dots &amp; \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} &amp; \\frac{\\partial f_m}{\\partial x_2} &amp; \\dots &amp; \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}. \\] <p>Each entry measures how one component of the output changes when one component of the input is varied. The Jacobian therefore captures all first-order sensitivities between inputs and outputs.</p> <p>Note</p> <p>In DL, layers often map vectors to vectors. Although Jacobians are rarely written explicitly, they are the objects through which changes propagate from one layer to the next. When the output is a scalar loss, the Jacobian reduces to a row vector. Conceptually, the gradient introduced earlier is simply the Jacobian of a scalar-valued function. Backpropagation avoids forming full Jacobian matrices explicitly. Instead, it efficiently computes vector\u2013Jacobian products, which is why gradients can be computed for models with millions of parameters at reasonable cost.</p> <p>Tip</p> <p>Jacobians are best understood through linear algebra. If matrices and vector transformations feel unfamiliar, you may want to read the Linear Algebra page alongside this section.</p>"},{"location":"mathematics/01_calculus/#chain-rule","title":"Chain Rule","text":"<p>DL models are built by composing functions. Instead of a single operation, a model applies many transformations one after another. Each transformation takes the output of the previous one as its input. To understand how changes propagate through such a model, consider a simple composition:</p> \\[ y = g(x), \\qquad L = f(y). \\] <p>Here, \\(x\\) influences \\(L\\) indirectly, through the intermediate variable \\(y\\). If we change \\(x\\) slightly, \\(y\\) will change, and that change in \\(y\\) will in turn affect \\(L\\). The chain rule formalizes this dependency.</p> <p>The chain rule states that the sensitivity of \\(L\\) with respect to \\(x\\) is the product of two sensitivities:</p> \\[ \\frac{dL}{dx} = \\frac{dL}{dy} \\cdot \\frac{dy}{dx}. \\] <p>This equation should be read step by step. First, \\(\\frac{dy}{dx}\\) tells us how a small change in \\(x\\) affects \\(y\\). Second, \\(\\frac{dL}{dy}\\) tells us how a small change in \\(y\\) affects the loss. Multiplying them gives the total effect of changing \\(x\\) on \\(L\\).</p> <p>This idea extends naturally to longer chains. If a model applies many functions in sequence, the chain rule is applied repeatedly, multiplying together the local sensitivities at each step. Each operation contributes a small piece to the overall gradient.</p>"},{"location":"mathematics/01_calculus/#taylor-expansion","title":"Taylor Expansion","text":"<p>Taylor series provides a systematic way to describe how a function behaves near a given point. It expresses a function as a sum of terms built from its derivatives at that point. Each term captures progressively finer details of how the function changes.</p> <p>For a function \\(f(x)\\) expanded around a point \\(x\\), the Taylor series in one dimension is</p> \\[ f(x+h) = f(x) + f'(x)h + \\tfrac{1}{2}f''(x)h^2 + \\tfrac{1}{6}f'''(x)h^3 + \\dots \\] <p>This expression says that the value of the function at \\(x+h\\) can be predicted by starting from the value at \\(x\\) and then adding corrections based on information about how the function changes at \\(x\\).</p> <p>In practice, we rarely use the full infinite series. Instead, we keep only the first few terms. This truncated version is called a Taylor expansion and is used as a local approximation.</p> <p>Keeping only the first-order term gives the linear approximation already used in gradient-based learning:</p> \\[ f(x+h) \\approx f(x) + f'(x)h. \\] <p>This approximation assumes that, for small updates, the function behaves almost like a straight line near the current point. It explains why gradients provide useful guidance for optimization.</p> <p>This local linear approximation relies on an important assumption: the function must be smooth enough near the point of expansion. Smoothness means that small changes in the input lead to small, predictable changes in the output, and that derivatives do not change abruptly.</p> <p>Note</p> <p>In DL, loss functions are often not perfectly smooth everywhere, but they are typically piecewise smooth. This is sufficient. Taylor expansions and gradient-based updates only rely on local behavior along the training trajectory, not on global smoothness of the loss surface. A common example is the ReLU activation, which is not differentiable at zero but is differentiable almost everywhere else. Gradient-based methods rely on this local behavior and use subgradients at nondifferentiable points.</p> <p>Keeping second-order terms reveals that this linear behavior is only approximate. These higher-order terms explain why the slope itself can change as we move, motivating the need to understand second-order structure.</p> <p>Note</p> <p>In DL, gradient-based learning relies on first-order Taylor approximations. Understanding why and when this approximation breaks down requires looking at second-order effects, which are captured by the Hessian.</p>"},{"location":"mathematics/01_calculus/#hessian","title":"Hessian","text":"<p>While the Jacobian describes first-order behavior\u2014how the loss changes under small parameter changes\u2014the Hessian describes second-order behavior.<sup>1</sup> It captures how these first-order sensitivities themselves change as we move in parameter space. The Hessian of \\(L\\) with respect to the parameter vector \\(\\theta\\) is a matrix of second-order partial derivatives:</p> \\[ H = \\begin{bmatrix} \\frac{\\partial^2 L}{\\partial \\theta_1^2} &amp; \\frac{\\partial^2 L}{\\partial \\theta_1 \\partial \\theta_2} &amp; \\dots &amp; \\frac{\\partial^2 L}{\\partial \\theta_1 \\partial \\theta_n} \\\\[0.5em] \\frac{\\partial^2 L}{\\partial \\theta_2 \\partial \\theta_1} &amp; \\frac{\\partial^2 L}{\\partial \\theta_2^2} &amp; \\dots &amp; \\frac{\\partial^2 L}{\\partial \\theta_2 \\partial \\theta_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 L}{\\partial \\theta_n \\partial \\theta_1} &amp; \\frac{\\partial^2 L}{\\partial \\theta_n \\partial \\theta_2} &amp; \\dots &amp; \\frac{\\partial^2 L}{\\partial \\theta_n^2} \\end{bmatrix}. \\] <p>Each entry tells us how the sensitivity with respect to one parameter changes when another parameter is varied. In this sense, the Hessian measures curvature: how the loss surface bends in different directions. Consider a simple two-parameter loss \\(L(\\theta_1, \\theta_2)\\). The diagonal entries of the Hessian describe how sharply the loss curves when we move along each parameter direction individually. The off-diagonal entries describe how changes in one parameter affect the sensitivity with respect to another parameter.</p> <p>Note</p> <p>In DL, this information explains important optimization behavior. Directions with strong positive curvature correspond to narrow valleys, where large updates can easily overshoot. Directions with weak curvature correspond to flat regions, where progress can be slow. Negative curvature indicates directions where the loss bends downward, which is typical near saddle points. Although full Hessians are rarely computed explicitly in DL due to their size and cost, their effects are always present. Learning rate selection, optimization stability, and the behavior of training near minima and saddle points are all influenced by second-order structure.</p> <p>Tip</p> <p>Like the Jacobian, the Hessian is a linear algebra object\u2014a matrix encoding directional behavior. If matrices, eigenvalues, or curvature interpretations feel unfamiliar, you may want to read the Linear Algebra page alongside this section.</p>"},{"location":"mathematics/01_calculus/#minima-saddle-points-and-convexity","title":"Minima, saddle points, and convexity","text":"<p>A minimum is a point where small changes in any direction increase the loss. At such a point, the gradient is zero and the surrounding curvature points upward.</p> <p>A saddle point is also a point where the gradient is zero, but the behavior is mixed: the loss increases in some directions and decreases in others. This means the point is neither a true minimum nor a maximum. The distinction between minima and saddle points is determined by the local curvature described by the Hessian.</p> <p>Note</p> <p>In high-dimensional DL models, saddle points are far more common than poor local minima. Gradient-based methods can often escape saddle points because curvature creates unstable directions, and stochastic noise from minibatches helps push parameters away from them.</p> <p>In classical optimization, convex loss functions play a special role. For a convex function, any point where the gradient is zero is guaranteed to be a global minimum. There are no saddle points and no spurious local minima.</p> <p>Note</p> <p>Most DL loss functions are not convex. As a result, global guarantees do not apply. Instead, training relies on local information provided by gradients and curvature. Despite the lack of convexity, gradient-based methods work well in practice due to overparameterization, stochastic gradients, and the structure induced by modern architectures, even though no global guarantees apply.</p> <p>Gradient-based learning does not require global convexity. What matters is that, locally, the loss behaves smoothly enough for gradients and Taylor approximations to provide reliable guidance along the training trajectory.</p>"},{"location":"mathematics/01_calculus/#fundamental-theorem-of-calculus","title":"Fundamental Theorem of Calculus","text":"<p>The Fundamental Theorem of Calculus explains the precise relationship between accumulation and change. If we define an accumulated quantity</p> \\[ F(x) = \\int_a^x f(t)\\,dt, \\] <p>then \\(F(x)\\) is differentiable and</p> \\[ \\frac{d}{dx} F(x) = f(x). \\] <p>This means that differentiation recovers the rate at which accumulation occurs. Conversely, if \\(F(x)\\) is any antiderivative of \\(f(x)\\), then total accumulation over an interval can be computed as</p> \\[ \\int_a^b f(x)\\,dx = F(b) - F(a). \\] <p>Together, these statements show that local change and total accumulation are two sides of the same idea.</p> <p>Note</p> <p>In DL, losses are defined as accumulated quantities, while gradients describe local change. Training works because following local gradients causes a consistent reduction in the accumulated loss over time.</p> <ol> <li> <p>For a further DL\u2013oriented treatment of gradients, Jacobians, Hessians, and numerical aspects of optimization, see Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press. Chapter 4: Numerical Computation.\u00a0\u21a9</p> </li> </ol>"},{"location":"mathematics/02_linear_algebra/","title":"Linear Algebra","text":""},{"location":"mathematics/03_probability/","title":"Probability Theory","text":""},{"location":"mathematics/04_information/","title":"Information Theory","text":""},{"location":"notebooks/01_backprop/","title":"01. From Derivatives to Backpropagation","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[2]: Copied! <pre>def f(x):\n  return x**2\n</pre> def f(x):   return x**2 In\u00a0[3]: Copied! <pre>x = 3.0\nfor h in [10, 1, 0.1, 0]:\n  print(f\"If we shift input by {h}, output becomes {f(x+h)}\")\n</pre> x = 3.0 for h in [10, 1, 0.1, 0]:   print(f\"If we shift input by {h}, output becomes {f(x+h)}\") <pre>If we shift input by 10, output becomes 169.0\nIf we shift input by 1, output becomes 16.0\nIf we shift input by 0.1, output becomes 9.610000000000001\nIf we shift input by 0, output becomes 9.0\n</pre> In\u00a0[4]: Copied! <pre>h = 1.0\n\ndx = h\ndy = f(x+h) - f(x)\n\nprint(f\"\u0394x: {dx}\")\nprint(f\"\u0394y: {dy}\")\nprint(f\"When you change x by {dx} unit, y changes by {dy} units.\")\n</pre> h = 1.0  dx = h dy = f(x+h) - f(x)  print(f\"\u0394x: {dx}\") print(f\"\u0394y: {dy}\") print(f\"When you change x by {dx} unit, y changes by {dy} units.\") <pre>\u0394x: 1.0\n\u0394y: 7.0\nWhen you change x by 1.0 unit, y changes by 7.0 units.\n</pre> In\u00a0[5]: Copied! <pre>def plot_delta(x, h, start=-4, stop=4, num=30):\n  # `np.linspace` returns an array of num inputs within a range.\n  x_all = np.linspace(start, stop, num)\n  y_all = f(x_all)\n\n  plt.figure(figsize=(4, 4))\n  plt.plot(x_all, y_all)\n\n  # dx &amp; dy\n  plt.plot([x, x + h], [f(x), f(x)], color='r')\n  plt.plot([x + h, x + h], [f(x), f(x + h)], color='r')\n</pre> def plot_delta(x, h, start=-4, stop=4, num=30):   # `np.linspace` returns an array of num inputs within a range.   x_all = np.linspace(start, stop, num)   y_all = f(x_all)    plt.figure(figsize=(4, 4))   plt.plot(x_all, y_all)    # dx &amp; dy   plt.plot([x, x + h], [f(x), f(x)], color='r')   plt.plot([x + h, x + h], [f(x), f(x + h)], color='r') In\u00a0[6]: Copied! <pre>plot_delta(x=2, h=1)\n</pre> plot_delta(x=2, h=1) <p>How to find if the ouput changes significantly when we change the input by some amount h?</p> In\u00a0[7]: Copied! <pre>def plot_roc(x, h):\n  dx = h\n  dy = f(x + h) - f(x)\n\n  plot_delta(x, h)\n  print(f\"Rate of change is {dy / dx}\")\n</pre> def plot_roc(x, h):   dx = h   dy = f(x + h) - f(x)    plot_delta(x, h)   print(f\"Rate of change is {dy / dx}\") In\u00a0[8]: Copied! <pre>plot_roc(3, 1)\n</pre> plot_roc(3, 1) <pre>Rate of change is 7.0\n</pre> In\u00a0[9]: Copied! <pre>plot_roc(3, 0.5)\n</pre> plot_roc(3, 0.5) <pre>Rate of change is 6.5\n</pre> In\u00a0[10]: Copied! <pre>plot_roc(1, 1)\n</pre> plot_roc(1, 1) <pre>Rate of change is 3.0\n</pre> In\u00a0[11]: Copied! <pre>plot_roc(-2, 0.5)\n</pre> plot_roc(-2, 0.5) <pre>Rate of change is -3.5\n</pre> <p>The rate of change for different values of h are different at the same point x. We would like to come up with a single value that would tell how significantly y changes at a given point x within the function (<code>a</code> in the formula corresponds to <code>x</code> in the code).</p> <p></p> <p>Simply, this limit tells us how much the value of y will change when we change x by just a very small amount. Note: Essentially, derivative is a function.</p> In\u00a0[12]: Copied! <pre>x = 3\nh = 0.000001 # limit of h approaches 0\nd = (f(x + h) - f(x)) / h\nprint(f\"The value of derivative function is {d}\")\n</pre> x = 3 h = 0.000001 # limit of h approaches 0 d = (f(x + h) - f(x)) / h print(f\"The value of derivative function is {d}\") <pre>The value of derivative function is 6.000001000927568\n</pre> <p>Partial derivative with respect to some variable basically means how much the output will change when we nudge that variable by a very small amount.</p> In\u00a0[13]: Copied! <pre>f = lambda x, y: x + y\n</pre> f = lambda x, y: x + y In\u00a0[14]: Copied! <pre>x = 2\ny = 3\n\nf(x, y)\n</pre> x = 2 y = 3  f(x, y) Out[14]: <pre>5</pre> In\u00a0[15]: Copied! <pre>h = 0.000001\nf(x + h, y)\n</pre> h = 0.000001 f(x + h, y) Out[15]: <pre>5.000001</pre> <p>Let\u2019s see partial derivatives with respect to x an y.</p> In\u00a0[16]: Copied! <pre># wrt x\n(f(x + h, y) - f(x, y)) / h\n</pre> # wrt x (f(x + h, y) - f(x, y)) / h Out[16]: <pre>1.000000000139778</pre> In\u00a0[17]: Copied! <pre># wrt y\n(f(x, y+h) - f(x, y)) / h\n</pre> # wrt y (f(x, y+h) - f(x, y)) / h Out[17]: <pre>1.000000000139778</pre> <p>It will always approach 1 for addition, no matter what are the input values.</p> In\u00a0[18]: Copied! <pre>for x, y in zip([-20, 2, 3], [300, 75, 10]):\n  print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}')\n</pre> for x, y in zip([-20, 2, 3], [300, 75, 10]):   print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}') <pre>x=-20, y=300: 0.9999999974752427\nx=2, y=75: 0.9999999974752427\nx=3, y=10: 1.0000000010279564\n</pre> <p>Indeed, if we have simple addition x + y, then increasing x or y by some amount will increase the result by the exact same amount. Assertion will work for any number h gets.</p> In\u00a0[19]: Copied! <pre>h = 10\nassert f(x+h, y) - f(x, y) == h\nassert f(x, y+h) - f(x, y) == h\n</pre> h = 10 assert f(x+h, y) - f(x, y) == h assert f(x, y+h) - f(x, y) == h In\u00a0[20]: Copied! <pre>f = lambda x, y: x * y\n</pre> f = lambda x, y: x * y In\u00a0[21]: Copied! <pre>x = 2\ny = 3\nh = 1e-5 # same as 0.00001\n(f(x + h, y) - f(x, y)) / h # wrt x\n</pre> x = 2 y = 3 h = 1e-5 # same as 0.00001 (f(x + h, y) - f(x, y)) / h # wrt x Out[21]: <pre>3.000000000064062</pre> In\u00a0[22]: Copied! <pre>for x in [-20, 2, 3]:\n  print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}')\n</pre> for x in [-20, 2, 3]:   print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}') <pre>x=-20, y=3: 2.999999999531155\nx=2, y=3: 3.000000000064062\nx=3, y=3: 3.000000000064062\n</pre> In\u00a0[23]: Copied! <pre>x = 10\nh = 5\npdx = (f(x+h, y) - f(x, y)) / h\nprint(pdx, y)\nassert round(pdx, 2) == round(y, 2)\n</pre> x = 10 h = 5 pdx = (f(x+h, y) - f(x, y)) / h print(pdx, y) assert round(pdx, 2) == round(y, 2) <pre>3.0 3\n</pre> In\u00a0[24]: Copied! <pre>def f(a, b, c):\n  return a**2 + b**3 - c\n</pre> def f(a, b, c):   return a**2 + b**3 - c In\u00a0[25]: Copied! <pre>a = 2\nb = 3\nc = 4\n\nf(a, b, c)\n</pre> a = 2 b = 3 c = 4  f(a, b, c) Out[25]: <pre>27</pre> In\u00a0[26]: Copied! <pre>h = 1\n\nf(a + h, b, c)\n</pre> h = 1  f(a + h, b, c) Out[26]: <pre>32</pre> In\u00a0[27]: Copied! <pre>f(a + h, b, c) - f(a, b, c)\n</pre> f(a + h, b, c) - f(a, b, c) Out[27]: <pre>5</pre> In\u00a0[28]: Copied! <pre>(f(a + h, b, c) - f(a, b, c)) / h\n</pre> (f(a + h, b, c) - f(a, b, c)) / h Out[28]: <pre>5.0</pre> In\u00a0[29]: Copied! <pre>h = 0.00001 # when the change is approaching zero\npda = (f(a + h, b, c) - f(a, b, c)) / h\npda\n</pre> h = 0.00001 # when the change is approaching zero pda = (f(a + h, b, c) - f(a, b, c)) / h pda Out[29]: <pre>4.000010000027032</pre> In\u00a0[30]: Copied! <pre>assert 2*a == round(pda)\n</pre> assert 2*a == round(pda) <p>Our function was <code>f(a,b,c) = a<sup>2</sup>+b<sup>3</sup>-c</code>. Partial derivative with respect to a is <code>2a</code> (by the power rule), and when <code>a=2</code> indeed we get 4.</p> <p>Exercise: Code the partial derivative with respect to b and c and verify if the result correct.</p> In\u00a0[31]: Copied! <pre># This is a graph visualization code from micrograd, no need to understand the details\n# https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb\nfrom graphviz import Digraph\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n\n    for n in nodes:\n        dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n\n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> # This is a graph visualization code from micrograd, no need to understand the details # https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb from graphviz import Digraph  def trace(root):     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root, format='svg', rankdir='LR'):     \"\"\"     format: png | svg | ...     rankdir: TB (top to bottom graph) | LR (left to right)     \"\"\"     assert rankdir in ['LR', 'TB']     nodes, edges = trace(root)     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})      for n in nodes:         dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')         if n._op:             dot.node(name=str(id(n)) + n._op, label=n._op)             dot.edge(str(id(n)) + n._op, str(id(n)))      for n1, n2 in edges:         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[32]: Copied! <pre># Value class stores a number and \"remembers\" information about its origins\nclass Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self.grad = 0\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+')\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), \"*\")\n    return out\n\n  def __repr__(self):\n    return f\"Value(data={self.data}, grad={self.grad})\"\n</pre> # Value class stores a number and \"remembers\" information about its origins class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self.grad = 0    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+')     return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), \"*\")     return out    def __repr__(self):     return f\"Value(data={self.data}, grad={self.grad})\" In\u00a0[33]: Copied! <pre>a = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b; c.label = 'c'\nd = Value(10, label='d')\nL = c * d; L.label = 'L'\n</pre> a = Value(5, label='a') b = Value(3, label='b') c = a + b; c.label = 'c' d = Value(10, label='d') L = c * d; L.label = 'L' In\u00a0[34]: Copied! <pre>print(a, a._prev)\nprint(L, L._prev)\n</pre> print(a, a._prev) print(L, L._prev) <pre>Value(data=5, grad=0) ()\nValue(data=80, grad=0) (Value(data=8, grad=0), Value(data=10, grad=0))\n</pre> In\u00a0[35]: Copied! <pre>draw_dot(c)\n</pre> draw_dot(c) Out[35]: In\u00a0[36]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[36]: <p>Gradient is vector of partial derivatives.</p> <p>We want to know how much changing each variable will affect the output of <code>L</code>. We will store those partial derivatives inside each <code>grad</code> variable of each <code>Value</code> object.</p> In\u00a0[37]: Copied! <pre>L.grad = 1.0\n</pre> L.grad = 1.0 <p>The derivative of a variable with respect to itself is 1 (you get the same dx/dy).</p> In\u00a0[38]: Copied! <pre>f = lambda x: x\nh = 1e-5\npdx = (f(x + h) - f(x)) / h\nassert round(pdx) == 1\n</pre> f = lambda x: x h = 1e-5 pdx = (f(x + h) - f(x)) / h assert round(pdx) == 1 <p>Now let's see how changing other variables will affect the eventual result.</p> In\u00a0[39]: Copied! <pre>def f(ha=0, hb=0, hc=0, hd=0):\n  # same function as before\n  a = Value(5 + ha, label='a')\n  b = Value(3 + hb, label='b')\n  c = a + b + Value(hc); c.label = 'c'\n  d = Value(10 + hd, label='d')\n  L = c * d; L.label = 'L'\n  return L.data\n</pre> def f(ha=0, hb=0, hc=0, hd=0):   # same function as before   a = Value(5 + ha, label='a')   b = Value(3 + hb, label='b')   c = a + b + Value(hc); c.label = 'c'   d = Value(10 + hd, label='d')   L = c * d; L.label = 'L'   return L.data In\u00a0[40]: Copied! <pre>h = 1e-5\n(f(hd=h) - f()) / h\n</pre> h = 1e-5 (f(hd=h) - f()) / h Out[40]: <pre>7.999999999697137</pre> <p>From the computational graph we can also see that <code>L=c*d</code>. When we change the value of d just a little bit (derivative of <code>L</code> with respect to <code>d</code>) the value of <code>L</code> will change by the amount of <code>c</code>, which is <code>8.0</code>. We saw it above in the partial derivative of a multiplication.</p> In\u00a0[41]: Copied! <pre>d.grad = c.data\nc.grad = d.data\n</pre> d.grad = c.data c.grad = d.data <p>With the same logic, the derivative of <code>L</code> wrt <code>c</code> will be the value of <code>d</code>, which is <code>10.0</code>. We can verify it.</p> In\u00a0[42]: Copied! <pre>(f(hc=h) - f()) / h\n</pre> (f(hc=h) - f()) / h Out[42]: <pre>10.000000000331966</pre> <p>To determine how much changing earlier variables in the computation graph will affect the <code>L</code> variable, we can apply the chain rule. Simply, the derivative of <code>L</code> with respect to <code>a</code> is the derivative of <code>c</code> with respect to <code>a</code> multiplied by the derivative of <code>L</code> with respect to <code>c</code>. God bless Leibniz.</p> <p></p> <p>The derivate of <code>c</code> both wrt <code>a</code> and 'b' is <code>1</code> due to the property of addition shown before (<code>c=a+b</code>). From here:</p> In\u00a0[43]: Copied! <pre>a.grad = 1.0 * c.grad\nb.grad = 1.0 * c.grad\n\na.grad, b.grad\n</pre> a.grad = 1.0 * c.grad b.grad = 1.0 * c.grad  a.grad, b.grad Out[43]: <pre>(10.0, 10.0)</pre> <p>We can verify it as well. Let's see how much <code>L</code> gets affected, when we shift <code>a</code> or <code>b</code> by a small amount.</p> In\u00a0[44]: Copied! <pre>(f(ha=h) - f()) / h\n</pre> (f(ha=h) - f()) / h Out[44]: <pre>10.000000000331966</pre> In\u00a0[45]: Copied! <pre>(f(hb=h) - f()) / h\n</pre> (f(hb=h) - f()) / h Out[45]: <pre>10.000000000331966</pre> <p>We will finally redraw the manually updated computation graph.</p> In\u00a0[46]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[46]: <p>It basically implies that, for example, changing the value of <code>a</code> by <code>1</code> unit (from 5 to 6) will increase the value of <code>L</code> by <code>10</code> units (from 80 to 90).</p> In\u00a0[47]: Copied! <pre>f(ha=1)\n</pre> f(ha=1) Out[47]: <pre>90</pre> In\u00a0[48]: Copied! <pre>f(hb=1), f(hc=1), f(hd=1) # the rest of the cases\n</pre> f(hb=1), f(hc=1), f(hd=1) # the rest of the cases Out[48]: <pre>(90, 90, 88)</pre> <p>What we saw above was one backward pass done manually. We are mainly interested in the signs of partial derivatives to know if they are positively or negatively influencing the eventual loss <code>L</code> of our model. In our case, all the derivatives are positive and influence loss positively.</p> <p>We have to simply nudge the values in the opposite direction of the gradient to bring the loss down. This is known as gradient descent.</p> In\u00a0[49]: Copied! <pre>lr = 0.01 # we will discuss learning rate later on\n\na.data -= lr * a.grad\nb.data -= lr * b.grad\nd.data -= lr * d.grad\n\n# we skip c which is controlled by the values of a and b\n# pay attention that the rest are leaf nodes in the computation graph\n</pre> lr = 0.01 # we will discuss learning rate later on  a.data -= lr * a.grad b.data -= lr * b.grad d.data -= lr * d.grad  # we skip c which is controlled by the values of a and b # pay attention that the rest are leaf nodes in the computation graph <p>In case the loss is a negative value (not common), we will need to \"gradient ascend\" the loss upwards towards zero and change the sign to <code>+=</code> from <code>-=</code>. Note that the values of parameters (a, b, d) can decrease or increase depending on the sign of <code>grad</code>.</p> <p>We will now do a single forward pass to see if loss has been decreased. Previous loss was <code>80</code>.</p> In\u00a0[50]: Copied! <pre># We will now forward pass\nc = a + b\nL = c * d\n\nL.data\n</pre> # We will now forward pass c = a + b L = c * d  L.data Out[50]: <pre>77.376</pre> <p>We optimized our values and brought down the loss.</p> <p>Manually calculating gradient is good only for educational purposes. We should implement automatic backward pass which will calculate gradients. We will rewrite our <code>Value</code> class for <code>backward()</code> function.</p> In\u00a0[51]: Copied! <pre>class Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self.grad = 0.0\n    self._backward = lambda: None # initially it is a function which does nothing\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+')\n\n    def _backward():\n      self.grad = 1.0 * out.grad\n      other.grad = 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), \"*\")\n\n    def _backward():\n      self.grad = other.data * out.grad\n      other.grad = self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __repr__(self):\n    return f\"Value(data={self.data}, grad={self.grad})\"\n</pre> class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self.grad = 0.0     self._backward = lambda: None # initially it is a function which does nothing    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+')      def _backward():       self.grad = 1.0 * out.grad       other.grad = 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), \"*\")      def _backward():       self.grad = other.data * out.grad       other.grad = self.data * out.grad     out._backward = _backward      return out    def __repr__(self):     return f\"Value(data={self.data}, grad={self.grad})\" In\u00a0[52]: Copied! <pre># Recreating the same function\na = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b; c.label = 'c'\nd = Value(10, label='d')\nL = c * d; L.label = 'L'\n</pre> # Recreating the same function a = Value(5, label='a') b = Value(3, label='b') c = a + b; c.label = 'c' d = Value(10, label='d') L = c * d; L.label = 'L' In\u00a0[53]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[53]: <p>We will initialize the gradient of the loss to be 1.0 and then call backward function. We should get the same results which we manually calculated previously.</p> In\u00a0[54]: Copied! <pre>L.grad = 1.0\nL._backward()\nc._backward()\n</pre> L.grad = 1.0 L._backward() c._backward() In\u00a0[55]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[55]: <p>Exercise: Make sure that all operations and their partial derivatives can be calculated (e.g. division, power).</p> <p>We can now call the optimization process, as well as forward and backward passes to reduce loss. Training model with the help of backward pass, optimization, and forward pass is called backpropagation.</p> In\u00a0[56]: Copied! <pre># optimization\nlr = 0.01\na.data -= lr * a.grad\nb.data -= lr * b.grad\nd.data -= lr * d.grad\n\n# forward pass\nc = a + b\nL = c * d\n\n# backward pass\nL.grad = 1.0\nL._backward()\nc._backward()\n\nL.data # loss\n</pre> # optimization lr = 0.01 a.data -= lr * a.grad b.data -= lr * b.grad d.data -= lr * d.grad  # forward pass c = a + b L = c * d  # backward pass L.grad = 1.0 L._backward() c._backward()  L.data # loss Out[56]: <pre>77.376</pre> <p>We have now trained the model for a single <code>epoch</code>. Even though what we do is oversimplistic and not precise, the main intuition and concepts behind training a neural network is the same.</p> <p>We will train the model for multiple epochs until we reduce the loss down to zero.</p> In\u00a0[57]: Copied! <pre>while True:\n  # optimization\n  a.data -= lr * a.grad\n  b.data -= lr * b.grad\n  d.data -= lr * d.grad\n\n  # forward pass\n  c = a + b\n  L = c * d\n\n  # backward pass\n  L.grad = 1.0\n  L._backward()\n  c._backward()\n\n  if L.data &lt; 0:\n    break\n\n  print(f'Loss: {round(L.data,2)}')\n</pre> while True:   # optimization   a.data -= lr * a.grad   b.data -= lr * b.grad   d.data -= lr * d.grad    # forward pass   c = a + b   L = c * d    # backward pass   L.grad = 1.0   L._backward()   c._backward()    if L.data &lt; 0:     break    print(f'Loss: {round(L.data,2)}') <pre>Loss: 74.81\nLoss: 72.31\nLoss: 69.87\nLoss: 67.49\nLoss: 65.16\nLoss: 62.88\nLoss: 60.66\nLoss: 58.48\nLoss: 56.35\nLoss: 54.27\nLoss: 52.23\nLoss: 50.24\nLoss: 48.28\nLoss: 46.37\nLoss: 44.49\nLoss: 42.65\nLoss: 40.84\nLoss: 39.07\nLoss: 37.33\nLoss: 35.62\nLoss: 33.94\nLoss: 32.29\nLoss: 30.66\nLoss: 29.06\nLoss: 27.48\nLoss: 25.93\nLoss: 24.39\nLoss: 22.88\nLoss: 21.39\nLoss: 19.91\nLoss: 18.45\nLoss: 17.0\nLoss: 15.57\nLoss: 14.16\nLoss: 12.75\nLoss: 11.36\nLoss: 9.97\nLoss: 8.59\nLoss: 7.22\nLoss: 5.86\nLoss: 4.5\nLoss: 3.15\nLoss: 1.8\nLoss: 0.45\n</pre> <p>All we did manually is built in to PyTorch. We will do a forward and backward pass and check if the gradients are what we had previosuly calculated. As gradients are not always calculated, for optimization purposes <code>requires_grad</code> is set to False by default. We cannot also calculate gradient for leaf nodes.</p> In\u00a0[58]: Copied! <pre>import torch\n\na = torch.tensor(5.0);    a.requires_grad = True\nb = torch.tensor(3.0);    b.requires_grad = True\nc = a + b\nd = torch.tensor(10.0);   d.requires_grad = True\nL = c * d\n</pre> import torch  a = torch.tensor(5.0);    a.requires_grad = True b = torch.tensor(3.0);    b.requires_grad = True c = a + b d = torch.tensor(10.0);   d.requires_grad = True L = c * d In\u00a0[59]: Copied! <pre>L.backward()\n</pre> L.backward() In\u00a0[60]: Copied! <pre>a.grad, b.grad, d.grad\n</pre> a.grad, b.grad, d.grad Out[60]: <pre>(tensor(10.), tensor(10.), tensor(8.))</pre> <p>We got the expected result.</p>"},{"location":"notebooks/01_backprop/#01-from-derivatives-to-backpropagation","title":"01. From Derivatives to Backpropagation\u00b6","text":"25 Jan 2025 \u00b7   18 Jan 2026 \u00b7   9 min <p>Important</p> <p>     The notebook is currently under revision.   </p> <p>Info</p> <p>The notebook is mainly based on Andrej Karpathy's lecture on Micrograd</p> <p>We will go from illustrating differentiation and finding derivatives in Python, all the way down till the implementation of the backpropagation algorithm.</p>"},{"location":"notebooks/01_backprop/#differentiation","title":"Differentiation\u00b6","text":""},{"location":"notebooks/01_backprop/#partial-derivatives","title":"Partial Derivatives\u00b6","text":""},{"location":"notebooks/01_backprop/#addition","title":"Addition\u00b6","text":""},{"location":"notebooks/01_backprop/#multiplication","title":"Multiplication\u00b6","text":""},{"location":"notebooks/01_backprop/#complex","title":"Complex\u00b6","text":""},{"location":"notebooks/01_backprop/#micrograd-and-computation-graph","title":"Micrograd and Computation Graph\u00b6","text":"<p>Based on Karpathy's https://github.com/karpathy/micrograd</p>"},{"location":"notebooks/01_backprop/#gradient","title":"Gradient\u00b6","text":""},{"location":"notebooks/01_backprop/#chain-rule","title":"Chain Rule\u00b6","text":""},{"location":"notebooks/01_backprop/#optimization-with-gradient-descent","title":"Optimization with Gradient Descent\u00b6","text":""},{"location":"notebooks/01_backprop/#forward-pass","title":"Forward Pass\u00b6","text":""},{"location":"notebooks/01_backprop/#backward-pass","title":"Backward Pass\u00b6","text":""},{"location":"notebooks/01_backprop/#training-model-with-backpropagation","title":"Training Model with Backpropagation\u00b6","text":""},{"location":"notebooks/01_backprop/#pytorch-implementation","title":"PyTorch Implementation\u00b6","text":""},{"location":"notebooks/02_neural_network/","title":"02. From Neuron to Neural Network","text":"8 Feb 2025 \u00b7   18 Jan 2026 \u00b7   9 min <p>Note</p> <p>The notebook is mainly based on Andrej Karpathy's lecture on Micrograd</p> In\u00a0[1]: Copied! <pre># This is a graph visualization code from micrograd, no need to understand the details\n# https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb\nfrom graphviz import Digraph\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n\n    for n in nodes:\n        dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n\n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> # This is a graph visualization code from micrograd, no need to understand the details # https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb from graphviz import Digraph  def trace(root):     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root, format='svg', rankdir='LR'):     \"\"\"     format: png | svg | ...     rankdir: TB (top to bottom graph) | LR (left to right)     \"\"\"     assert rankdir in ['LR', 'TB']     nodes, edges = trace(root)     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})      for n in nodes:         dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')         if n._op:             dot.node(name=str(id(n)) + n._op, label=n._op)             dot.edge(str(id(n)) + n._op, str(id(n)))      for n1, n2 in edges:         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[2]: Copied! <pre># This is the code from previous lecture\n# It is important to understand _backward function\nclass Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self._backward = lambda: None\n    self.grad = 0.0\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other),'+')\n\n    def _backward():\n      self.grad = 1.0 * out.grad\n      other.grad = 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other),'*')\n\n    def _backward():\n      self.grad = other.data * out.grad\n      other.grad = self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __sub__(self, other):\n    return self + (Value(-1) * other) # self + (-other)\n\n  def __repr__(self):\n    return f'{self.label}: {self.data}'\n</pre> # This is the code from previous lecture # It is important to understand _backward function class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self._backward = lambda: None     self.grad = 0.0    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other),'+')      def _backward():       self.grad = 1.0 * out.grad       other.grad = 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other),'*')      def _backward():       self.grad = other.data * out.grad       other.grad = self.data * out.grad     out._backward = _backward      return out    def __sub__(self, other):     return self + (Value(-1) * other) # self + (-other)    def __repr__(self):     return f'{self.label}: {self.data}' In\u00a0[3]: Copied! <pre>a = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b;   c.label = 'c'\nd = Value(10, label='d')\nL = c * d;   L.label = 'L'\n</pre> a = Value(5, label='a') b = Value(3, label='b') c = a + b;   c.label = 'c' d = Value(10, label='d') L = c * d;   L.label = 'L' In\u00a0[4]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[4]: In\u00a0[5]: Copied! <pre>epochs = 10\nlearning_rate = 0.01\n\nfor _ in range(epochs):\n  L.grad = 1.0\n\n  # backward pass\n  L._backward()\n  c._backward()\n\n  # optimization (gradient descent)\n  a.data -= learning_rate * a.grad\n  b.data -= learning_rate * b.grad\n  d.data -= learning_rate * d.grad\n\n  # forward pass\n  c = a + b\n  L = c * d\n\n  print(f'Loss: {L.data:.2f}')\n</pre> epochs = 10 learning_rate = 0.01  for _ in range(epochs):   L.grad = 1.0    # backward pass   L._backward()   c._backward()    # optimization (gradient descent)   a.data -= learning_rate * a.grad   b.data -= learning_rate * b.grad   d.data -= learning_rate * d.grad    # forward pass   c = a + b   L = c * d    print(f'Loss: {L.data:.2f}') <pre>Loss: 77.38\nLoss: 74.81\nLoss: 72.31\nLoss: 69.87\nLoss: 67.49\nLoss: 65.16\nLoss: 62.88\nLoss: 60.66\nLoss: 58.48\nLoss: 56.35\n</pre> In\u00a0[6]: Copied! <pre># Equivalent implementation in PyTorch\n# pay attention to requires_grad, no_grad() and zero_()\n\nimport torch\n\na = torch.tensor(5.0, requires_grad = True);\nb = torch.tensor(3.0, requires_grad = True);\nc = a + b\nd = torch.tensor(10.0,requires_grad = True);\nL = c * d\n\nfor _ in range(epochs):\n  # backward pass\n  L.backward()\n\n  # optimization (gradient descent)\n  with torch.no_grad():\n    a -= learning_rate * a.grad\n    b -= learning_rate * b.grad\n    d -= learning_rate * d.grad\n\n  # avoids accumulating gradients\n  # comment this out to see how it affects the learning\n  a.grad.zero_()\n  b.grad.zero_()\n  d.grad.zero_()\n\n  # forward pass\n  c = a + b\n  L = c * d\n\n  print(f'Loss: {L.data:.2f}')\n</pre> # Equivalent implementation in PyTorch # pay attention to requires_grad, no_grad() and zero_()  import torch  a = torch.tensor(5.0, requires_grad = True); b = torch.tensor(3.0, requires_grad = True); c = a + b d = torch.tensor(10.0,requires_grad = True); L = c * d  for _ in range(epochs):   # backward pass   L.backward()    # optimization (gradient descent)   with torch.no_grad():     a -= learning_rate * a.grad     b -= learning_rate * b.grad     d -= learning_rate * d.grad    # avoids accumulating gradients   # comment this out to see how it affects the learning   a.grad.zero_()   b.grad.zero_()   d.grad.zero_()    # forward pass   c = a + b   L = c * d    print(f'Loss: {L.data:.2f}') <pre>Loss: 77.38\nLoss: 74.81\nLoss: 72.31\nLoss: 69.87\nLoss: 67.49\nLoss: 65.16\nLoss: 62.88\nLoss: 60.66\nLoss: 58.48\nLoss: 56.35\n</pre> <p>The function <code>f(x) = x * w</code> is a linear function always passing from origin. The real world data, however, will be much more complex, and in order to describe a pattern in the data our Machine Learning model should return a more flexible function. For that, we will do two things: add bias <code>b</code> and bring non-linearity with an activation function. We can choose different non-linear activation functions with the condition that it should be differentiable (otherwise we won't be able to calculate gradients for backpropagation). We will implement <code>sigmoid</code>(logistic) activation function which has the following formula:</p> <p></p> <p>Sigmoid function not only makes a linear function non-linear and continuous, but also maps any value of <code>x</code> to be between 0 and 1. It may be useful when we want to predict probabilities for different output classes.</p> In\u00a0[7]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[8]: Copied! <pre>def sigmoid(x):\n  return 1.0 / (1 + np.exp(-x)) # see formula above\n</pre> def sigmoid(x):   return 1.0 / (1 + np.exp(-x)) # see formula above In\u00a0[9]: Copied! <pre># a simple linear function where activation will be applied\ndef f(x, w=0.5, b=10, activation=None):\n  out = x * w + b\n  return activation(out) if activation else out\n</pre> # a simple linear function where activation will be applied def f(x, w=0.5, b=10, activation=None):   out = x * w + b   return activation(out) if activation else out In\u00a0[10]: Copied! <pre>def plot(f, x, activation=None):\n  plt.figure(figsize=(4, 4))\n  x_all = np.linspace(-50, 50, 100)\n  y_all = f(x_all, activation=activation)\n  plt.plot(x_all, y_all)\n  plt.scatter(x, f(x, activation=activation), color='r')\n  plt.show()\n</pre> def plot(f, x, activation=None):   plt.figure(figsize=(4, 4))   x_all = np.linspace(-50, 50, 100)   y_all = f(x_all, activation=activation)   plt.plot(x_all, y_all)   plt.scatter(x, f(x, activation=activation), color='r')   plt.show() In\u00a0[11]: Copied! <pre>x = -20\nplot(f, x)\n</pre> x = -20 plot(f, x) <p>Now we will plot the exact same point mapped into the non-linear function between <code>0</code> and <code>1</code>. Try out different <code>x</code> values and see the plots.</p> In\u00a0[12]: Copied! <pre>plot(f, x, sigmoid)\n</pre> plot(f, x, sigmoid) <p>Exercise: Implement other activation functions (e.g. <code>tanh</code>, <code>relu</code>) and see the plot.</p> <p>Exercise: What could be the distadvantage of using sigmoid activation function?</p> <p>Funcs - Own work (CC0 | Wikimedia Commons)</p> <p></p> <p>An artificial neuron is simply a linear function passing through an activation function  (e.g. <code>sigmoid(x * w + b)</code>). The illustration above describes an N-dimensional neuron, accepting inputs between <code>x<sub>1</sub> ... x<sub>n</sub></code>. The function <code>f</code> we had above is a very simple neuron with 1-dimensional input.</p> <p>Question: What could be input values for predicting the probability of a customer cancelling their subscription?</p> <p>Before creating our neuron, we will first make some updates to the <code>Value</code> class.</p> <p>Not only value class should have a <code>sigmoid(x)</code> function, but also it should be able to calculate a derivative for it.</p> <p>Exercise: Find the derivative of the <code>sigmoid</code> function:</p> <p></p> <p>The <code>requires_grad</code> flag (similar to PyTorch) will tell which parameters are trainable and requires gradient calculation and update (Note that this feature is not implemented in <code>micrograd</code>).</p> <p>For example, it doesn't make sense to modify the real-life training inputs <code>x1</code> and <code>x2</code> for our neuron. We shouldn't spend resources for calculating unnecessary gradients. Our goal is to nudge only the weight and bias (i.e. parameter) values, as well as the nodes dependent on them, in order to minimize the eventual loss.</p> In\u00a0[13]: Copied! <pre>class Value:\n  def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self._backward = lambda: None\n    self.grad = 0.0\n    self.requires_grad = requires_grad\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad = 1.0 * out.grad\n      if other.requires_grad:\n        other.grad = 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad = other.data * out.grad\n      if other.requires_grad:\n        other.grad = self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __sub__(self, other):\n    return self + (Value(-1) * other) # self + (-other)\n\n  def sigmoid(self):\n    s = 1.0 / (1 + np.exp(-self.data))\n    out = Value(s, (self, ), 'sigmoid', self.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad = s * (1.0 - s) * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __repr__(self):\n    return f'Value({self.data:.4f})'\n</pre> class Value:   def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self._backward = lambda: None     self.grad = 0.0     self.requires_grad = requires_grad    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad = 1.0 * out.grad       if other.requires_grad:         other.grad = 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad = other.data * out.grad       if other.requires_grad:         other.grad = self.data * out.grad     out._backward = _backward      return out    def __sub__(self, other):     return self + (Value(-1) * other) # self + (-other)    def sigmoid(self):     s = 1.0 / (1 + np.exp(-self.data))     out = Value(s, (self, ), 'sigmoid', self.requires_grad)      def _backward():       if self.requires_grad:         self.grad = s * (1.0 - s) * out.grad     out._backward = _backward      return out    def __repr__(self):     return f'Value({self.data:.4f})' <p>We will initially implement a simple <code>Neuron</code> class in 3D (2-dimensional input values and an output value). The function will have two inputs <code>x1</code> and <code>x2</code>, which will become <code>Value</code> objects. Their weights <code>w1</code> and <code>w2</code> will determine how much input (e.g. age of a customer) influences outcome.</p> In\u00a0[14]: Copied! <pre>from mpl_toolkits.mplot3d import Axes3D\n\nclass Neuron:\n  def __init__(self):\n    self.w1 = Value(np.random.uniform(-1, 1), label='w1', requires_grad=True)\n    self.w2 = Value(np.random.uniform(-1, 1), label='w2', requires_grad=True)\n    self.b = Value(0, label='b', requires_grad=True)\n\n  def __call__(self, x1, x2):\n    out = x1 * self.w1 + x2 * self.w2 + self.b\n    return out.sigmoid()\n\n  # this code here is for plotting, no need to understand, works for only 3D\n  def plot(self):\n    x1_vals = np.linspace(-5, 5, 100)\n    x2_vals = np.linspace(-5, 5, 100)\n    X1, X2 = np.meshgrid(x1_vals, x2_vals)\n    Z = np.zeros_like(X1)\n\n    for i in range(X1.shape[0]):\n      for j in range(X1.shape[1]):\n        x1 = Value(X1[i, j])\n        x2 = Value(X2[i, j])\n        output = self(x1, x2)\n        Z[i, j] = output.data\n\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot_surface(X1, X2, Z, cmap='viridis')\n    ax.set_title(f'Neuron Output with Sigmoid Activation)')\n    plt.show()\n</pre> from mpl_toolkits.mplot3d import Axes3D  class Neuron:   def __init__(self):     self.w1 = Value(np.random.uniform(-1, 1), label='w1', requires_grad=True)     self.w2 = Value(np.random.uniform(-1, 1), label='w2', requires_grad=True)     self.b = Value(0, label='b', requires_grad=True)    def __call__(self, x1, x2):     out = x1 * self.w1 + x2 * self.w2 + self.b     return out.sigmoid()    # this code here is for plotting, no need to understand, works for only 3D   def plot(self):     x1_vals = np.linspace(-5, 5, 100)     x2_vals = np.linspace(-5, 5, 100)     X1, X2 = np.meshgrid(x1_vals, x2_vals)     Z = np.zeros_like(X1)      for i in range(X1.shape[0]):       for j in range(X1.shape[1]):         x1 = Value(X1[i, j])         x2 = Value(X2[i, j])         output = self(x1, x2)         Z[i, j] = output.data      fig = plt.figure(figsize=(10, 8))     ax = fig.add_subplot(111, projection='3d')     ax.plot_surface(X1, X2, Z, cmap='viridis')     ax.set_title(f'Neuron Output with Sigmoid Activation)')     plt.show() <p>Now we can initialize our inputs and neuron to see our computation graph. Our loss will be simple: the ground truth label <code>y</code> minus the predicted probability. Let's assume that, our input values <code>x1</code> and <code>x2</code> correspond to a customer who made the purchase (<code>y = 1</code>). We will try out both activation functions and see their plots.</p> In\u00a0[15]: Copied! <pre>x1 = Value(2, label='x1')\nx2 = Value(3, label='x2')\ny  = Value(1, label= 'y')\n\nn = Neuron()\n\npred = n(x1, x2);      pred.label = 'pred'\nL = y - pred;          L.label = 'loss'\n</pre> x1 = Value(2, label='x1') x2 = Value(3, label='x2') y  = Value(1, label= 'y')  n = Neuron()  pred = n(x1, x2);      pred.label = 'pred' L = y - pred;          L.label = 'loss' In\u00a0[16]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[16]: In\u00a0[17]: Copied! <pre>n.plot()\n</pre> n.plot() <p>The ground truth label <code>1</code> tells us that we should push our probability towards <code>1.0</code>. In other words, as our loss <code>L</code> here is a simple error value corresponding to <code>1 - prob</code>, we should try to minimize the loss down to zero with backpropagation. However, our computation graph is bigger than how it was before. The <code>_backward</code> pass function we call manually on each node is not scalable. Ideally, we should have a single function <code>backward()</code> to calculate all the gradients, which we previously saw in the PyTorch implementation. For that, we will need to sort the nodes of the computation graph (in this case, from input/weight nodes until the probability node). We can achieve that with a topological sort function implemented for micrograd.</p> In\u00a0[18]: Copied! <pre>topo = []\nvisited = set()\ndef build_topo(v):\n  if v not in visited:\n    visited.add(v)\n    for child in v._prev:\n      build_topo(child)\n    topo.append(v)\nbuild_topo(pred)\ntopo\n</pre> topo = [] visited = set() def build_topo(v):   if v not in visited:     visited.add(v)     for child in v._prev:       build_topo(child)     topo.append(v) build_topo(pred) topo Out[18]: <pre>[Value(2.0000),\n Value(-0.7886),\n Value(-1.5772),\n Value(3.0000),\n Value(0.0694),\n Value(0.2081),\n Value(-1.3691),\n Value(0.0000),\n Value(-1.3691),\n Value(0.2028)]</pre> <p>We will integrate topological sort into our <code>Value</code> object and implement complete backward pass. We can also add a simple gradient descent function <code>optimize()</code> which will use this topology. Finally, instead of overriding gradients (<code>=</code>), we will accumulate them (<code>+=</code>) to avoid gradient update bugs when using the same node more than once in an operation. And as a consequence, we will have to reset gradients with <code>zero_()</code> (similar to PyTorch) so that the gradients of different backward passes will not affect each other (it does the exact same thing as <code>self.grad = 0.0</code> was doing before gradient accumulation). Although, to be precise, <code>zero_()</code> function should reset only the gradient of <code>self</code>, and it is actually a function called <code>zero_grad()</code> of <code>optimizer</code> in PyTorch which resets gradients accross all nodes.</p> In\u00a0[\u00a0]: Copied! <pre>class Value:\n  def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self._backward = lambda: None\n    self.grad = 0.0\n    self.requires_grad = requires_grad\n    self.topo = self.build_topo()\n    self.params = [node for node in self.topo if node.requires_grad]\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad += 1.0 * out.grad\n      if other.requires_grad:\n        other.grad += 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad += other.data * out.grad\n      if other.requires_grad:\n        other.grad += self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __sub__(self, other):\n    return self + (Value(-1) * other) # self + (-other)\n\n  def sigmoid(self):\n    s = 1.0 / (1 + np.exp(-self.data))\n    out = Value(s, (self, ), 'sigmoid', self.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad += s * (1.0 - s) * out.grad\n    out._backward = _backward\n\n    return out\n\n  def build_topo(self):\n    # topological order all of the children in the graph\n    topo = []\n    visited = set()\n\n    def _build_topo(node):\n      if node not in visited:\n        visited.add(node)\n        for child in node._prev:\n          _build_topo(child)\n        topo.append(node)\n    _build_topo(self)\n\n    return topo\n\n  def backward(self):\n    if self.requires_grad:\n      self.grad = 1.0\n      for node in reversed(self.params):\n        node._backward()\n\n  def optimize(self, learning_rate=0.01):\n    for node in self.params:\n      node.data -= learning_rate * node.grad\n\n  def zero_(self):\n    self.grad = 0.0\n\n  def zero_grad(self):\n    for node in self.params:\n      node.grad = 0.0\n\n  def __repr__(self):\n    return f'Value({self.data})'\n</pre> class Value:   def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self._backward = lambda: None     self.grad = 0.0     self.requires_grad = requires_grad     self.topo = self.build_topo()     self.params = [node for node in self.topo if node.requires_grad]    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad += 1.0 * out.grad       if other.requires_grad:         other.grad += 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad += other.data * out.grad       if other.requires_grad:         other.grad += self.data * out.grad     out._backward = _backward      return out    def __sub__(self, other):     return self + (Value(-1) * other) # self + (-other)    def sigmoid(self):     s = 1.0 / (1 + np.exp(-self.data))     out = Value(s, (self, ), 'sigmoid', self.requires_grad)      def _backward():       if self.requires_grad:         self.grad += s * (1.0 - s) * out.grad     out._backward = _backward      return out    def build_topo(self):     # topological order all of the children in the graph     topo = []     visited = set()      def _build_topo(node):       if node not in visited:         visited.add(node)         for child in node._prev:           _build_topo(child)         topo.append(node)     _build_topo(self)      return topo    def backward(self):     if self.requires_grad:       self.grad = 1.0       for node in reversed(self.params):         node._backward()    def optimize(self, learning_rate=0.01):     for node in self.params:       node.data -= learning_rate * node.grad    def zero_(self):     self.grad = 0.0    def zero_grad(self):     for node in self.params:       node.grad = 0.0    def __repr__(self):     return f'Value({self.data})' <p>In addition to the <code>_backward()</code> function which calculated the derivatives for only immediate previous nodes, we now have the <code>backward()</code> function which calculates derivates for all the nodes (we also won't forget to set the gradient to <code>1.0</code> in the beginning). Once we plot the graph, pay attention that the input and leaf node gradients which we have no control over are not calculated, thanks to <code>requires_grad</code>.</p> In\u00a0[20]: Copied! <pre>x1 = Value(2, label='x1')\nx2 = Value(5, label='x2')\ny  = Value(1, label= 'y')\n\nn = Neuron()\n\npred = n(x1, x2);      pred.label = 'pred'\nL = y - pred;          L.label = 'loss'\n</pre> x1 = Value(2, label='x1') x2 = Value(5, label='x2') y  = Value(1, label= 'y')  n = Neuron()  pred = n(x1, x2);      pred.label = 'pred' L = y - pred;          L.label = 'loss' In\u00a0[21]: Copied! <pre>L.backward()\ndraw_dot(L)\n</pre> L.backward() draw_dot(L) Out[21]: <p>We can finally implement complete backpropogatation with the goal of increasing the final probability to <code>1.0</code> (decreasing the loss down to zero).</p> In\u00a0[22]: Copied! <pre># gradient descent\nL.optimize()\n\n# forward pass\npred = n(x1, x2);      pred.label = 'pred'\nL = y - pred;          L.label = 'loss'\n\ndraw_dot(L)\n</pre> # gradient descent L.optimize()  # forward pass pred = n(x1, x2);      pred.label = 'pred' L = y - pred;          L.label = 'loss'  draw_dot(L) Out[22]: <p>Let's repeat the backpropagation in multiple epochs until we achieve a minimal loss. We will also print the parameters to see when our neuron function returns a maximum probability for the given input values. And we will make sure to not forget to reset the gradients.</p> In\u00a0[\u00a0]: Copied! <pre>while True:\n  L.zero_grad()\n\n  # backward pass\n  L.backward()\n\n  # gradient descent\n  L.optimize()\n\n  # forward pass\n  pred = n(x1, x2);\n  L = y - pred;\n\n  print(f'Loss {L.data:.4f}')\n\n  if L.data &lt; 0.01:\n    print(f'\\nInputs: {x1} {x2}')\n    print(f'Parameters: {n.w1} {n.w2} {n.b}')\n    print(f'Prediction Probability: {pred.data}')\n    break\n</pre> while True:   L.zero_grad()    # backward pass   L.backward()    # gradient descent   L.optimize()    # forward pass   pred = n(x1, x2);   L = y - pred;    print(f'Loss {L.data:.4f}')    if L.data &lt; 0.01:     print(f'\\nInputs: {x1} {x2}')     print(f'Parameters: {n.w1} {n.w2} {n.b}')     print(f'Prediction Probability: {pred.data}')     break <pre>Loss 0.0148\nLoss 0.0148\nLoss 0.0147\nLoss 0.0147\nLoss 0.0146\nLoss 0.0145\nLoss 0.0145\nLoss 0.0144\nLoss 0.0144\nLoss 0.0143\nLoss 0.0142\nLoss 0.0142\nLoss 0.0141\nLoss 0.0141\nLoss 0.0140\nLoss 0.0139\nLoss 0.0139\nLoss 0.0138\nLoss 0.0138\nLoss 0.0137\nLoss 0.0137\nLoss 0.0136\nLoss 0.0136\nLoss 0.0135\nLoss 0.0134\nLoss 0.0134\nLoss 0.0133\nLoss 0.0133\nLoss 0.0132\nLoss 0.0132\nLoss 0.0131\nLoss 0.0131\nLoss 0.0130\nLoss 0.0130\nLoss 0.0129\nLoss 0.0129\nLoss 0.0128\nLoss 0.0128\nLoss 0.0127\nLoss 0.0127\nLoss 0.0127\nLoss 0.0126\nLoss 0.0126\nLoss 0.0125\nLoss 0.0125\nLoss 0.0124\nLoss 0.0124\nLoss 0.0123\nLoss 0.0123\nLoss 0.0122\nLoss 0.0122\nLoss 0.0122\nLoss 0.0121\nLoss 0.0121\nLoss 0.0120\nLoss 0.0120\nLoss 0.0119\nLoss 0.0119\nLoss 0.0119\nLoss 0.0118\nLoss 0.0118\nLoss 0.0117\nLoss 0.0117\nLoss 0.0117\nLoss 0.0116\nLoss 0.0116\nLoss 0.0115\nLoss 0.0115\nLoss 0.0115\nLoss 0.0114\nLoss 0.0114\nLoss 0.0113\nLoss 0.0113\nLoss 0.0113\nLoss 0.0112\nLoss 0.0112\nLoss 0.0112\nLoss 0.0111\nLoss 0.0111\nLoss 0.0111\nLoss 0.0110\nLoss 0.0110\nLoss 0.0109\nLoss 0.0109\nLoss 0.0109\nLoss 0.0108\nLoss 0.0108\nLoss 0.0108\nLoss 0.0107\nLoss 0.0107\nLoss 0.0107\nLoss 0.0106\nLoss 0.0106\nLoss 0.0106\nLoss 0.0105\nLoss 0.0105\nLoss 0.0105\nLoss 0.0104\nLoss 0.0104\nLoss 0.0104\nLoss 0.0103\nLoss 0.0103\nLoss 0.0103\nLoss 0.0103\nLoss 0.0102\nLoss 0.0102\nLoss 0.0102\nLoss 0.0101\nLoss 0.0101\nLoss 0.0101\nLoss 0.0100\nLoss 0.0100\nLoss 0.0100\n\nInputs: Value(2) Value(5)\nParameters: Value(0.15289672859342332) Value(0.8555124993367387) Value(0.013701999952082168)\nPrediction Probability: 0.9900191690162561\n</pre> <p>We have just now trained our 2-dimensional input neuron to find suitable parameter values for achieving a maximum probability for input values <code>x1</code> and <code>x2</code>. Now we would like to create N-dimensional neuron which will accept much more inputs, similar to what we saw in the illustration of artificial neuron: <code>x<sub>1</sub> ... x<sub>n</sub></code>. As a consequence, our neuron will have to learn the parameter values for N-dimensional weights <code>w<sub>1</sub> ... w<sub>n</sub></code>.</p> In\u00a0[24]: Copied! <pre>class Neuron:\n  def __init__(self, N):\n    self.W = [Value(np.random.uniform(-1, 1), label=f'w{i}', requires_grad=True) for i in range(N)]\n    self.b = Value(0, label='b', requires_grad=True)\n\n  def __call__(self, X):\n    out = sum((x * w for x, w in zip(X, self.W)), self.b)\n    return out.sigmoid()\n</pre> class Neuron:   def __init__(self, N):     self.W = [Value(np.random.uniform(-1, 1), label=f'w{i}', requires_grad=True) for i in range(N)]     self.b = Value(0, label='b', requires_grad=True)    def __call__(self, X):     out = sum((x * w for x, w in zip(X, self.W)), self.b)     return out.sigmoid() <p>We will now see the training output of our N-dimensional neuron which will accept N <code>Value</code> inputs as a list. Note that our <code>Neuron</code> which implements <code>sigmoid</code> (logistic) activation is known as Logistic Regression.</p> In\u00a0[25]: Copied! <pre>X = [Value(x, label=f'x{i}') for i, x in enumerate([5, 0.4, -1, -2])]\n\nn = Neuron(len(X))\n\npred = n(X);           pred.label = 'pred'\nL = y - pred;          L.label = 'loss'\n</pre> X = [Value(x, label=f'x{i}') for i, x in enumerate([5, 0.4, -1, -2])]  n = Neuron(len(X))  pred = n(X);           pred.label = 'pred' L = y - pred;          L.label = 'loss' In\u00a0[26]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[26]: In\u00a0[\u00a0]: Copied! <pre>while True:\n  L.zero_grad()\n\n  # backward pass\n  L.backward()\n\n  # gradient descent\n  L.optimize()\n\n  # forward pass\n  pred = n(X)\n  L = y - pred\n\n  print(f'Loss {L.data:.4f}')\n\n  if L.data &lt; 0.01:\n    print(f'\\nInputs: {X}')\n    print(f'Parameters: {n.W} {n.b}')\n    print(f'Prediction Probability: {pred.data}')\n    break\n</pre> while True:   L.zero_grad()    # backward pass   L.backward()    # gradient descent   L.optimize()    # forward pass   pred = n(X)   L = y - pred    print(f'Loss {L.data:.4f}')    if L.data &lt; 0.01:     print(f'\\nInputs: {X}')     print(f'Parameters: {n.W} {n.b}')     print(f'Prediction Probability: {pred.data}')     break <pre>Loss 0.1273\nLoss 0.1235\nLoss 0.1199\nLoss 0.1165\nLoss 0.1132\nLoss 0.1101\nLoss 0.1072\nLoss 0.1043\nLoss 0.1016\nLoss 0.0991\nLoss 0.0966\nLoss 0.0943\nLoss 0.0920\nLoss 0.0899\nLoss 0.0878\nLoss 0.0858\nLoss 0.0839\nLoss 0.0821\nLoss 0.0804\nLoss 0.0787\nLoss 0.0770\nLoss 0.0755\nLoss 0.0740\nLoss 0.0725\nLoss 0.0711\nLoss 0.0698\nLoss 0.0685\nLoss 0.0672\nLoss 0.0660\nLoss 0.0648\nLoss 0.0637\nLoss 0.0626\nLoss 0.0615\nLoss 0.0605\nLoss 0.0595\nLoss 0.0585\nLoss 0.0576\nLoss 0.0567\nLoss 0.0558\nLoss 0.0549\nLoss 0.0541\nLoss 0.0533\nLoss 0.0525\nLoss 0.0517\nLoss 0.0510\nLoss 0.0503\nLoss 0.0496\nLoss 0.0489\nLoss 0.0482\nLoss 0.0476\nLoss 0.0469\nLoss 0.0463\nLoss 0.0457\nLoss 0.0451\nLoss 0.0445\nLoss 0.0440\nLoss 0.0434\nLoss 0.0429\nLoss 0.0424\nLoss 0.0419\nLoss 0.0414\nLoss 0.0409\nLoss 0.0404\nLoss 0.0399\nLoss 0.0395\nLoss 0.0390\nLoss 0.0386\nLoss 0.0382\nLoss 0.0378\nLoss 0.0373\nLoss 0.0369\nLoss 0.0366\nLoss 0.0362\nLoss 0.0358\nLoss 0.0354\nLoss 0.0351\nLoss 0.0347\nLoss 0.0344\nLoss 0.0340\nLoss 0.0337\nLoss 0.0334\nLoss 0.0330\nLoss 0.0327\nLoss 0.0324\nLoss 0.0321\nLoss 0.0318\nLoss 0.0315\nLoss 0.0312\nLoss 0.0309\nLoss 0.0307\nLoss 0.0304\nLoss 0.0301\nLoss 0.0298\nLoss 0.0296\nLoss 0.0293\nLoss 0.0291\nLoss 0.0288\nLoss 0.0286\nLoss 0.0283\nLoss 0.0281\nLoss 0.0279\nLoss 0.0277\nLoss 0.0274\nLoss 0.0272\nLoss 0.0270\nLoss 0.0268\nLoss 0.0266\nLoss 0.0264\nLoss 0.0262\nLoss 0.0260\nLoss 0.0258\nLoss 0.0256\nLoss 0.0254\nLoss 0.0252\nLoss 0.0250\nLoss 0.0248\nLoss 0.0246\nLoss 0.0244\nLoss 0.0243\nLoss 0.0241\nLoss 0.0239\nLoss 0.0238\nLoss 0.0236\nLoss 0.0234\nLoss 0.0233\nLoss 0.0231\nLoss 0.0229\nLoss 0.0228\nLoss 0.0226\nLoss 0.0225\nLoss 0.0223\nLoss 0.0222\nLoss 0.0220\nLoss 0.0219\nLoss 0.0217\nLoss 0.0216\nLoss 0.0215\nLoss 0.0213\nLoss 0.0212\nLoss 0.0211\nLoss 0.0209\nLoss 0.0208\nLoss 0.0207\nLoss 0.0205\nLoss 0.0204\nLoss 0.0203\nLoss 0.0202\nLoss 0.0200\nLoss 0.0199\nLoss 0.0198\nLoss 0.0197\nLoss 0.0196\nLoss 0.0195\nLoss 0.0193\nLoss 0.0192\nLoss 0.0191\nLoss 0.0190\nLoss 0.0189\nLoss 0.0188\nLoss 0.0187\nLoss 0.0186\nLoss 0.0185\nLoss 0.0184\nLoss 0.0183\nLoss 0.0182\nLoss 0.0181\nLoss 0.0180\nLoss 0.0179\nLoss 0.0178\nLoss 0.0177\nLoss 0.0176\nLoss 0.0175\nLoss 0.0174\nLoss 0.0173\nLoss 0.0172\nLoss 0.0172\nLoss 0.0171\nLoss 0.0170\nLoss 0.0169\nLoss 0.0168\nLoss 0.0167\nLoss 0.0166\nLoss 0.0166\nLoss 0.0165\nLoss 0.0164\nLoss 0.0163\nLoss 0.0162\nLoss 0.0161\nLoss 0.0161\nLoss 0.0160\nLoss 0.0159\nLoss 0.0158\nLoss 0.0158\nLoss 0.0157\nLoss 0.0156\nLoss 0.0155\nLoss 0.0155\nLoss 0.0154\nLoss 0.0153\nLoss 0.0153\nLoss 0.0152\nLoss 0.0151\nLoss 0.0150\nLoss 0.0150\nLoss 0.0149\nLoss 0.0148\nLoss 0.0148\nLoss 0.0147\nLoss 0.0146\nLoss 0.0146\nLoss 0.0145\nLoss 0.0145\nLoss 0.0144\nLoss 0.0143\nLoss 0.0143\nLoss 0.0142\nLoss 0.0141\nLoss 0.0141\nLoss 0.0140\nLoss 0.0140\nLoss 0.0139\nLoss 0.0138\nLoss 0.0138\nLoss 0.0137\nLoss 0.0137\nLoss 0.0136\nLoss 0.0136\nLoss 0.0135\nLoss 0.0134\nLoss 0.0134\nLoss 0.0133\nLoss 0.0133\nLoss 0.0132\nLoss 0.0132\nLoss 0.0131\nLoss 0.0131\nLoss 0.0130\nLoss 0.0130\nLoss 0.0129\nLoss 0.0129\nLoss 0.0128\nLoss 0.0128\nLoss 0.0127\nLoss 0.0127\nLoss 0.0126\nLoss 0.0126\nLoss 0.0125\nLoss 0.0125\nLoss 0.0124\nLoss 0.0124\nLoss 0.0123\nLoss 0.0123\nLoss 0.0122\nLoss 0.0122\nLoss 0.0122\nLoss 0.0121\nLoss 0.0121\nLoss 0.0120\nLoss 0.0120\nLoss 0.0119\nLoss 0.0119\nLoss 0.0118\nLoss 0.0118\nLoss 0.0118\nLoss 0.0117\nLoss 0.0117\nLoss 0.0116\nLoss 0.0116\nLoss 0.0116\nLoss 0.0115\nLoss 0.0115\nLoss 0.0114\nLoss 0.0114\nLoss 0.0114\nLoss 0.0113\nLoss 0.0113\nLoss 0.0112\nLoss 0.0112\nLoss 0.0112\nLoss 0.0111\nLoss 0.0111\nLoss 0.0110\nLoss 0.0110\nLoss 0.0110\nLoss 0.0109\nLoss 0.0109\nLoss 0.0109\nLoss 0.0108\nLoss 0.0108\nLoss 0.0108\nLoss 0.0107\nLoss 0.0107\nLoss 0.0107\nLoss 0.0106\nLoss 0.0106\nLoss 0.0106\nLoss 0.0105\nLoss 0.0105\nLoss 0.0104\nLoss 0.0104\nLoss 0.0104\nLoss 0.0104\nLoss 0.0103\nLoss 0.0103\nLoss 0.0103\nLoss 0.0102\nLoss 0.0102\nLoss 0.0102\nLoss 0.0101\nLoss 0.0101\nLoss 0.0101\nLoss 0.0100\nLoss 0.0100\nLoss 0.0100\n\nInputs: [Value(5), Value(0.4), Value(-1), Value(-2)]\nParameters: [Value(0.6195076510193583), Value(0.988462556700058), Value(0.6814715991439656), Value(-0.8498024629867303)] Value(0.08692061974641961)\nPrediction Probability: 0.9900282484345291\n</pre> <p>Glosser.ca - Own work, Derivative of Artificial neural network.svg (CC BY-SA 3.0 | Wikimedia Commons)</p> <p></p> <p>We managed to train our single neuron to learn a function for our input values. In reality, however, data is much more complex and we need to learn more complication functions. How to achieve that? By chaining many neurons together, similar to biological neuron. Each neuron will basically learn some portion of the overall function.</p> <p>What we see above is an illustration of an artificial neural network. In the input layer we have three neurons, each separately accepting N-dimensional input values. The output values of each neuron are then fully connected, as inputs to the hidden layer with four neurons (note that there can be more than one hidden layer). And finally, the output of hidden layer neurons are passed as inputs to the output layer, which may, for example, predict probability scores for two classes.</p> <p>We will now try to implement a fully connected feedforward neural network, which is often referred to as Multi-Layer Perceptron (MLP).</p> In\u00a0[28]: Copied! <pre>class Layer:\n  def __init__(self, N, count):\n    self.neurons = [Neuron(N) for _ in range(count)]\n\n  def __call__(self, X):\n    outs = [n(X) for n in self.neurons]\n    return outs[0] if len(outs) == 1 else outs # flattening dimension if a single element\n</pre> class Layer:   def __init__(self, N, count):     self.neurons = [Neuron(N) for _ in range(count)]    def __call__(self, X):     outs = [n(X) for n in self.neurons]     return outs[0] if len(outs) == 1 else outs # flattening dimension if a single element <p>The code above creates a list of <code>count</code> number of neurons, each accepting <code>N</code> dimensional input. Let's build our layers shown in the illustration above and connect them. Note that the input dimension of the next layer is the amount of neurons in the previous layer.</p> In\u00a0[29]: Copied! <pre># input data and its dimension\nX = [Value(x, label=f'x{i}') for i, x in enumerate([1, 4, -3, -2, 3])]\nN = len(X)\n</pre> # input data and its dimension X = [Value(x, label=f'x{i}') for i, x in enumerate([1, 4, -3, -2, 3])] N = len(X) In\u00a0[30]: Copied! <pre># creating layers\nin_layer = Layer(N, 3)\nhid_layer = Layer(3, 4)\nout_layer = Layer(4, 2)\n</pre> # creating layers in_layer = Layer(N, 3) hid_layer = Layer(3, 4) out_layer = Layer(4, 2) In\u00a0[31]: Copied! <pre># output of each layer is input to the next\nX_hidden = in_layer(X)\nX_output = hid_layer(X_hidden)\nout = out_layer(X_output)\n</pre> # output of each layer is input to the next X_hidden = in_layer(X) X_output = hid_layer(X_hidden) out = out_layer(X_output) In\u00a0[32]: Copied! <pre># let's plot either one of the outputs\ndraw_dot(out[0])\n</pre> # let's plot either one of the outputs draw_dot(out[0]) Out[32]: <p>We will further abstract away the neuron and layer creation inside the <code>MLP</code> class. We will then reimplement the exact same network.</p> In\u00a0[33]: Copied! <pre>class MLP:\n  def __init__(self, N, counts):\n    dims = [N] + counts # concatenates dimensions\n    self.layers = [Layer(dims[i], dims[i+1]) for i in range(len(dims)-1)]\n\n  def __call__(self, X):\n    out = X\n    for layer in self.layers:\n      out = layer(out)\n    return out\n</pre> class MLP:   def __init__(self, N, counts):     dims = [N] + counts # concatenates dimensions     self.layers = [Layer(dims[i], dims[i+1]) for i in range(len(dims)-1)]    def __call__(self, X):     out = X     for layer in self.layers:       out = layer(out)     return out In\u00a0[34]: Copied! <pre>nn = MLP(N, [3, 4, 2])\nout = nn(X)\ndraw_dot(out[0]) # out[1] will return the second output\n</pre> nn = MLP(N, [3, 4, 2]) out = nn(X) draw_dot(out[0]) # out[1] will return the second output Out[34]: <p>It is time to judge our network by applying it to the real dataset. Even though applying neural networks to Fisher's Iris dataset is a little overkill (as the dataset is simple), it will be a nice demonstration of our MLP's capacity.</p> <p>Iris dataset has 4-dimensional input samples with three possible output classes. We should be able to predict the class of the Iris flower based on the width and height values of its two elements. We will load our dataset and split it into train and test sets.</p> In\u00a0[35]: Copied! <pre>from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\n\nX = iris.data  # 50x3 4-dimensional samples\ny = iris.target # 3 classes (0, 1, 2)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nprint(f'Train data shape: {X_train.shape}, {y_train.shape}')\nprint(f'Test data shape: {X_test.shape}, {y_test.shape}')\nprint(f'Input Samples:\\n {X_train[:5]}')\nprint(f'Labels:\\n {y_train[:5]}')\n</pre> from sklearn import datasets from sklearn.model_selection import train_test_split  iris = datasets.load_iris()  X = iris.data  # 50x3 4-dimensional samples y = iris.target # 3 classes (0, 1, 2)  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  print(f'Train data shape: {X_train.shape}, {y_train.shape}') print(f'Test data shape: {X_test.shape}, {y_test.shape}') print(f'Input Samples:\\n {X_train[:5]}') print(f'Labels:\\n {y_train[:5]}') <pre>Train data shape: (120, 4), (120,)\nTest data shape: (30, 4), (30,)\nInput Samples:\n [[4.4 2.9 1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [6.5 3.  5.5 1.8]\n [6.4 3.1 5.5 1.8]\n [6.3 2.5 5.  1.9]]\nLabels:\n [0 0 2 2 2]\n</pre> <p>We should then convert each element to be a <code>Value</code> object. But before that, let's try out two ready scikit-learn classifiers, <code>LogisticRegression</code> and <code>MLPClassifier</code>, the latter of which can be seen as the extension of the former, which we will implement in its simplistic form. As we have already discussed, <code>LogisticRegression</code> is basically our <code>Neuron</code> class which uses <code>sigmoid</code> (logistic) function as its activation. And in fact, Logistic Regression will simply be our MLP with the layer size for just a single neuron. Thanks to <code>numpy</code> vectorization and other optimizations, the <code>sklearn</code> implementations will be extremely quick.</p> In\u00a0[36]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\naccuracy = accuracy_score(y_test, preds)\nprint(f\"Logistic Regression Accuracy: {accuracy:.2f}\")\n\nmodel = MLPClassifier()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\naccuracy = accuracy_score(y_test, preds)\nprint(f\"MLP Classifier Accuracy: {accuracy:.2f}\")\n</pre> from sklearn.linear_model import LogisticRegression from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score  model = LogisticRegression() model.fit(X_train, y_train) preds = model.predict(X_test) accuracy = accuracy_score(y_test, preds) print(f\"Logistic Regression Accuracy: {accuracy:.2f}\")  model = MLPClassifier() model.fit(X_train, y_train) preds = model.predict(X_test) accuracy = accuracy_score(y_test, preds) print(f\"MLP Classifier Accuracy: {accuracy:.2f}\") <pre>Logistic Regression Accuracy: 1.00\nMLP Classifier Accuracy: 1.00\n</pre> <pre>/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n</pre> <p>When converting labels to <code>Value</code> objects, we can technically scale them down to be between <code>0</code> and <code>1</code> (by multiplying them to <code>0.5</code>). Then we can apply the simple Mean Squared Error (MSE). Because there are only three labels and the dataset is small, it will work, yet it will penalize ordinal labels unnecessarily. For example, our loss should give the same penalty if we predict <code>0</code> instead of <code>1</code> or <code>2</code>  (it is either one type of flower or another). MSE, however, will give more penalty when predicting <code>0</code> instead of <code>2</code> and less penalty when predicting <code>0</code> instead of <code>1</code> because of the imaginary distances between numbers, which do not exist among the real labels.</p> In\u00a0[37]: Copied! <pre># converting numpy float arrays into Value lists\n# scaling labels to be between 0 and 1 to simplify our code\n# again, it is not a right way to treat the labels\nX_train = [[Value(x) for x in X] for X in X_train]\nX_test = [[Value(x) for x in X] for X in X_test]\ny_train = [Value(y) * Value(0.5) for y in y_train]\ny_test = [Value(y) * Value(0.5) for y in y_test]\n</pre> # converting numpy float arrays into Value lists # scaling labels to be between 0 and 1 to simplify our code # again, it is not a right way to treat the labels X_train = [[Value(x) for x in X] for X in X_train] X_test = [[Value(x) for x in X] for X in X_test] y_train = [Value(y) * Value(0.5) for y in y_train] y_test = [Value(y) * Value(0.5) for y in y_test] <p>We will now create a class for our model, in the style of <code>scikit-learn</code> models, more specifically MLPClassifier.</p> <p>As noted above, what we do is simple and will work in this case, but is not right. Ideally, we should initially one hot encode the ground truth labels to describe them with only zeros and ones. Our final layer for multiclass classification, instead of <code>sigmoid</code> activation, should output <code>logits</code> (unprocessed predictions) passing through <code>softmax</code> function. The loss in this case should be Cross Entropy instead of MSE.</p> <p>Exercise (Advanced): Write code for the correct implementation noted above. See Josh Starmer's (StatQuest) videos which explain its theory, including the Iris dataset, argmax and softmax functions, as well as Cross Entropy.</p> In\u00a0[\u00a0]: Copied! <pre>class Classifier:\n  def __init__(self, layer_sizes=[2, 3, 1]):\n    self.layer_sizes = layer_sizes\n    self.nn = None\n    self.L = None\n    self.iterations = 0\n\n  def forward(self, Xs):\n    out = [self.nn(X) for X in Xs]\n    return out\n\n  def predict(self, X_test):\n    return self.forward(X_test)\n\n  def train(self, X_train, y_train, learning_rate=0.01):\n    preds = self.forward(X_train)\n    self.L = self.mean_squared_error(y_train, preds)\n    self.L.zero_grad()\n    self.L.backward()\n    self.L.optimize(learning_rate=learning_rate)\n    print(f'Loss: {self.L.data:.4f}')\n\n  def fit(self, X_train, y_train, learning_rate=0.01, num_epochs=50):\n    if not self.nn: # in order to not restart training if nn exists\n      self.nn = MLP(len(X_train), self.layer_sizes)\n    for i in range(num_epochs):\n      print(f'Training epoch {self.iterations + i + 1}')\n      self.train(X_train, y_train, learning_rate)\n    self.iterations += i + 1\n\n  def mean_squared_error(self, y_train, preds):\n    return sum([(y-y_hat)*(y-y_hat) for y, y_hat in zip(y_train, preds)], Value(0))\n\n  def score(self, y_test, preds):\n    return self.mean_squared_error(y_test, preds).data / len(y_test)\n\n  def accuracy_score(self, y_test, preds):\n    # due to incorrect handling of the labels\n    # we need to scale y_test and preds values back\n    y_test = [y * Value(2) for y in y_test]\n    preds = [y_hat * Value(2) for y_hat in preds]\n    correct = sum(1 for y, y_hat in zip(y_test, preds) if round(y_hat.data) == y.data)\n    total = len(y_test)\n    return (correct/total)\n</pre> class Classifier:   def __init__(self, layer_sizes=[2, 3, 1]):     self.layer_sizes = layer_sizes     self.nn = None     self.L = None     self.iterations = 0    def forward(self, Xs):     out = [self.nn(X) for X in Xs]     return out    def predict(self, X_test):     return self.forward(X_test)    def train(self, X_train, y_train, learning_rate=0.01):     preds = self.forward(X_train)     self.L = self.mean_squared_error(y_train, preds)     self.L.zero_grad()     self.L.backward()     self.L.optimize(learning_rate=learning_rate)     print(f'Loss: {self.L.data:.4f}')    def fit(self, X_train, y_train, learning_rate=0.01, num_epochs=50):     if not self.nn: # in order to not restart training if nn exists       self.nn = MLP(len(X_train), self.layer_sizes)     for i in range(num_epochs):       print(f'Training epoch {self.iterations + i + 1}')       self.train(X_train, y_train, learning_rate)     self.iterations += i + 1    def mean_squared_error(self, y_train, preds):     return sum([(y-y_hat)*(y-y_hat) for y, y_hat in zip(y_train, preds)], Value(0))    def score(self, y_test, preds):     return self.mean_squared_error(y_test, preds).data / len(y_test)    def accuracy_score(self, y_test, preds):     # due to incorrect handling of the labels     # we need to scale y_test and preds values back     y_test = [y * Value(2) for y in y_test]     preds = [y_hat * Value(2) for y_hat in preds]     correct = sum(1 for y, y_hat in zip(y_test, preds) if round(y_hat.data) == y.data)     total = len(y_test)     return (correct/total) <p>Our naive classifer is ready and we can now train our model and note the accuracy. However, unlike the optimized classifiers of the <code>sklearn</code> library, it will be much slower and inefficient. Try out experiments with different layer sizes and learning rates, and notice how it affects the training process and loss. As we have mentioned before, we can implement Logistic Regression by simply passing <code>layer_sizes=[1]</code> to our MLP classifier.</p> In\u00a0[45]: Copied! <pre>model = Classifier([1])\n# model = Classifier([4, 1])\n</pre> model = Classifier([1]) # model = Classifier([4, 1]) In\u00a0[48]: Copied! <pre>model.fit(X_train, y_train, learning_rate=0.002, num_epochs=30)\n</pre> model.fit(X_train, y_train, learning_rate=0.002, num_epochs=30) <pre>Training epoch 61\nLoss: 1.3948\nTraining epoch 62\nLoss: 1.3938\nTraining epoch 63\nLoss: 1.3929\nTraining epoch 64\nLoss: 1.3920\nTraining epoch 65\nLoss: 1.3911\nTraining epoch 66\nLoss: 1.3901\nTraining epoch 67\nLoss: 1.3892\nTraining epoch 68\nLoss: 1.3883\nTraining epoch 69\nLoss: 1.3874\nTraining epoch 70\nLoss: 1.3865\nTraining epoch 71\nLoss: 1.3856\nTraining epoch 72\nLoss: 1.3847\nTraining epoch 73\nLoss: 1.3838\nTraining epoch 74\nLoss: 1.3830\nTraining epoch 75\nLoss: 1.3821\nTraining epoch 76\nLoss: 1.3812\nTraining epoch 77\nLoss: 1.3803\nTraining epoch 78\nLoss: 1.3795\nTraining epoch 79\nLoss: 1.3786\nTraining epoch 80\nLoss: 1.3777\nTraining epoch 81\nLoss: 1.3769\nTraining epoch 82\nLoss: 1.3760\nTraining epoch 83\nLoss: 1.3752\nTraining epoch 84\nLoss: 1.3743\nTraining epoch 85\nLoss: 1.3735\nTraining epoch 86\nLoss: 1.3726\nTraining epoch 87\nLoss: 1.3718\nTraining epoch 88\nLoss: 1.3710\nTraining epoch 89\nLoss: 1.3701\nTraining epoch 90\nLoss: 1.3693\n</pre> In\u00a0[49]: Copied! <pre>preds = model.predict(X_train)\nprint(f'Custom MLP classifier accuracy on train Data: {model.accuracy_score(y_train, preds):.2f}')\n</pre> preds = model.predict(X_train) print(f'Custom MLP classifier accuracy on train Data: {model.accuracy_score(y_train, preds):.2f}') <pre>Custom MLP classifier accuracy on train Data: 0.97\n</pre> In\u00a0[50]: Copied! <pre>preds = model.predict(X_test)\nprint(f'Custom MLP classifier accuracy on test Data: {model.accuracy_score(y_test, preds):.2f}')\n</pre> preds = model.predict(X_test) print(f'Custom MLP classifier accuracy on test Data: {model.accuracy_score(y_test, preds):.2f}') <pre>Custom MLP classifier accuracy on test Data: 0.97\n</pre> In\u00a0[51]: Copied! <pre>for i in range(len(y_test)):\n  print(y_test[i].data * 2, preds[i].data * 2)\n</pre> for i in range(len(y_test)):   print(y_test[i].data * 2, preds[i].data * 2) <pre>2.0 1.7378279677947062\n1.0 1.3786491598121862\n2.0 1.7797233124612888\n2.0 1.5546211532302696\n1.0 0.7808593322180695\n1.0 1.1323777346177633\n0.0 0.03179964376674919\n2.0 1.6900839613983492\n2.0 1.818651271834627\n2.0 1.5712929781499119\n2.0 1.9175600703929185\n2.0 1.8478802623012778\n2.0 1.8502101169387906\n0.0 0.024109593550568645\n1.0 1.6944624014543954\n1.0 1.1691956406120396\n2.0 1.6617236866912894\n2.0 1.7796042897673732\n1.0 1.186491156964572\n2.0 1.7267232886353379\n1.0 1.197352283588582\n0.0 0.01104177713999268\n2.0 1.860650974045892\n0.0 0.029735232803150373\n1.0 1.021880542808286\n1.0 0.7892688298891827\n0.0 0.026875629893556307\n1.0 1.219772374852777\n0.0 0.015342452967957023\n2.0 1.5336924121298994\n</pre>"},{"location":"notebooks/02_neural_network/#02-from-neuron-to-neural-network","title":"02. From Neuron to Neural Network\u00b6","text":""},{"location":"notebooks/02_neural_network/#recall-backpropagation","title":"Recall: Backpropagation\u00b6","text":""},{"location":"notebooks/02_neural_network/#activation-function","title":"Activation Function\u00b6","text":""},{"location":"notebooks/02_neural_network/#artificial-neuron","title":"Artificial Neuron\u00b6","text":""},{"location":"notebooks/02_neural_network/#n-dimensional-neuron","title":"N-dimensional Neuron\u00b6","text":""},{"location":"notebooks/02_neural_network/#artificial-neural-network","title":"Artificial Neural Network\u00b6","text":""},{"location":"notebooks/02_neural_network/#iris-dataset","title":"Iris Dataset\u00b6","text":""},{"location":"notebooks/02_neural_network/#training-custom-mlp-classifier","title":"Training Custom MLP Classifier\u00b6","text":""}]}