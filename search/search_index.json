{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to CSCI 4701!","text":"<p>This is the website of the CSCI 4701: Deep Learning course I teach at ADA University. See the course syllabus for the current semester in navigation bar.</p>"},{"location":"syllabus/","title":"Spring 2026","text":"<p>Important</p> <p>test</p>"},{"location":"Main_Content/01_backprop/","title":"01 Python Code from Derivatives to Backpropagation","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[2]: Copied! <pre>def f(x):\n  return x**2\n</pre> def f(x):   return x**2 In\u00a0[3]: Copied! <pre>x = 3.0\nfor h in [10, 1, 0.1, 0]:\n  print(f\"If we shift input by {h}, output becomes {f(x+h)}\")\n</pre> x = 3.0 for h in [10, 1, 0.1, 0]:   print(f\"If we shift input by {h}, output becomes {f(x+h)}\") <pre>If we shift input by 10, output becomes 169.0\nIf we shift input by 1, output becomes 16.0\nIf we shift input by 0.1, output becomes 9.610000000000001\nIf we shift input by 0, output becomes 9.0\n</pre> In\u00a0[4]: Copied! <pre>h = 1.0\n\ndx = h\ndy = f(x+h) - f(x)\n\nprint(f\"\u0394x: {dx}\")\nprint(f\"\u0394y: {dy}\")\nprint(f\"When you change x by {dx} unit, y changes by {dy} units.\")\n</pre> h = 1.0  dx = h dy = f(x+h) - f(x)  print(f\"\u0394x: {dx}\") print(f\"\u0394y: {dy}\") print(f\"When you change x by {dx} unit, y changes by {dy} units.\") <pre>\u0394x: 1.0\n\u0394y: 7.0\nWhen you change x by 1.0 unit, y changes by 7.0 units.\n</pre> In\u00a0[5]: Copied! <pre>def plot_delta(x, h, start=-4, stop=4, num=30):\n  # `np.linspace` returns an array of num inputs within a range.\n  x_all = np.linspace(start, stop, num)\n  y_all = f(x_all)\n\n  plt.figure(figsize=(4, 4))\n  plt.plot(x_all, y_all)\n\n  # dx &amp; dy\n  plt.plot([x, x + h], [f(x), f(x)], color='r')\n  plt.plot([x + h, x + h], [f(x), f(x + h)], color='r')\n</pre> def plot_delta(x, h, start=-4, stop=4, num=30):   # `np.linspace` returns an array of num inputs within a range.   x_all = np.linspace(start, stop, num)   y_all = f(x_all)    plt.figure(figsize=(4, 4))   plt.plot(x_all, y_all)    # dx &amp; dy   plt.plot([x, x + h], [f(x), f(x)], color='r')   plt.plot([x + h, x + h], [f(x), f(x + h)], color='r') In\u00a0[6]: Copied! <pre>plot_delta(x=2, h=1)\n</pre> plot_delta(x=2, h=1) <p>How to find if the ouput changes significantly when we change the input by some amount h?</p> In\u00a0[7]: Copied! <pre>def plot_roc(x, h):\n  dx = h\n  dy = f(x + h) - f(x)\n\n  plot_delta(x, h)\n  print(f\"Rate of change is {dy / dx}\")\n</pre> def plot_roc(x, h):   dx = h   dy = f(x + h) - f(x)    plot_delta(x, h)   print(f\"Rate of change is {dy / dx}\") In\u00a0[8]: Copied! <pre>plot_roc(3, 1)\n</pre> plot_roc(3, 1) <pre>Rate of change is 7.0\n</pre> In\u00a0[9]: Copied! <pre>plot_roc(3, 0.5)\n</pre> plot_roc(3, 0.5) <pre>Rate of change is 6.5\n</pre> In\u00a0[10]: Copied! <pre>plot_roc(1, 1)\n</pre> plot_roc(1, 1) <pre>Rate of change is 3.0\n</pre> In\u00a0[11]: Copied! <pre>plot_roc(-2, 0.5)\n</pre> plot_roc(-2, 0.5) <pre>Rate of change is -3.5\n</pre> <p>The rate of change for different values of h are different at the same point x. We would like to come up with a single value that would tell how significantly y changes at a given point x within the function (<code>a</code> in the formula corresponds to <code>x</code> in the code).</p> <p></p> <p>Simply, this limit tells us how much the value of y will change when we change x by just a very small amount. Note: Essentially, derivative is a function.</p> In\u00a0[12]: Copied! <pre>x = 3\nh = 0.000001 # limit of h approaches 0\nd = (f(x + h) - f(x)) / h\nprint(f\"The value of derivative function is {d}\")\n</pre> x = 3 h = 0.000001 # limit of h approaches 0 d = (f(x + h) - f(x)) / h print(f\"The value of derivative function is {d}\") <pre>The value of derivative function is 6.000001000927568\n</pre> <p>Partial derivative with respect to some variable basically means how much the output will change when we nudge that variable by a very small amount.</p> In\u00a0[13]: Copied! <pre>f = lambda x, y: x + y\n</pre> f = lambda x, y: x + y In\u00a0[14]: Copied! <pre>x = 2\ny = 3\n\nf(x, y)\n</pre> x = 2 y = 3  f(x, y) Out[14]: <pre>5</pre> In\u00a0[15]: Copied! <pre>h = 0.000001\nf(x + h, y)\n</pre> h = 0.000001 f(x + h, y) Out[15]: <pre>5.000001</pre> <p>Let\u2019s see partial derivatives with respect to x an y.</p> In\u00a0[16]: Copied! <pre># wrt x\n(f(x + h, y) - f(x, y)) / h\n</pre> # wrt x (f(x + h, y) - f(x, y)) / h Out[16]: <pre>1.000000000139778</pre> In\u00a0[17]: Copied! <pre># wrt y\n(f(x, y+h) - f(x, y)) / h\n</pre> # wrt y (f(x, y+h) - f(x, y)) / h Out[17]: <pre>1.000000000139778</pre> <p>It will always approach 1 for addition, no matter what are the input values.</p> In\u00a0[18]: Copied! <pre>for x, y in zip([-20, 2, 3], [300, 75, 10]):\n  print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}')\n</pre> for x, y in zip([-20, 2, 3], [300, 75, 10]):   print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}') <pre>x=-20, y=300: 0.9999999974752427\nx=2, y=75: 0.9999999974752427\nx=3, y=10: 1.0000000010279564\n</pre> <p>Indeed, if we have simple addition x + y, then increasing x or y by some amount will increase the result by the exact same amount. Assertion will work for any number h gets.</p> In\u00a0[19]: Copied! <pre>h = 10\nassert f(x+h, y) - f(x, y) == h\nassert f(x, y+h) - f(x, y) == h\n</pre> h = 10 assert f(x+h, y) - f(x, y) == h assert f(x, y+h) - f(x, y) == h In\u00a0[20]: Copied! <pre>f = lambda x, y: x * y\n</pre> f = lambda x, y: x * y In\u00a0[21]: Copied! <pre>x = 2\ny = 3\nh = 1e-5 # same as 0.00001\n(f(x + h, y) - f(x, y)) / h # wrt x\n</pre> x = 2 y = 3 h = 1e-5 # same as 0.00001 (f(x + h, y) - f(x, y)) / h # wrt x Out[21]: <pre>3.000000000064062</pre> In\u00a0[22]: Copied! <pre>for x in [-20, 2, 3]:\n  print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}')\n</pre> for x in [-20, 2, 3]:   print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}') <pre>x=-20, y=3: 2.999999999531155\nx=2, y=3: 3.000000000064062\nx=3, y=3: 3.000000000064062\n</pre> In\u00a0[23]: Copied! <pre>x = 10\nh = 5\npdx = (f(x+h, y) - f(x, y)) / h\nprint(pdx, y)\nassert round(pdx, 2) == round(y, 2)\n</pre> x = 10 h = 5 pdx = (f(x+h, y) - f(x, y)) / h print(pdx, y) assert round(pdx, 2) == round(y, 2) <pre>3.0 3\n</pre> In\u00a0[24]: Copied! <pre>def f(a, b, c):\n  return a**2 + b**3 - c\n</pre> def f(a, b, c):   return a**2 + b**3 - c In\u00a0[25]: Copied! <pre>a = 2\nb = 3\nc = 4\n\nf(a, b, c)\n</pre> a = 2 b = 3 c = 4  f(a, b, c) Out[25]: <pre>27</pre> In\u00a0[26]: Copied! <pre>h = 1\n\nf(a + h, b, c)\n</pre> h = 1  f(a + h, b, c) Out[26]: <pre>32</pre> In\u00a0[27]: Copied! <pre>f(a + h, b, c) - f(a, b, c)\n</pre> f(a + h, b, c) - f(a, b, c) Out[27]: <pre>5</pre> In\u00a0[28]: Copied! <pre>(f(a + h, b, c) - f(a, b, c)) / h\n</pre> (f(a + h, b, c) - f(a, b, c)) / h Out[28]: <pre>5.0</pre> In\u00a0[29]: Copied! <pre>h = 0.00001 # when the change is approaching zero\npda = (f(a + h, b, c) - f(a, b, c)) / h\npda\n</pre> h = 0.00001 # when the change is approaching zero pda = (f(a + h, b, c) - f(a, b, c)) / h pda Out[29]: <pre>4.000010000027032</pre> In\u00a0[30]: Copied! <pre>assert 2*a == round(pda)\n</pre> assert 2*a == round(pda) <p>Our function was <code>f(a,b,c) = a<sup>2</sup>+b<sup>3</sup>-c</code>. Partial derivative with respect to a is <code>2a</code> (by the power rule), and when <code>a=2</code> indeed we get 4.</p> <p>Exercise: Code the partial derivative with respect to b and c and verify if the result correct.</p> In\u00a0[31]: Copied! <pre># This is a graph visualization code from micrograd, no need to understand the details\n# https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb\nfrom graphviz import Digraph\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n\n    for n in nodes:\n        dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n\n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> # This is a graph visualization code from micrograd, no need to understand the details # https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb from graphviz import Digraph  def trace(root):     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root, format='svg', rankdir='LR'):     \"\"\"     format: png | svg | ...     rankdir: TB (top to bottom graph) | LR (left to right)     \"\"\"     assert rankdir in ['LR', 'TB']     nodes, edges = trace(root)     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})      for n in nodes:         dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')         if n._op:             dot.node(name=str(id(n)) + n._op, label=n._op)             dot.edge(str(id(n)) + n._op, str(id(n)))      for n1, n2 in edges:         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[32]: Copied! <pre># Value class stores a number and \"remembers\" information about its origins\nclass Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self.grad = 0\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+')\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), \"*\")\n    return out\n\n  def __repr__(self):\n    return f\"Value(data={self.data}, grad={self.grad})\"\n</pre> # Value class stores a number and \"remembers\" information about its origins class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self.grad = 0    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+')     return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), \"*\")     return out    def __repr__(self):     return f\"Value(data={self.data}, grad={self.grad})\" In\u00a0[33]: Copied! <pre>a = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b; c.label = 'c'\nd = Value(10, label='d')\nL = c * d; L.label = 'L'\n</pre> a = Value(5, label='a') b = Value(3, label='b') c = a + b; c.label = 'c' d = Value(10, label='d') L = c * d; L.label = 'L' In\u00a0[34]: Copied! <pre>print(a, a._prev)\nprint(L, L._prev)\n</pre> print(a, a._prev) print(L, L._prev) <pre>Value(data=5, grad=0) ()\nValue(data=80, grad=0) (Value(data=8, grad=0), Value(data=10, grad=0))\n</pre> In\u00a0[35]: Copied! <pre>draw_dot(c)\n</pre> draw_dot(c) Out[35]: In\u00a0[36]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[36]: <p>Gradient is vector of partial derivatives.</p> <p>We want to know how much changing each variable will affect the output of <code>L</code>. We will store those partial derivatives inside each <code>grad</code> variable of each <code>Value</code> object.</p> In\u00a0[37]: Copied! <pre>L.grad = 1.0\n</pre> L.grad = 1.0 <p>The derivative of a variable with respect to itself is 1 (you get the same dx/dy).</p> In\u00a0[38]: Copied! <pre>f = lambda x: x\nh = 1e-5\npdx = (f(x + h) - f(x)) / h\nassert round(pdx) == 1\n</pre> f = lambda x: x h = 1e-5 pdx = (f(x + h) - f(x)) / h assert round(pdx) == 1 <p>Now let's see how changing other variables will affect the eventual result.</p> In\u00a0[39]: Copied! <pre>def f(ha=0, hb=0, hc=0, hd=0):\n  # same function as before\n  a = Value(5 + ha, label='a')\n  b = Value(3 + hb, label='b')\n  c = a + b + Value(hc); c.label = 'c'\n  d = Value(10 + hd, label='d')\n  L = c * d; L.label = 'L'\n  return L.data\n</pre> def f(ha=0, hb=0, hc=0, hd=0):   # same function as before   a = Value(5 + ha, label='a')   b = Value(3 + hb, label='b')   c = a + b + Value(hc); c.label = 'c'   d = Value(10 + hd, label='d')   L = c * d; L.label = 'L'   return L.data In\u00a0[40]: Copied! <pre>h = 1e-5\n(f(hd=h) - f()) / h\n</pre> h = 1e-5 (f(hd=h) - f()) / h Out[40]: <pre>7.999999999697137</pre> <p>From the computational graph we can also see that <code>L=c*d</code>. When we change the value of d just a little bit (derivative of <code>L</code> with respect to <code>d</code>) the value of <code>L</code> will change by the amount of <code>c</code>, which is <code>8.0</code>. We saw it above in the partial derivative of a multiplication.</p> In\u00a0[41]: Copied! <pre>d.grad = c.data\nc.grad = d.data\n</pre> d.grad = c.data c.grad = d.data <p>With the same logic, the derivative of <code>L</code> wrt <code>c</code> will be the value of <code>d</code>, which is <code>10.0</code>. We can verify it.</p> In\u00a0[42]: Copied! <pre>(f(hc=h) - f()) / h\n</pre> (f(hc=h) - f()) / h Out[42]: <pre>10.000000000331966</pre> <p>To determine how much changing earlier variables in the computation graph will affect the <code>L</code> variable, we can apply the chain rule. Simply, the derivative of <code>L</code> with respect to <code>a</code> is the derivative of <code>c</code> with respect to <code>a</code> multiplied by the derivative of <code>L</code> with respect to <code>c</code>. God bless Leibniz.</p> <p></p> <p>The derivate of <code>c</code> both wrt <code>a</code> and 'b' is <code>1</code> due to the property of addition shown before (<code>c=a+b</code>). From here:</p> In\u00a0[43]: Copied! <pre>a.grad = 1.0 * c.grad\nb.grad = 1.0 * c.grad\n\na.grad, b.grad\n</pre> a.grad = 1.0 * c.grad b.grad = 1.0 * c.grad  a.grad, b.grad Out[43]: <pre>(10.0, 10.0)</pre> <p>We can verify it as well. Let's see how much <code>L</code> gets affected, when we shift <code>a</code> or <code>b</code> by a small amount.</p> In\u00a0[44]: Copied! <pre>(f(ha=h) - f()) / h\n</pre> (f(ha=h) - f()) / h Out[44]: <pre>10.000000000331966</pre> In\u00a0[45]: Copied! <pre>(f(hb=h) - f()) / h\n</pre> (f(hb=h) - f()) / h Out[45]: <pre>10.000000000331966</pre> <p>We will finally redraw the manually updated computation graph.</p> In\u00a0[46]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[46]: <p>It basically implies that, for example, changing the value of <code>a</code> by <code>1</code> unit (from 5 to 6) will increase the value of <code>L</code> by <code>10</code> units (from 80 to 90).</p> In\u00a0[47]: Copied! <pre>f(ha=1)\n</pre> f(ha=1) Out[47]: <pre>90</pre> In\u00a0[48]: Copied! <pre>f(hb=1), f(hc=1), f(hd=1) # the rest of the cases\n</pre> f(hb=1), f(hc=1), f(hd=1) # the rest of the cases Out[48]: <pre>(90, 90, 88)</pre> <p>What we saw above was one backward pass done manually. We are mainly interested in the signs of partial derivatives to know if they are positively or negatively influencing the eventual loss <code>L</code> of our model. In our case, all the derivatives are positive and influence loss positively.</p> <p>We have to simply nudge the values in the opposite direction of the gradient to bring the loss down. This is known as gradient descent.</p> In\u00a0[49]: Copied! <pre>lr = 0.01 # we will discuss learning rate later on\n\na.data -= lr * a.grad\nb.data -= lr * b.grad\nd.data -= lr * d.grad\n\n# we skip c which is controlled by the values of a and b\n# pay attention that the rest are leaf nodes in the computation graph\n</pre> lr = 0.01 # we will discuss learning rate later on  a.data -= lr * a.grad b.data -= lr * b.grad d.data -= lr * d.grad  # we skip c which is controlled by the values of a and b # pay attention that the rest are leaf nodes in the computation graph <p>In case the loss is a negative value (not common), we will need to \"gradient ascend\" the loss upwards towards zero and change the sign to <code>+=</code> from <code>-=</code>. Note that the values of parameters (a, b, d) can decrease or increase depending on the sign of <code>grad</code>.</p> <p>We will now do a single forward pass to see if loss has been decreased. Previous loss was <code>80</code>.</p> In\u00a0[50]: Copied! <pre># We will now forward pass\nc = a + b\nL = c * d\n\nL.data\n</pre> # We will now forward pass c = a + b L = c * d  L.data Out[50]: <pre>77.376</pre> <p>We optimized our values and brought down the loss.</p> <p>Manually calculating gradient is good only for educational purposes. We should implement automatic backward pass which will calculate gradients. We will rewrite our <code>Value</code> class for <code>backward()</code> function.</p> In\u00a0[51]: Copied! <pre>class Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self.grad = 0.0\n    self._backward = lambda: None # initially it is a function which does nothing\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+')\n\n    def _backward():\n      self.grad = 1.0 * out.grad\n      other.grad = 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), \"*\")\n\n    def _backward():\n      self.grad = other.data * out.grad\n      other.grad = self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __repr__(self):\n    return f\"Value(data={self.data}, grad={self.grad})\"\n</pre> class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self.grad = 0.0     self._backward = lambda: None # initially it is a function which does nothing    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+')      def _backward():       self.grad = 1.0 * out.grad       other.grad = 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), \"*\")      def _backward():       self.grad = other.data * out.grad       other.grad = self.data * out.grad     out._backward = _backward      return out    def __repr__(self):     return f\"Value(data={self.data}, grad={self.grad})\" In\u00a0[52]: Copied! <pre># Recreating the same function\na = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b; c.label = 'c'\nd = Value(10, label='d')\nL = c * d; L.label = 'L'\n</pre> # Recreating the same function a = Value(5, label='a') b = Value(3, label='b') c = a + b; c.label = 'c' d = Value(10, label='d') L = c * d; L.label = 'L' In\u00a0[53]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[53]: <p>We will initialize the gradient of the loss to be 1.0 and then call backward function. We should get the same results which we manually calculated previously.</p> In\u00a0[54]: Copied! <pre>L.grad = 1.0\nL._backward()\nc._backward()\n</pre> L.grad = 1.0 L._backward() c._backward() In\u00a0[55]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[55]: <p>Exercise: Make sure that all operations and their partial derivatives can be calculated (e.g. division, power).</p> <p>We can now call the optimization process, as well as forward and backward passes to reduce loss. Training model with the help of backward pass, optimization, and forward pass is called backpropagation.</p> In\u00a0[56]: Copied! <pre># optimization\nlr = 0.01\na.data -= lr * a.grad\nb.data -= lr * b.grad\nd.data -= lr * d.grad\n\n# forward pass\nc = a + b\nL = c * d\n\n# backward pass\nL.grad = 1.0\nL._backward()\nc._backward()\n\nL.data # loss\n</pre> # optimization lr = 0.01 a.data -= lr * a.grad b.data -= lr * b.grad d.data -= lr * d.grad  # forward pass c = a + b L = c * d  # backward pass L.grad = 1.0 L._backward() c._backward()  L.data # loss Out[56]: <pre>77.376</pre> <p>We have now trained the model for a single <code>epoch</code>. Even though what we do is oversimplistic and not precise, the main intuition and concepts behind training a neural network is the same.</p> <p>We will train the model for multiple epochs until we reduce the loss down to zero.</p> In\u00a0[57]: Copied! <pre>while True:\n  # optimization\n  a.data -= lr * a.grad\n  b.data -= lr * b.grad\n  d.data -= lr * d.grad\n\n  # forward pass\n  c = a + b\n  L = c * d\n\n  # backward pass\n  L.grad = 1.0\n  L._backward()\n  c._backward()\n\n  if L.data &lt; 0:\n    break\n\n  print(f'Loss: {round(L.data,2)}')\n</pre> while True:   # optimization   a.data -= lr * a.grad   b.data -= lr * b.grad   d.data -= lr * d.grad    # forward pass   c = a + b   L = c * d    # backward pass   L.grad = 1.0   L._backward()   c._backward()    if L.data &lt; 0:     break    print(f'Loss: {round(L.data,2)}') <pre>Loss: 74.81\nLoss: 72.31\nLoss: 69.87\nLoss: 67.49\nLoss: 65.16\nLoss: 62.88\nLoss: 60.66\nLoss: 58.48\nLoss: 56.35\nLoss: 54.27\nLoss: 52.23\nLoss: 50.24\nLoss: 48.28\nLoss: 46.37\nLoss: 44.49\nLoss: 42.65\nLoss: 40.84\nLoss: 39.07\nLoss: 37.33\nLoss: 35.62\nLoss: 33.94\nLoss: 32.29\nLoss: 30.66\nLoss: 29.06\nLoss: 27.48\nLoss: 25.93\nLoss: 24.39\nLoss: 22.88\nLoss: 21.39\nLoss: 19.91\nLoss: 18.45\nLoss: 17.0\nLoss: 15.57\nLoss: 14.16\nLoss: 12.75\nLoss: 11.36\nLoss: 9.97\nLoss: 8.59\nLoss: 7.22\nLoss: 5.86\nLoss: 4.5\nLoss: 3.15\nLoss: 1.8\nLoss: 0.45\n</pre> <p>All we did manually is built in to PyTorch. We will do a forward and backward pass and check if the gradients are what we had previosuly calculated. As gradients are not always calculated, for optimization purposes <code>requires_grad</code> is set to False by default. We cannot also calculate gradient for leaf nodes.</p> In\u00a0[58]: Copied! <pre>import torch\n\na = torch.tensor(5.0);    a.requires_grad = True\nb = torch.tensor(3.0);    b.requires_grad = True\nc = a + b\nd = torch.tensor(10.0);   d.requires_grad = True\nL = c * d\n</pre> import torch  a = torch.tensor(5.0);    a.requires_grad = True b = torch.tensor(3.0);    b.requires_grad = True c = a + b d = torch.tensor(10.0);   d.requires_grad = True L = c * d In\u00a0[59]: Copied! <pre>L.backward()\n</pre> L.backward() In\u00a0[60]: Copied! <pre>a.grad, b.grad, d.grad\n</pre> a.grad, b.grad, d.grad Out[60]: <pre>(tensor(10.), tensor(10.), tensor(8.))</pre> <p>We got the expected result.</p>"},{"location":"Main_Content/01_backprop/#01-python-code-from-derivatives-to-backpropagation","title":"01 Python Code from Derivatives to Backpropagation\u00b6","text":"<p>Note: The notebook is mainly based on Andrej Karpathy's lecture on Micrograd.</p> <p>We will go from illustrating differentiation and finding derivatives in Python, all the way down till the implementation of the backpropagation algorithm.</p>"},{"location":"Main_Content/01_backprop/#differentiation","title":"Differentiation\u00b6","text":""},{"location":"Main_Content/01_backprop/#partial-derivatives","title":"Partial Derivatives\u00b6","text":""},{"location":"Main_Content/01_backprop/#addition","title":"Addition\u00b6","text":""},{"location":"Main_Content/01_backprop/#multiplication","title":"Multiplication\u00b6","text":""},{"location":"Main_Content/01_backprop/#complex","title":"Complex\u00b6","text":""},{"location":"Main_Content/01_backprop/#micrograd-and-computation-graph","title":"Micrograd and Computation Graph\u00b6","text":"<p>Based on Karpathy's https://github.com/karpathy/micrograd</p>"},{"location":"Main_Content/01_backprop/#gradient","title":"Gradient\u00b6","text":""},{"location":"Main_Content/01_backprop/#chain-rule","title":"Chain Rule\u00b6","text":""},{"location":"Main_Content/01_backprop/#optimization-with-gradient-descent","title":"Optimization with Gradient Descent\u00b6","text":""},{"location":"Main_Content/01_backprop/#forward-pass","title":"Forward Pass\u00b6","text":""},{"location":"Main_Content/01_backprop/#backward-pass","title":"Backward Pass\u00b6","text":""},{"location":"Main_Content/01_backprop/#training-model-with-backpropagation","title":"Training Model with Backpropagation\u00b6","text":""},{"location":"Main_Content/01_backprop/#pytorch-implementation","title":"PyTorch Implementation\u00b6","text":""},{"location":"Main_Content/02_neural_network/","title":"02 Python Code from Neuron to Neural Network","text":"<p>Note: The notebook is mainly based on Andrej Karpathy's lecture on Micrograd.</p> In\u00a0[1]: Copied! <pre># This is a graph visualization code from micrograd, no need to understand the details\n# https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb\nfrom graphviz import Digraph\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n\n    for n in nodes:\n        dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n\n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> # This is a graph visualization code from micrograd, no need to understand the details # https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb from graphviz import Digraph  def trace(root):     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root, format='svg', rankdir='LR'):     \"\"\"     format: png | svg | ...     rankdir: TB (top to bottom graph) | LR (left to right)     \"\"\"     assert rankdir in ['LR', 'TB']     nodes, edges = trace(root)     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})      for n in nodes:         dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')         if n._op:             dot.node(name=str(id(n)) + n._op, label=n._op)             dot.edge(str(id(n)) + n._op, str(id(n)))      for n1, n2 in edges:         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[2]: Copied! <pre># This is the code from previous lecture\n# It is important to understand _backward function\nclass Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self._backward = lambda: None\n    self.grad = 0.0\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other),'+')\n\n    def _backward():\n      self.grad = 1.0 * out.grad\n      other.grad = 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other),'*')\n\n    def _backward():\n      self.grad = other.data * out.grad\n      other.grad = self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __sub__(self, other):\n    return self + (Value(-1) * other) # self + (-other)\n\n  def __repr__(self):\n    return f'{self.label}: {self.data}'\n</pre> # This is the code from previous lecture # It is important to understand _backward function class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self._backward = lambda: None     self.grad = 0.0    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other),'+')      def _backward():       self.grad = 1.0 * out.grad       other.grad = 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other),'*')      def _backward():       self.grad = other.data * out.grad       other.grad = self.data * out.grad     out._backward = _backward      return out    def __sub__(self, other):     return self + (Value(-1) * other) # self + (-other)    def __repr__(self):     return f'{self.label}: {self.data}' In\u00a0[3]: Copied! <pre>a = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b;   c.label = 'c'\nd = Value(10, label='d')\nL = c * d;   L.label = 'L'\n</pre> a = Value(5, label='a') b = Value(3, label='b') c = a + b;   c.label = 'c' d = Value(10, label='d') L = c * d;   L.label = 'L' In\u00a0[4]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[4]: In\u00a0[5]: Copied! <pre>epochs = 10\nlearning_rate = 0.01\n\nfor _ in range(epochs):\n  L.grad = 1.0\n\n  # backward pass\n  L._backward()\n  c._backward()\n\n  # optimization (gradient descent)\n  a.data -= learning_rate * a.grad\n  b.data -= learning_rate * b.grad\n  d.data -= learning_rate * d.grad\n\n  # forward pass\n  c = a + b\n  L = c * d\n\n  print(f'Loss: {L.data:.2f}')\n</pre> epochs = 10 learning_rate = 0.01  for _ in range(epochs):   L.grad = 1.0    # backward pass   L._backward()   c._backward()    # optimization (gradient descent)   a.data -= learning_rate * a.grad   b.data -= learning_rate * b.grad   d.data -= learning_rate * d.grad    # forward pass   c = a + b   L = c * d    print(f'Loss: {L.data:.2f}') <pre>Loss: 77.38\nLoss: 74.81\nLoss: 72.31\nLoss: 69.87\nLoss: 67.49\nLoss: 65.16\nLoss: 62.88\nLoss: 60.66\nLoss: 58.48\nLoss: 56.35\n</pre> In\u00a0[6]: Copied! <pre># Equivalent implementation in PyTorch\n# pay attention to requires_grad, no_grad() and zero_()\n\nimport torch\n\na = torch.tensor(5.0, requires_grad = True);\nb = torch.tensor(3.0, requires_grad = True);\nc = a + b\nd = torch.tensor(10.0,requires_grad = True);\nL = c * d\n\nfor _ in range(epochs):\n  # backward pass\n  L.backward()\n\n  # optimization (gradient descent)\n  with torch.no_grad():\n    a -= learning_rate * a.grad\n    b -= learning_rate * b.grad\n    d -= learning_rate * d.grad\n\n  # avoids accumulating gradients\n  # comment this out to see how it affects the learning\n  a.grad.zero_()\n  b.grad.zero_()\n  d.grad.zero_()\n\n  # forward pass\n  c = a + b\n  L = c * d\n\n  print(f'Loss: {L.data:.2f}')\n</pre> # Equivalent implementation in PyTorch # pay attention to requires_grad, no_grad() and zero_()  import torch  a = torch.tensor(5.0, requires_grad = True); b = torch.tensor(3.0, requires_grad = True); c = a + b d = torch.tensor(10.0,requires_grad = True); L = c * d  for _ in range(epochs):   # backward pass   L.backward()    # optimization (gradient descent)   with torch.no_grad():     a -= learning_rate * a.grad     b -= learning_rate * b.grad     d -= learning_rate * d.grad    # avoids accumulating gradients   # comment this out to see how it affects the learning   a.grad.zero_()   b.grad.zero_()   d.grad.zero_()    # forward pass   c = a + b   L = c * d    print(f'Loss: {L.data:.2f}') <pre>Loss: 77.38\nLoss: 74.81\nLoss: 72.31\nLoss: 69.87\nLoss: 67.49\nLoss: 65.16\nLoss: 62.88\nLoss: 60.66\nLoss: 58.48\nLoss: 56.35\n</pre> <p>The function <code>f(x) = x * w</code> is a linear function always passing from origin. The real world data, however, will be much more complex, and in order to describe a pattern in the data our Machine Learning model should return a more flexible function. For that, we will do two things: add bias <code>b</code> and bring non-linearity with an activation function. We can choose different non-linear activation functions with the condition that it should be differentiable (otherwise we won't be able to calculate gradients for backpropagation). We will implement <code>sigmoid</code>(logistic) activation function which has the following formula:</p> <p></p> <p>Sigmoid function not only makes a linear function non-linear and continuous, but also maps any value of <code>x</code> to be between 0 and 1. It may be useful when we want to predict probabilities for different output classes.</p> In\u00a0[7]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[8]: Copied! <pre>def sigmoid(x):\n  return 1.0 / (1 + np.exp(-x)) # see formula above\n</pre> def sigmoid(x):   return 1.0 / (1 + np.exp(-x)) # see formula above In\u00a0[9]: Copied! <pre># a simple linear function where activation will be applied\ndef f(x, w=0.5, b=10, activation=None):\n  out = x * w + b\n  return activation(out) if activation else out\n</pre> # a simple linear function where activation will be applied def f(x, w=0.5, b=10, activation=None):   out = x * w + b   return activation(out) if activation else out In\u00a0[10]: Copied! <pre>def plot(f, x, activation=None):\n  plt.figure(figsize=(4, 4))\n  x_all = np.linspace(-50, 50, 100)\n  y_all = f(x_all, activation=activation)\n  plt.plot(x_all, y_all)\n  plt.scatter(x, f(x, activation=activation), color='r')\n  plt.show()\n</pre> def plot(f, x, activation=None):   plt.figure(figsize=(4, 4))   x_all = np.linspace(-50, 50, 100)   y_all = f(x_all, activation=activation)   plt.plot(x_all, y_all)   plt.scatter(x, f(x, activation=activation), color='r')   plt.show() In\u00a0[11]: Copied! <pre>x = -20\nplot(f, x)\n</pre> x = -20 plot(f, x) <p>Now we will plot the exact same point mapped into the non-linear function between <code>0</code> and <code>1</code>. Try out different <code>x</code> values and see the plots.</p> In\u00a0[12]: Copied! <pre>plot(f, x, sigmoid)\n</pre> plot(f, x, sigmoid) <p>Exercise: Implement other activation functions (e.g. <code>tanh</code>, <code>relu</code>) and see the plot.</p> <p>Exercise: What could be the distadvantage of using sigmoid activation function?</p> <p>Funcs - Own work (CC0 | Wikimedia Commons)</p> <p></p> <p>An artificial neuron is simply a linear function passing through an activation function  (e.g. <code>sigmoid(x * w + b)</code>). The illustration above describes an N-dimensional neuron, accepting inputs between <code>x<sub>1</sub> ... x<sub>n</sub></code>. The function <code>f</code> we had above is a very simple neuron with 1-dimensional input.</p> <p>Question: What could be input values for predicting the probability of a customer cancelling their subscription?</p> <p>Before creating our neuron, we will first make some updates to the <code>Value</code> class.</p> <p>Not only value class should have a <code>sigmoid(x)</code> function, but also it should be able to calculate a derivative for it.</p> <p>Exercise: Find the derivative of the <code>sigmoid</code> function:</p> <p></p> <p>The <code>requires_grad</code> flag (similar to PyTorch) will tell which parameters are trainable and requires gradient calculation and update (Note that this feature is not implemented in <code>micrograd</code>).</p> <p>For example, it doesn't make sense to modify the real-life training inputs <code>x1</code> and <code>x2</code> for our neuron. We shouldn't spend resources for calculating unnecessary gradients. Our goal is to nudge only the weight and bias (i.e. parameter) values, as well as the nodes dependent on them, in order to minimize the eventual loss.</p> In\u00a0[13]: Copied! <pre>class Value:\n  def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self._backward = lambda: None\n    self.grad = 0.0\n    self.requires_grad = requires_grad\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad = 1.0 * out.grad\n      if other.requires_grad:\n        other.grad = 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad = other.data * out.grad\n      if other.requires_grad:\n        other.grad = self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __sub__(self, other):\n    return self + (Value(-1) * other) # self + (-other)\n\n  def sigmoid(self):\n    s = 1.0 / (1 + np.exp(-self.data))\n    out = Value(s, (self, ), 'sigmoid', self.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad = s * (1.0 - s) * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __repr__(self):\n    return f'Value({self.data:.4f})'\n</pre> class Value:   def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self._backward = lambda: None     self.grad = 0.0     self.requires_grad = requires_grad    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad = 1.0 * out.grad       if other.requires_grad:         other.grad = 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad = other.data * out.grad       if other.requires_grad:         other.grad = self.data * out.grad     out._backward = _backward      return out    def __sub__(self, other):     return self + (Value(-1) * other) # self + (-other)    def sigmoid(self):     s = 1.0 / (1 + np.exp(-self.data))     out = Value(s, (self, ), 'sigmoid', self.requires_grad)      def _backward():       if self.requires_grad:         self.grad = s * (1.0 - s) * out.grad     out._backward = _backward      return out    def __repr__(self):     return f'Value({self.data:.4f})' <p>We will initially implement a simple <code>Neuron</code> class in 3D (2-dimensional input values and an output value). The function will have two inputs <code>x1</code> and <code>x2</code>, which will become <code>Value</code> objects. Their weights <code>w1</code> and <code>w2</code> will determine how much input (e.g. age of a customer) influences outcome.</p> In\u00a0[14]: Copied! <pre>from mpl_toolkits.mplot3d import Axes3D\n\nclass Neuron:\n  def __init__(self):\n    self.w1 = Value(np.random.uniform(-1, 1), label='w1', requires_grad=True)\n    self.w2 = Value(np.random.uniform(-1, 1), label='w2', requires_grad=True)\n    self.b = Value(0, label='b', requires_grad=True)\n\n  def __call__(self, x1, x2):\n    out = x1 * self.w1 + x2 * self.w2 + self.b\n    return out.sigmoid()\n\n  # this code here is for plotting, no need to understand, works for only 3D\n  def plot(self):\n    x1_vals = np.linspace(-5, 5, 100)\n    x2_vals = np.linspace(-5, 5, 100)\n    X1, X2 = np.meshgrid(x1_vals, x2_vals)\n    Z = np.zeros_like(X1)\n\n    for i in range(X1.shape[0]):\n      for j in range(X1.shape[1]):\n        x1 = Value(X1[i, j])\n        x2 = Value(X2[i, j])\n        output = self(x1, x2)\n        Z[i, j] = output.data\n\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot_surface(X1, X2, Z, cmap='viridis')\n    ax.set_title(f'Neuron Output with Sigmoid Activation)')\n    plt.show()\n</pre> from mpl_toolkits.mplot3d import Axes3D  class Neuron:   def __init__(self):     self.w1 = Value(np.random.uniform(-1, 1), label='w1', requires_grad=True)     self.w2 = Value(np.random.uniform(-1, 1), label='w2', requires_grad=True)     self.b = Value(0, label='b', requires_grad=True)    def __call__(self, x1, x2):     out = x1 * self.w1 + x2 * self.w2 + self.b     return out.sigmoid()    # this code here is for plotting, no need to understand, works for only 3D   def plot(self):     x1_vals = np.linspace(-5, 5, 100)     x2_vals = np.linspace(-5, 5, 100)     X1, X2 = np.meshgrid(x1_vals, x2_vals)     Z = np.zeros_like(X1)      for i in range(X1.shape[0]):       for j in range(X1.shape[1]):         x1 = Value(X1[i, j])         x2 = Value(X2[i, j])         output = self(x1, x2)         Z[i, j] = output.data      fig = plt.figure(figsize=(10, 8))     ax = fig.add_subplot(111, projection='3d')     ax.plot_surface(X1, X2, Z, cmap='viridis')     ax.set_title(f'Neuron Output with Sigmoid Activation)')     plt.show() <p>Now we can initialize our inputs and neuron to see our computation graph. Our loss will be simple: the ground truth label <code>y</code> minus the predicted probability. Let's assume that, our input values <code>x1</code> and <code>x2</code> correspond to a customer who made the purchase (<code>y = 1</code>). We will try out both activation functions and see their plots.</p> In\u00a0[15]: Copied! <pre>x1 = Value(2, label='x1')\nx2 = Value(3, label='x2')\ny  = Value(1, label= 'y')\n\nn = Neuron()\n\npred = n(x1, x2);      pred.label = 'pred'\nL = y - pred;          L.label = 'loss'\n</pre> x1 = Value(2, label='x1') x2 = Value(3, label='x2') y  = Value(1, label= 'y')  n = Neuron()  pred = n(x1, x2);      pred.label = 'pred' L = y - pred;          L.label = 'loss' In\u00a0[16]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[16]: In\u00a0[17]: Copied! <pre>n.plot()\n</pre> n.plot() <p>The ground truth label <code>1</code> tells us that we should push our probability towards <code>1.0</code>. In other words, as our loss <code>L</code> here is a simple error value corresponding to <code>1 - prob</code>, we should try to minimize the loss down to zero with backpropagation. However, our computation graph is bigger than how it was before. The <code>_backward</code> pass function we call manually on each node is not scalable. Ideally, we should have a single function <code>backward()</code> to calculate all the gradients, which we previously saw in the PyTorch implementation. For that, we will need to sort the nodes of the computation graph (in this case, from input/weight nodes until the probability node). We can achieve that with a topological sort function implemented for micrograd.</p> In\u00a0[18]: Copied! <pre>topo = []\nvisited = set()\ndef build_topo(v):\n  if v not in visited:\n    visited.add(v)\n    for child in v._prev:\n      build_topo(child)\n    topo.append(v)\nbuild_topo(pred)\ntopo\n</pre> topo = [] visited = set() def build_topo(v):   if v not in visited:     visited.add(v)     for child in v._prev:       build_topo(child)     topo.append(v) build_topo(pred) topo Out[18]: <pre>[Value(2.0000),\n Value(-0.7886),\n Value(-1.5772),\n Value(3.0000),\n Value(0.0694),\n Value(0.2081),\n Value(-1.3691),\n Value(0.0000),\n Value(-1.3691),\n Value(0.2028)]</pre> <p>We will integrate topological sort into our <code>Value</code> object and implement complete backward pass. We can also add a simple gradient descent function <code>optimize()</code> which will use this topology. Finally, instead of overriding gradients (<code>=</code>), we will accumulate them (<code>+=</code>) to avoid gradient update bugs when using the same node more than once in an operation. And as a consequence, we will have to reset gradients with <code>zero_()</code> (similar to PyTorch) so that the gradients of different backward passes will not affect each other (it does the exact same thing as <code>self.grad = 0.0</code> was doing before gradient accumulation). Although, to be precise, <code>zero_()</code> function should reset only the gradient of <code>self</code>, and it is actually a function called <code>zero_grad()</code> of <code>optimizer</code> in PyTorch which resets gradients accross all nodes.</p> In\u00a0[\u00a0]: Copied! <pre>class Value:\n  def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self._backward = lambda: None\n    self.grad = 0.0\n    self.requires_grad = requires_grad\n    self.topo = self.build_topo()\n    self.params = [node for node in self.topo if node.requires_grad]\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad += 1.0 * out.grad\n      if other.requires_grad:\n        other.grad += 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad += other.data * out.grad\n      if other.requires_grad:\n        other.grad += self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __sub__(self, other):\n    return self + (Value(-1) * other) # self + (-other)\n\n  def sigmoid(self):\n    s = 1.0 / (1 + np.exp(-self.data))\n    out = Value(s, (self, ), 'sigmoid', self.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad += s * (1.0 - s) * out.grad\n    out._backward = _backward\n\n    return out\n\n  def build_topo(self):\n    # topological order all of the children in the graph\n    topo = []\n    visited = set()\n\n    def _build_topo(node):\n      if node not in visited:\n        visited.add(node)\n        for child in node._prev:\n          _build_topo(child)\n        topo.append(node)\n    _build_topo(self)\n\n    return topo\n\n  def backward(self):\n    if self.requires_grad:\n      self.grad = 1.0\n      for node in reversed(self.params):\n        node._backward()\n\n  def optimize(self, learning_rate=0.01):\n    for node in self.params:\n      node.data -= learning_rate * node.grad\n\n  def zero_(self):\n    self.grad = 0.0\n\n  def zero_grad(self):\n    for node in self.params:\n      node.grad = 0.0\n\n  def __repr__(self):\n    return f'Value({self.data})'\n</pre> class Value:   def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self._backward = lambda: None     self.grad = 0.0     self.requires_grad = requires_grad     self.topo = self.build_topo()     self.params = [node for node in self.topo if node.requires_grad]    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad += 1.0 * out.grad       if other.requires_grad:         other.grad += 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad += other.data * out.grad       if other.requires_grad:         other.grad += self.data * out.grad     out._backward = _backward      return out    def __sub__(self, other):     return self + (Value(-1) * other) # self + (-other)    def sigmoid(self):     s = 1.0 / (1 + np.exp(-self.data))     out = Value(s, (self, ), 'sigmoid', self.requires_grad)      def _backward():       if self.requires_grad:         self.grad += s * (1.0 - s) * out.grad     out._backward = _backward      return out    def build_topo(self):     # topological order all of the children in the graph     topo = []     visited = set()      def _build_topo(node):       if node not in visited:         visited.add(node)         for child in node._prev:           _build_topo(child)         topo.append(node)     _build_topo(self)      return topo    def backward(self):     if self.requires_grad:       self.grad = 1.0       for node in reversed(self.params):         node._backward()    def optimize(self, learning_rate=0.01):     for node in self.params:       node.data -= learning_rate * node.grad    def zero_(self):     self.grad = 0.0    def zero_grad(self):     for node in self.params:       node.grad = 0.0    def __repr__(self):     return f'Value({self.data})' <p>In addition to the <code>_backward()</code> function which calculated the derivatives for only immediate previous nodes, we now have the <code>backward()</code> function which calculates derivates for all the nodes (we also won't forget to set the gradient to <code>1.0</code> in the beginning). Once we plot the graph, pay attention that the input and leaf node gradients which we have no control over are not calculated, thanks to <code>requires_grad</code>.</p> In\u00a0[20]: Copied! <pre>x1 = Value(2, label='x1')\nx2 = Value(5, label='x2')\ny  = Value(1, label= 'y')\n\nn = Neuron()\n\npred = n(x1, x2);      pred.label = 'pred'\nL = y - pred;          L.label = 'loss'\n</pre> x1 = Value(2, label='x1') x2 = Value(5, label='x2') y  = Value(1, label= 'y')  n = Neuron()  pred = n(x1, x2);      pred.label = 'pred' L = y - pred;          L.label = 'loss' In\u00a0[21]: Copied! <pre>L.backward()\ndraw_dot(L)\n</pre> L.backward() draw_dot(L) Out[21]: <p>We can finally implement complete backpropogatation with the goal of increasing the final probability to <code>1.0</code> (decreasing the loss down to zero).</p> In\u00a0[22]: Copied! <pre># gradient descent\nL.optimize()\n\n# forward pass\npred = n(x1, x2);      pred.label = 'pred'\nL = y - pred;          L.label = 'loss'\n\ndraw_dot(L)\n</pre> # gradient descent L.optimize()  # forward pass pred = n(x1, x2);      pred.label = 'pred' L = y - pred;          L.label = 'loss'  draw_dot(L) Out[22]: <p>Let's repeat the backpropagation in multiple epochs until we achieve a minimal loss. We will also print the parameters to see when our neuron function returns a maximum probability for the given input values. And we will make sure to not forget to reset the gradients.</p> In\u00a0[\u00a0]: Copied! <pre>while True:\n  L.zero_grad()\n\n  # backward pass\n  L.backward()\n\n  # gradient descent\n  L.optimize()\n\n  # forward pass\n  pred = n(x1, x2);\n  L = y - pred;\n\n  print(f'Loss {L.data:.4f}')\n\n  if L.data &lt; 0.01:\n    print(f'\\nInputs: {x1} {x2}')\n    print(f'Parameters: {n.w1} {n.w2} {n.b}')\n    print(f'Prediction Probability: {pred.data}')\n    break\n</pre> while True:   L.zero_grad()    # backward pass   L.backward()    # gradient descent   L.optimize()    # forward pass   pred = n(x1, x2);   L = y - pred;    print(f'Loss {L.data:.4f}')    if L.data &lt; 0.01:     print(f'\\nInputs: {x1} {x2}')     print(f'Parameters: {n.w1} {n.w2} {n.b}')     print(f'Prediction Probability: {pred.data}')     break <pre>Loss 0.0148\nLoss 0.0148\nLoss 0.0147\nLoss 0.0147\nLoss 0.0146\nLoss 0.0145\nLoss 0.0145\nLoss 0.0144\nLoss 0.0144\nLoss 0.0143\nLoss 0.0142\nLoss 0.0142\nLoss 0.0141\nLoss 0.0141\nLoss 0.0140\nLoss 0.0139\nLoss 0.0139\nLoss 0.0138\nLoss 0.0138\nLoss 0.0137\nLoss 0.0137\nLoss 0.0136\nLoss 0.0136\nLoss 0.0135\nLoss 0.0134\nLoss 0.0134\nLoss 0.0133\nLoss 0.0133\nLoss 0.0132\nLoss 0.0132\nLoss 0.0131\nLoss 0.0131\nLoss 0.0130\nLoss 0.0130\nLoss 0.0129\nLoss 0.0129\nLoss 0.0128\nLoss 0.0128\nLoss 0.0127\nLoss 0.0127\nLoss 0.0127\nLoss 0.0126\nLoss 0.0126\nLoss 0.0125\nLoss 0.0125\nLoss 0.0124\nLoss 0.0124\nLoss 0.0123\nLoss 0.0123\nLoss 0.0122\nLoss 0.0122\nLoss 0.0122\nLoss 0.0121\nLoss 0.0121\nLoss 0.0120\nLoss 0.0120\nLoss 0.0119\nLoss 0.0119\nLoss 0.0119\nLoss 0.0118\nLoss 0.0118\nLoss 0.0117\nLoss 0.0117\nLoss 0.0117\nLoss 0.0116\nLoss 0.0116\nLoss 0.0115\nLoss 0.0115\nLoss 0.0115\nLoss 0.0114\nLoss 0.0114\nLoss 0.0113\nLoss 0.0113\nLoss 0.0113\nLoss 0.0112\nLoss 0.0112\nLoss 0.0112\nLoss 0.0111\nLoss 0.0111\nLoss 0.0111\nLoss 0.0110\nLoss 0.0110\nLoss 0.0109\nLoss 0.0109\nLoss 0.0109\nLoss 0.0108\nLoss 0.0108\nLoss 0.0108\nLoss 0.0107\nLoss 0.0107\nLoss 0.0107\nLoss 0.0106\nLoss 0.0106\nLoss 0.0106\nLoss 0.0105\nLoss 0.0105\nLoss 0.0105\nLoss 0.0104\nLoss 0.0104\nLoss 0.0104\nLoss 0.0103\nLoss 0.0103\nLoss 0.0103\nLoss 0.0103\nLoss 0.0102\nLoss 0.0102\nLoss 0.0102\nLoss 0.0101\nLoss 0.0101\nLoss 0.0101\nLoss 0.0100\nLoss 0.0100\nLoss 0.0100\n\nInputs: Value(2) Value(5)\nParameters: Value(0.15289672859342332) Value(0.8555124993367387) Value(0.013701999952082168)\nPrediction Probability: 0.9900191690162561\n</pre> <p>We have just now trained our 2-dimensional input neuron to find suitable parameter values for achieving a maximum probability for input values <code>x1</code> and <code>x2</code>. Now we would like to create N-dimensional neuron which will accept much more inputs, similar to what we saw in the illustration of artificial neuron: <code>x<sub>1</sub> ... x<sub>n</sub></code>. As a consequence, our neuron will have to learn the parameter values for N-dimensional weights <code>w<sub>1</sub> ... w<sub>n</sub></code>.</p> In\u00a0[24]: Copied! <pre>class Neuron:\n  def __init__(self, N):\n    self.W = [Value(np.random.uniform(-1, 1), label=f'w{i}', requires_grad=True) for i in range(N)]\n    self.b = Value(0, label='b', requires_grad=True)\n\n  def __call__(self, X):\n    out = sum((x * w for x, w in zip(X, self.W)), self.b)\n    return out.sigmoid()\n</pre> class Neuron:   def __init__(self, N):     self.W = [Value(np.random.uniform(-1, 1), label=f'w{i}', requires_grad=True) for i in range(N)]     self.b = Value(0, label='b', requires_grad=True)    def __call__(self, X):     out = sum((x * w for x, w in zip(X, self.W)), self.b)     return out.sigmoid() <p>We will now see the training output of our N-dimensional neuron which will accept N <code>Value</code> inputs as a list. Note that our <code>Neuron</code> which implements <code>sigmoid</code> (logistic) activation is known as Logistic Regression.</p> In\u00a0[25]: Copied! <pre>X = [Value(x, label=f'x{i}') for i, x in enumerate([5, 0.4, -1, -2])]\n\nn = Neuron(len(X))\n\npred = n(X);           pred.label = 'pred'\nL = y - pred;          L.label = 'loss'\n</pre> X = [Value(x, label=f'x{i}') for i, x in enumerate([5, 0.4, -1, -2])]  n = Neuron(len(X))  pred = n(X);           pred.label = 'pred' L = y - pred;          L.label = 'loss' In\u00a0[26]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[26]: In\u00a0[\u00a0]: Copied! <pre>while True:\n  L.zero_grad()\n\n  # backward pass\n  L.backward()\n\n  # gradient descent\n  L.optimize()\n\n  # forward pass\n  pred = n(X)\n  L = y - pred\n\n  print(f'Loss {L.data:.4f}')\n\n  if L.data &lt; 0.01:\n    print(f'\\nInputs: {X}')\n    print(f'Parameters: {n.W} {n.b}')\n    print(f'Prediction Probability: {pred.data}')\n    break\n</pre> while True:   L.zero_grad()    # backward pass   L.backward()    # gradient descent   L.optimize()    # forward pass   pred = n(X)   L = y - pred    print(f'Loss {L.data:.4f}')    if L.data &lt; 0.01:     print(f'\\nInputs: {X}')     print(f'Parameters: {n.W} {n.b}')     print(f'Prediction Probability: {pred.data}')     break <pre>Loss 0.1273\nLoss 0.1235\nLoss 0.1199\nLoss 0.1165\nLoss 0.1132\nLoss 0.1101\nLoss 0.1072\nLoss 0.1043\nLoss 0.1016\nLoss 0.0991\nLoss 0.0966\nLoss 0.0943\nLoss 0.0920\nLoss 0.0899\nLoss 0.0878\nLoss 0.0858\nLoss 0.0839\nLoss 0.0821\nLoss 0.0804\nLoss 0.0787\nLoss 0.0770\nLoss 0.0755\nLoss 0.0740\nLoss 0.0725\nLoss 0.0711\nLoss 0.0698\nLoss 0.0685\nLoss 0.0672\nLoss 0.0660\nLoss 0.0648\nLoss 0.0637\nLoss 0.0626\nLoss 0.0615\nLoss 0.0605\nLoss 0.0595\nLoss 0.0585\nLoss 0.0576\nLoss 0.0567\nLoss 0.0558\nLoss 0.0549\nLoss 0.0541\nLoss 0.0533\nLoss 0.0525\nLoss 0.0517\nLoss 0.0510\nLoss 0.0503\nLoss 0.0496\nLoss 0.0489\nLoss 0.0482\nLoss 0.0476\nLoss 0.0469\nLoss 0.0463\nLoss 0.0457\nLoss 0.0451\nLoss 0.0445\nLoss 0.0440\nLoss 0.0434\nLoss 0.0429\nLoss 0.0424\nLoss 0.0419\nLoss 0.0414\nLoss 0.0409\nLoss 0.0404\nLoss 0.0399\nLoss 0.0395\nLoss 0.0390\nLoss 0.0386\nLoss 0.0382\nLoss 0.0378\nLoss 0.0373\nLoss 0.0369\nLoss 0.0366\nLoss 0.0362\nLoss 0.0358\nLoss 0.0354\nLoss 0.0351\nLoss 0.0347\nLoss 0.0344\nLoss 0.0340\nLoss 0.0337\nLoss 0.0334\nLoss 0.0330\nLoss 0.0327\nLoss 0.0324\nLoss 0.0321\nLoss 0.0318\nLoss 0.0315\nLoss 0.0312\nLoss 0.0309\nLoss 0.0307\nLoss 0.0304\nLoss 0.0301\nLoss 0.0298\nLoss 0.0296\nLoss 0.0293\nLoss 0.0291\nLoss 0.0288\nLoss 0.0286\nLoss 0.0283\nLoss 0.0281\nLoss 0.0279\nLoss 0.0277\nLoss 0.0274\nLoss 0.0272\nLoss 0.0270\nLoss 0.0268\nLoss 0.0266\nLoss 0.0264\nLoss 0.0262\nLoss 0.0260\nLoss 0.0258\nLoss 0.0256\nLoss 0.0254\nLoss 0.0252\nLoss 0.0250\nLoss 0.0248\nLoss 0.0246\nLoss 0.0244\nLoss 0.0243\nLoss 0.0241\nLoss 0.0239\nLoss 0.0238\nLoss 0.0236\nLoss 0.0234\nLoss 0.0233\nLoss 0.0231\nLoss 0.0229\nLoss 0.0228\nLoss 0.0226\nLoss 0.0225\nLoss 0.0223\nLoss 0.0222\nLoss 0.0220\nLoss 0.0219\nLoss 0.0217\nLoss 0.0216\nLoss 0.0215\nLoss 0.0213\nLoss 0.0212\nLoss 0.0211\nLoss 0.0209\nLoss 0.0208\nLoss 0.0207\nLoss 0.0205\nLoss 0.0204\nLoss 0.0203\nLoss 0.0202\nLoss 0.0200\nLoss 0.0199\nLoss 0.0198\nLoss 0.0197\nLoss 0.0196\nLoss 0.0195\nLoss 0.0193\nLoss 0.0192\nLoss 0.0191\nLoss 0.0190\nLoss 0.0189\nLoss 0.0188\nLoss 0.0187\nLoss 0.0186\nLoss 0.0185\nLoss 0.0184\nLoss 0.0183\nLoss 0.0182\nLoss 0.0181\nLoss 0.0180\nLoss 0.0179\nLoss 0.0178\nLoss 0.0177\nLoss 0.0176\nLoss 0.0175\nLoss 0.0174\nLoss 0.0173\nLoss 0.0172\nLoss 0.0172\nLoss 0.0171\nLoss 0.0170\nLoss 0.0169\nLoss 0.0168\nLoss 0.0167\nLoss 0.0166\nLoss 0.0166\nLoss 0.0165\nLoss 0.0164\nLoss 0.0163\nLoss 0.0162\nLoss 0.0161\nLoss 0.0161\nLoss 0.0160\nLoss 0.0159\nLoss 0.0158\nLoss 0.0158\nLoss 0.0157\nLoss 0.0156\nLoss 0.0155\nLoss 0.0155\nLoss 0.0154\nLoss 0.0153\nLoss 0.0153\nLoss 0.0152\nLoss 0.0151\nLoss 0.0150\nLoss 0.0150\nLoss 0.0149\nLoss 0.0148\nLoss 0.0148\nLoss 0.0147\nLoss 0.0146\nLoss 0.0146\nLoss 0.0145\nLoss 0.0145\nLoss 0.0144\nLoss 0.0143\nLoss 0.0143\nLoss 0.0142\nLoss 0.0141\nLoss 0.0141\nLoss 0.0140\nLoss 0.0140\nLoss 0.0139\nLoss 0.0138\nLoss 0.0138\nLoss 0.0137\nLoss 0.0137\nLoss 0.0136\nLoss 0.0136\nLoss 0.0135\nLoss 0.0134\nLoss 0.0134\nLoss 0.0133\nLoss 0.0133\nLoss 0.0132\nLoss 0.0132\nLoss 0.0131\nLoss 0.0131\nLoss 0.0130\nLoss 0.0130\nLoss 0.0129\nLoss 0.0129\nLoss 0.0128\nLoss 0.0128\nLoss 0.0127\nLoss 0.0127\nLoss 0.0126\nLoss 0.0126\nLoss 0.0125\nLoss 0.0125\nLoss 0.0124\nLoss 0.0124\nLoss 0.0123\nLoss 0.0123\nLoss 0.0122\nLoss 0.0122\nLoss 0.0122\nLoss 0.0121\nLoss 0.0121\nLoss 0.0120\nLoss 0.0120\nLoss 0.0119\nLoss 0.0119\nLoss 0.0118\nLoss 0.0118\nLoss 0.0118\nLoss 0.0117\nLoss 0.0117\nLoss 0.0116\nLoss 0.0116\nLoss 0.0116\nLoss 0.0115\nLoss 0.0115\nLoss 0.0114\nLoss 0.0114\nLoss 0.0114\nLoss 0.0113\nLoss 0.0113\nLoss 0.0112\nLoss 0.0112\nLoss 0.0112\nLoss 0.0111\nLoss 0.0111\nLoss 0.0110\nLoss 0.0110\nLoss 0.0110\nLoss 0.0109\nLoss 0.0109\nLoss 0.0109\nLoss 0.0108\nLoss 0.0108\nLoss 0.0108\nLoss 0.0107\nLoss 0.0107\nLoss 0.0107\nLoss 0.0106\nLoss 0.0106\nLoss 0.0106\nLoss 0.0105\nLoss 0.0105\nLoss 0.0104\nLoss 0.0104\nLoss 0.0104\nLoss 0.0104\nLoss 0.0103\nLoss 0.0103\nLoss 0.0103\nLoss 0.0102\nLoss 0.0102\nLoss 0.0102\nLoss 0.0101\nLoss 0.0101\nLoss 0.0101\nLoss 0.0100\nLoss 0.0100\nLoss 0.0100\n\nInputs: [Value(5), Value(0.4), Value(-1), Value(-2)]\nParameters: [Value(0.6195076510193583), Value(0.988462556700058), Value(0.6814715991439656), Value(-0.8498024629867303)] Value(0.08692061974641961)\nPrediction Probability: 0.9900282484345291\n</pre> <p>Glosser.ca - Own work, Derivative of Artificial neural network.svg (CC BY-SA 3.0 | Wikimedia Commons)</p> <p></p> <p>We managed to train our single neuron to learn a function for our input values. In reality, however, data is much more complex and we need to learn more complication functions. How to achieve that? By chaining many neurons together, similar to biological neuron. Each neuron will basically learn some portion of the overall function.</p> <p>What we see above is an illustration of an artificial neural network. In the input layer we have three neurons, each separately accepting N-dimensional input values. The output values of each neuron are then fully connected, as inputs to the hidden layer with four neurons (note that there can be more than one hidden layer). And finally, the output of hidden layer neurons are passed as inputs to the output layer, which may, for example, predict probability scores for two classes.</p> <p>We will now try to implement a fully connected feedforward neural network, which is often referred to as Multi-Layer Perceptron (MLP).</p> In\u00a0[28]: Copied! <pre>class Layer:\n  def __init__(self, N, count):\n    self.neurons = [Neuron(N) for _ in range(count)]\n\n  def __call__(self, X):\n    outs = [n(X) for n in self.neurons]\n    return outs[0] if len(outs) == 1 else outs # flattening dimension if a single element\n</pre> class Layer:   def __init__(self, N, count):     self.neurons = [Neuron(N) for _ in range(count)]    def __call__(self, X):     outs = [n(X) for n in self.neurons]     return outs[0] if len(outs) == 1 else outs # flattening dimension if a single element <p>The code above creates a list of <code>count</code> number of neurons, each accepting <code>N</code> dimensional input. Let's build our layers shown in the illustration above and connect them. Note that the input dimension of the next layer is the amount of neurons in the previous layer.</p> In\u00a0[29]: Copied! <pre># input data and its dimension\nX = [Value(x, label=f'x{i}') for i, x in enumerate([1, 4, -3, -2, 3])]\nN = len(X)\n</pre> # input data and its dimension X = [Value(x, label=f'x{i}') for i, x in enumerate([1, 4, -3, -2, 3])] N = len(X) In\u00a0[30]: Copied! <pre># creating layers\nin_layer = Layer(N, 3)\nhid_layer = Layer(3, 4)\nout_layer = Layer(4, 2)\n</pre> # creating layers in_layer = Layer(N, 3) hid_layer = Layer(3, 4) out_layer = Layer(4, 2) In\u00a0[31]: Copied! <pre># output of each layer is input to the next\nX_hidden = in_layer(X)\nX_output = hid_layer(X_hidden)\nout = out_layer(X_output)\n</pre> # output of each layer is input to the next X_hidden = in_layer(X) X_output = hid_layer(X_hidden) out = out_layer(X_output) In\u00a0[32]: Copied! <pre># let's plot either one of the outputs\ndraw_dot(out[0])\n</pre> # let's plot either one of the outputs draw_dot(out[0]) Out[32]: <p>We will further abstract away the neuron and layer creation inside the <code>MLP</code> class. We will then reimplement the exact same network.</p> In\u00a0[33]: Copied! <pre>class MLP:\n  def __init__(self, N, counts):\n    dims = [N] + counts # concatenates dimensions\n    self.layers = [Layer(dims[i], dims[i+1]) for i in range(len(dims)-1)]\n\n  def __call__(self, X):\n    out = X\n    for layer in self.layers:\n      out = layer(out)\n    return out\n</pre> class MLP:   def __init__(self, N, counts):     dims = [N] + counts # concatenates dimensions     self.layers = [Layer(dims[i], dims[i+1]) for i in range(len(dims)-1)]    def __call__(self, X):     out = X     for layer in self.layers:       out = layer(out)     return out In\u00a0[34]: Copied! <pre>nn = MLP(N, [3, 4, 2])\nout = nn(X)\ndraw_dot(out[0]) # out[1] will return the second output\n</pre> nn = MLP(N, [3, 4, 2]) out = nn(X) draw_dot(out[0]) # out[1] will return the second output Out[34]: <p>It is time to judge our network by applying it to the real dataset. Even though applying neural networks to Fisher's Iris dataset is a little overkill (as the dataset is simple), it will be a nice demonstration of our MLP's capacity.</p> <p>Iris dataset has 4-dimensional input samples with three possible output classes. We should be able to predict the class of the Iris flower based on the width and height values of its two elements. We will load our dataset and split it into train and test sets.</p> In\u00a0[35]: Copied! <pre>from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\n\nX = iris.data  # 50x3 4-dimensional samples\ny = iris.target # 3 classes (0, 1, 2)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nprint(f'Train data shape: {X_train.shape}, {y_train.shape}')\nprint(f'Test data shape: {X_test.shape}, {y_test.shape}')\nprint(f'Input Samples:\\n {X_train[:5]}')\nprint(f'Labels:\\n {y_train[:5]}')\n</pre> from sklearn import datasets from sklearn.model_selection import train_test_split  iris = datasets.load_iris()  X = iris.data  # 50x3 4-dimensional samples y = iris.target # 3 classes (0, 1, 2)  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  print(f'Train data shape: {X_train.shape}, {y_train.shape}') print(f'Test data shape: {X_test.shape}, {y_test.shape}') print(f'Input Samples:\\n {X_train[:5]}') print(f'Labels:\\n {y_train[:5]}') <pre>Train data shape: (120, 4), (120,)\nTest data shape: (30, 4), (30,)\nInput Samples:\n [[4.4 2.9 1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [6.5 3.  5.5 1.8]\n [6.4 3.1 5.5 1.8]\n [6.3 2.5 5.  1.9]]\nLabels:\n [0 0 2 2 2]\n</pre> <p>We should then convert each element to be a <code>Value</code> object. But before that, let's try out two ready scikit-learn classifiers, <code>LogisticRegression</code> and <code>MLPClassifier</code>, the latter of which can be seen as the extension of the former, which we will implement in its simplistic form. As we have already discussed, <code>LogisticRegression</code> is basically our <code>Neuron</code> class which uses <code>sigmoid</code> (logistic) function as its activation. And in fact, Logistic Regression will simply be our MLP with the layer size for just a single neuron. Thanks to <code>numpy</code> vectorization and other optimizations, the <code>sklearn</code> implementations will be extremely quick.</p> In\u00a0[36]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\naccuracy = accuracy_score(y_test, preds)\nprint(f\"Logistic Regression Accuracy: {accuracy:.2f}\")\n\nmodel = MLPClassifier()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\naccuracy = accuracy_score(y_test, preds)\nprint(f\"MLP Classifier Accuracy: {accuracy:.2f}\")\n</pre> from sklearn.linear_model import LogisticRegression from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score  model = LogisticRegression() model.fit(X_train, y_train) preds = model.predict(X_test) accuracy = accuracy_score(y_test, preds) print(f\"Logistic Regression Accuracy: {accuracy:.2f}\")  model = MLPClassifier() model.fit(X_train, y_train) preds = model.predict(X_test) accuracy = accuracy_score(y_test, preds) print(f\"MLP Classifier Accuracy: {accuracy:.2f}\") <pre>Logistic Regression Accuracy: 1.00\nMLP Classifier Accuracy: 1.00\n</pre> <pre>/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n</pre> <p>When converting labels to <code>Value</code> objects, we can technically scale them down to be between <code>0</code> and <code>1</code> (by multiplying them to <code>0.5</code>). Then we can apply the simple Mean Squared Error (MSE). Because there are only three labels and the dataset is small, it will work, yet it will penalize ordinal labels unnecessarily. For example, our loss should give the same penalty if we predict <code>0</code> instead of <code>1</code> or <code>2</code>  (it is either one type of flower or another). MSE, however, will give more penalty when predicting <code>0</code> instead of <code>2</code> and less penalty when predicting <code>0</code> instead of <code>1</code> because of the imaginary distances between numbers, which do not exist among the real labels.</p> In\u00a0[37]: Copied! <pre># converting numpy float arrays into Value lists\n# scaling labels to be between 0 and 1 to simplify our code\n# again, it is not a right way to treat the labels\nX_train = [[Value(x) for x in X] for X in X_train]\nX_test = [[Value(x) for x in X] for X in X_test]\ny_train = [Value(y) * Value(0.5) for y in y_train]\ny_test = [Value(y) * Value(0.5) for y in y_test]\n</pre> # converting numpy float arrays into Value lists # scaling labels to be between 0 and 1 to simplify our code # again, it is not a right way to treat the labels X_train = [[Value(x) for x in X] for X in X_train] X_test = [[Value(x) for x in X] for X in X_test] y_train = [Value(y) * Value(0.5) for y in y_train] y_test = [Value(y) * Value(0.5) for y in y_test] <p>We will now create a class for our model, in the style of <code>scikit-learn</code> models, more specifically MLPClassifier.</p> <p>As noted above, what we do is simple and will work in this case, but is not right. Ideally, we should initially one hot encode the ground truth labels to describe them with only zeros and ones. Our final layer for multiclass classification, instead of <code>sigmoid</code> activation, should output <code>logits</code> (unprocessed predictions) passing through <code>softmax</code> function. The loss in this case should be Cross Entropy instead of MSE.</p> <p>Exercise (Advanced): Write code for the correct implementation noted above. See Josh Starmer's (StatQuest) videos which explain its theory, including the Iris dataset, argmax and softmax functions, as well as Cross Entropy.</p> In\u00a0[\u00a0]: Copied! <pre>class Classifier:\n  def __init__(self, layer_sizes=[2, 3, 1]):\n    self.layer_sizes = layer_sizes\n    self.nn = None\n    self.L = None\n    self.iterations = 0\n\n  def forward(self, Xs):\n    out = [self.nn(X) for X in Xs]\n    return out\n\n  def predict(self, X_test):\n    return self.forward(X_test)\n\n  def train(self, X_train, y_train, learning_rate=0.01):\n    preds = self.forward(X_train)\n    self.L = self.mean_squared_error(y_train, preds)\n    self.L.zero_grad()\n    self.L.backward()\n    self.L.optimize(learning_rate=learning_rate)\n    print(f'Loss: {self.L.data:.4f}')\n\n  def fit(self, X_train, y_train, learning_rate=0.01, num_epochs=50):\n    if not self.nn: # in order to not restart training if nn exists\n      self.nn = MLP(len(X_train), self.layer_sizes)\n    for i in range(num_epochs):\n      print(f'Training epoch {self.iterations + i + 1}')\n      self.train(X_train, y_train, learning_rate)\n    self.iterations += i + 1\n\n  def mean_squared_error(self, y_train, preds):\n    return sum([(y-y_hat)*(y-y_hat) for y, y_hat in zip(y_train, preds)], Value(0))\n\n  def score(self, y_test, preds):\n    return self.mean_squared_error(y_test, preds).data / len(y_test)\n\n  def accuracy_score(self, y_test, preds):\n    # due to incorrect handling of the labels\n    # we need to scale y_test and preds values back\n    y_test = [y * Value(2) for y in y_test]\n    preds = [y_hat * Value(2) for y_hat in preds]\n    correct = sum(1 for y, y_hat in zip(y_test, preds) if round(y_hat.data) == y.data)\n    total = len(y_test)\n    return (correct/total)\n</pre> class Classifier:   def __init__(self, layer_sizes=[2, 3, 1]):     self.layer_sizes = layer_sizes     self.nn = None     self.L = None     self.iterations = 0    def forward(self, Xs):     out = [self.nn(X) for X in Xs]     return out    def predict(self, X_test):     return self.forward(X_test)    def train(self, X_train, y_train, learning_rate=0.01):     preds = self.forward(X_train)     self.L = self.mean_squared_error(y_train, preds)     self.L.zero_grad()     self.L.backward()     self.L.optimize(learning_rate=learning_rate)     print(f'Loss: {self.L.data:.4f}')    def fit(self, X_train, y_train, learning_rate=0.01, num_epochs=50):     if not self.nn: # in order to not restart training if nn exists       self.nn = MLP(len(X_train), self.layer_sizes)     for i in range(num_epochs):       print(f'Training epoch {self.iterations + i + 1}')       self.train(X_train, y_train, learning_rate)     self.iterations += i + 1    def mean_squared_error(self, y_train, preds):     return sum([(y-y_hat)*(y-y_hat) for y, y_hat in zip(y_train, preds)], Value(0))    def score(self, y_test, preds):     return self.mean_squared_error(y_test, preds).data / len(y_test)    def accuracy_score(self, y_test, preds):     # due to incorrect handling of the labels     # we need to scale y_test and preds values back     y_test = [y * Value(2) for y in y_test]     preds = [y_hat * Value(2) for y_hat in preds]     correct = sum(1 for y, y_hat in zip(y_test, preds) if round(y_hat.data) == y.data)     total = len(y_test)     return (correct/total) <p>Our naive classifer is ready and we can now train our model and note the accuracy. However, unlike the optimized classifiers of the <code>sklearn</code> library, it will be much slower and inefficient. Try out experiments with different layer sizes and learning rates, and notice how it affects the training process and loss. As we have mentioned before, we can implement Logistic Regression by simply passing <code>layer_sizes=[1]</code> to our MLP classifier.</p> In\u00a0[45]: Copied! <pre>model = Classifier([1])\n# model = Classifier([4, 1])\n</pre> model = Classifier([1]) # model = Classifier([4, 1]) In\u00a0[48]: Copied! <pre>model.fit(X_train, y_train, learning_rate=0.002, num_epochs=30)\n</pre> model.fit(X_train, y_train, learning_rate=0.002, num_epochs=30) <pre>Training epoch 61\nLoss: 1.3948\nTraining epoch 62\nLoss: 1.3938\nTraining epoch 63\nLoss: 1.3929\nTraining epoch 64\nLoss: 1.3920\nTraining epoch 65\nLoss: 1.3911\nTraining epoch 66\nLoss: 1.3901\nTraining epoch 67\nLoss: 1.3892\nTraining epoch 68\nLoss: 1.3883\nTraining epoch 69\nLoss: 1.3874\nTraining epoch 70\nLoss: 1.3865\nTraining epoch 71\nLoss: 1.3856\nTraining epoch 72\nLoss: 1.3847\nTraining epoch 73\nLoss: 1.3838\nTraining epoch 74\nLoss: 1.3830\nTraining epoch 75\nLoss: 1.3821\nTraining epoch 76\nLoss: 1.3812\nTraining epoch 77\nLoss: 1.3803\nTraining epoch 78\nLoss: 1.3795\nTraining epoch 79\nLoss: 1.3786\nTraining epoch 80\nLoss: 1.3777\nTraining epoch 81\nLoss: 1.3769\nTraining epoch 82\nLoss: 1.3760\nTraining epoch 83\nLoss: 1.3752\nTraining epoch 84\nLoss: 1.3743\nTraining epoch 85\nLoss: 1.3735\nTraining epoch 86\nLoss: 1.3726\nTraining epoch 87\nLoss: 1.3718\nTraining epoch 88\nLoss: 1.3710\nTraining epoch 89\nLoss: 1.3701\nTraining epoch 90\nLoss: 1.3693\n</pre> In\u00a0[49]: Copied! <pre>preds = model.predict(X_train)\nprint(f'Custom MLP classifier accuracy on train Data: {model.accuracy_score(y_train, preds):.2f}')\n</pre> preds = model.predict(X_train) print(f'Custom MLP classifier accuracy on train Data: {model.accuracy_score(y_train, preds):.2f}') <pre>Custom MLP classifier accuracy on train Data: 0.97\n</pre> In\u00a0[50]: Copied! <pre>preds = model.predict(X_test)\nprint(f'Custom MLP classifier accuracy on test Data: {model.accuracy_score(y_test, preds):.2f}')\n</pre> preds = model.predict(X_test) print(f'Custom MLP classifier accuracy on test Data: {model.accuracy_score(y_test, preds):.2f}') <pre>Custom MLP classifier accuracy on test Data: 0.97\n</pre> In\u00a0[51]: Copied! <pre>for i in range(len(y_test)):\n  print(y_test[i].data * 2, preds[i].data * 2)\n</pre> for i in range(len(y_test)):   print(y_test[i].data * 2, preds[i].data * 2) <pre>2.0 1.7378279677947062\n1.0 1.3786491598121862\n2.0 1.7797233124612888\n2.0 1.5546211532302696\n1.0 0.7808593322180695\n1.0 1.1323777346177633\n0.0 0.03179964376674919\n2.0 1.6900839613983492\n2.0 1.818651271834627\n2.0 1.5712929781499119\n2.0 1.9175600703929185\n2.0 1.8478802623012778\n2.0 1.8502101169387906\n0.0 0.024109593550568645\n1.0 1.6944624014543954\n1.0 1.1691956406120396\n2.0 1.6617236866912894\n2.0 1.7796042897673732\n1.0 1.186491156964572\n2.0 1.7267232886353379\n1.0 1.197352283588582\n0.0 0.01104177713999268\n2.0 1.860650974045892\n0.0 0.029735232803150373\n1.0 1.021880542808286\n1.0 0.7892688298891827\n0.0 0.026875629893556307\n1.0 1.219772374852777\n0.0 0.015342452967957023\n2.0 1.5336924121298994\n</pre>"},{"location":"Main_Content/02_neural_network/#02-python-code-from-neuron-to-neural-network","title":"02 Python Code from Neuron to Neural Network\u00b6","text":""},{"location":"Main_Content/02_neural_network/#recall-backpropagation","title":"Recall: Backpropagation\u00b6","text":""},{"location":"Main_Content/02_neural_network/#activation-function","title":"Activation Function\u00b6","text":""},{"location":"Main_Content/02_neural_network/#artificial-neuron","title":"Artificial Neuron\u00b6","text":""},{"location":"Main_Content/02_neural_network/#n-dimensional-neuron","title":"N-dimensional Neuron\u00b6","text":""},{"location":"Main_Content/02_neural_network/#artificial-neural-network","title":"Artificial Neural Network\u00b6","text":""},{"location":"Main_Content/02_neural_network/#iris-dataset","title":"Iris Dataset\u00b6","text":""},{"location":"Main_Content/02_neural_network/#training-custom-mlp-classifier","title":"Training Custom MLP Classifier\u00b6","text":""},{"location":"Main_Content/03_cnn_torch/","title":"03 PyTorch Code from Kernel to Convolutional Neural Network","text":"<p>In previous lectures, we learned how to build a neural network. We will come back to discuss many optimization and regularization techniques (e.g. parameter initialization, momentum, weight decay, dropout, batch normalization, etc.) to improve the efficiency of our training. But before coming back to these important notions, we will discuss Convolutional Neural Networks (CNN) which are used in image processing. And even before that, we wil learn what an image is.</p> <p>A grayscale image is simply a 2D array holding pixel values, often ranging between <code>0</code> and <code>1</code> (normalized) or <code>0</code> and <code>255</code> (8-bit image), representing the range of intensities between black and white. A colored image often is represented by the RGB color model, where we have 3 sets (channels) of 2D arrays for Red, Green, and Blue. Combinations of three values represent additional colors to black and white (e.g. <code>[255, 0, 0]</code> is red).</p> <p>We will create a toy image to get a feeling of how images are stored in the memory. We will also delibarately use <code>torch</code> library to familiarize ourselves with <code>PyTorch</code> functions. In <code>PyTorch</code>, images are stored in <code>(C, H, W)</code> format, corresponding to channel, height, and width. Note that batch size can also be included <code>(B, C, H, W)</code>. Batch is simply a subset of a dataset. We  should process our dataset in batches in order to not load all the images at once to the RAM memory, which is usually limited to several <code>Gigabytes</code> (see the top right corner of a <code>Colab</code> notebook for RAM information). Tensor is the main data abstraction in PyTorch, similar to <code>numpy</code> arrays. Tensors are optimized for GPU and can calculate gradients with its autograd engine (i.e. advanced micrograd), which we saw in the previous lecture.</p> In\u00a0[1]: Copied! <pre>import torch\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import torch from torchvision import datasets, transforms  import matplotlib.pyplot as plt %matplotlib inline In\u00a0[2]: Copied! <pre># this is for plotting, skip for now\ndef plot(img_tensors, titles=None):\n  if not isinstance(img_tensors, list):\n    img_tensors = [img_tensors]\n  if not isinstance(titles, list):\n    titles = [titles] * len(img_tensors)\n\n  fig, axes = plt.subplots(1, len(img_tensors), figsize=(len(img_tensors) * 4, 4))\n  if len(img_tensors) == 1:\n    axes = [axes]\n\n  for ax, img_tensor, title in zip(axes, img_tensors, titles):\n\n    img_tensor = img_tensor.cpu()\n    img = img_tensor.permute(1, 2, 0).numpy()\n    ax.imshow(img, cmap='gray')\n    ax.set_title(title)\n    ax.axis(\"off\")\n\n  plt.show()\n</pre> # this is for plotting, skip for now def plot(img_tensors, titles=None):   if not isinstance(img_tensors, list):     img_tensors = [img_tensors]   if not isinstance(titles, list):     titles = [titles] * len(img_tensors)    fig, axes = plt.subplots(1, len(img_tensors), figsize=(len(img_tensors) * 4, 4))   if len(img_tensors) == 1:     axes = [axes]    for ax, img_tensor, title in zip(axes, img_tensors, titles):      img_tensor = img_tensor.cpu()     img = img_tensor.permute(1, 2, 0).numpy()     ax.imshow(img, cmap='gray')     ax.set_title(title)     ax.axis(\"off\")    plt.show() In\u00a0[3]: Copied! <pre>color = ['gray','rgb'][0] # change 0 to 1 for rgb\n\nH = W = 8\nC = 3 if color == 'rgb' else 1\n\nimg = torch.rand((C, H, W))\nplot(img)\n</pre> color = ['gray','rgb'][0] # change 0 to 1 for rgb  H = W = 8 C = 3 if color == 'rgb' else 1  img = torch.rand((C, H, W)) plot(img) In\u00a0[4]: Copied! <pre>img\n</pre> img Out[4]: <pre>tensor([[[0.6681, 0.9885, 0.3858, 0.6401, 0.8704, 0.5164, 0.6299, 0.8198],\n         [0.5352, 0.3675, 0.8261, 0.6846, 0.7454, 0.7086, 0.6866, 0.0161],\n         [0.5431, 0.9798, 0.7918, 0.1760, 0.0203, 0.2780, 0.8877, 0.1514],\n         [0.4076, 0.0945, 0.1907, 0.3847, 0.2569, 0.0359, 0.3500, 0.5701],\n         [0.0235, 0.6880, 0.2567, 0.5896, 0.3844, 0.9977, 0.2146, 0.1763],\n         [0.3725, 0.7660, 0.5657, 0.1217, 0.5217, 0.2664, 0.6946, 0.1770],\n         [0.1950, 0.0631, 0.5228, 0.1239, 0.7185, 0.7971, 0.2741, 0.3123],\n         [0.8518, 0.9311, 0.3031, 0.2274, 0.1595, 0.2751, 0.4300, 0.4698]]])</pre> <p>Technically, if we have a label for the image, and enough images in the dataset, we can already train a neural network to make predictions. For that, we can turn our 2D (grayscale) or 3D (RGB) tensors into vectors and feed in to our model. For example, and image with input dimensions (3,32,32), which corresponds to a 32x32 pixel RGB image, can be reshaped into a single dimensional vector of size (3 * 32 * 32) which will be the input dimension of our network.</p> <p>Question: What will be the shape of 8x8 grayscale image after reshaping?</p> In\u00a0[5]: Copied! <pre>img.view(-1)\n</pre> img.view(-1) Out[5]: <pre>tensor([0.6681, 0.9885, 0.3858, 0.6401, 0.8704, 0.5164, 0.6299, 0.8198, 0.5352,\n        0.3675, 0.8261, 0.6846, 0.7454, 0.7086, 0.6866, 0.0161, 0.5431, 0.9798,\n        0.7918, 0.1760, 0.0203, 0.2780, 0.8877, 0.1514, 0.4076, 0.0945, 0.1907,\n        0.3847, 0.2569, 0.0359, 0.3500, 0.5701, 0.0235, 0.6880, 0.2567, 0.5896,\n        0.3844, 0.9977, 0.2146, 0.1763, 0.3725, 0.7660, 0.5657, 0.1217, 0.5217,\n        0.2664, 0.6946, 0.1770, 0.1950, 0.0631, 0.5228, 0.1239, 0.7185, 0.7971,\n        0.2741, 0.3123, 0.8518, 0.9311, 0.3031, 0.2274, 0.1595, 0.2751, 0.4300,\n        0.4698])</pre> <p>We will download and train our model on the MNIST dataset, which has 70,000 grayscale 28x28 images of hand-written digits with their labels. Our main goal is to familiarize ourselves with <code>PyTorch</code>. Make sure to go through the Datasets and DataLoaders tutorial. Even though it won't be essential for understanding this notebook, try to answer the following questions: What other transforms can we apply to our dataset and why? Why do we shuffle the <code>train</code> data but not the <code>test</code> data? What are <code>num_workers</code> and <code>batch_size</code>?</p> In\u00a0[7]: Copied! <pre>train_data = datasets.MNIST(root='./data', train=True,  transform=transforms.ToTensor(), download=True)\ntest_data  = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n</pre> train_data = datasets.MNIST(root='./data', train=True,  transform=transforms.ToTensor(), download=True) test_data  = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True) In\u00a0[9]: Copied! <pre>train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, num_workers=2, shuffle=True)\ntest_loader  = torch.utils.data.DataLoader(test_data,  batch_size=64, num_workers=2, shuffle=False)\n</pre> train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, num_workers=2, shuffle=True) test_loader  = torch.utils.data.DataLoader(test_data,  batch_size=64, num_workers=2, shuffle=False) In\u00a0[15]: Copied! <pre>X_train, y_train = next(iter(train_loader)) # gets the images of the batch\nplot(X_train[0], f'Label: {y_train[0].item()} | {X_train[0].shape}')\n</pre> X_train, y_train = next(iter(train_loader)) # gets the images of the batch plot(X_train[0], f'Label: {y_train[0].item()} | {X_train[0].shape}') <p>There are many ways to collapse our multi-dimensional image tensor into a single dimension. It is worth understanding the internals of PyTorch and the workings of the equivalent methods below, especially the difference of memory management between <code>reshape</code> and <code>view</code> functions. We can also see the number of elements with <code>numel</code>.</p> In\u00a0[16]: Copied! <pre># X_train[0].view(-1).shape[0]\n# X_train[0].reshape(-1).shape[0]\n# X_train[0].flatten().shape[0]\nX_train[0].numel() # 1x28x28\n</pre> # X_train[0].view(-1).shape[0] # X_train[0].reshape(-1).shape[0] # X_train[0].flatten().shape[0] X_train[0].numel() # 1x28x28 Out[16]: <pre>784</pre> <p>We will now build a simple neural network and train our model on MNIST, but not from scratch :). When we build our custom network, we need to inherit from <code>torch.nn.Module</code> which will handle many useful operations (e.g. autograd). It will also enforce us to define our <code>forward</code> pass function. Our activation will be <code>ReLU</code> function. Fully Connected (FC) layer (also known as Dense Layer) is simply a network layer where all the neurons are connected to the previous layer neurons (recall: MLP). When applying forward pass, we should reshape our input dimensions from <code>(B, C, H, W)</code> to <code>(B, C*H*W)</code>, so that we can pass the input from <code>train_loader</code> to our FC layer.</p> <p>Exercise: Implement different ways of reshaping tensor <code>(B, C, H, W)</code> to <code>(B, C*H*W)</code>.</p> In\u00a0[17]: Copied! <pre>import torch.nn as nn\nimport torch.optim as optim\n\nclass MLP(nn.Module):\n  def __init__(self, input_size, hidden_size, output_size):\n    super(MLP, self).__init__()\n    self.fc1 = nn.Linear(input_size, hidden_size)\n    self.fc2 = nn.Linear(hidden_size, output_size)\n    self.relu = nn.ReLU()\n\n  def forward(self, X):\n    X = X.view(X.shape[0], -1)\n    return self.fc2(self.relu(self.fc1(X)))\n</pre> import torch.nn as nn import torch.optim as optim  class MLP(nn.Module):   def __init__(self, input_size, hidden_size, output_size):     super(MLP, self).__init__()     self.fc1 = nn.Linear(input_size, hidden_size)     self.fc2 = nn.Linear(hidden_size, output_size)     self.relu = nn.ReLU()    def forward(self, X):     X = X.view(X.shape[0], -1)     return self.fc2(self.relu(self.fc1(X))) <p>We will now initialize our model, define our loss and optimizer. Stochastic Gradient Descent approximates gradient descent accross data batches to achieve faster performance. We will discuss <code>Adam</code> optimizer in the future as well.</p> <p>Question: What should be the input and output layer sizes of our model?</p> <p>Question: What should be our loss function and why?</p> <p>Exercise: Change <code>SGD</code> optimizer to <code>Adam</code> and retrain the model and note the training performance and inference accuracy.</p> In\u00a0[18]: Copied! <pre>model = MLP(784, 128, 10)\ncel = nn.CrossEntropyLoss() # for multiclass classification\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n</pre> model = MLP(784, 128, 10) cel = nn.CrossEntropyLoss() # for multiclass classification optimizer = optim.SGD(model.parameters(), lr=0.001) <p>It is possible to run faster training calculations on GPU with <code>cuda</code>. It is possible to change the runtime in Google Colab through the menu <code>Runtime -&gt; Change runtime type</code>. We will move our model's parameters to the available device with <code>.to()</code>.</p> In\u00a0[19]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model.to(device) Out[19]: <pre>MLP(\n  (fc1): Linear(in_features=784, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n  (relu): ReLU()\n)</pre> <p>As <code>PyTorch</code> is highly optimized and the dataset images are small in size, training will be quick, especially with GPU. We should move our data in batches to the GPU memory as well in order to make it compatible with the model. Backpropagation steps we have repeatedly discussed in the previous lectures. We will train out model for only three epochs to quickly see interesting mistakes of our model, but feel free to train the model for a longer period to achieve a higher accuracy.</p> In\u00a0[20]: Copied! <pre>num_epochs = 3\nfor epoch in range(num_epochs):\n  loss = 0.0\n  for X_train, y_train in train_loader:\n    X_train, y_train = X_train.to(device), y_train.to(device)\n    optimizer.zero_grad()\n    preds = model(X_train)\n    batch_loss = cel(preds, y_train)\n    batch_loss.backward()\n    optimizer.step()\n    loss += batch_loss.item()\n  print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}\")\n</pre> num_epochs = 3 for epoch in range(num_epochs):   loss = 0.0   for X_train, y_train in train_loader:     X_train, y_train = X_train.to(device), y_train.to(device)     optimizer.zero_grad()     preds = model(X_train)     batch_loss = cel(preds, y_train)     batch_loss.backward()     optimizer.step()     loss += batch_loss.item()   print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}\") <pre>Epoch: 1/3, Loss: 2.2294\nEpoch: 2/3, Loss: 2.0287\nEpoch: 3/3, Loss: 1.7497\n</pre> <p>Forward passing unseen data through a trained network to measure how well the model is doing is called inference. In this stage, we do not need to calculate gradients (<code>torch.no_grad()</code>). We will also call <code>model.eval()</code> which is not necessary for our simple network, yet is a good practice to follow. As <code>y_test</code> is a tensor of batch size and the output dimension <code>(B, 10)</code>, we will find top prediction labels on each batch (<code>dim=1</code>) and store them for future visualization. We will then sum the correct predictions and divide by the image count in the test dataset to get the final accuracy score.</p> In\u00a0[21]: Copied! <pre>model.eval()\n\ncorrect = 0\nall_preds = []\nwith torch.no_grad():\n  for X_test, y_test in test_loader:\n    X_test, y_test = X_test.to(device), y_test.to(device)\n    preds = model(X_test)\n    _, top_preds = torch.max(preds, dim=1)\n    correct += (top_preds == y_test).sum().item()\n    all_preds.append(top_preds)\n\naccuracy = 100 * correct / len(test_loader.dataset)\nprint(f\"Accuracy on test data: {accuracy:.2f}%\")\n</pre> model.eval()  correct = 0 all_preds = [] with torch.no_grad():   for X_test, y_test in test_loader:     X_test, y_test = X_test.to(device), y_test.to(device)     preds = model(X_test)     _, top_preds = torch.max(preds, dim=1)     correct += (top_preds == y_test).sum().item()     all_preds.append(top_preds)  accuracy = 100 * correct / len(test_loader.dataset) print(f\"Accuracy on test data: {accuracy:.2f}%\") <pre>Accuracy on test data: 76.14%\n</pre> <p>Let's visualize some of our predictions in the last test batch. Pay attention to the false predictions of our model. Can you guess why the model could have made those mistakes?</p> In\u00a0[25]: Copied! <pre>plot(\n    [X_test[i] for i in range(4)],\n    [f\"Pred: {all_preds[-1][i]}, True: {y_test[i]}\" for i in range(4)]\n)\n</pre> plot(     [X_test[i] for i in range(4)],     [f\"Pred: {all_preds[-1][i]}, True: {y_test[i]}\" for i in range(4)] ) <p>Our simple MLP with two FC layers acheived a pretty high accuracy. But that was mainly due to the simplicity of the dataset. Processing bigger datasets, also with a more challenging goal in mind (e.g. object detection, segmentation, etc), in addition to huge computational resources, requires from a model to understand spatial relationship in the images. When we reshape our image pixels into a single dimension two major things happen: 1) input size of our neurons drastically increases (a 224x224 pixel RGB image is more than 150,000 input dimension for a FC neuron), and 2) we lose important information about the pixels: their spatial location. It would make sense, if we could somehow also train our model to learn, for example, which pixels are close to each other. Most probably a combination of pixels (superpixels) make up a wheel of a car, or an ear of an animal. It would be great to look at pixels not as distinct and unrelated items (which FC layer does), but as connected and spatially related items.</p> <p>The word \"convolution\" is actually a misnomer, and \"cross-correlation\" would be a more precise name for the operation which we will describe now. The image below is <code>figure 7.2.1</code> from the Dive into Deep Learning book. As can be seen, we have a 2D input tensor (image) and a kernel of size <code>2x2</code>. We can put full kernel onto four different locations in the image (top left, which we see in the image, top right, bottom left and bottom right) and calculate the output. And the output is calculated by a simple dot product: all the overlapping values are multiplied to each other and summed up. The output in the image is calculated as <code>0*0+1*1+3*2+4*3=19</code>.</p> <p>Question: Given the image dimensions and the kernel size, how many times can we move kernel over the image?</p> <p></p> In\u00a0[26]: Copied! <pre>img = torch.arange(9).reshape((1,3,3))\nkernel = torch.arange(4).reshape((1,2,2))\nplot([img, kernel], [f'{img}', f'{kernel}'])\n</pre> img = torch.arange(9).reshape((1,3,3)) kernel = torch.arange(4).reshape((1,2,2)) plot([img, kernel], [f'{img}', f'{kernel}']) <p>If the kernel width is the same as image width, we can put kernel only once on each row of the image. If kernel width is <code>1</code> pixel less than the image width dimension, we can put kernel twice on the image row. The same rule applies to height and vertical movement of the kernel over the image. From that, we can determine the number of steps kernel will move over the image.</p> In\u00a0[27]: Copied! <pre>horizontal_steps = img.shape[1] - kernel.shape[1] + 1 # height\nvertical_steps   = img.shape[2] - kernel.shape[2] + 1 # width\n</pre> horizontal_steps = img.shape[1] - kernel.shape[1] + 1 # height vertical_steps   = img.shape[2] - kernel.shape[2] + 1 # width <p>We will now use for loops for calculating cross-correlation operation, but note that we do it for simplicity, and using loops isn't an efficient choice. Whenever we can, we should use vectorized operations provided by <code>Pytroch</code>. Finally, pay attention how <code>squeeze/unsqueeze</code> functions take care of the channel dimension below.</p> In\u00a0[28]: Copied! <pre>out = torch.zeros((horizontal_steps, vertical_steps))\nfor i in range(horizontal_steps):\n  for j in range(vertical_steps):\n    patch = img.squeeze()[i:kernel.shape[1]+i, j:kernel.shape[2]+j]\n    out[i, j] = torch.sum(kernel.squeeze() * patch)\nout = out.unsqueeze(0)\n\nplot(out, f'{out}')\n</pre> out = torch.zeros((horizontal_steps, vertical_steps)) for i in range(horizontal_steps):   for j in range(vertical_steps):     patch = img.squeeze()[i:kernel.shape[1]+i, j:kernel.shape[2]+j]     out[i, j] = torch.sum(kernel.squeeze() * patch) out = out.unsqueeze(0)  plot(out, f'{out}') <p>The good news is that we do not have to implement complicated (and inefficient) cross-correlation operations in higher dimensions. <code>torch.nn.functional</code> provides functions for that, which, however accept arguments in 4-dimensions, the first dimension being batch size. Again, pay attention to <code>squeeze</code> and <code>unsqueeze</code> dimension values, which will add and remove the fourth (batch) dimension.</p> In\u00a0[29]: Copied! <pre>import torch.nn.functional as F\n\nout = F.conv2d(img.unsqueeze(0), kernel.unsqueeze(0)).squeeze(1)\nplot(out, f'{out}')\n</pre> import torch.nn.functional as F  out = F.conv2d(img.unsqueeze(0), kernel.unsqueeze(0)).squeeze(1) plot(out, f'{out}') <p>Kernels are capable of extracting relevant features from images. We can choose different kernels, depending on what we try to detect in the image. To see how kernel values influence the output of the cross-correlation operation, let's see the following edge detection example. Edges in the toy image below are the points where pixel values change extremely (from 0 to 1 or vice versa).</p> In\u00a0[30]: Copied! <pre>img = torch.zeros((1, 6, 8))\nimg[:, :, 2:6] = 1.0\nplot(img, f'{img}\\n{img.shape}')\n</pre> img = torch.zeros((1, 6, 8)) img[:, :, 2:6] = 1.0 plot(img, f'{img}\\n{img.shape}') <p>Question: What kind of kernel would be appropriate for detecting edges?</p> <p>Try to understand why the code below detects the edges. Hint: manually calculate a cross-correlation step.</p> In\u00a0[31]: Copied! <pre>kernel = torch.tensor([[[1.0, -1.0]]])\nout = F.conv2d(img.unsqueeze(0), kernel.unsqueeze(0)).squeeze(1)\nplot(out, f'{out}\\n{out.shape}')\n</pre> kernel = torch.tensor([[[1.0, -1.0]]]) out = F.conv2d(img.unsqueeze(0), kernel.unsqueeze(0)).squeeze(1) plot(out, f'{out}\\n{out.shape}') <p>There exists many ready kernels#Details) for getting different image features (e.g. sharpening, blurring, edge detection, etc). But what if we wanted to learn more complicated kernels, how to achieve that? Can we learn correct kernel values, say, for detecting an ear of a dog? It turns out backpropagation will help with that as well. Given image and the edge detected output, we can learn the kernel. Instead of MLP with linear (FC, Dense) layers, we will use a single convolutional layer provided by <code>PyTorch</code> to learn the kernel. <code>nn.Conv2d</code> requires from us to specify the input dimensions, when <code>nn.LazyConv2d</code> will determine the dimension dynamically, when we pass the input image to the model. Note that we are overfitting the model (layer) to a single example. Our goal is to learn a single kernel of size <code>(1,2)</code> for a grayscale image. What the convolutional layer does under the hood we will learn soon.</p> In\u00a0[32]: Copied! <pre>model = nn.LazyConv2d(out_channels=1, kernel_size=(1, 2)).to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nX_img = img.to(device).unsqueeze(0)\ny_out = out.to(device).unsqueeze(0)\n\nnum_epochs = 50\nfor i in range(num_epochs):\n  optimizer.zero_grad()\n  preds = model(X_img)\n  loss = torch.sum((preds - y_out) ** 2) # mse\n  loss.backward()\n  optimizer.step()\n\nprint(f'Epoch: {i+1}/{num_epochs}, Loss: {loss:.4f}')\nprint(f'Learned Kernel: {model.weight.data}')\n</pre> model = nn.LazyConv2d(out_channels=1, kernel_size=(1, 2)).to(device) optimizer = optim.SGD(model.parameters(), lr=0.01)  X_img = img.to(device).unsqueeze(0) y_out = out.to(device).unsqueeze(0)  num_epochs = 50 for i in range(num_epochs):   optimizer.zero_grad()   preds = model(X_img)   loss = torch.sum((preds - y_out) ** 2) # mse   loss.backward()   optimizer.step()  print(f'Epoch: {i+1}/{num_epochs}, Loss: {loss:.4f}') print(f'Learned Kernel: {model.weight.data}') <pre>Epoch: 50/50, Loss: 0.0000\nLearned Kernel: tensor([[[[ 0.9987, -0.9987]]]])\n</pre> <p>Just to see what kernels are capable of, you can try out different kernels and see their outputs on an image in an interactive visualization. But here, let's take 1) a simple blur kernel which averages pixels within its frame, and 2) a Gaussian blur kernel which is basically giving more weight to the pixels that are closer to the middle, by considering Gaussian (normal) distribution (see nice demonstration). We will apply them on a photo and see the output. Note that Gaussian blur is commonly used for detecting edges (increase the size of the kernel and see the output). 3Blue1Brown video called \"But what is a convolution?\" explains cross-correlation and kernel operations in a nice visual way.</p> In\u00a0[34]: Copied! <pre>from skimage import data\n\ntest_img = data.astronaut()\ntest_img = torch.tensor(test_img, dtype=torch.float32).permute(2,0,1) # C, H, W\ntest_img = test_img.mean(0).unsqueeze(0) # RGB to grayscale\n\nH = W = 5 # try out bigger kernel sizes\nmean, std = 0.0, 1.0 # mean and standard deviation of Gaussian distribution\n\nblur_kernel = torch.ones((1,H,W)) * 1/H*W # averaging pixels\ngaussian_blur_kernel = torch.normal(mean, std, size=(1,H,W)) # pixels closer to middle get more weight\n\nblurred  = F.conv2d(test_img.unsqueeze(0), blur_kernel.unsqueeze(0)).squeeze(1)\ngaussian = F.conv2d(test_img.unsqueeze(0), gaussian_blur_kernel.unsqueeze(0)).squeeze(1)\n\nplot([test_img, blurred, gaussian], ['original', 'blurred', 'gaussian'])\n</pre> from skimage import data  test_img = data.astronaut() test_img = torch.tensor(test_img, dtype=torch.float32).permute(2,0,1) # C, H, W test_img = test_img.mean(0).unsqueeze(0) # RGB to grayscale  H = W = 5 # try out bigger kernel sizes mean, std = 0.0, 1.0 # mean and standard deviation of Gaussian distribution  blur_kernel = torch.ones((1,H,W)) * 1/H*W # averaging pixels gaussian_blur_kernel = torch.normal(mean, std, size=(1,H,W)) # pixels closer to middle get more weight  blurred  = F.conv2d(test_img.unsqueeze(0), blur_kernel.unsqueeze(0)).squeeze(1) gaussian = F.conv2d(test_img.unsqueeze(0), gaussian_blur_kernel.unsqueeze(0)).squeeze(1)  plot([test_img, blurred, gaussian], ['original', 'blurred', 'gaussian']) <p>By Michael Plotke - Own work (CC BY-SA 3.0 / Wikimedia Commons)</p> <p></p> <p>When we find <code>valid</code> cross-correlation, we move kernel within the constraints of the image. As a consequence, we pass through boundaries only once, losing some spatial information (pixels on the corners are barely used). <code>Padding</code> resolves this issue.</p> <p>By default, we step with kernel one row/column at a time (as in the image above). <code>Striding</code> determines the step size of our kernel, which is used when we want to reduce the output dimensionality after cross-correlation even further. It may be useful when the kernel size is big, we need faster computation, etc.</p> <p>From this point on, we will replace the more accurate word \"cross-correlation\" with the accepted terminology \"convolution\". Simply put, convolution operation is equivalent to the 90 degrees rotated cross-correlation operation.</p> In\u00a0[35]: Copied! <pre>img = torch.arange(1., 10).reshape((1,3,3))\npadded_img = F.pad(img, (1,1,1,1)) # left right top bottom\nplot([img, padded_img], [f'{img}', f'{padded_img}'])\n</pre> img = torch.arange(1., 10).reshape((1,3,3)) padded_img = F.pad(img, (1,1,1,1)) # left right top bottom plot([img, padded_img], [f'{img}', f'{padded_img}']) In\u00a0[36]: Copied! <pre>kernel = torch.ones((1,1,2,2))\n\nmanual_padding = F.conv2d(padded_img.unsqueeze(0), kernel).squeeze(1)\nconv2d_padding = F.conv2d(img.unsqueeze(0), kernel, padding=1).squeeze(1)\nconv2d_stride  = F.conv2d(img.unsqueeze(0), kernel, padding=1, stride=2).squeeze(1)\n\nplot([manual_padding, conv2d_padding, conv2d_stride],\n     [f'{manual_padding}', f'{conv2d_padding}', f'{conv2d_stride}'])\n</pre> kernel = torch.ones((1,1,2,2))  manual_padding = F.conv2d(padded_img.unsqueeze(0), kernel).squeeze(1) conv2d_padding = F.conv2d(img.unsqueeze(0), kernel, padding=1).squeeze(1) conv2d_stride  = F.conv2d(img.unsqueeze(0), kernel, padding=1, stride=2).squeeze(1)  plot([manual_padding, conv2d_padding, conv2d_stride],      [f'{manual_padding}', f'{conv2d_padding}', f'{conv2d_stride}']) <p>Exercise: Manually calculate the strided convolution above.</p> <p>Exercise: Create 100 random images (colored) and calculate the output dimension after the convolution operation. Calculate and print out how many features will be passed to the next layer after flattening the feature maps for all the cases:</p> Image Size Kernel Size Stride Padding 30x30 3x3 1 0 224x224 5x5 2 1 528x528 7x7 3 2 In\u00a0[37]: Copied! <pre>img = torch.cat((torch.arange(9.), torch.arange(1.,10))).reshape((1,2,3,3))\nimg\n</pre> img = torch.cat((torch.arange(9.), torch.arange(1.,10))).reshape((1,2,3,3)) img Out[37]: <pre>tensor([[[[0., 1., 2.],\n          [3., 4., 5.],\n          [6., 7., 8.]],\n\n         [[1., 2., 3.],\n          [4., 5., 6.],\n          [7., 8., 9.]]]])</pre> In\u00a0[38]: Copied! <pre>kernels = torch.cat((torch.arange(4.), torch.arange(1.,5))).reshape((1,2,2,2))\nkernels\n</pre> kernels = torch.cat((torch.arange(4.), torch.arange(1.,5))).reshape((1,2,2,2)) kernels Out[38]: <pre>tensor([[[[0., 1.],\n          [2., 3.]],\n\n         [[1., 2.],\n          [3., 4.]]]])</pre> In\u00a0[39]: Copied! <pre>conv1 = F.conv2d(img[:,0:1,:,:], kernels[:,0:1,:,:])\nconv2 = F.conv2d(img[:,1:2,:,:], kernels[:,1:2,:,:])\n\nconv1 + conv2\n</pre> conv1 = F.conv2d(img[:,0:1,:,:], kernels[:,0:1,:,:]) conv2 = F.conv2d(img[:,1:2,:,:], kernels[:,1:2,:,:])  conv1 + conv2 Out[39]: <pre>tensor([[[[ 56.,  72.],\n          [104., 120.]]]])</pre> In\u00a0[40]: Copied! <pre># or equivalently\nF.conv2d(img, kernels)\n</pre> # or equivalently F.conv2d(img, kernels) Out[40]: <pre>tensor([[[[ 56.,  72.],\n          [104., 120.]]]])</pre> <p>We may want to not only deal with multiple channels, but also output multiple channels. The simplistic reasoning for it could be that each output channel may hold different feature information. For that, we will apply convolution of each kernel set to the whole image (all dimensions) and eventually concatenate the results along the channel axis.</p> In\u00a0[41]: Copied! <pre>torch.cat([F.conv2d(img, kernels+i) for i in range(3)], dim=0)\n</pre> torch.cat([F.conv2d(img, kernels+i) for i in range(3)], dim=0) Out[41]: <pre>tensor([[[[ 56.,  72.],\n          [104., 120.]]],\n\n\n        [[[ 76., 100.],\n          [148., 172.]]],\n\n\n        [[[ 96., 128.],\n          [192., 224.]]]])</pre> <p>As we know, the advantage of convolutional layers is that they take into account spatial information about the pixels (their locations, neighbors, etc). But how can we make sure that our predictions will not be very sensitive to small changes in pixel locations? Because a cat image where the cat is streching is still a cat image, and our model should correctly classify it, even if it hasn't seen a stretching cat during training. As we will see, applying a <code>pooling</code> layer will help with spatial invariance, as well as downsample the representations of the previous layer.</p> <p>The image below (figure 7.5.1) describes maximum pooling operation, which looks like a kernel, but instead of convolution, simply takes the maximum pixel value within that range. Similarly, an average pooling operation, averages all the corresponding pixels and is equivalent to the simple blur kernel which we saw previously.</p> <p></p> In\u00a0[42]: Copied! <pre>img_channel = img.unbind(1)[0] # unbinds from the channel dimension and takes the first channel\nimg_channel\n</pre> img_channel = img.unbind(1)[0] # unbinds from the channel dimension and takes the first channel img_channel Out[42]: <pre>tensor([[[0., 1., 2.],\n         [3., 4., 5.],\n         [6., 7., 8.]]])</pre> In\u00a0[43]: Copied! <pre>operation = ['avg','max'][1] # change to 0 for average poolig\n\n# Compare the code below with the convolution code\nkernel_height = kernel_width = 2\n\nhorizontal_steps = img_channel.shape[1] - kernel_height + 1\nvertical_steps   = img_channel.shape[2] - kernel_width  + 1\n\nout = torch.zeros((horizontal_steps, vertical_steps))\nfor i in range(horizontal_steps):\n  for j in range(vertical_steps):\n    patch = img_channel.squeeze(0)[i:i+kernel_height, j:j+kernel_width]\n    out[i, j] = patch.mean() if operation == 'avg' else patch.max()\n\nout.unsqueeze(0)\n</pre> operation = ['avg','max'][1] # change to 0 for average poolig  # Compare the code below with the convolution code kernel_height = kernel_width = 2  horizontal_steps = img_channel.shape[1] - kernel_height + 1 vertical_steps   = img_channel.shape[2] - kernel_width  + 1  out = torch.zeros((horizontal_steps, vertical_steps)) for i in range(horizontal_steps):   for j in range(vertical_steps):     patch = img_channel.squeeze(0)[i:i+kernel_height, j:j+kernel_width]     out[i, j] = patch.mean() if operation == 'avg' else patch.max()  out.unsqueeze(0) Out[43]: <pre>tensor([[[4., 5.],\n         [7., 8.]]])</pre> In\u00a0[44]: Copied! <pre># or equivalently\nnn.MaxPool2d(kernel_size=(2,2), stride=1, padding=0)(img_channel)\n</pre> # or equivalently nn.MaxPool2d(kernel_size=(2,2), stride=1, padding=0)(img_channel) Out[44]: <pre>tensor([[[4., 5.],\n         [7., 8.]]])</pre> In\u00a0[45]: Copied! <pre>nn.AvgPool2d(kernel_size=(2,2), stride=1, padding=0)(img_channel)\n</pre> nn.AvgPool2d(kernel_size=(2,2), stride=1, padding=0)(img_channel) Out[45]: <pre>tensor([[[2., 3.],\n         [5., 6.]]])</pre> <p>In multiple dimensions, instead of summing channel outputs of each kernel, we will concatenate them after the pooling operation, which makes sense, as we have to retain channel information. Notice that the output channel is 2 dimensions, as we pass both image channels to <code>nn.MaxPool2d()</code>.</p> In\u00a0[46]: Copied! <pre>nn.MaxPool2d(kernel_size=2, stride=1, padding=0)(img)\n</pre> nn.MaxPool2d(kernel_size=2, stride=1, padding=0)(img) Out[46]: <pre>tensor([[[[4., 5.],\n          [7., 8.]],\n\n         [[5., 6.],\n          [8., 9.]]]])</pre> <p>The origins of Convolutional Neural Networks (CNN) goes back to the 1980 paper called Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position by Kunihiko Fukushima. As the computational power increased and the area developed, infamous CNN implementation by Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner was introduced in the 1998 paper called Gradient-Based Learning Applied to Document Recognition. The convolutional network architecture proposed for detecting handwritten digits (and more) was named after its first author LeNet.</p> <p></p> <p>The figure 7.6.1 describes the LeNet architecture. Convolutional block consists of convolution layer, sigmoid activation function (convolution is a linear function), and average pooling layer. Note that the superiority of max-pooling over average pooling, as well as ReLU activation over Sigmoid activation function weren't yet discovered. Kernel size of convolutional layers is <code>5x5</code> (with initial padding of <code>2</code>). Two convolutional layers output <code>6</code> and <code>16</code> feature maps respectively. Average pooling kernel size is <code>2x2</code> (with stride <code>2</code>).</p> <p>Once we extract the relevant features with convolutional blocks, it is time to make predictions with FC (Dense) layers. For that, we must flatten the feature maps similar to what we did when using our simple <code>MLP</code>, which was directly accepting the image as its inputs, instead of convolutional feature maps. FC layers sizes proposed in the paper are <code>120, 84, 10</code>, classifying 10 handwritten digits.</p> <p>Exercise: Code the model architecture by using the knowledge of <code>MLP class</code>.</p> <p>We will now code and train a modern implementation for <code>LeNet</code>, similar to <code>MLP</code>. We will replace average pool with max-pool layer, Sigmoid with ReLU activation, and the original Gaussian decoder with softmax function. We will use <code>Lazy</code> versions of the layers in order to not hard-code input dimensions.</p> In\u00a0[47]: Copied! <pre>class LeNet(nn.Module):\n  def __init__(self):\n    super(LeNet, self).__init__()\n    self.conv1 = nn.LazyConv2d(out_channels=6, kernel_size=(5,5), padding=2)\n    self.conv2 = nn.LazyConv2d(out_channels=16, kernel_size=(5,5))\n    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n    self.fc1 = nn.LazyLinear(out_features=84)\n    self.fc2 = nn.LazyLinear(out_features=10)\n    self.act = nn.ReLU()\n\n  def forward(self, X):\n    block1 = self.pool(self.act(self.conv1(X)))\n    block2 = self.pool(self.act(self.conv2(block1)))\n    flatten = block2.view(block2.shape[0], -1)\n    logits = self.fc2(self.act(self.fc1(flatten)))\n    return nn.Softmax(dim=-1)(logits)\n</pre> class LeNet(nn.Module):   def __init__(self):     super(LeNet, self).__init__()     self.conv1 = nn.LazyConv2d(out_channels=6, kernel_size=(5,5), padding=2)     self.conv2 = nn.LazyConv2d(out_channels=16, kernel_size=(5,5))     self.pool = nn.MaxPool2d(kernel_size=2, stride=2)     self.fc1 = nn.LazyLinear(out_features=84)     self.fc2 = nn.LazyLinear(out_features=10)     self.act = nn.ReLU()    def forward(self, X):     block1 = self.pool(self.act(self.conv1(X)))     block2 = self.pool(self.act(self.conv2(block1)))     flatten = block2.view(block2.shape[0], -1)     logits = self.fc2(self.act(self.fc1(flatten)))     return nn.Softmax(dim=-1)(logits) In\u00a0[48]: Copied! <pre>mlp = nn.Sequential(\n    nn.Flatten(),\n    nn.LazyLinear(out_features=128),\n    nn.ReLU(),\n    nn.LazyLinear(out_features=10),\n    nn.Softmax(dim=-1)\n)\n\nlenet = nn.Sequential(\n    nn.LazyConv2d(out_channels=6, kernel_size=5, padding=2),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.LazyConv2d(out_channels=16, kernel_size=5),\n    nn.ReLU(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Flatten(),\n    nn.LazyLinear(out_features=84),\n    nn.ReLU(),\n    nn.LazyLinear(out_features=10),\n    nn.Softmax(dim=-1)\n)\n</pre> mlp = nn.Sequential(     nn.Flatten(),     nn.LazyLinear(out_features=128),     nn.ReLU(),     nn.LazyLinear(out_features=10),     nn.Softmax(dim=-1) )  lenet = nn.Sequential(     nn.LazyConv2d(out_channels=6, kernel_size=5, padding=2),     nn.ReLU(),     nn.MaxPool2d(kernel_size=2, stride=2),     nn.LazyConv2d(out_channels=16, kernel_size=5),     nn.ReLU(),     nn.AvgPool2d(kernel_size=2, stride=2),     nn.Flatten(),     nn.LazyLinear(out_features=84),     nn.ReLU(),     nn.LazyLinear(out_features=10),     nn.Softmax(dim=-1) ) In\u00a0[49]: Copied! <pre>class Classifier:\n  def __init__(self, model, lr=0.01):\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.model = model.to(device)\n    self.optimizer = optim.SGD(model.parameters(), lr=lr)\n    self.loss_fn = nn.CrossEntropyLoss()\n\n  def fit(self, train_loader, num_epochs=10):\n    self.model.train()\n    for epoch in range(num_epochs):\n      loss = 0.0\n      for X_train, y_train in train_loader:\n        X_train, y_train = X_train.to(device), y_train.to(device)\n        self.optimizer.zero_grad()\n        preds = self.model(X_train)\n        batch_loss = self.loss_fn(preds, y_train)\n        batch_loss.backward()\n        loss += batch_loss.item()\n        self.optimizer.step()\n      print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}\")\n\n  def inference(self, test_loader):\n    self.model.eval()\n    correct = 0\n    all_preds = []\n    with torch.no_grad():\n      for X_test, y_test in test_loader:\n        X_test, y_test = X_test.to(device), y_test.to(device)\n        preds = self.model(X_test)\n        _, top_preds = torch.max(preds, dim=1)\n        correct += (top_preds == y_test).sum().item()\n        all_preds.append(top_preds)\n    accuracy = 100 * correct / len(test_loader.dataset)\n    print(f\"Accuracy on test data: {accuracy:.2f}%\")\n</pre> class Classifier:   def __init__(self, model, lr=0.01):     self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     self.model = model.to(device)     self.optimizer = optim.SGD(model.parameters(), lr=lr)     self.loss_fn = nn.CrossEntropyLoss()    def fit(self, train_loader, num_epochs=10):     self.model.train()     for epoch in range(num_epochs):       loss = 0.0       for X_train, y_train in train_loader:         X_train, y_train = X_train.to(device), y_train.to(device)         self.optimizer.zero_grad()         preds = self.model(X_train)         batch_loss = self.loss_fn(preds, y_train)         batch_loss.backward()         loss += batch_loss.item()         self.optimizer.step()       print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}\")    def inference(self, test_loader):     self.model.eval()     correct = 0     all_preds = []     with torch.no_grad():       for X_test, y_test in test_loader:         X_test, y_test = X_test.to(device), y_test.to(device)         preds = self.model(X_test)         _, top_preds = torch.max(preds, dim=1)         correct += (top_preds == y_test).sum().item()         all_preds.append(top_preds)     accuracy = 100 * correct / len(test_loader.dataset)     print(f\"Accuracy on test data: {accuracy:.2f}%\") In\u00a0[50]: Copied! <pre>clf_mlp = Classifier(mlp)\nclf_mlp.fit(train_loader)\nclf_mlp.inference(test_loader)\n</pre> clf_mlp = Classifier(mlp) clf_mlp.fit(train_loader) clf_mlp.inference(test_loader) <pre>Epoch: 1/10, Loss: 2.2903\nEpoch: 2/10, Loss: 2.2232\nEpoch: 3/10, Loss: 2.0272\nEpoch: 4/10, Loss: 1.8475\nEpoch: 5/10, Loss: 1.7862\nEpoch: 6/10, Loss: 1.7610\nEpoch: 7/10, Loss: 1.7387\nEpoch: 8/10, Loss: 1.7022\nEpoch: 9/10, Loss: 1.6840\nEpoch: 10/10, Loss: 1.6726\nAccuracy on test data: 83.36%\n</pre> In\u00a0[51]: Copied! <pre>clf_lenet = Classifier(lenet)\nclf_lenet.fit(train_loader)\nclf_lenet.inference(test_loader)\n</pre> clf_lenet = Classifier(lenet) clf_lenet.fit(train_loader) clf_lenet.inference(test_loader) <pre>Epoch: 1/10, Loss: 2.3026\nEpoch: 2/10, Loss: 2.3020\nEpoch: 3/10, Loss: 2.3011\nEpoch: 4/10, Loss: 2.2996\nEpoch: 5/10, Loss: 2.2953\nEpoch: 6/10, Loss: 2.2337\nEpoch: 7/10, Loss: 1.8479\nEpoch: 8/10, Loss: 1.6735\nEpoch: 9/10, Loss: 1.6107\nEpoch: 10/10, Loss: 1.5920\nAccuracy on test data: 88.74%\n</pre> <p>Finally, it is important to note that convolutions are expensive matrix operations, when GPUs can come in aid. Also, do not rush to compare <code>MLP</code> with <code>LeNet</code>, as our <code>MNIST</code> dataset is very simple. In the next lecture, we will discuss many optimization and regularization techniques of artificial neural networks.</p>"},{"location":"Main_Content/03_cnn_torch/#03-pytorch-code-from-kernel-to-convolutional-neural-network","title":"03 PyTorch Code from Kernel to Convolutional Neural Network\u00b6","text":""},{"location":"Main_Content/03_cnn_torch/#image-dimensions","title":"Image Dimensions\u00b6","text":""},{"location":"Main_Content/03_cnn_torch/#mnist-dataset","title":"MNIST Dataset\u00b6","text":""},{"location":"Main_Content/03_cnn_torch/#training-custom-pytorch-model-on-mnist","title":"Training Custom PyTorch Model on MNIST\u00b6","text":""},{"location":"Main_Content/03_cnn_torch/#cross-correlation-operation","title":"Cross-Correlation Operation\u00b6","text":""},{"location":"Main_Content/03_cnn_torch/#kernels","title":"Kernels\u00b6","text":""},{"location":"Main_Content/03_cnn_torch/#padding-stride","title":"Padding &amp; Stride\u00b6","text":""},{"location":"Main_Content/03_cnn_torch/#increasing-dimensions","title":"Increasing Dimensions\u00b6","text":"<p>So far we have been working with grayscale images. When we increase the dimension of channels <code>C</code>, we will simply have <code>C</code> number of kernels. We will apply convolution of each kernel to their corresponding channels and eventually sum them up. Below is figure 7.4.1.</p> <p></p>"},{"location":"Main_Content/03_cnn_torch/#maximum-average-pooling","title":"Maximum &amp; Average Pooling\u00b6","text":""},{"location":"Main_Content/03_cnn_torch/#convolutional-neural-network","title":"Convolutional Neural Network\u00b6","text":""},{"location":"Main_Content/03_cnn_torch/#sequential-module-in-pytorch","title":"Sequential Module in PyTorch\u00b6","text":"<p>Before testing our convnet, we will rewrite our model classes in order to demonstrate the power and clarity of <code>nn.Sequential</code> module in <code>PyTorch</code>. The code below is self-explanatory and is equivalent to <code>MLP</code> and <code>LeNet</code> classes we had initilaized previously. Notice how simple it is.</p>"},{"location":"Main_Content/03_cnn_torch/#training-inference","title":"Training &amp; Inference\u00b6","text":"<p>We will abstract away our model initalization, train and inference codes before testing our models.</p>"},{"location":"Main_Content/04_regul_optim/","title":"04 Python Code for Deep Learning Regularization and Optimization","text":"<p>Note: The notebook is highly based on the examples given in different chapters of the Dive Deep into Deep Learning book.</p> <p>Our artificial neural networks which we built from scratch in previous classes were in their simplistic form and very inefficient, causing slow and less accurate training. In this notebook we will introduce many regularization/optimization techniques which will improve our model performance.</p> <p>The prerequisite of this course, Machine Learning, should have introduced the problems of underfitting and overfitting (See StatQuest video and MLU vizualization on bias and variance). Regularization) techniques like weight decay tackle high variance (overfitting) without getting more high-quality data, which is often costly.</p> <p>Loss function $L$ (e.g. MSE) can be regularized by adding to it regularizer $R$, where \u03bb is the regularization hyperparameter that controls the strength of the regularization function (we saw <code>learning rate</code> hyperparameter previously).</p> <p>$$ L(w, b) + \\lambda R(w) $$</p> <p>Even if there are many different regularizers for loss, the most common and practically efficient one is $l_2$ (ridge) regularization, which is called weight decay in the context of Deep Learning. It has the following formula:</p> <p>$$ L(w, b) + \\frac{\\lambda}{2} \\sum_{i=1}^{d} w_i^2 $$</p> <p>As can be seen, the eventual loss increases for higher weight values. Hence, backpropagation will not only reduce the chosen loss function (e.g. MSE) but will also strive for smaller weights. You can imagine that in the limit weights approach zero, reducing the impact of the corresponding neuron on the outcome. Less neuron impact means getting simpler function to avoid overfitting.</p> <p>When $\u03bb$ is zero we restore the original loss function, when $\u03bb$ is large it forces shifts the attention from the original loss function to $R$, constraining $w$ further. We divide by $2$ so that when the derivative of the square will be found, it will get cancelled out with it, simplifying our derivative expression to $ \u03bbw_i $. Recall linear algebra that we are finding the square of the euclidean norm of $d$-dimensional vector, again to ease computation: we remove the burden of finding the square root in $l_2$.</p> In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import torch import torch.nn as nn import torch.optim as optim  import numpy as np import matplotlib.pyplot as plt %matplotlib inline <p>We will now implement the simplified and clearer version of the example noted in the d2l book. We will generate random data corresponding to an N-dimensional linear function, where each data point will be shifted by some amount of noise.</p> In\u00a0[3]: Copied! <pre>def generate_data(samples=100, N=100, test_size=0.5, b=0.05, scale=0.01):\n  X = torch.randn(samples, N)\n  w = torch.randn(N, 1) * scale # scaling will ease training\n  n = torch.randn(samples, 1) * scale # noise is not a bias\n  y = torch.matmul(X, w) + n + b\n\n  size = int(samples * test_size)\n  X_train, X_test = X[:-size], X[-size:]\n  y_train, y_test = y[:-size], y[-size:]\n\n  return X_train, X_test, y_train, y_test\n</pre> def generate_data(samples=100, N=100, test_size=0.5, b=0.05, scale=0.01):   X = torch.randn(samples, N)   w = torch.randn(N, 1) * scale # scaling will ease training   n = torch.randn(samples, 1) * scale # noise is not a bias   y = torch.matmul(X, w) + n + b    size = int(samples * test_size)   X_train, X_test = X[:-size], X[-size:]   y_train, y_test = y[:-size], y[-size:]    return X_train, X_test, y_train, y_test In\u00a0[4]: Copied! <pre>X_train, _, y_train, _ = generate_data(100, 1) # 1D input for plotting\nplt.scatter(X_train, y_train);\n</pre> X_train, _, y_train, _ = generate_data(100, 1) # 1D input for plotting plt.scatter(X_train, y_train); <p>We will now generate a bigger dataset (both in the number of samples and dimensions), a good deal of which will be used for testing (so that we can illustrate overfitting). We will then create simple <code>Linear Regression</code> model and train it. When training we will implement the $l_2$ regularization discussed above to see how it affects the test loss.</p> In\u00a0[5]: Copied! <pre>X_train, X_test, y_train, y_test = generate_data(samples=100, N=200, test_size=0.5, scale=0.01)\n</pre> X_train, X_test, y_train, y_test = generate_data(samples=100, N=200, test_size=0.5, scale=0.01) In\u00a0[6]: Copied! <pre>def train(num_epochs=1000, lambda_=0.1):\n  model = nn.LazyLinear(1)\n  optimizer = optim.SGD(model.parameters(), lr=0.001)\n  mse = nn.MSELoss()\n\n  # needed for plotting\n  train_losses = []\n  test_losses = []\n\n  for epoch in range(num_epochs):\n    optimizer.zero_grad()\n    pred = model(X_train)\n    # regularization\n    l2 = sum(p.pow(2).sum() for p in model.parameters())/2\n    loss = mse(pred, y_train) + lambda_ * l2 # see formula above\n    loss.backward()\n    optimizer.step()\n\n    # inference during training\n    with torch.no_grad():\n      test_pred = model(X_test)\n      test_loss = mse(test_pred, y_test)\n\n    # needed for plotting\n    train_losses.append(loss.item())\n    test_losses.append(test_loss.item())\n\n    if epoch % 200 == 0:\n      print(f'Epoch {epoch}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n\n  # plotting\n  plt.figure(figsize=(6, 4))\n  plt.plot(range(num_epochs), train_losses, label='Train Loss')\n  plt.plot(range(num_epochs), test_losses, label='Test Loss')\n  plt.xlabel('Epochs')\n  plt.ylabel('Loss')\n</pre> def train(num_epochs=1000, lambda_=0.1):   model = nn.LazyLinear(1)   optimizer = optim.SGD(model.parameters(), lr=0.001)   mse = nn.MSELoss()    # needed for plotting   train_losses = []   test_losses = []    for epoch in range(num_epochs):     optimizer.zero_grad()     pred = model(X_train)     # regularization     l2 = sum(p.pow(2).sum() for p in model.parameters())/2     loss = mse(pred, y_train) + lambda_ * l2 # see formula above     loss.backward()     optimizer.step()      # inference during training     with torch.no_grad():       test_pred = model(X_test)       test_loss = mse(test_pred, y_test)      # needed for plotting     train_losses.append(loss.item())     test_losses.append(test_loss.item())      if epoch % 200 == 0:       print(f'Epoch {epoch}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')    # plotting   plt.figure(figsize=(6, 4))   plt.plot(range(num_epochs), train_losses, label='Train Loss')   plt.plot(range(num_epochs), test_losses, label='Test Loss')   plt.xlabel('Epochs')   plt.ylabel('Loss') <p>Try to understand why the line which calculates regularization function's output has two <code>sum</code> functions: <code>l2 = sum(p.pow(2).sum() for p in model.parameters())/2</code>. We have plotting logic in the function above for demonstration purposes, and we will give different $\u03bb$ values to customize our regularizer $R$ ($\u03bb$=0 implies no regularization).</p> In\u00a0[7]: Copied! <pre>train(lambda_=0.0) # no regularization\n</pre> train(lambda_=0.0) # no regularization <pre>Epoch 0, Train Loss: 0.3230, Test Loss: 0.4413\nEpoch 200, Train Loss: 0.0165, Test Loss: 0.3265\nEpoch 400, Train Loss: 0.0023, Test Loss: 0.3051\nEpoch 600, Train Loss: 0.0005, Test Loss: 0.2979\nEpoch 800, Train Loss: 0.0002, Test Loss: 0.2950\n</pre> In\u00a0[8]: Copied! <pre>train(lambda_=5.0) # with regularization\n</pre> train(lambda_=5.0) # with regularization <pre>Epoch 0, Train Loss: 1.1423, Test Loss: 0.2795\nEpoch 200, Train Loss: 0.1009, Test Loss: 0.0429\nEpoch 400, Train Loss: 0.0218, Test Loss: 0.0185\nEpoch 600, Train Loss: 0.0116, Test Loss: 0.0153\nEpoch 800, Train Loss: 0.0103, Test Loss: 0.0150\n</pre> In\u00a0[9]: Copied! <pre>X = torch.arange(1,7)\nprint(\"Initial layer activations:\", X)\n\nprob = 0.5 # try out different values between 0 and 1\nmask = (torch.rand(X.shape) &gt; prob ).float()\nprint(f\"Random mask with dropout probability {prob}: {mask}\")\n\nX_dropout = X * mask\nprint(f\"Layer activations after dropout {X_dropout}\")\n\nX_dropout_scaled = X_dropout / (1.0 - prob + 1e-9)\nprint(f\"Scaled activations after dropout {X_dropout_scaled}\")\n</pre> X = torch.arange(1,7) print(\"Initial layer activations:\", X)  prob = 0.5 # try out different values between 0 and 1 mask = (torch.rand(X.shape) &gt; prob ).float() print(f\"Random mask with dropout probability {prob}: {mask}\")  X_dropout = X * mask print(f\"Layer activations after dropout {X_dropout}\")  X_dropout_scaled = X_dropout / (1.0 - prob + 1e-9) print(f\"Scaled activations after dropout {X_dropout_scaled}\") <pre>Initial layer activations: tensor([1, 2, 3, 4, 5, 6])\nRandom mask with dropout probability 0.5: tensor([0., 0., 0., 0., 1., 1.])\nLayer activations after dropout tensor([0., 0., 0., 0., 5., 6.])\nScaled activations after dropout tensor([ 0.,  0.,  0.,  0., 10., 12.])\n</pre> <p>The idea behind dropout is that it injects noise to the network during training. If a model can achieve good accuracy with noise, that probably implies that it has learned a more generalizable function. Dropping out certain proportion of neurons in each iteration basically guides the model to train a smaller network. Scaling is done to so that the expected sum of activations will roughly remain the same to compensate for missing activations. See Andrew Ng's explanation to get a better intuition of dropout. While watching the video, note that in other framework implementations <code>keep_prob</code> can be used for dropout, which keeps all the nodes in case of being set to <code>1.0</code>. In the <code>PyTorch</code> implementation, the same is achieved by doing the opposite and setting the probability to <code>0.0</code>. Although a useful technique, dropout should be used carefully in order to not hurt the overall performance of the model.</p> <p>We will now implement a <code>Dropout</code> class (an equivalent of <code>nn.Dropout</code> module of <code>PyTorch</code>) and build MLP. Dropout probability is usually set higher for bigger layers and layer closer to the input (but not the input layer, as it doesn't make sense to drop out the input features). Once the training is over, dropout is usually turned off. However, it can also be used in the test time as a mean of estimating how uncertain the neural network is.</p> In\u00a0[10]: Copied! <pre>class Dropout(nn.Module):\n  def __init__(self, prob):\n    super().__init__()\n    assert 0 &lt;= prob &lt;= 1\n    self.prob = prob\n    self.epsilon = 1e-8\n\n  def forward(self, X):\n    if self.training: # model.eval() will turn it off\n      mask = (torch.rand_like(X) &gt; self.prob).float()\n      X = X * mask / (1.0 - self.prob + self.epsilon)\n    return X\n</pre> class Dropout(nn.Module):   def __init__(self, prob):     super().__init__()     assert 0 &lt;= prob &lt;= 1     self.prob = prob     self.epsilon = 1e-8    def forward(self, X):     if self.training: # model.eval() will turn it off       mask = (torch.rand_like(X) &gt; self.prob).float()       X = X * mask / (1.0 - self.prob + self.epsilon)     return X In\u00a0[11]: Copied! <pre>model = nn.Sequential(\n    nn.Flatten(),\n    nn.LazyLinear(256),\n    nn.ReLU(),\n    Dropout(0.5), # nn.Dropout(0.5)\n    nn.LazyLinear(128),\n    nn.ReLU(),\n    Dropout(0.3), # nn.Dropout(0.3)\n    nn.LazyLinear(10)\n)\n</pre> model = nn.Sequential(     nn.Flatten(),     nn.LazyLinear(256),     nn.ReLU(),     Dropout(0.5), # nn.Dropout(0.5)     nn.LazyLinear(128),     nn.ReLU(),     Dropout(0.3), # nn.Dropout(0.3)     nn.LazyLinear(10) ) <p>So far we have looked at two regularization techniques: $l_2$ regularization and dropout. There is also a technique called early stopping which simply stops the training when validation loss stops improving. Regulization methods had the goal of reducing overfitting, and thus increasing the generalization ability of our model. Additionally, we would like to optimize our models to efficiently minimize the loss and converge to a good solution.</p> In\u00a0[12]: Copied! <pre>import scipy.stats as stats\n\nW = torch.normal(mean=0, std=1, size=(10000,))\nx = np.linspace(-4, 4, 1000)\npdf = stats.norm.pdf(x, 0, 1)\n\nplt.figure(figsize=(6, 4))\nplt.hist(W.numpy(), bins=50, density=True, alpha=0.6)\nplt.plot(x, pdf, linewidth=2)\nplt.title('Random Normal (Gaussian) Distribution');\n</pre> import scipy.stats as stats  W = torch.normal(mean=0, std=1, size=(10000,)) x = np.linspace(-4, 4, 1000) pdf = stats.norm.pdf(x, 0, 1)  plt.figure(figsize=(6, 4)) plt.hist(W.numpy(), bins=50, density=True, alpha=0.6) plt.plot(x, pdf, linewidth=2) plt.title('Random Normal (Gaussian) Distribution'); <p>By chain rule, we know that the node gradients are multiplied. Hence, multiplying many gradients that have big value along our neural network will explode the final gradient, resulting in unnessarily big jumps and missing the minimum of the function and thus not converging. The opposite is also true: if the gradients are too small then multiplying lost of small values will result in a number that is almost zero, causing the vanishing gradient problem. With gradients almost zero, values during backpropagation will not get updated and the learning will stop. Below are some examples from the textbook.</p> In\u00a0[13]: Copied! <pre>W = torch.normal(mean=0, std=1, size=(3, 3)) # Gaussian distribution matrix\n\nnum_epochs = 20\nfor _ in range(num_epochs):\n  W = W @ torch.normal(mean=0, std=1, size=(3, 3)) # matrix multiplication of N such matrices\nprint('Exploding gradients:\\n', W)\n</pre> W = torch.normal(mean=0, std=1, size=(3, 3)) # Gaussian distribution matrix  num_epochs = 20 for _ in range(num_epochs):   W = W @ torch.normal(mean=0, std=1, size=(3, 3)) # matrix multiplication of N such matrices print('Exploding gradients:\\n', W) <pre>Exploding gradients:\n tensor([[-2657.0173,   939.6129,  3719.5845],\n        [-2549.2896,   842.5388,  3360.0208],\n        [ -697.6809,   168.5173,   699.8742]])\n</pre> <p>We will now take three activation functions (<code>sigmoid</code>, <code>relu</code>, <code>leaky_relu</code>) and plot their functions together with the graident values. Try to interpret what the plots mean.</p> In\u00a0[14]: Copied! <pre>plt.figure(figsize=(14,3))\n\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny_sigmoid = torch.sigmoid(x)\ny_sigmoid.backward(torch.ones_like(x))\n\nplt.subplot(1, 3, 1)\nplt.plot(x.detach().numpy(), y_sigmoid.detach().numpy(), label='Sigmoid')\nplt.plot(x.detach().numpy(), x.grad.numpy(), label='Gradient')\nplt.legend()\n\nx.grad.zero_()\ny_relu = torch.relu(x)\ny_relu.backward(torch.ones_like(x))\n\nplt.subplot(1, 3, 2)\nplt.plot(x.detach().numpy(), y_relu.detach().numpy(), label='ReLU')\nplt.plot(x.detach().numpy(), x.grad.numpy(), label='Gradient')\nplt.legend()\n\nx.grad.zero_()\ny_leaky_relu = torch.nn.functional.leaky_relu(x, 0.2)\ny_leaky_relu.backward(torch.ones_like(x))\n\nplt.subplot(1, 3, 3)\nplt.plot(x.detach().numpy(), y_leaky_relu.detach().numpy(), label='Leaky ReLU')\nplt.plot(x.detach().numpy(), x.grad.numpy(), label='Gradient')\nplt.legend();\n</pre> plt.figure(figsize=(14,3))  x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True) y_sigmoid = torch.sigmoid(x) y_sigmoid.backward(torch.ones_like(x))  plt.subplot(1, 3, 1) plt.plot(x.detach().numpy(), y_sigmoid.detach().numpy(), label='Sigmoid') plt.plot(x.detach().numpy(), x.grad.numpy(), label='Gradient') plt.legend()  x.grad.zero_() y_relu = torch.relu(x) y_relu.backward(torch.ones_like(x))  plt.subplot(1, 3, 2) plt.plot(x.detach().numpy(), y_relu.detach().numpy(), label='ReLU') plt.plot(x.detach().numpy(), x.grad.numpy(), label='Gradient') plt.legend()  x.grad.zero_() y_leaky_relu = torch.nn.functional.leaky_relu(x, 0.2) y_leaky_relu.backward(torch.ones_like(x))  plt.subplot(1, 3, 3) plt.plot(x.detach().numpy(), y_leaky_relu.detach().numpy(), label='Leaky ReLU') plt.plot(x.detach().numpy(), x.grad.numpy(), label='Gradient') plt.legend(); <p>As can be seen, for the sigmoid function, the gradients are almost zero for low and high ends of the function (saddle points) due to its gradient being $\u03c3(x)(1\u2212\u03c3(x))$. Since the AlexNet paper, for the reason of vanishing gradient problem, <code>relu</code> activation is usually preferred over <code>sigmoid</code>. Note that when the output is mapped to the negative values on the <code>relu</code>, derivates become zero, still causing so-called dying relu problem, which can be solved to some degree with the help of <code>leaky_relu</code> where the gradients do not become exactly zero when the output is mapped to the negative value. Still, in practice, <code>relu</code> is often robust.</p> <p>For simplicity, let's take a simple neural network layer without activation and bias. We know that the next layer's values will be affected by the following sum: $o_{i} = \\sum_{j=1}^{n_\\textrm{in}} w_{ij} x_j$. Let's also assume that the weights are initializated with the Guassian distribution with zero mean. Then we can calculate the Expectation $E$ (mean). Recall that $E$ of a random variable represents the average value you would expect if you repeated experiment many times.</p> <p>\\begin{split}\\begin{aligned}     E[o_i] &amp; = \\sum_{j=1}^{n_\\textrm{in}} E[w_{ij} x_j] = \\sum_{j=1}^{n_\\textrm{in}} E[w_{ij}] E[x_j] = \\sum_{j=1}^{n_\\textrm{in}} 0 \\cdot E[x_j] = 0 \\end{aligned}\\end{split}</p> <p>Recall that variance measures the spread of data around its mean. Squaring is applied so that negative and positive values will not cancel each others out. Considering that weights and inputs are independent:</p> <p>\\begin{split}\\begin{aligned}     \\textrm{Var}[o_i] &amp; = E[o_i^2] - (E[o_i])^2 = \\sum_{j=1}^{n_\\textrm{in}} E[w^2_{ij} x^2_j] - 0  = \\sum_{j=1}^{n_\\textrm{in}} E[w^2_{ij}] E[x^2_j] = n_\\textrm{in} \\sigma^2 \\gamma^2 \\end{aligned}\\end{split}</p> <p>It implies that the variance of the output will be proportional to the number of input neurons.</p> In\u00a0[\u00a0]: Copied! <pre>n_out = 10\nfor n_in in [10**i for i in range(5)]:\n  X = torch.randn(n_in)\n  W = torch.normal(mean=0, std=1, size=(n_in, n_out))\n  O = X @ W\n  print(f'n_in: {n_in}, var(o): {O.var().item()}')\n</pre> n_out = 10 for n_in in [10**i for i in range(5)]:   X = torch.randn(n_in)   W = torch.normal(mean=0, std=1, size=(n_in, n_out))   O = X @ W   print(f'n_in: {n_in}, var(o): {O.var().item()}') <pre>n_in: 1, var(o): 0.6714984774589539\nn_in: 10, var(o): 2.69901967048645\nn_in: 100, var(o): 45.740028381347656\nn_in: 1000, var(o): 1667.195068359375\nn_in: 10000, var(o): 12416.2021484375\n</pre> <p>To mitigate the exploding gradient problem, an obvious trick is to We can achieve the stability of variance by ensuring that $n_\\textrm{in} \\sigma^2 = 1$. Hence we retrieve the standard deviation to be $\\sigma = \\sqrt{\\frac{1}{n_\\textrm{in}}}$. The paper called Understanding the difficulty of training deep feedforward neural networks by Xavier Glorot and Yoshua Bengio propose this solution together with considering the case for backpropagation. With the same logic, we can realize that to keep the gradient variance consistent, we should ensure $n_\\textrm{out} \\sigma^2 = 1$, where $n_\\textrm{out}$ is the number of output neurons. To satisfy both cases, we simply average the variance scaling: $(n_\\textrm{in} + n_\\textrm{out}) \\sigma^2 = 1$ or equivalently $\\sigma = \\sqrt{\\frac{2}{n_\\textrm{in} + n_\\textrm{out}}}$.</p> <p>Xavier initialization is mainly used for <code>sigmoid</code> and <code>tanh</code> activations. The paper Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification addresses rectified non-linearities. As <code>relu</code> function doesn't struggle with the vanishing gradient problem, the backpropagation doesn't have much effect on the variance. Hence Kaiming He initialization gets the form of $\\sigma = \\sqrt{\\frac{2}{n_\\textrm{in}}}$.</p> In\u00a0[\u00a0]: Copied! <pre>n_out = 10\nfor n_in in [10**i for i in range(5)]:\n  X = torch.randn(n_in)\n  W = torch.normal(mean=0, std=np.sqrt(2.0/n_in), size=(n_in, n_out))\n  O = X @ W\n  print(f'n_in: {n_in}, var(o): {O.var().item()}')\n</pre> n_out = 10 for n_in in [10**i for i in range(5)]:   X = torch.randn(n_in)   W = torch.normal(mean=0, std=np.sqrt(2.0/n_in), size=(n_in, n_out))   O = X @ W   print(f'n_in: {n_in}, var(o): {O.var().item()}') <pre>n_in: 1, var(o): 0.12620338797569275\nn_in: 10, var(o): 2.6602299213409424\nn_in: 100, var(o): 2.959010124206543\nn_in: 1000, var(o): 2.462198257446289\nn_in: 10000, var(o): 2.8828887939453125\n</pre> <p>When minimizing the loss with gradient descent, we often deal with complicated functions with local and global minimums. When we are in local minima, we need certain noise to kick our parameter out of it so that we can find global minimum value for the loss. It is also possible to have a function where the gradients almost disappear (vanish), yet it is not a local or global minima. Optimization will not improve when we are at that point of the function due to very small gradients. We will plot both of the mentioned cases in 2D, but they can be extended to higher dimensions. For example, the function $f(x,y)=x^2-y^2$ indeed looks like a horse saddle when plotted, with its saddle point at $(0,0,0)$, hence the terminology. Saddle points are more frequent than local minima.</p> In\u00a0[\u00a0]: Copied! <pre>f1 = lambda x: x * np.cos(np.pi * x)\nf2 = lambda x: x**3\n\nx1 = np.linspace(-1, 2, 100)\ny1 = f1(x1)\n\nlocal_min = (-0.3, f1(-0.3))\nglobal_min = (1.1, f1(1.1))\n\nx2 = np.linspace(-2, 2, 100)\ny2 = f2(x2)\n\nsaddle_point = (0.0, f2(0.0))\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 4))\n\naxs[0].plot(x1, y1, label=r'$f(x) = x \\cos(\\pi x)$', color='b')\naxs[0].scatter(*local_min, color='r', label=\"Local Minimum\")\naxs[0].scatter(*global_min, color='g', label=\"Global Minimum\")\naxs[0].legend()\naxs[0].set_title(\"Local &amp; Global Minima\")\n\naxs[1].plot(x2, y2, label=r'$f(x) = x^3$', color='m')\naxs[1].scatter(*saddle_point, color='orange', label=\"Saddle Point\", zorder=3)\naxs[1].legend()\naxs[1].set_title(\"Saddle Point\");\n</pre> f1 = lambda x: x * np.cos(np.pi * x) f2 = lambda x: x**3  x1 = np.linspace(-1, 2, 100) y1 = f1(x1)  local_min = (-0.3, f1(-0.3)) global_min = (1.1, f1(1.1))  x2 = np.linspace(-2, 2, 100) y2 = f2(x2)  saddle_point = (0.0, f2(0.0))  fig, axs = plt.subplots(1, 2, figsize=(10, 4))  axs[0].plot(x1, y1, label=r'$f(x) = x \\cos(\\pi x)$', color='b') axs[0].scatter(*local_min, color='r', label=\"Local Minimum\") axs[0].scatter(*global_min, color='g', label=\"Global Minimum\") axs[0].legend() axs[0].set_title(\"Local &amp; Global Minima\")  axs[1].plot(x2, y2, label=r'$f(x) = x^3$', color='m') axs[1].scatter(*saddle_point, color='orange', label=\"Saddle Point\", zorder=3) axs[1].legend() axs[1].set_title(\"Saddle Point\"); In\u00a0[\u00a0]: Copied! <pre>def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps):\n  path = [initial_point]\n  x = initial_point\n  for _ in range(num_steps):\n    grad = grad_f(x)\n    x -= learning_rate * grad\n    path.append(x)\n  return np.array(path)\n\ndef plot_descent(f, grad_f, point, steps, start, end, learning_rates=[0.1, 0.3, 0.9, 1.0], figsize=(14,4)):\n  x_vals = np.linspace(start, end, 100)\n  y_vals = f(x_vals)\n\n  fig, axs = plt.subplots(1, len(learning_rates), figsize=figsize)\n  for i, lr in enumerate(learning_rates):\n    path = gradient_descent(f, grad_f, lr, point, steps)\n    axs[i].plot(x_vals, y_vals, color='b')\n    axs[i].plot(path, f(path), marker='o', label=f'LR = {lr}', color=[1.0, 0.5, 0.5])\n    axs[i].set_title(f'LR = {lr}')\n</pre> def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps):   path = [initial_point]   x = initial_point   for _ in range(num_steps):     grad = grad_f(x)     x -= learning_rate * grad     path.append(x)   return np.array(path)  def plot_descent(f, grad_f, point, steps, start, end, learning_rates=[0.1, 0.3, 0.9, 1.0], figsize=(14,4)):   x_vals = np.linspace(start, end, 100)   y_vals = f(x_vals)    fig, axs = plt.subplots(1, len(learning_rates), figsize=figsize)   for i, lr in enumerate(learning_rates):     path = gradient_descent(f, grad_f, lr, point, steps)     axs[i].plot(x_vals, y_vals, color='b')     axs[i].plot(path, f(path), marker='o', label=f'LR = {lr}', color=[1.0, 0.5, 0.5])     axs[i].set_title(f'LR = {lr}') In\u00a0[\u00a0]: Copied! <pre>f = lambda x: x**2\ngrad_f = lambda x: 2*x\n\nplot_descent(f, grad_f, 1.5, 10, -2, 2)\n</pre> f = lambda x: x**2 grad_f = lambda x: 2*x  plot_descent(f, grad_f, 1.5, 10, -2, 2) In\u00a0[\u00a0]: Copied! <pre>c = np.array(0.15 * np.pi)\nf = lambda x: x * np.cos(c * x)\ngrad_f = lambda x: np.cos(c * x) - c * x * np.sin(c * x)\n</pre> c = np.array(0.15 * np.pi) f = lambda x: x * np.cos(c * x) grad_f = lambda x: np.cos(c * x) - c * x * np.sin(c * x) In\u00a0[\u00a0]: Copied! <pre>plot_descent(f, grad_f, point=10, steps=10, start=-13, end=13, learning_rates=[0.3, 1.3, 2.0])\n</pre> plot_descent(f, grad_f, point=10, steps=10, start=-13, end=13, learning_rates=[0.3, 1.3, 2.0]) In\u00a0[\u00a0]: Copied! <pre>plot_descent(f, grad_f, point=-7, steps=5, start=-13, end=13, learning_rates=[0.5, 2.7, 3.5])\n</pre> plot_descent(f, grad_f, point=-7, steps=5, start=-13, end=13, learning_rates=[0.5, 2.7, 3.5]) In\u00a0[\u00a0]: Copied! <pre>def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, momentum=0.9):\n  assert momentum &lt;= 1 and momentum &gt;= 0\n  path = [initial_point]\n  x = initial_point\n\n  beta = momentum\n  v = 0.0\n  for _ in range(num_steps):\n    grad = grad_f(x)\n    v = beta * v + (1 - beta) * grad\n    x -= learning_rate * v\n    path.append(x)\n  return np.array(path)\n\ndef plot_descent(f, grad_f, point, steps, start, end, learning_rate, momentums=[0.1, 0.5, 0.9], figsize=(14,4)):\n  x_vals = np.linspace(start, end, 100)\n  y_vals = f(x_vals)\n\n  fig, axs = plt.subplots(1, len(momentums), figsize=figsize)\n  for i, momentum in enumerate(momentums):\n    path = gradient_descent(f, grad_f, learning_rate, point, steps, momentum)\n    axs[i].plot(x_vals, y_vals, color='b')\n    axs[i].plot(path, f(path), marker='o', label=f'M = {momentum}', color=[1.0, 0.5, 0.5])\n    axs[i].set_title(f'M = {momentum}')\n</pre> def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, momentum=0.9):   assert momentum &lt;= 1 and momentum &gt;= 0   path = [initial_point]   x = initial_point    beta = momentum   v = 0.0   for _ in range(num_steps):     grad = grad_f(x)     v = beta * v + (1 - beta) * grad     x -= learning_rate * v     path.append(x)   return np.array(path)  def plot_descent(f, grad_f, point, steps, start, end, learning_rate, momentums=[0.1, 0.5, 0.9], figsize=(14,4)):   x_vals = np.linspace(start, end, 100)   y_vals = f(x_vals)    fig, axs = plt.subplots(1, len(momentums), figsize=figsize)   for i, momentum in enumerate(momentums):     path = gradient_descent(f, grad_f, learning_rate, point, steps, momentum)     axs[i].plot(x_vals, y_vals, color='b')     axs[i].plot(path, f(path), marker='o', label=f'M = {momentum}', color=[1.0, 0.5, 0.5])     axs[i].set_title(f'M = {momentum}') In\u00a0[\u00a0]: Copied! <pre>plot_descent(f, grad_f, point=-7, steps=35, start=-13, end=13, learning_rate=1.1, momentums=[0.0, 0.6, 0.9])\n</pre> plot_descent(f, grad_f, point=-7, steps=35, start=-13, end=13, learning_rate=1.1, momentums=[0.0, 0.6, 0.9]) In\u00a0[\u00a0]: Copied! <pre>def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, gamma=0.9):\n  assert gamma &lt;= 1 and gamma &gt;= 0\n  path = [initial_point]\n  x = initial_point\n\n  s = 0.0\n  epsilon=1e-8\n  for _ in range(num_steps):\n    grad = grad_f(x)\n    s = gamma * s + (1 - gamma) * grad**2\n    x -= learning_rate * grad / (np.sqrt(s) + epsilon)\n    path.append(x)\n  return np.array(path)\n\ndef plot_descent(f, grad_f, point, steps, start, end, learning_rate, gammas=[0.9, 0.99, 0.999], figsize=(14,4)):\n  x_vals = np.linspace(start, end, 100)\n  y_vals = f(x_vals)\n\n  fig, axs = plt.subplots(1, len(gammas), figsize=figsize)\n  for i, gamma in enumerate(gammas):\n    path = gradient_descent(f, grad_f, learning_rate, point, steps, gamma)\n    axs[i].plot(x_vals, y_vals, color='b')\n    axs[i].plot(path, f(path), marker='o', label=f'G = {gamma}', color=[1.0, 0.5, 0.5])\n    axs[i].set_title(f'G = {gamma}')\n</pre> def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, gamma=0.9):   assert gamma &lt;= 1 and gamma &gt;= 0   path = [initial_point]   x = initial_point    s = 0.0   epsilon=1e-8   for _ in range(num_steps):     grad = grad_f(x)     s = gamma * s + (1 - gamma) * grad**2     x -= learning_rate * grad / (np.sqrt(s) + epsilon)     path.append(x)   return np.array(path)  def plot_descent(f, grad_f, point, steps, start, end, learning_rate, gammas=[0.9, 0.99, 0.999], figsize=(14,4)):   x_vals = np.linspace(start, end, 100)   y_vals = f(x_vals)    fig, axs = plt.subplots(1, len(gammas), figsize=figsize)   for i, gamma in enumerate(gammas):     path = gradient_descent(f, grad_f, learning_rate, point, steps, gamma)     axs[i].plot(x_vals, y_vals, color='b')     axs[i].plot(path, f(path), marker='o', label=f'G = {gamma}', color=[1.0, 0.5, 0.5])     axs[i].set_title(f'G = {gamma}') In\u00a0[\u00a0]: Copied! <pre>plot_descent(f, grad_f, point=12, steps=10, start=-13, end=13, learning_rate=0.5, gammas=[0, 0.7, 0.9])\n</pre> plot_descent(f, grad_f, point=12, steps=10, start=-13, end=13, learning_rate=0.5, gammas=[0, 0.7, 0.9]) In\u00a0[\u00a0]: Copied! <pre>def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, b1=0.9, b2=0.999, normalize=False):\n  assert b1 &lt;= 1 and b1 &gt;= 0\n  assert b2 &lt;= 1 and b2 &gt;= 0\n  path = [initial_point]\n  x = initial_point\n\n  v = 0.0\n  s = 0.0\n  epsilon=1e-8\n  for t in range(1, num_steps + 1):\n    grad = grad_f(x)\n    v = b1 * v + (1 - b1) * grad\n    s = b2 * s + (1 - b2) * grad**2\n    # as our function and plot are simple, we will keep\n    # unnormalized option as well for demonstration purposes\n    v_hat = v / (1 - b1**t) if normalize else v\n    s_hat = s / (1 - b2**t) if normalize else s\n    x -= learning_rate * v_hat / (np.sqrt(s_hat) + epsilon)\n    path.append(x)\n  return np.array(path)\n\ndef plot_descent(f, grad_f, point, steps, start, end, learning_rate, b1_vals=[0.9, 0.99, 0.999], b2_vals=[0.1, 0.99, 0.999], figsize=(14,4)):\n  x_vals = np.linspace(start, end, 100)\n  y_vals = f(x_vals)\n\n  fig, axs = plt.subplots(len(b1_vals), len(b2_vals), figsize=figsize)\n  for i, b1 in enumerate(b1_vals):\n    for j, b2 in enumerate(b2_vals):\n      path = gradient_descent(f, grad_f, learning_rate, point, steps, b1, b2)\n      axs[i, j].plot(x_vals, y_vals, color='b')\n      axs[i, j].plot(path, f(path), marker='o', label=f'B1 = {b1}, B2 = {b2}', color=[1.0, 0.5, 0.5])\n      axs[i, j].set_title(f'B1 = {b1}, B2 = {b2}')\n</pre> def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, b1=0.9, b2=0.999, normalize=False):   assert b1 &lt;= 1 and b1 &gt;= 0   assert b2 &lt;= 1 and b2 &gt;= 0   path = [initial_point]   x = initial_point    v = 0.0   s = 0.0   epsilon=1e-8   for t in range(1, num_steps + 1):     grad = grad_f(x)     v = b1 * v + (1 - b1) * grad     s = b2 * s + (1 - b2) * grad**2     # as our function and plot are simple, we will keep     # unnormalized option as well for demonstration purposes     v_hat = v / (1 - b1**t) if normalize else v     s_hat = s / (1 - b2**t) if normalize else s     x -= learning_rate * v_hat / (np.sqrt(s_hat) + epsilon)     path.append(x)   return np.array(path)  def plot_descent(f, grad_f, point, steps, start, end, learning_rate, b1_vals=[0.9, 0.99, 0.999], b2_vals=[0.1, 0.99, 0.999], figsize=(14,4)):   x_vals = np.linspace(start, end, 100)   y_vals = f(x_vals)    fig, axs = plt.subplots(len(b1_vals), len(b2_vals), figsize=figsize)   for i, b1 in enumerate(b1_vals):     for j, b2 in enumerate(b2_vals):       path = gradient_descent(f, grad_f, learning_rate, point, steps, b1, b2)       axs[i, j].plot(x_vals, y_vals, color='b')       axs[i, j].plot(path, f(path), marker='o', label=f'B1 = {b1}, B2 = {b2}', color=[1.0, 0.5, 0.5])       axs[i, j].set_title(f'B1 = {b1}, B2 = {b2}') In\u00a0[\u00a0]: Copied! <pre>plot_descent(f, grad_f, point=15, steps=20, start=-15, end=25, learning_rate=0.7, b1_vals=[0, 0.6, 0.9], b2_vals=[0, 0.6, 0.9], figsize=(12, 12))\n</pre> plot_descent(f, grad_f, point=15, steps=20, start=-15, end=25, learning_rate=0.7, b1_vals=[0, 0.6, 0.9], b2_vals=[0, 0.6, 0.9], figsize=(12, 12)) <p>Instead of simple grayscale MNIST dataset, we will choose CIFAR-10 this time to train our model on. It consists of <code>50,000</code> training and <code>10,000</code> test images of size <code>32x32</code> divided into 10 classes. An alternative version of the dataset called CIFAR-100 has 100 classes. Unlike MNIST, images have three channels (RGB) which makes computation more expensive and the task of the model more difficult.</p> In\u00a0[\u00a0]: Copied! <pre>from torchvision import datasets, transforms\n\ntrain_data = datasets.CIFAR10(root='./data', train=True,  download=True, transform=transforms.ToTensor())\ntest_data  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</pre> from torchvision import datasets, transforms  train_data = datasets.CIFAR10(root='./data', train=True,  download=True, transform=transforms.ToTensor()) test_data  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor()) <pre>Files already downloaded and verified\nFiles already downloaded and verified\n</pre> In\u00a0[\u00a0]: Copied! <pre>from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_data, batch_size=64, pin_memory=True, shuffle=True)\ntest_loader  = DataLoader(test_data,  batch_size=64, pin_memory=True, shuffle=False)\n</pre> from torch.utils.data import DataLoader  train_loader = DataLoader(train_data, batch_size=64, pin_memory=True, shuffle=True) test_loader  = DataLoader(test_data,  batch_size=64, pin_memory=True, shuffle=False) In\u00a0[\u00a0]: Copied! <pre>idx_to_class = {v: k for k, v in train_data.class_to_idx.items()}\nidx_to_class\n</pre> idx_to_class = {v: k for k, v in train_data.class_to_idx.items()} idx_to_class Out[\u00a0]: <pre>{0: 'airplane',\n 1: 'automobile',\n 2: 'bird',\n 3: 'cat',\n 4: 'deer',\n 5: 'dog',\n 6: 'frog',\n 7: 'horse',\n 8: 'ship',\n 9: 'truck'}</pre> In\u00a0[\u00a0]: Copied! <pre>X_train, y_train = next(iter(train_loader))\nplt.imshow(X_train[0].permute(1, 2, 0))\nplt.title(idx_to_class[y_train[0].item()]);\n</pre> X_train, y_train = next(iter(train_loader)) plt.imshow(X_train[0].permute(1, 2, 0)) plt.title(idx_to_class[y_train[0].item()]); In\u00a0[\u00a0]: Copied! <pre>model = nn.Sequential(\n    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Flatten(),\n    nn.Linear(128 * 4 * 4, 256),\n    nn.ReLU(),\n    nn.Dropout(0.5), # Dropout\n    nn.Linear(256, 10)\n)\n</pre> model = nn.Sequential(     nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),     nn.ReLU(),     nn.MaxPool2d(kernel_size=2, stride=2),     nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),     nn.ReLU(),     nn.MaxPool2d(kernel_size=2, stride=2),     nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),     nn.ReLU(),     nn.MaxPool2d(kernel_size=2, stride=2),     nn.Flatten(),     nn.Linear(128 * 4 * 4, 256),     nn.ReLU(),     nn.Dropout(0.5), # Dropout     nn.Linear(256, 10) ) In\u00a0[\u00a0]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model.to(device) Out[\u00a0]: <pre>Sequential(\n  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (4): ReLU()\n  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (7): ReLU()\n  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (9): Flatten(start_dim=1, end_dim=-1)\n  (10): Linear(in_features=2048, out_features=256, bias=True)\n  (11): ReLU()\n  (12): Dropout(p=0.5, inplace=False)\n  (13): Linear(in_features=256, out_features=10, bias=True)\n)</pre> In\u00a0[\u00a0]: Copied! <pre># Moment weigths are by default 0.9 and 0.999 for Adam optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.005, betas=(0.9, 0.999), weight_decay=1e-4)\nloss_fn = nn.CrossEntropyLoss()\n</pre> # Moment weigths are by default 0.9 and 0.999 for Adam optimizer optimizer = optim.Adam(model.parameters(), lr=0.005, betas=(0.9, 0.999), weight_decay=1e-4) loss_fn = nn.CrossEntropyLoss() In\u00a0[\u00a0]: Copied! <pre># Kaiming He Initialization\nfor layer in model:\n  if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n    nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n    if layer.bias is not None:\n      nn.init.zeros_(layer.bias)\n</pre> # Kaiming He Initialization for layer in model:   if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):     nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')     if layer.bias is not None:       nn.init.zeros_(layer.bias) In\u00a0[\u00a0]: Copied! <pre>model.train()\n\nnum_epochs = 50\nfor epoch in range(num_epochs):\n  loss = 0.0\n  for X_train, y_train in train_loader:\n    X_train, y_train = X_train.to(device), y_train.to(device)\n    optimizer.zero_grad()\n    preds = model(X_train)\n    batch_loss = loss_fn(preds, y_train)\n    batch_loss.backward()\n    loss += batch_loss.item()\n    optimizer.step()\n  print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}')\n</pre> model.train()  num_epochs = 50 for epoch in range(num_epochs):   loss = 0.0   for X_train, y_train in train_loader:     X_train, y_train = X_train.to(device), y_train.to(device)     optimizer.zero_grad()     preds = model(X_train)     batch_loss = loss_fn(preds, y_train)     batch_loss.backward()     loss += batch_loss.item()     optimizer.step()   print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}') <pre>Epoch: 1/50, Loss: 1.6063\nEpoch: 2/50, Loss: 1.3142\nEpoch: 3/50, Loss: 1.2205\nEpoch: 4/50, Loss: 1.1668\nEpoch: 5/50, Loss: 1.1335\nEpoch: 6/50, Loss: 1.0996\nEpoch: 7/50, Loss: 1.0810\nEpoch: 8/50, Loss: 1.0594\nEpoch: 9/50, Loss: 1.0465\nEpoch: 10/50, Loss: 1.0295\nEpoch: 11/50, Loss: 1.0175\nEpoch: 12/50, Loss: 1.0196\nEpoch: 13/50, Loss: 0.9891\nEpoch: 14/50, Loss: 0.9799\nEpoch: 15/50, Loss: 0.9781\nEpoch: 16/50, Loss: 0.9729\nEpoch: 17/50, Loss: 0.9789\nEpoch: 18/50, Loss: 0.9651\nEpoch: 19/50, Loss: 0.9611\nEpoch: 20/50, Loss: 0.9605\nEpoch: 21/50, Loss: 0.9545\nEpoch: 22/50, Loss: 0.9485\nEpoch: 23/50, Loss: 0.9362\nEpoch: 24/50, Loss: 0.9436\nEpoch: 25/50, Loss: 0.9309\nEpoch: 26/50, Loss: 0.9296\nEpoch: 27/50, Loss: 0.9240\nEpoch: 28/50, Loss: 0.9205\nEpoch: 29/50, Loss: 0.9164\nEpoch: 30/50, Loss: 0.9104\nEpoch: 31/50, Loss: 0.9124\nEpoch: 32/50, Loss: 0.9160\nEpoch: 33/50, Loss: 0.9211\nEpoch: 34/50, Loss: 0.9067\nEpoch: 35/50, Loss: 0.9168\nEpoch: 36/50, Loss: 0.9151\nEpoch: 37/50, Loss: 0.9065\nEpoch: 38/50, Loss: 0.9048\nEpoch: 39/50, Loss: 0.9024\nEpoch: 40/50, Loss: 0.8971\nEpoch: 41/50, Loss: 0.9015\nEpoch: 42/50, Loss: 0.9055\nEpoch: 43/50, Loss: 0.8923\nEpoch: 44/50, Loss: 0.8804\nEpoch: 45/50, Loss: 0.8855\nEpoch: 46/50, Loss: 0.8931\nEpoch: 47/50, Loss: 0.8802\nEpoch: 48/50, Loss: 0.8820\nEpoch: 49/50, Loss: 0.8748\nEpoch: 50/50, Loss: 0.8847\n</pre> In\u00a0[\u00a0]: Copied! <pre>model.eval()\n\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n  for X_test, y_test in test_loader:\n    X_test, y_test = X_test.to(device), y_test.to(device)\n    preds = model(X_test)\n    _, predicted = torch.max(preds, 1)\n    total += y_test.size(0)\n    correct += (predicted == y_test).sum().item()\n\naccuracy = 100 * correct / total\nprint(f'Accuracy on the test set: {accuracy:.2f}%')\n</pre> model.eval()  correct = 0 total = 0 with torch.no_grad():   for X_test, y_test in test_loader:     X_test, y_test = X_test.to(device), y_test.to(device)     preds = model(X_test)     _, predicted = torch.max(preds, 1)     total += y_test.size(0)     correct += (predicted == y_test).sum().item()  accuracy = 100 * correct / total print(f'Accuracy on the test set: {accuracy:.2f}%') <pre>Accuracy on the test set: 68.98%\n</pre>"},{"location":"Main_Content/04_regul_optim/#04-python-code-for-deep-learning-regularization-and-optimization","title":"04 Python Code for Deep Learning Regularization and Optimization\u00b6","text":""},{"location":"Main_Content/04_regul_optim/#weight-decay","title":"Weight Decay\u00b6","text":""},{"location":"Main_Content/04_regul_optim/#dropout","title":"Dropout\u00b6","text":"<p>The more complicated the model is, the more chance it has to overfit to the train data. In order to improve generalization ability of our model to the unseen data, we may aim for a simpler model. Regularization with weight decay achieved that to some degree but it's not the only solution. Srivastava et al. building on top of the previous idea developed by Bishop came up with the concept called Dropout. Dropout literally drops out some neurons during training by setting their activations to zero, which is illustrated in the figure 5.6.1 of d2l book below.</p> <p></p>"},{"location":"Main_Content/04_regul_optim/#exploding-vanishing-gradients","title":"Exploding &amp; Vanishing Gradients\u00b6","text":"<p>When building neural network from scratch, we have been initilializing our parameters randomly. By following a certain guideline when initializing weights, however, it is possible to influence the speed and convergence of our training. Poor parameter initialization can cause problems like exploding and vanishing gradients.</p>"},{"location":"Main_Content/04_regul_optim/#xavier-he-parameter-initialization","title":"Xavier / He Parameter Initialization\u00b6","text":""},{"location":"Main_Content/04_regul_optim/#local-minima-saddle-point","title":"Local Minima &amp; Saddle Point\u00b6","text":""},{"location":"Main_Content/04_regul_optim/#learning-rate","title":"Learning Rate\u00b6","text":"<p>We have seen from previous experiments how learning rate can affect the training process. When the learning rate is low, parameters take smaller steps towards the minimum, and training becomes slow. When the learning rate is high, it can overshoot the local minima and may even never converge. Therefore, it is common to use a learning rate scheduler which schedules the learning rate values to initially take larger steps to speed up the training, but as the parameter values approach local minima, the scheduler gradually reduces the learning rate to avoid overshooting. For example, we can reduce the learning rate by a factor of <code>0.1</code> or so every <code>N</code> epochs. We can simulate the gradient descent in a 2D plot to illustrate the impact of different learning rates.</p>"},{"location":"Main_Content/04_regul_optim/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)\u00b6","text":"<p>As mentioned before, it is possible for the parameters to get stuck in the local minima. A slightly modified version of the gradient descent, the minibatch Stochastic Gradient Descent (SGD) has the ability to avoid local minima, as the variation of gradients in the minibatches bring noise and affect the parameter update. In SGD, instead of averaging the gradients of all data (batch) samples, we randomly choose only a single sample to approximate the average gradient. In case of minibatch SGD, we choose a small data subset (minibatch) instead of a single data point (SGD) or all data points (BGD) and average the minibatch gradients to approximate the average of full batch gradients.</p> <p>In the examples below, we can see how different values of learning rates may postiviely or negatively influence the outcome depending on whether we are close to the local or global minima.</p>"},{"location":"Main_Content/04_regul_optim/#momentum","title":"Momentum\u00b6","text":"<p>We have observed that the learning rate is sensitive to small changes in its values. We can look at the direction of the gradient and encourage the learning rate to move smoothly in that direction (e.g. instead of oscillating a lot in a zig-zag pattern). It can be achieved with momentum, which adds a velocity term with acceleration to parameter updates. It behaves similar to how a ball would roll down a hill. Momentum builds speed as we move in the direction of the gradient, helping the training converge faster and more smoothly. This explanation, however, is oversimplistic. If you have a good mathematical background, you can find a nice demo and better explanation of momentum in this distill article.</p> <p>Recall that gradient descent for parameter $\\theta$ had the following formula where $\\nabla_{\\theta} L(\\theta_t)$ is gradient of loss and $\\eta $ is learning rate:</p> <p>$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} L(\\theta_t)$$</p> <p>Momentum has a velocity term $v$ which accumulates the values of previous gradients. It is also scaled by a momentum hyperparamter $\\beta$ where $\\beta \\in (0, 1)$. Unlike  $\\eta$ (learning rate hyperparamter), $\\beta$ is usually set to $0.9$. Gradient update instead of pointing towards the direction of steepest descent on a single instance, becomes the direction of a weighted average of previous gradients. Momentum will play the role of an accelerator, From this point on, and parameters will be updated by the accumulated gradient value.</p> <p>$$ v_t = \\beta v_{t-1} +\\nabla_{\\theta} L(\\theta_t) \\\\ \\theta_{t+1} = \\theta_t - \\eta v_t $$</p> <p>You will usually see a slighly modified version of the velocity, which scales the current gradient by $(1-\\beta)$:</p> <p>$$ v_t = \\beta v_{t-1} + (1 - \\beta)\\nabla_{\\theta} L(\\theta_t)$$</p> <p>You can imagine $\\beta$ value to be an averaging factor. As the gradients accumulate, we may want to give less weight to the current gradient, which $1-\\beta$ achieves. We can also notice that $\\beta=0.0$ will turn off the momentum update and restore the original gradient descent, when $\\beta=1.0$ would completely remove the effect of the gradient update. For this reason, you will see values for the momentum hyperparameter to be $0.99$ or even $0.999$ but never $1.0$.</p>"},{"location":"Main_Content/04_regul_optim/#rmsprop","title":"RMSProp\u00b6","text":"<p>We can further optimize learning with RMSProp algorithm. RMSProp adjusts the parameter step size based on the historical magnitude of the parameter gradients. For that, step size (learning rate) is divided by a running average of squared gradients. This helps to avoid large updates in regions where gradients are large, while allowing for bigger steps in flatter regions of the function. Squaring and then desquaring (finding the square root) is a common technique (similar to finding MSE or standard deviation) which deals with negative values. RMSProp has the common abbreviation as MSE: the algorithm's full title is Root Mean Squared Propagation. The state $s$ in the formula below (which should seem familiar) holds the running average of squared gradients, which is then used to scale the learning rate for each parameter:</p> <p>$$s_t = \\gamma s_{t-1} + (1 - \\gamma) \\nabla_{\\theta} L(\\theta_t)^2$$</p> <p>The updated parameter $\\theta$ is then divided by the square root of this state vector. To avoid division by zero and achieve numerical stability $\u03f5$ (a very small number approaching zero) is added. The term $\\sqrt{s_t}$ represents the root of the running average of squared gradients, normalizing the gradient for each parameter.</p> <p>$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{s_t} + \\epsilon} \\nabla_{\\theta} L(\\theta_t)$$</p> <p>RMSprop often performs well when dealing with noisy or sparse gradients, and the parameter $\\gamma$ typically ranges between $0.9$ and $0.99$. Again note that RMSProp updates each parameter individually. The formula and all may seem complicated, yet the reasoning is very simple: you adjust the learning rate of each parameter by its past gradient. If the gradient was big, learning rate for that parameter will get smaller and vice versa.</p> <p>In our example below, we will work with optimizing a single paramter to visualize it in 2D which should give a feeling on how RMSProp works. But it is important to note that, just like many concepts, the idea can be extended to higher dimensions. Learning rates for each parameter may get different values as a result.</p>"},{"location":"Main_Content/04_regul_optim/#adam","title":"Adam\u00b6","text":"<p>You might have noted certain similarities in momentum and RSMProp formulas. But can't we simply make the best out of both worlds and integrate them into a single gradient descent function? It turns out we can. Adam stands for Adaptive Moment Estimation. Adam: A Method for Stochastic Optimization does exactly that.</p> <p>$$ \\begin{aligned} v_t &amp;= \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla_{\\theta} L(\\theta_t) \\\\ s_t &amp;= \\beta_2 s_{t-1} + (1 - \\beta_2) \\nabla_{\\theta} L(\\theta_t)^2 \\end{aligned} $$</p> <p>We should note a couple of more things. The common values for weights are $\\beta_1=0.9$ and $\\beta_2=0.999$ (why not $\\beta_2=1.0$ we discussed above), as variance estimate $s_t$ is much slower than the momentum. As we initilize it together with velocity to zero ($v=s=0$) we initially get bias towards smaller values. It can be fixed with normalization:</p> <p>$$\\hat{v_t} = \\frac{v_t}{1 - \\beta_1^t}, \\quad \\hat{s_t} = \\frac{s_t}{1 - \\beta_2^t}$$</p> <p>Here $v_t$ and $s_t$ are called moments. Let's see how normalized moments get affected after the first two iterations:</p> <p>$$ \\beta_1 = 0.9^1 = 0.9 \\quad \\Rightarrow \\quad \\frac{1}{1 - \\beta_1^1} = \\frac{1}{0.1} = 10 \\\\ \\beta_1^2 = 0.9^2 = 0.81 \\quad \\Rightarrow \\quad \\frac{1}{1 - \\beta_1^2} = \\frac{1}{0.19} \\approx 5.26 \\\\ \\beta_2^1 = 0.999^1 = 0.999 \\quad \\Rightarrow \\quad \\frac{1}{1 - \\beta_2^1} = \\frac{1}{0.001} = 1000 \\\\ \\beta_2^2 = 0.999^2 = 0.998001 \\quad \\Rightarrow \\quad \\frac{1}{1 - \\beta_2^2} = \\frac{1}{0.001999} \\approx 500 $$</p> <p>Finally, we can integrate the accumulated values into our gradient descent formula to get the final step size:</p> <p>$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{s_t}} + \\epsilon} \\hat{v_t}$$</p>"},{"location":"Main_Content/04_regul_optim/#cifar-10-dataset","title":"CIFAR-10 Dataset\u00b6","text":""},{"location":"Main_Content/04_regul_optim/#training-efficient-model","title":"Training Efficient Model\u00b6","text":"<p>We will now train our custom CNN model (see LeNet model in the previous lecture) by applying regularization / optimization methods we have discussed above. Pay attention to the architecture, try to understand the input dimensions. We have included a <code>Dropout</code> layer. In order to initialize our weights non-randomly, we will not use <code>nn.Lazy</code> modules.</p>"},{"location":"Main_Content/04_regul_optim/#hyperparameter-tuning","title":"Hyperparameter Tuning\u00b6","text":"<p>When building our model architecture we should consider activation function, number of layers and number of neurons per layer, kernel size, stride, padding. We have also come across to many different hyperparameters up until now: learning rate, batch size, initialization method, number of epochs, optimizer type (e.g. SGD, Adam), momentum (beta values for Adam), weight decay, dropout rate, etc. It is often not very clear which values are the best for our model architecture and dataset. We have to try out many different values for our hyperparameters to find the most efficient training conditions.</p> <p>Hyperparameter tuning is the process of finding the best set of hyperparameters. Grid search, random search, Bayesian optimization help us to explore different hyperparameter combinations. It is recommended to start with common default values, and gradually adjust one or more hyperparameters during experiments. You can use cross-validation to evaluate model performance for each hypermeter combination.</p> <p>Learning rate and optimizer affect the convergence of training. Batch size and number of epochs affect the training time and model accuracy. Regularization  (dropout, weight decay) parameters should aid with preventing overfitting.</p> <p>Exercise: Tune hyperparameters for the model and note down the results. Which hyperparameter set achieved the best accuracy on test data?</p>"},{"location":"Main_Content/05_batchnorm_resnet/","title":"05 Training Deeper Networks: Batch Normalization and Residual Blocks","text":"<p>Increasing the number of layers in neural networks for learning more advanced functions is challenging due to issues like vanishing gradients. VGGNet partially addressed this problem by using repetitive blocks that stack multiple convolutional layers before downsampling with max-pooling. For instance, two consecutive <code>3x3</code> convolutional layers achieve the same receptive field as a single <code>5x5</code> convolution, while preserving a higher spatial resolution for the next layer. In simpler terms, repeating a smaller kernel allows the network to access the same input pixels while retaining more detail for subsequent processing. Larger kernels blur (downsample) the image more aggressively, which can lead to the loss of important details and force the network to reduce resolution earlier in the architecture and stop.</p> <p>Despite this breakthrough, VGGNet was still limited and showed diminishing returns beyond <code>19</code> layers (hence, <code>vgg19</code> architecture). Another architecture was introduced the same year with the paper titled Going Deeper with Convolutions. It was named Inception because of the internet meme from the infamous Inception movie. I am not joking. If you don't believe me, scroll down the paper for references section and check out the very first reference.</p> <p>Inception architecture, and its implementation, <code>GoogLeNet</code> model (a play on words: 1) was developed by Google researchers, and 2) pays homage to the LeNet architecture), significantly reduced parameter count and leveraged the advantages of the <code>1x1</code> convolution kernel (see the Network in Network paper which also introduced <code>Global Average Pooling (GAP)</code> layer). Despite enabling deeper networks with far fewer parameters, Inception did not fully resolve the core training and convergence problems faced by very deep models.</p> <p>Batch Normalization and Residual Networks emerged as two major solutions for efficiently training neural networks as deep as <code>100</code> layers and more. We will now set up the data environment and go on discussing the core ideas and implementations of both papers.</p> In\u00a0[1]: Copied! <pre>import requests\nimport random\nimport string\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n</pre> import requests import random import string import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import TensorDataset, DataLoader In\u00a0[2]: Copied! <pre>########## DATA SETUP ##########\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\nresponse = requests.get(url)\nwords = response.text.splitlines()\nrandom.shuffle(words)\n\nchars = list(string.ascii_lowercase)\nstoi = {ch: i for i, ch in enumerate(chars)}\nstoi['&lt;START&gt;'] = len(stoi)\nstoi['&lt;END&gt;'] = len(stoi)\nitos = {i: ch for ch, i in stoi.items()}\n\nBLOCK_SIZE = 3\nVOCAB_SIZE = len(stoi)\nEMBED_SIZE = 10\nLAYER_SIZE = 100\n\nlen(words), BLOCK_SIZE, VOCAB_SIZE, words[0]\n</pre> ########## DATA SETUP ##########  url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\" response = requests.get(url) words = response.text.splitlines() random.shuffle(words)  chars = list(string.ascii_lowercase) stoi = {ch: i for i, ch in enumerate(chars)} stoi[''] = len(stoi) stoi[''] = len(stoi) itos = {i: ch for ch, i in stoi.items()}  BLOCK_SIZE = 3 VOCAB_SIZE = len(stoi) EMBED_SIZE = 10 LAYER_SIZE = 100  len(words), BLOCK_SIZE, VOCAB_SIZE, words[0] Out[2]: <pre>(32033, 3, 28, 'wafi')</pre> <p>A quick sidenote: it is encouraged to split the data into training, validation (also called dev), and test sets. When the dataset is not large, an <code>80/10/10</code> split is a reasonable ratio for allocation. For larger datasets (e.g. with one million images), it is fine to allocate <code>90%</code> or more of your data for training. The training set is used to update the model's parameters. The validation set is used for tuning hyperparameters (e.g. testing different learning rates, regularization strengths, etc.). The test split should ideally be used only once to report the final performance of the selected model (e.g. for inclusion in a research paper).</p> In\u00a0[3]: Copied! <pre>########## DATA PREP ##########\n\ndef get_ngrams(start=0, end=None):\n  X, Y = [], []\n  for word in words[start:end]:\n    context = ['&lt;START&gt;'] * BLOCK_SIZE\n    for ch in list(word) + ['&lt;END&gt;']:\n      X.append([stoi[c] for c in context])\n      Y.append(stoi[ch])\n      context = context[1:] + [ch]\n  return torch.tensor(X), torch.tensor(Y)\n\ndef split_data(p=80):\n  train_end = int(p/100 * len(words))\n  remaining = len(words) - train_end\n  val_end = train_end + remaining // 2\n\n  X_train, Y_train = get_ngrams(end=train_end)\n  X_val, Y_val = get_ngrams(start=train_end, end=val_end)\n  X_test, Y_test = get_ngrams(start=val_end, end=len(words))\n\n  return {\n    'train': (X_train, Y_train),\n    'val':   (X_val, Y_val),\n    'test':  (X_test, Y_test),\n  }\n\ndata = split_data()\n\nX_train, Y_train = data['train']\nX_val, Y_val = data['val']\nX_test, Y_test = data['test']\n\nlen(X_train), len(X_val), len(X_test)\n</pre> ########## DATA PREP ##########  def get_ngrams(start=0, end=None):   X, Y = [], []   for word in words[start:end]:     context = [''] * BLOCK_SIZE     for ch in list(word) + ['']:       X.append([stoi[c] for c in context])       Y.append(stoi[ch])       context = context[1:] + [ch]   return torch.tensor(X), torch.tensor(Y)  def split_data(p=80):   train_end = int(p/100 * len(words))   remaining = len(words) - train_end   val_end = train_end + remaining // 2    X_train, Y_train = get_ngrams(end=train_end)   X_val, Y_val = get_ngrams(start=train_end, end=val_end)   X_test, Y_test = get_ngrams(start=val_end, end=len(words))    return {     'train': (X_train, Y_train),     'val':   (X_val, Y_val),     'test':  (X_test, Y_test),   }  data = split_data()  X_train, Y_train = data['train'] X_val, Y_val = data['val'] X_test, Y_test = data['test']  len(X_train), len(X_val), len(X_test) Out[3]: <pre>(182535, 22720, 22891)</pre> <p>Batch normalization normalizes the inputs within a mini-batch before passing them to the next layer. That is, for each input feature $x_i$, we subtract the batch mean and divide by the batch standard deviation. A small constant  $\\epsilon$ is commonly added for maintaining numerical stability (to avoid zero division):</p> <p>$$ \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} $$</p> <p>This standardization gives $\\hat{x}_i$ a mean close to 0 and a standard deviation close to 1 over the batch. This may limit the model's capacity if left unchanged. Therefore, we introduce learnable parameters $\\gamma$ (scale) and $\\beta$ (shift) for flexibility:</p> <p>$$ BN = \\gamma \\hat{x}_i + \\beta $$</p> <p>Batch normalization is typically applied after the affine transformation ($Wx + b$) and before the non-linearity (e.g., ReLU):</p> <p>$$ act = \\phi(\\textrm{BN}(Wx)) $$</p> <p>Pay attention that we omitted $b$ when using batch normalization. In practice, the bias $b$ becomes redundant, because the shifting role is already handled by $\\beta$. Recall that <code>PyTorch</code> has <code>bias=False</code> option  as well (e.g. in <code>nn.Conv2d()</code>).</p> <p>Batch normalization improves convergence in optimization and has regularization effect. The original paper by Ioffe and Szegedyattributes this to reducing internal covariate shift \u2014 i.e. the shift in the distribution of layer inputs during training as parameters in earlier layers change. But this intuition is challenged. You can read more about that in d2l book chapter dedicated to batch normalization.</p> In\u00a0[4]: Copied! <pre>########## PARAMETER SETUP ##########\n\ndef get_params(batch_norm=True):\n  C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)\n\n  W1 = torch.randn((BLOCK_SIZE * EMBED_SIZE, LAYER_SIZE), requires_grad = True)\n  b1 = torch.zeros(LAYER_SIZE, requires_grad=True)\n\n  W2 = torch.randn((LAYER_SIZE, VOCAB_SIZE), requires_grad = True)\n  b2 = torch.zeros(VOCAB_SIZE, requires_grad=True)\n\n  params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n\n  if batch_norm:\n    gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)\n    beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)\n    params['gamma'] = gamma\n    params['beta'] = beta\n    # we can add additional code for omitting b1 in case of using beta (BN bias)\n\n  return params\n</pre> ########## PARAMETER SETUP ##########  def get_params(batch_norm=True):   C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)    W1 = torch.randn((BLOCK_SIZE * EMBED_SIZE, LAYER_SIZE), requires_grad = True)   b1 = torch.zeros(LAYER_SIZE, requires_grad=True)    W2 = torch.randn((LAYER_SIZE, VOCAB_SIZE), requires_grad = True)   b2 = torch.zeros(VOCAB_SIZE, requires_grad=True)    params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}    if batch_norm:     gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)     beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)     params['gamma'] = gamma     params['beta'] = beta     # we can add additional code for omitting b1 in case of using beta (BN bias)    return params In\u00a0[5]: Copied! <pre>########## FORWARD PASS ##########\n\n@torch.no_grad() # applies \"with torch.no_grad()\" to the whole function\ndef get_bn_stats(X_train, params):\n  emb = params['C'][X_train]\n  out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']\n  mean, std = out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5\n  return mean, std\n\ndef forward(X, params, batch_norm=False, bn_stats=None):\n  emb = params['C'][X]\n  out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']\n\n  if batch_norm:\n    mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)\n    out = (out - mean) / std\n    out = params['gamma'] * out + params['beta']\n\n  act = torch.tanh(out)\n  logits = act @ params['W2'] + params['b2']\n  return logits\n</pre> ########## FORWARD PASS ##########  @torch.no_grad() # applies \"with torch.no_grad()\" to the whole function def get_bn_stats(X_train, params):   emb = params['C'][X_train]   out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']   mean, std = out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5   return mean, std  def forward(X, params, batch_norm=False, bn_stats=None):   emb = params['C'][X]   out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']    if batch_norm:     mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)     out = (out - mean) / std     out = params['gamma'] * out + params['beta']    act = torch.tanh(out)   logits = act @ params['W2'] + params['b2']   return logits In\u00a0[6]: Copied! <pre>########## TRAINING &amp; INFERENCE ##########\n\ndef train(X, Y, params, num_epochs=100, lr=0.1, batch_size=None, batch_norm=False):\n  for epoch in range(1, num_epochs+1):\n    if batch_size:\n      idx = torch.randint(0, X.size(0), (batch_size,))\n      batch_X, batch_Y = X[idx], Y[idx]\n    else:\n      batch_X, batch_Y = X, Y\n\n    logits = forward(batch_X, params, batch_norm)\n    loss = F.cross_entropy(logits, batch_Y)\n\n    for p in params.values():\n      if p.grad is not None:\n        p.grad.zero_()\n    loss.backward()\n\n    with torch.no_grad():\n      for p in params.values():\n        p.data -= lr * p.grad\n\n    if epoch % (1000 if batch_size else 10) == 0:\n      print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n@torch.no_grad()\ndef evaluate(X, Y, params, batch_norm=False, bn_stats=None):\n  logits = forward(X, params, batch_norm, bn_stats)\n  loss = F.cross_entropy(logits, Y)\n  print(f\"Loss: {loss.item():.4f}\")\n</pre> ########## TRAINING &amp; INFERENCE ##########  def train(X, Y, params, num_epochs=100, lr=0.1, batch_size=None, batch_norm=False):   for epoch in range(1, num_epochs+1):     if batch_size:       idx = torch.randint(0, X.size(0), (batch_size,))       batch_X, batch_Y = X[idx], Y[idx]     else:       batch_X, batch_Y = X, Y      logits = forward(batch_X, params, batch_norm)     loss = F.cross_entropy(logits, batch_Y)      for p in params.values():       if p.grad is not None:         p.grad.zero_()     loss.backward()      with torch.no_grad():       for p in params.values():         p.data -= lr * p.grad      if epoch % (1000 if batch_size else 10) == 0:       print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")  @torch.no_grad() def evaluate(X, Y, params, batch_norm=False, bn_stats=None):   logits = forward(X, params, batch_norm, bn_stats)   loss = F.cross_entropy(logits, Y)   print(f\"Loss: {loss.item():.4f}\") In\u00a0[7]: Copied! <pre>########## TEST ##########\n\nepochs = 10_000\nlr = 0.01\nbatch_size = 32\nbatch_norm = True\ninit = True\n</pre> ########## TEST ##########  epochs = 10_000 lr = 0.01 batch_size = 32 batch_norm = True init = True In\u00a0[8]: Copied! <pre>params = get_params(batch_norm=batch_norm)\n\n# xavier for tanh, kaiming for relu\nif init:\n  nn.init.xavier_uniform_(params['W1'])\n  nn.init.xavier_uniform_(params['W2'])\n</pre> params = get_params(batch_norm=batch_norm)  # xavier for tanh, kaiming for relu if init:   nn.init.xavier_uniform_(params['W1'])   nn.init.xavier_uniform_(params['W2']) In\u00a0[9]: Copied! <pre>train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm)\n</pre> train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm) <pre>Epoch 1000, Loss: 2.7862\nEpoch 2000, Loss: 2.5897\nEpoch 3000, Loss: 2.3340\nEpoch 4000, Loss: 2.4985\nEpoch 5000, Loss: 2.7501\nEpoch 6000, Loss: 2.2715\nEpoch 7000, Loss: 2.6008\nEpoch 8000, Loss: 2.0696\nEpoch 9000, Loss: 2.3910\nEpoch 10000, Loss: 2.4777\n</pre> In\u00a0[10]: Copied! <pre>bn_stats = get_bn_stats(X_train, params) if batch_norm else None\n\nprint('Train and Validation losses:')\nevaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats)\nevaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats)\n</pre> bn_stats = get_bn_stats(X_train, params) if batch_norm else None  print('Train and Validation losses:') evaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats) evaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats) <pre>Train and Validation losses:\nLoss: 2.3337\nLoss: 2.3345\n</pre> In\u00a0[11]: Copied! <pre>########## SAMPLING ##########\n\n# minor changes to what we had previously for adapting to new code\ndef sample(params, n=10, batch_norm=False, bn_stats=None):\n  names = []\n  for _ in range(n):\n    context = ['&lt;START&gt;'] * BLOCK_SIZE\n    name = ''\n    while True:\n      X = torch.tensor([[stoi[c] for c in context]])\n      logits = forward(X, params, batch_norm, bn_stats)\n      probs = F.softmax(logits, dim=1)\n      id = torch.multinomial(probs, num_samples=1).item()\n      char = itos[id]\n      if char == '&lt;END&gt;':\n        break\n      name += char\n      context = context[1:] + [char]\n    names.append(name)\n  return names\n</pre> ########## SAMPLING ##########  # minor changes to what we had previously for adapting to new code def sample(params, n=10, batch_norm=False, bn_stats=None):   names = []   for _ in range(n):     context = [''] * BLOCK_SIZE     name = ''     while True:       X = torch.tensor([[stoi[c] for c in context]])       logits = forward(X, params, batch_norm, bn_stats)       probs = F.softmax(logits, dim=1)       id = torch.multinomial(probs, num_samples=1).item()       char = itos[id]       if char == '':         break       name += char       context = context[1:] + [char]     names.append(name)   return names In\u00a0[12]: Copied! <pre>sample(params, batch_norm=batch_norm, bn_stats=bn_stats)\n</pre> sample(params, batch_norm=batch_norm, bn_stats=bn_stats) Out[12]: <pre>['khuc',\n 'boka',\n 'lyq',\n 'rasyanrith',\n 'onna',\n 'helia',\n 'brhaylanio',\n 'boleiklak',\n 'ekbnqron',\n 'aren']</pre> <p>Hence, the idea of the residual connection is very simple. Before the second activation function, we add the previous input to the affine transformation. You can imagine the simplified code as below:</p> In\u00a0[13]: Copied! <pre>def residual_block(X):\n  act = torch.relu(X @ params['W1'] + params['b1'])\n  out = act @ params['W2'] + params['b2']\n  return torch.relu(out + X)\n</pre> def residual_block(X):   act = torch.relu(X @ params['W1'] + params['b1'])   out = act @ params['W2'] + params['b2']   return torch.relu(out + X) <p>However, If we attempt to directly run the code above, we will see a shape mismatch, as our final layer returns a matrix of dimension <code>VOCAB_SIZE</code> which is not equal to the input dimension <code>BLOCK_SIZE * EMBED_SIZE</code>.</p> <p>Exercise: Modifying the <code>forward</code> function by adding a residual connection.</p> In\u00a0[14]: Copied! <pre>def forward(X, params, batch_norm=False, bn_stats=None, residual=True):\n  emb = params['C'][X]\n  out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']\n\n  if batch_norm:\n    mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)\n    out = (out - mean) / std\n    out = params['gamma'] * out + params['beta']\n\n  act = torch.tanh(out + emb) if residual else torch.tanh(out)\n  logits = act @ params['W2'] + params['b2']\n  return logits\n</pre> def forward(X, params, batch_norm=False, bn_stats=None, residual=True):   emb = params['C'][X]   out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']    if batch_norm:     mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)     out = (out - mean) / std     out = params['gamma'] * out + params['beta']    act = torch.tanh(out + emb) if residual else torch.tanh(out)   logits = act @ params['W2'] + params['b2']   return logits In\u00a0[15]: Copied! <pre>X = params['C'][X_train].view(X_train.shape[0], -1)\nX.shape\n</pre> X = params['C'][X_train].view(X_train.shape[0], -1) X.shape Out[15]: <pre>torch.Size([182535, 30])</pre> <p>What to do? For demonstration purposes we will have to add another layer.</p> <p>Exercise (Advanced): Train a three layer model with batch normalization and residual connections.</p> In\u00a0[16]: Copied! <pre>def get_params(batch_norm=True):\n  C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)\n\n  in_features = BLOCK_SIZE * EMBED_SIZE\n\n  W1 = torch.randn((in_features, LAYER_SIZE), requires_grad = True)\n  b1 = torch.zeros(LAYER_SIZE, requires_grad=True)\n\n  W2 = torch.randn((LAYER_SIZE, in_features), requires_grad = True)\n  b2 = torch.zeros(in_features, requires_grad=True)\n\n  W3 = torch.randn((in_features, VOCAB_SIZE), requires_grad = True)\n  b3 = torch.zeros(VOCAB_SIZE, requires_grad=True)\n\n  params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}\n\n  if batch_norm:\n    gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)\n    beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)\n    params['gamma'] = gamma\n    params['beta'] = beta\n\n  return params\n</pre> def get_params(batch_norm=True):   C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)    in_features = BLOCK_SIZE * EMBED_SIZE    W1 = torch.randn((in_features, LAYER_SIZE), requires_grad = True)   b1 = torch.zeros(LAYER_SIZE, requires_grad=True)    W2 = torch.randn((LAYER_SIZE, in_features), requires_grad = True)   b2 = torch.zeros(in_features, requires_grad=True)    W3 = torch.randn((in_features, VOCAB_SIZE), requires_grad = True)   b3 = torch.zeros(VOCAB_SIZE, requires_grad=True)    params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}    if batch_norm:     gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)     beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)     params['gamma'] = gamma     params['beta'] = beta    return params In\u00a0[17]: Copied! <pre>def forward(X, params, batch_norm=False, bn_stats=None, residual=True):\n  emb = params['C'][X].view(X.shape[0], -1)\n  out = emb @ params['W1'] + params['b1']\n\n  if batch_norm:\n    mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)\n    out = (out - mean) / std\n    out = params['gamma'] * out + params['beta']\n\n  act = torch.relu(out)\n  out2 = act @ params['W2'] + params['b2']\n\n  if residual:\n    out2 = out2 + emb\n\n  logits = torch.tanh(out2) @ params['W3'] + params['b3']\n  return logits\n</pre> def forward(X, params, batch_norm=False, bn_stats=None, residual=True):   emb = params['C'][X].view(X.shape[0], -1)   out = emb @ params['W1'] + params['b1']    if batch_norm:     mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)     out = (out - mean) / std     out = params['gamma'] * out + params['beta']    act = torch.relu(out)   out2 = act @ params['W2'] + params['b2']    if residual:     out2 = out2 + emb    logits = torch.tanh(out2) @ params['W3'] + params['b3']   return logits In\u00a0[18]: Copied! <pre>params = get_params()\nparams.keys()\n</pre> params = get_params() params.keys() Out[18]: <pre>dict_keys(['C', 'W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'gamma', 'beta'])</pre> In\u00a0[19]: Copied! <pre># we are using relu in intermediate layer\nif init:\n  nn.init.kaiming_uniform_(params['W1'])\n  nn.init.kaiming_uniform_(params['W2']);\n</pre> # we are using relu in intermediate layer if init:   nn.init.kaiming_uniform_(params['W1'])   nn.init.kaiming_uniform_(params['W2']); In\u00a0[20]: Copied! <pre>train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm)\n</pre> train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm) <pre>Epoch 1000, Loss: 2.5227\nEpoch 2000, Loss: 2.9970\nEpoch 3000, Loss: 2.5845\nEpoch 4000, Loss: 2.3321\nEpoch 5000, Loss: 2.2630\nEpoch 6000, Loss: 2.5062\nEpoch 7000, Loss: 2.8853\nEpoch 8000, Loss: 2.3080\nEpoch 9000, Loss: 2.7023\nEpoch 10000, Loss: 2.8854\n</pre> In\u00a0[21]: Copied! <pre>bn_stats = get_bn_stats(X_train, params) if batch_norm else None\n\nprint('Train and Validation losses:')\nevaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats)\nevaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats)\n</pre> bn_stats = get_bn_stats(X_train, params) if batch_norm else None  print('Train and Validation losses:') evaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats) evaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats) <pre>Train and Validation losses:\nLoss: 2.3690\nLoss: 2.3698\n</pre> In\u00a0[22]: Copied! <pre>class ResidualBlock(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.emb = nn.Embedding(VOCAB_SIZE, EMBED_SIZE)\n    self.fc1 = nn.Linear(in_features=EMBED_SIZE, out_features=LAYER_SIZE, bias=False)\n    self.fc2 = nn.Linear(in_features=LAYER_SIZE, out_features=EMBED_SIZE, bias=False)\n    self.fc3 = nn.Linear(in_features=EMBED_SIZE, out_features=VOCAB_SIZE, bias=True)\n    self.bn1 = nn.LazyBatchNorm1d()\n    self.bn2 = nn.LazyBatchNorm1d()\n    nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n\n  # nn.LazyBatchNorm1d in 3D input expects shape (batch, channels, length) = (B, C, T)\n  # it normalizes across the batch and time (token, block) dimensions for each channel, independently\n  # we need to move that dimension to the middle (axis 1) with transpose(1, 2)\n  def forward(self, X):\n    emb = self.emb(X)                     # (BATCH_SIZE, BLOCK_SIZE, EMBED_SIZE)\n    out = self.fc1(emb).transpose(1, 2)   # (BATCH_SIZE, LAYER_SIZE, BLOCK_SIZE) for BatchNorm1d\n    out = self.bn1(out).transpose(1, 2)   # back to our dimensions\n    act = F.relu(out)\n    out = self.fc2(act).transpose(1, 2)\n    out = self.bn2(out).transpose(1, 2)\n    out += emb                            # shortcut connection\n    logits = self.fc3(out)                # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)\n    return logits\n</pre> class ResidualBlock(nn.Module):   def __init__(self):     super().__init__()     self.emb = nn.Embedding(VOCAB_SIZE, EMBED_SIZE)     self.fc1 = nn.Linear(in_features=EMBED_SIZE, out_features=LAYER_SIZE, bias=False)     self.fc2 = nn.Linear(in_features=LAYER_SIZE, out_features=EMBED_SIZE, bias=False)     self.fc3 = nn.Linear(in_features=EMBED_SIZE, out_features=VOCAB_SIZE, bias=True)     self.bn1 = nn.LazyBatchNorm1d()     self.bn2 = nn.LazyBatchNorm1d()     nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')    # nn.LazyBatchNorm1d in 3D input expects shape (batch, channels, length) = (B, C, T)   # it normalizes across the batch and time (token, block) dimensions for each channel, independently   # we need to move that dimension to the middle (axis 1) with transpose(1, 2)   def forward(self, X):     emb = self.emb(X)                     # (BATCH_SIZE, BLOCK_SIZE, EMBED_SIZE)     out = self.fc1(emb).transpose(1, 2)   # (BATCH_SIZE, LAYER_SIZE, BLOCK_SIZE) for BatchNorm1d     out = self.bn1(out).transpose(1, 2)   # back to our dimensions     act = F.relu(out)     out = self.fc2(act).transpose(1, 2)     out = self.bn2(out).transpose(1, 2)     out += emb                            # shortcut connection     logits = self.fc3(out)                # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)     return logits In\u00a0[23]: Copied! <pre>model = ResidualBlock()\ncel = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel.train()\n</pre> model = ResidualBlock() cel = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.01) model.train() Out[23]: <pre>ResidualBlock(\n  (emb): Embedding(28, 10)\n  (fc1): Linear(in_features=10, out_features=100, bias=False)\n  (fc2): Linear(in_features=100, out_features=10, bias=False)\n  (fc3): Linear(in_features=10, out_features=28, bias=True)\n  (bn1): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bn2): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)</pre> In\u00a0[24]: Copied! <pre>num_epochs = 10_000\nbatch_size = 32\n\nfor epoch in range(1, num_epochs+1):\n  model.train()\n  idx = torch.randint(0, X_train.size(0), (batch_size,))\n  batch_X, batch_Y = X_train[idx], Y_train[idx]\n  optimizer.zero_grad()\n  logits = model(batch_X)     # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)\n  logits = logits[:, -1, :]   # (BATCH_SIZE, VOCAB_SIZE)\n  loss = cel(logits, batch_Y)\n  loss.backward()\n  optimizer.step()\n  if epoch % 1000 == 0 or epoch == 1:\n    print(f'Epoch {epoch}, Loss: {loss.item()}')\n</pre> num_epochs = 10_000 batch_size = 32  for epoch in range(1, num_epochs+1):   model.train()   idx = torch.randint(0, X_train.size(0), (batch_size,))   batch_X, batch_Y = X_train[idx], Y_train[idx]   optimizer.zero_grad()   logits = model(batch_X)     # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)   logits = logits[:, -1, :]   # (BATCH_SIZE, VOCAB_SIZE)   loss = cel(logits, batch_Y)   loss.backward()   optimizer.step()   if epoch % 1000 == 0 or epoch == 1:     print(f'Epoch {epoch}, Loss: {loss.item()}') <pre>Epoch 1, Loss: 3.6320574283599854\nEpoch 1000, Loss: 2.374105930328369\nEpoch 2000, Loss: 2.6409666538238525\nEpoch 3000, Loss: 2.6358656883239746\nEpoch 4000, Loss: 2.36672043800354\nEpoch 5000, Loss: 2.696502208709717\nEpoch 6000, Loss: 2.4992451667785645\nEpoch 7000, Loss: 2.413964033126831\nEpoch 8000, Loss: 2.83028507232666\nEpoch 9000, Loss: 2.3721745014190674\nEpoch 10000, Loss: 2.6832263469696045\n</pre> In\u00a0[25]: Copied! <pre>model.eval()\nwith torch.no_grad():\n  logits_train = model(X_train)[:, -1, :]\n  logits_val   = model(X_val)[:, -1, :]\n\n  full_loss_train = cel(logits_train, Y_train)\n  full_loss_val   = cel(logits_val, Y_val)\n\n  print(f'Train loss: {full_loss_train.item()}')\n  print(f'Validation loss: {full_loss_val.item()}')\n</pre> model.eval() with torch.no_grad():   logits_train = model(X_train)[:, -1, :]   logits_val   = model(X_val)[:, -1, :]    full_loss_train = cel(logits_train, Y_train)   full_loss_val   = cel(logits_val, Y_val)    print(f'Train loss: {full_loss_train.item()}')   print(f'Validation loss: {full_loss_val.item()}') <pre>Train loss: 2.4901065826416016\nValidation loss: 2.4812421798706055\n</pre> In\u00a0[26]: Copied! <pre># modifying code to suit our needs\ndef sample(model, n=10, block_size=3):\n  model.eval()\n  names = []\n  for _ in range(n):\n    context = ['&lt;START&gt;'] * block_size\n    name = ''\n    while True:\n      idx = [stoi[c] for c in context]\n      X = torch.tensor([idx], dtype=torch.long)\n      with torch.no_grad():\n        logits = model(X)[0, -1] # VOCAB_SIZE\n      probs = F.softmax(logits, dim=0)\n      idx_next = torch.multinomial(probs, num_samples=1).item()\n      char = itos[idx_next]\n      if char == '&lt;END&gt;':\n        break\n      name += char\n      context = context[1:] + [char]\n    names.append(name)\n  return names\n</pre> # modifying code to suit our needs def sample(model, n=10, block_size=3):   model.eval()   names = []   for _ in range(n):     context = [''] * block_size     name = ''     while True:       idx = [stoi[c] for c in context]       X = torch.tensor([idx], dtype=torch.long)       with torch.no_grad():         logits = model(X)[0, -1] # VOCAB_SIZE       probs = F.softmax(logits, dim=0)       idx_next = torch.multinomial(probs, num_samples=1).item()       char = itos[idx_next]       if char == '':         break       name += char       context = context[1:] + [char]     names.append(name)   return names In\u00a0[27]: Copied! <pre>sample(model)\n</pre> sample(model) Out[27]: <pre>['kelifo',\n 'ja',\n 'tha',\n 'elarhncasoria',\n 'ka',\n 'voratte',\n 'eniysh',\n 'th',\n 'kelld',\n 'edm']</pre> In\u00a0[27]: Copied! <pre>\n</pre>"},{"location":"Main_Content/05_batchnorm_resnet/#05-training-deeper-networks-batch-normalization-and-residual-blocks","title":"05 Training Deeper Networks: Batch Normalization and Residual Blocks\u00b6","text":""},{"location":"Main_Content/05_batchnorm_resnet/#batch-normalization","title":"Batch Normalization\u00b6","text":""},{"location":"Main_Content/05_batchnorm_resnet/#running_stats","title":"running_stats\u00b6","text":"<p>In <code>PyTorch</code> we use <code>model.eval()</code> during inference to switch the model into evaluation mode. This is important because layers like dropout and batch normalization behave differently during training and evaluation.</p> <p>During inference, normalization should be done using statistics over the whole dataset instead of mini-batches. Without <code>bn_stats</code> in the code below, the model would normalize using the current batch's mean and standard deviation, leading to inconsistent results depending on the batch.</p> <p>The implemented <code>PyTorch</code> layers like nn.BatchNorm1d automatically calculate running statistics during training. These statistics include a running mean and a running variance for each feature channel, which are stored as non-learnable buffers inside the <code>BatchNorm</code> layer.</p> <p>$$ \\mu_{\\text{running}} = \\alpha \\, \\mu_{\\text{batch}} + (1 - \\alpha) \\, \\mu_{\\text{running}} $$</p> <p>$$ \\sigma^2_{\\text{running}} = \\alpha \\, \\sigma^2_{\\text{batch}} + (1 - \\alpha) \\, \\sigma^2_{\\text{running}} $$</p> <p>In <code>BatchNorm</code>, $\\alpha$ is defined as <code>momentum</code> which is a misnomer and has nothing to do with the momentum we had previously learned for optimization. Its values controls how quickly the <code>running_stats</code> adapt. If momentum is high, the running statistics update quickly based on new batches which can make them unstable and noisy if batches vary a lot. If it is low (by default it is set to <code>0.1</code>, but you may want to reduce it further depending on circumstances), the updates are smoother and slower, averaging batch statistics over time.</p> <p>During evaluation <code>BatchNorm</code> uses the stored running mean and variance for normalization. This ensures deterministic behavior, regardless of the input batch. These buffers are automatically updated and used unless you disable tracking by setting <code>track_running_stats=False</code>.</p> <p>A manual implementation of <code>running_stats</code> is demonstrated in Andrej Karpathy's video as well. In this notebook, we will only implemented the simpler <code>bn_stats</code>.</p>"},{"location":"Main_Content/05_batchnorm_resnet/#layer-normalization","title":"Layer Normalization\u00b6","text":"<p>A rule of thumb is that batch sizes between <code>50-100</code> generally work well for batch normalization: the batch is large enough to return reliable statistics but not so large that it causes memory issues or slows down training unnecessarily. Batch size of <code>32</code> is usually the lower bound where batch normalization still provides relatively stable estimates. Batch size of <code>128</code> is also effective if the hardware allows, and can produce even smoother estimates. Beyond that the benefit often diminishes.</p> <p>If the batch size is very small due to memory limitations, batch normalization may lose its effectiveness. In such cases, it's better to consider alternatives like Layer Normalization which do not depend on the batch dimension.</p> <p>Layer normalization normalizes across features for each individual sample, not across the batch and works well for transformers where batch sizes may be small or variable. Basically, batch normalization depends on the batch, but layer normalization does not.</p> <p>Furthermore, in fully connected layers, each feature is just a single number per sample, so batch normalization computes the mean and variance across the batch for each feature. Fully connected layers don't have spatial structure, so there's nothing to average across except the batch. In convolutional layers, each feature channel height and width and is a 2D map (hence, <code>nn.BatchNorm2d</code>), so batch normalization uses not just the batch dimension, but also all the spatial positions to compute statistics. This gives more stable estimates because there are more values per channel.</p>"},{"location":"Main_Content/05_batchnorm_resnet/#residual-block","title":"Residual Block\u00b6","text":"<p>Residual Network (ResNet) consists of repeated residual blocks, in the style of the VGGNet architecture. Each residual block consists of a residual (skip/shortcut) connection . We will first see what it does and then will attempt to understand the reasoning behind this simple breakthrough idea.</p>"},{"location":"Main_Content/05_batchnorm_resnet/#implementation","title":"Implementation\u00b6","text":"<p>Figure 8.6.2 of Dive into Deep Learning (Chapter 8) by d2l.ai authors and contributors. Licensed under Apache 2.0</p>"},{"location":"Main_Content/05_batchnorm_resnet/#reasoning","title":"Reasoning\u00b6","text":"<p>As our model is implementing a single residual block, we don't see any performance improvement. However, similar to batch normalization, the advantages will be obvious in case of 50 layers or more, with repeated residual blocks. But why adding input of the layer to the second affine transformation boosts training?</p> <p>Let's take any deep learning model. The types of functions this model can learn depend on its design (e.g. number of layers, activation functions, etc). All these possible functions we can denote as class $\\mathcal{F}$. If we cannot learn a perfect function for our data, which is usually the case, we can at least try to appoximate this function as closely as possible by minimizing a loss. We may assume that a more powerful model can learn more types of functions and show better performance. But that's not always the case. To achieve a better performance than a simpler model, our model must be capable of learning not only more functions but also all the functions the simpler model can learn. Simply, the possible function class of the more powerful model should be a superclass of the simpler model's function class $\\mathcal{F} \\subseteq \\mathcal{F}'$. If the ${F}'$ isn't an expanded version of {F}$, the new model might actually learn a function that is farther from the truth, and even show worse performance.</p> <p>Refer to the figure above, where our residual output is $f(x) = g(x) + x$. One advantage of residual blocks is their regularization effect. What if some activation nodes in our network are unnecessary and increase complexity or learn bad representations? Instead of learning weights and biases, our residual block can now learn an identity function $f(x) = x$ by simply setting that nodes parameters to zero. As a result, our inputs will propagate faster while ensuring that the learned functions are within the biggest function domain. Residual blocks not only act as a regularizer, but also, unlike, say, dropout which stops input from propagating, allow the network to learn more functions by helping inputs to \"jump over\" (skip) the nodes. And it is very important that the function classes of the model with residual blocks is a superset of the same model without such blocks. Finally, along the way, it deals with the vanishing gradient problem by simply increasing the output of each layer. To sum up, residual connection allows the model to learn more complex functions, while allowing it to easily learn simpler ones, which tackles the vanishing gradient problem and has a regularizing effect.</p>"},{"location":"Main_Content/05_batchnorm_resnet/#residual-network-for-nlp-in-pytorch","title":"Residual Network for NLP in PyTorch\u00b6","text":"<p>Originally, the complete Residual Network was developed for image classification tasks, winning ImageNet competition. Each of its residual block consisted of two <code>3x3</code> convolutions (inspired  by VGGNet), both integrating batch normalization, followed by a skip connection. Even though, ResNet model relies on convolutional layer, the concept of residual connections has been adapted for NLP models as well. The infamous Transformer model, introduced in the paper titled Attention is All You Need incorporates residual connections heavily in its design, which is very similar to ResNet.</p>"},{"location":"Main_Content/06_nn_ngram/","title":"06 Python Code for Neural Network N-Gram Model","text":"In\u00a0[1]: Copied! <pre>import requests\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\nresponse = requests.get(url)\ndata = response.text\nwords = data.splitlines()\n</pre> import requests  url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\" response = requests.get(url) data = response.text words = data.splitlines() In\u00a0[2]: Copied! <pre>print('Length:', len(words))\nprint(words[:5] )\n</pre> print('Length:', len(words)) print(words[:5] ) <pre>Length: 32033\n['emma', 'olivia', 'ava', 'isabella', 'sophia']\n</pre> <p>This data provides information about names. For example, by having five examples <code>['emma', 'olivia', 'ava', 'isabella', 'sophia']</code> we may conclude that the probability of <code>'a'</code> being the last letter is <code>1.0</code> after which the word will certainly end, or that the letter <code>'o'</code> is more likely to be at the beginning of the name. Our goal will be to to predict the most probable next character. A common technique is to take track of bigrams of characters (there can be N-grams of words, etc., which have both advantages and disadvantages).</p> In\u00a0[3]: Copied! <pre>def get_bigrams(n):\n  bigrams = {}\n  for w in words[:n]:\n    w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n    for ch1, ch2 in zip(w, w[1:]):\n      b = (ch1, ch2)\n      bigrams[b] = bigrams.get(b, 0) + 1\n  return bigrams\n</pre> def get_bigrams(n):   bigrams = {}   for w in words[:n]:     w = [''] + list(w) + ['']     for ch1, ch2 in zip(w, w[1:]):       b = (ch1, ch2)       bigrams[b] = bigrams.get(b, 0) + 1   return bigrams <p>Change <code>n</code> up to 5 and take note of the bigram counts.</p> In\u00a0[4]: Copied! <pre>get_bigrams(n=2)\n</pre> get_bigrams(n=2) Out[4]: <pre>{('&lt;START&gt;', 'e'): 1,\n ('e', 'm'): 1,\n ('m', 'm'): 1,\n ('m', 'a'): 1,\n ('a', '&lt;END&gt;'): 2,\n ('&lt;START&gt;', 'o'): 1,\n ('o', 'l'): 1,\n ('l', 'i'): 1,\n ('i', 'v'): 1,\n ('v', 'i'): 1,\n ('i', 'a'): 1}</pre> In\u00a0[5]: Copied! <pre>from collections import Counter\n\nbigrams = get_bigrams(len(words))\nCounter(bigrams).most_common()\n</pre> from collections import Counter  bigrams = get_bigrams(len(words)) Counter(bigrams).most_common() Out[5]: <pre>[(('n', '&lt;END&gt;'), 6763),\n (('a', '&lt;END&gt;'), 6640),\n (('a', 'n'), 5438),\n (('&lt;START&gt;', 'a'), 4410),\n (('e', '&lt;END&gt;'), 3983),\n (('a', 'r'), 3264),\n (('e', 'l'), 3248),\n (('r', 'i'), 3033),\n (('n', 'a'), 2977),\n (('&lt;START&gt;', 'k'), 2963),\n (('l', 'e'), 2921),\n (('e', 'n'), 2675),\n (('l', 'a'), 2623),\n (('m', 'a'), 2590),\n (('&lt;START&gt;', 'm'), 2538),\n (('a', 'l'), 2528),\n (('i', '&lt;END&gt;'), 2489),\n (('l', 'i'), 2480),\n (('i', 'a'), 2445),\n (('&lt;START&gt;', 'j'), 2422),\n (('o', 'n'), 2411),\n (('h', '&lt;END&gt;'), 2409),\n (('r', 'a'), 2356),\n (('a', 'h'), 2332),\n (('h', 'a'), 2244),\n (('y', 'a'), 2143),\n (('i', 'n'), 2126),\n (('&lt;START&gt;', 's'), 2055),\n (('a', 'y'), 2050),\n (('y', '&lt;END&gt;'), 2007),\n (('e', 'r'), 1958),\n (('n', 'n'), 1906),\n (('y', 'n'), 1826),\n (('k', 'a'), 1731),\n (('n', 'i'), 1725),\n (('r', 'e'), 1697),\n (('&lt;START&gt;', 'd'), 1690),\n (('i', 'e'), 1653),\n (('a', 'i'), 1650),\n (('&lt;START&gt;', 'r'), 1639),\n (('a', 'm'), 1634),\n (('l', 'y'), 1588),\n (('&lt;START&gt;', 'l'), 1572),\n (('&lt;START&gt;', 'c'), 1542),\n (('&lt;START&gt;', 'e'), 1531),\n (('j', 'a'), 1473),\n (('r', '&lt;END&gt;'), 1377),\n (('n', 'e'), 1359),\n (('l', 'l'), 1345),\n (('i', 'l'), 1345),\n (('i', 's'), 1316),\n (('l', '&lt;END&gt;'), 1314),\n (('&lt;START&gt;', 't'), 1308),\n (('&lt;START&gt;', 'b'), 1306),\n (('d', 'a'), 1303),\n (('s', 'h'), 1285),\n (('d', 'e'), 1283),\n (('e', 'e'), 1271),\n (('m', 'i'), 1256),\n (('s', 'a'), 1201),\n (('s', '&lt;END&gt;'), 1169),\n (('&lt;START&gt;', 'n'), 1146),\n (('a', 's'), 1118),\n (('y', 'l'), 1104),\n (('e', 'y'), 1070),\n (('o', 'r'), 1059),\n (('a', 'd'), 1042),\n (('t', 'a'), 1027),\n (('&lt;START&gt;', 'z'), 929),\n (('v', 'i'), 911),\n (('k', 'e'), 895),\n (('s', 'e'), 884),\n (('&lt;START&gt;', 'h'), 874),\n (('r', 'o'), 869),\n (('e', 's'), 861),\n (('z', 'a'), 860),\n (('o', '&lt;END&gt;'), 855),\n (('i', 'r'), 849),\n (('b', 'r'), 842),\n (('a', 'v'), 834),\n (('m', 'e'), 818),\n (('e', 'i'), 818),\n (('c', 'a'), 815),\n (('i', 'y'), 779),\n (('r', 'y'), 773),\n (('e', 'm'), 769),\n (('s', 't'), 765),\n (('h', 'i'), 729),\n (('t', 'e'), 716),\n (('n', 'd'), 704),\n (('l', 'o'), 692),\n (('a', 'e'), 692),\n (('a', 't'), 687),\n (('s', 'i'), 684),\n (('e', 'a'), 679),\n (('d', 'i'), 674),\n (('h', 'e'), 674),\n (('&lt;START&gt;', 'g'), 669),\n (('t', 'o'), 667),\n (('c', 'h'), 664),\n (('b', 'e'), 655),\n (('t', 'h'), 647),\n (('v', 'a'), 642),\n (('o', 'l'), 619),\n (('&lt;START&gt;', 'i'), 591),\n (('i', 'o'), 588),\n (('e', 't'), 580),\n (('v', 'e'), 568),\n (('a', 'k'), 568),\n (('a', 'a'), 556),\n (('c', 'e'), 551),\n (('a', 'b'), 541),\n (('i', 't'), 541),\n (('&lt;START&gt;', 'y'), 535),\n (('t', 'i'), 532),\n (('s', 'o'), 531),\n (('m', '&lt;END&gt;'), 516),\n (('d', '&lt;END&gt;'), 516),\n (('&lt;START&gt;', 'p'), 515),\n (('i', 'c'), 509),\n (('k', 'i'), 509),\n (('o', 's'), 504),\n (('n', 'o'), 496),\n (('t', '&lt;END&gt;'), 483),\n (('j', 'o'), 479),\n (('u', 's'), 474),\n (('a', 'c'), 470),\n (('n', 'y'), 465),\n (('e', 'v'), 463),\n (('s', 's'), 461),\n (('m', 'o'), 452),\n (('i', 'k'), 445),\n (('n', 't'), 443),\n (('i', 'd'), 440),\n (('j', 'e'), 440),\n (('a', 'z'), 435),\n (('i', 'g'), 428),\n (('i', 'm'), 427),\n (('r', 'r'), 425),\n (('d', 'r'), 424),\n (('&lt;START&gt;', 'f'), 417),\n (('u', 'r'), 414),\n (('r', 'l'), 413),\n (('y', 's'), 401),\n (('&lt;START&gt;', 'o'), 394),\n (('e', 'd'), 384),\n (('a', 'u'), 381),\n (('c', 'o'), 380),\n (('k', 'y'), 379),\n (('d', 'o'), 378),\n (('&lt;START&gt;', 'v'), 376),\n (('t', 't'), 374),\n (('z', 'e'), 373),\n (('z', 'i'), 364),\n (('k', '&lt;END&gt;'), 363),\n (('g', 'h'), 360),\n (('t', 'r'), 352),\n (('k', 'o'), 344),\n (('t', 'y'), 341),\n (('g', 'e'), 334),\n (('g', 'a'), 330),\n (('l', 'u'), 324),\n (('b', 'a'), 321),\n (('d', 'y'), 317),\n (('c', 'k'), 316),\n (('&lt;START&gt;', 'w'), 307),\n (('k', 'h'), 307),\n (('u', 'l'), 301),\n (('y', 'e'), 301),\n (('y', 'r'), 291),\n (('m', 'y'), 287),\n (('h', 'o'), 287),\n (('w', 'a'), 280),\n (('s', 'l'), 279),\n (('n', 's'), 278),\n (('i', 'z'), 277),\n (('u', 'n'), 275),\n (('o', 'u'), 275),\n (('n', 'g'), 273),\n (('y', 'd'), 272),\n (('c', 'i'), 271),\n (('y', 'o'), 271),\n (('i', 'v'), 269),\n (('e', 'o'), 269),\n (('o', 'm'), 261),\n (('r', 'u'), 252),\n (('f', 'a'), 242),\n (('b', 'i'), 217),\n (('s', 'y'), 215),\n (('n', 'c'), 213),\n (('h', 'y'), 213),\n (('p', 'a'), 209),\n (('r', 't'), 208),\n (('q', 'u'), 206),\n (('p', 'h'), 204),\n (('h', 'r'), 204),\n (('j', 'u'), 202),\n (('g', 'r'), 201),\n (('p', 'e'), 197),\n (('n', 'l'), 195),\n (('y', 'i'), 192),\n (('g', 'i'), 190),\n (('o', 'd'), 190),\n (('r', 's'), 190),\n (('r', 'd'), 187),\n (('h', 'l'), 185),\n (('s', 'u'), 185),\n (('a', 'x'), 182),\n (('e', 'z'), 181),\n (('e', 'k'), 178),\n (('o', 'v'), 176),\n (('a', 'j'), 175),\n (('o', 'h'), 171),\n (('u', 'e'), 169),\n (('m', 'm'), 168),\n (('a', 'g'), 168),\n (('h', 'u'), 166),\n (('x', '&lt;END&gt;'), 164),\n (('u', 'a'), 163),\n (('r', 'm'), 162),\n (('a', 'w'), 161),\n (('f', 'i'), 160),\n (('z', '&lt;END&gt;'), 160),\n (('u', '&lt;END&gt;'), 155),\n (('u', 'm'), 154),\n (('e', 'c'), 153),\n (('v', 'o'), 153),\n (('e', 'h'), 152),\n (('p', 'r'), 151),\n (('d', 'd'), 149),\n (('o', 'a'), 149),\n (('w', 'e'), 149),\n (('w', 'i'), 148),\n (('y', 'm'), 148),\n (('z', 'y'), 147),\n (('n', 'z'), 145),\n (('y', 'u'), 141),\n (('r', 'n'), 140),\n (('o', 'b'), 140),\n (('k', 'l'), 139),\n (('m', 'u'), 139),\n (('l', 'd'), 138),\n (('h', 'n'), 138),\n (('u', 'd'), 136),\n (('&lt;START&gt;', 'x'), 134),\n (('t', 'l'), 134),\n (('a', 'f'), 134),\n (('o', 'e'), 132),\n (('e', 'x'), 132),\n (('e', 'g'), 125),\n (('f', 'e'), 123),\n (('z', 'l'), 123),\n (('u', 'i'), 121),\n (('v', 'y'), 121),\n (('e', 'b'), 121),\n (('r', 'h'), 121),\n (('j', 'i'), 119),\n (('o', 't'), 118),\n (('d', 'h'), 118),\n (('h', 'm'), 117),\n (('c', 'l'), 116),\n (('o', 'o'), 115),\n (('y', 'c'), 115),\n (('o', 'w'), 114),\n (('o', 'c'), 114),\n (('f', 'r'), 114),\n (('b', '&lt;END&gt;'), 114),\n (('m', 'b'), 112),\n (('z', 'o'), 110),\n (('i', 'b'), 110),\n (('i', 'u'), 109),\n (('k', 'r'), 109),\n (('g', '&lt;END&gt;'), 108),\n (('y', 'v'), 106),\n (('t', 'z'), 105),\n (('b', 'o'), 105),\n (('c', 'y'), 104),\n (('y', 't'), 104),\n (('u', 'b'), 103),\n (('u', 'c'), 103),\n (('x', 'a'), 103),\n (('b', 'l'), 103),\n (('o', 'y'), 103),\n (('x', 'i'), 102),\n (('i', 'f'), 101),\n (('r', 'c'), 99),\n (('c', '&lt;END&gt;'), 97),\n (('m', 'r'), 97),\n (('n', 'u'), 96),\n (('o', 'p'), 95),\n (('i', 'h'), 95),\n (('k', 's'), 95),\n (('l', 's'), 94),\n (('u', 'k'), 93),\n (('&lt;START&gt;', 'q'), 92),\n (('d', 'u'), 92),\n (('s', 'm'), 90),\n (('r', 'k'), 90),\n (('i', 'x'), 89),\n (('v', '&lt;END&gt;'), 88),\n (('y', 'k'), 86),\n (('u', 'w'), 86),\n (('g', 'u'), 85),\n (('b', 'y'), 83),\n (('e', 'p'), 83),\n (('g', 'o'), 83),\n (('s', 'k'), 82),\n (('u', 't'), 82),\n (('a', 'p'), 82),\n (('e', 'f'), 82),\n (('i', 'i'), 82),\n (('r', 'v'), 80),\n (('f', '&lt;END&gt;'), 80),\n (('t', 'u'), 78),\n (('y', 'z'), 78),\n (('&lt;START&gt;', 'u'), 78),\n (('l', 't'), 77),\n (('r', 'g'), 76),\n (('c', 'r'), 76),\n (('i', 'j'), 76),\n (('w', 'y'), 73),\n (('z', 'u'), 73),\n (('l', 'v'), 72),\n (('h', 't'), 71),\n (('j', '&lt;END&gt;'), 71),\n (('x', 't'), 70),\n (('o', 'i'), 69),\n (('e', 'u'), 69),\n (('o', 'k'), 68),\n (('b', 'd'), 65),\n (('a', 'o'), 63),\n (('p', 'i'), 61),\n (('s', 'c'), 60),\n (('d', 'l'), 60),\n (('l', 'm'), 60),\n (('a', 'q'), 60),\n (('f', 'o'), 60),\n (('p', 'o'), 59),\n (('n', 'k'), 58),\n (('w', 'n'), 58),\n (('u', 'h'), 58),\n (('e', 'j'), 55),\n (('n', 'v'), 55),\n (('s', 'r'), 55),\n (('o', 'z'), 54),\n (('i', 'p'), 53),\n (('l', 'b'), 52),\n (('i', 'q'), 52),\n (('w', '&lt;END&gt;'), 51),\n (('m', 'c'), 51),\n (('s', 'p'), 51),\n (('e', 'w'), 50),\n (('k', 'u'), 50),\n (('v', 'r'), 48),\n (('u', 'g'), 47),\n (('o', 'x'), 45),\n (('u', 'z'), 45),\n (('z', 'z'), 45),\n (('j', 'h'), 45),\n (('b', 'u'), 45),\n (('o', 'g'), 44),\n (('n', 'r'), 44),\n (('f', 'f'), 44),\n (('n', 'j'), 44),\n (('z', 'h'), 43),\n (('c', 'c'), 42),\n (('r', 'b'), 41),\n (('x', 'o'), 41),\n (('b', 'h'), 41),\n (('p', 'p'), 39),\n (('x', 'l'), 39),\n (('h', 'v'), 39),\n (('b', 'b'), 38),\n (('m', 'p'), 38),\n (('x', 'x'), 38),\n (('u', 'v'), 37),\n (('x', 'e'), 36),\n (('w', 'o'), 36),\n (('c', 't'), 35),\n (('z', 'm'), 35),\n (('t', 's'), 35),\n (('m', 's'), 35),\n (('c', 'u'), 35),\n (('o', 'f'), 34),\n (('u', 'x'), 34),\n (('k', 'w'), 34),\n (('p', '&lt;END&gt;'), 33),\n (('g', 'l'), 32),\n (('z', 'r'), 32),\n (('d', 'n'), 31),\n (('g', 't'), 31),\n (('g', 'y'), 31),\n (('h', 's'), 31),\n (('x', 's'), 31),\n (('g', 's'), 30),\n (('x', 'y'), 30),\n (('y', 'g'), 30),\n (('d', 'm'), 30),\n (('d', 's'), 29),\n (('h', 'k'), 29),\n (('y', 'x'), 28),\n (('q', '&lt;END&gt;'), 28),\n (('g', 'n'), 27),\n (('y', 'b'), 27),\n (('g', 'w'), 26),\n (('n', 'h'), 26),\n (('k', 'n'), 26),\n (('g', 'g'), 25),\n (('d', 'g'), 25),\n (('l', 'c'), 25),\n (('r', 'j'), 25),\n (('w', 'u'), 25),\n (('l', 'k'), 24),\n (('m', 'd'), 24),\n (('s', 'w'), 24),\n (('s', 'n'), 24),\n (('h', 'd'), 24),\n (('w', 'h'), 23),\n (('y', 'j'), 23),\n (('y', 'y'), 23),\n (('r', 'z'), 23),\n (('d', 'w'), 23),\n (('w', 'r'), 22),\n (('t', 'n'), 22),\n (('l', 'f'), 22),\n (('y', 'h'), 22),\n (('r', 'w'), 21),\n (('s', 'b'), 21),\n (('m', 'n'), 20),\n (('f', 'l'), 20),\n (('w', 's'), 20),\n (('k', 'k'), 20),\n (('h', 'z'), 20),\n (('g', 'd'), 19),\n (('l', 'h'), 19),\n (('n', 'm'), 19),\n (('x', 'z'), 19),\n (('u', 'f'), 19),\n (('f', 't'), 18),\n (('l', 'r'), 18),\n (('p', 't'), 17),\n (('t', 'c'), 17),\n (('k', 't'), 17),\n (('d', 'v'), 17),\n (('u', 'p'), 16),\n (('p', 'l'), 16),\n (('l', 'w'), 16),\n (('p', 's'), 16),\n (('o', 'j'), 16),\n (('r', 'q'), 16),\n (('y', 'p'), 15),\n (('l', 'p'), 15),\n (('t', 'v'), 15),\n (('r', 'p'), 14),\n (('l', 'n'), 14),\n (('e', 'q'), 14),\n (('f', 'y'), 14),\n (('s', 'v'), 14),\n (('u', 'j'), 14),\n (('v', 'l'), 14),\n (('q', 'a'), 13),\n (('u', 'y'), 13),\n (('q', 'i'), 13),\n (('w', 'l'), 13),\n (('p', 'y'), 12),\n (('y', 'f'), 12),\n (('c', 'q'), 11),\n (('j', 'r'), 11),\n (('n', 'w'), 11),\n (('n', 'f'), 11),\n (('t', 'w'), 11),\n (('m', 'z'), 11),\n (('u', 'o'), 10),\n (('f', 'u'), 10),\n (('l', 'z'), 10),\n (('h', 'w'), 10),\n (('u', 'q'), 10),\n (('j', 'y'), 10),\n (('s', 'z'), 10),\n (('s', 'd'), 9),\n (('j', 'l'), 9),\n (('d', 'j'), 9),\n (('k', 'm'), 9),\n (('r', 'f'), 9),\n (('h', 'j'), 9),\n (('v', 'n'), 8),\n (('n', 'b'), 8),\n (('i', 'w'), 8),\n (('h', 'b'), 8),\n (('b', 's'), 8),\n (('w', 't'), 8),\n (('w', 'd'), 8),\n (('v', 'v'), 7),\n (('v', 'u'), 7),\n (('j', 's'), 7),\n (('m', 'j'), 7),\n (('f', 's'), 6),\n (('l', 'g'), 6),\n (('l', 'j'), 6),\n (('j', 'w'), 6),\n (('n', 'x'), 6),\n (('y', 'q'), 6),\n (('w', 'k'), 6),\n (('g', 'm'), 6),\n (('x', 'u'), 5),\n (('m', 'h'), 5),\n (('m', 'l'), 5),\n (('j', 'm'), 5),\n (('c', 's'), 5),\n (('j', 'v'), 5),\n (('n', 'p'), 5),\n (('d', 'f'), 5),\n (('x', 'd'), 5),\n (('z', 'b'), 4),\n (('f', 'n'), 4),\n (('x', 'c'), 4),\n (('m', 't'), 4),\n (('t', 'm'), 4),\n (('z', 'n'), 4),\n (('z', 't'), 4),\n (('p', 'u'), 4),\n (('c', 'z'), 4),\n (('b', 'n'), 4),\n (('z', 's'), 4),\n (('f', 'w'), 4),\n (('d', 't'), 4),\n (('j', 'd'), 4),\n (('j', 'c'), 4),\n (('y', 'w'), 4),\n (('v', 'k'), 3),\n (('x', 'w'), 3),\n (('t', 'j'), 3),\n (('c', 'j'), 3),\n (('q', 'w'), 3),\n (('g', 'b'), 3),\n (('o', 'q'), 3),\n (('r', 'x'), 3),\n (('d', 'c'), 3),\n (('g', 'j'), 3),\n (('x', 'f'), 3),\n (('z', 'w'), 3),\n (('d', 'k'), 3),\n (('u', 'u'), 3),\n (('m', 'v'), 3),\n (('c', 'x'), 3),\n (('l', 'q'), 3),\n (('p', 'b'), 2),\n (('t', 'g'), 2),\n (('q', 's'), 2),\n (('t', 'x'), 2),\n (('f', 'k'), 2),\n (('b', 't'), 2),\n (('j', 'n'), 2),\n (('k', 'c'), 2),\n (('z', 'k'), 2),\n (('s', 'j'), 2),\n (('s', 'f'), 2),\n (('z', 'j'), 2),\n (('n', 'q'), 2),\n (('f', 'z'), 2),\n (('h', 'g'), 2),\n (('w', 'w'), 2),\n (('k', 'j'), 2),\n (('j', 'k'), 2),\n (('w', 'm'), 2),\n (('z', 'c'), 2),\n (('z', 'v'), 2),\n (('w', 'f'), 2),\n (('q', 'm'), 2),\n (('k', 'z'), 2),\n (('j', 'j'), 2),\n (('z', 'p'), 2),\n (('j', 't'), 2),\n (('k', 'b'), 2),\n (('m', 'w'), 2),\n (('h', 'f'), 2),\n (('c', 'g'), 2),\n (('t', 'f'), 2),\n (('h', 'c'), 2),\n (('q', 'o'), 2),\n (('k', 'd'), 2),\n (('k', 'v'), 2),\n (('s', 'g'), 2),\n (('z', 'd'), 2),\n (('q', 'r'), 1),\n (('d', 'z'), 1),\n (('p', 'j'), 1),\n (('q', 'l'), 1),\n (('p', 'f'), 1),\n (('q', 'e'), 1),\n (('b', 'c'), 1),\n (('c', 'd'), 1),\n (('m', 'f'), 1),\n (('p', 'n'), 1),\n (('w', 'b'), 1),\n (('p', 'c'), 1),\n (('h', 'p'), 1),\n (('f', 'h'), 1),\n (('b', 'j'), 1),\n (('f', 'g'), 1),\n (('z', 'g'), 1),\n (('c', 'p'), 1),\n (('p', 'k'), 1),\n (('p', 'm'), 1),\n (('x', 'n'), 1),\n (('s', 'q'), 1),\n (('k', 'f'), 1),\n (('m', 'k'), 1),\n (('x', 'h'), 1),\n (('g', 'f'), 1),\n (('v', 'b'), 1),\n (('j', 'p'), 1),\n (('g', 'z'), 1),\n (('v', 'd'), 1),\n (('d', 'b'), 1),\n (('v', 'h'), 1),\n (('h', 'h'), 1),\n (('g', 'v'), 1),\n (('d', 'q'), 1),\n (('x', 'b'), 1),\n (('w', 'z'), 1),\n (('h', 'q'), 1),\n (('j', 'b'), 1),\n (('x', 'm'), 1),\n (('w', 'g'), 1),\n (('t', 'b'), 1),\n (('z', 'x'), 1)]</pre> <p>We will store bigrams in a <code>PyTorch</code> tensor instead of a dictionary. Each character will be mapped to an id.</p> In\u00a0[7]: Copied! <pre>import string\n\n# character mapping from string to integer\nchars = list(string.ascii_lowercase)\nstoi = {ch: i for i, ch in enumerate(chars)}\nstoi['&lt;START&gt;'] = 26\nstoi['&lt;END&gt;'] = 27\nstoi\n</pre> import string  # character mapping from string to integer chars = list(string.ascii_lowercase) stoi = {ch: i for i, ch in enumerate(chars)} stoi[''] = 26 stoi[''] = 27 stoi Out[7]: <pre>{'a': 0,\n 'b': 1,\n 'c': 2,\n 'd': 3,\n 'e': 4,\n 'f': 5,\n 'g': 6,\n 'h': 7,\n 'i': 8,\n 'j': 9,\n 'k': 10,\n 'l': 11,\n 'm': 12,\n 'n': 13,\n 'o': 14,\n 'p': 15,\n 'q': 16,\n 'r': 17,\n 's': 18,\n 't': 19,\n 'u': 20,\n 'v': 21,\n 'w': 22,\n 'x': 23,\n 'y': 24,\n 'z': 25,\n '&lt;START&gt;': 26,\n '&lt;END&gt;': 27}</pre> In\u00a0[8]: Copied! <pre>import torch\n\nSIZE = len(stoi)\n\ndef get_bigrams(n):\n  bigrams = torch.zeros((SIZE, SIZE))\n  for w in words[:n]:\n    w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n    for ch1, ch2 in zip(w, w[1:]):\n      bigrams[stoi[ch1], stoi[ch2]] += 1\n  return bigrams\n</pre> import torch  SIZE = len(stoi)  def get_bigrams(n):   bigrams = torch.zeros((SIZE, SIZE))   for w in words[:n]:     w = [''] + list(w) + ['']     for ch1, ch2 in zip(w, w[1:]):       bigrams[stoi[ch1], stoi[ch2]] += 1   return bigrams In\u00a0[9]: Copied! <pre>bigrams = get_bigrams(len(words))\n</pre> bigrams = get_bigrams(len(words)) <p>We can refer to bigrams by indices and slicing. Modify <code>i</code> and <code>j</code> and run the code to get a sense of the table.</p> In\u00a0[10]: Copied! <pre>itos = {ch: i for i, ch in stoi.items()} # reverse stoi\n\ni, j = 0, 1\ncount = bigrams[i, j]\nprint(f'({itos[i]}, {itos[j]}): {count}')\n</pre> itos = {ch: i for i, ch in stoi.items()} # reverse stoi  i, j = 0, 1 count = bigrams[i, j] print(f'({itos[i]}, {itos[j]}): {count}') <pre>(a, b): 541.0\n</pre> <p>Exercise: Find the probability of a character (e.g. <code>b</code>) being the first character (hint: it will follow <code>&lt;START&gt;</code>).</p> In\u00a0[11]: Copied! <pre>counts = bigrams[stoi['&lt;START&gt;']]\nprobs = counts / counts.sum()\nprobs[stoi['b']]\n</pre> counts = bigrams[stoi['']] probs = counts / counts.sum() probs[stoi['b']] Out[11]: <pre>tensor(0.0408)</pre> <p>To generate some output we need to understand <code>torch.multinomial</code>. Let's have a simpler probability distribution for three classes (<code>0</code>, <code>1</code>, <code>2</code>). Our goal is to generate <code>n</code> samples according to the given probabilities. Setting <code>replacement=True</code> means the same class index can be picked multiple times. The higher a class' probability, the more often it is likely to appear in the samples. Rerun the cell below and notice how probabilities are related to the generated samples.</p> In\u00a0[12]: Copied! <pre>p = torch.rand(3)\np /= p.sum()\nsamples = torch.multinomial(p, num_samples=10, replacement=True)\n\nprint(p)\nprint(samples)\n</pre> p = torch.rand(3) p /= p.sum() samples = torch.multinomial(p, num_samples=10, replacement=True)  print(p) print(samples) <pre>tensor([0.5375, 0.1891, 0.2735])\ntensor([0, 0, 0, 2, 0, 1, 0, 0, 1, 0])\n</pre> <p>Once we understand the logic of <code>torch.multinomial</code>, we will randomly pick a next character based on our probability distribution. The higher is the frequency of the bigram, the more likely is that the random sampler will return us that character.</p> In\u00a0[13]: Copied! <pre>next_char = torch.multinomial(probs, num_samples=1, replacement=True)\nnext_char, itos[next_char.item()]\n</pre> next_char = torch.multinomial(probs, num_samples=1, replacement=True) next_char, itos[next_char.item()] Out[13]: <pre>(tensor([17]), 'r')</pre> <p>We will start with bigrams of <code>&lt;START&gt;</code>. Once we randomly generate the next character based on its probability distribution, we will start looking for bigrams starting with that generated character. This process will continue until we our sampling returns <code>&lt;END&gt;</code>.</p> <p>We will work with the probability matrix from now on, instead of the frequency matrix. Below, <code>dim=1</code> ensures that we sum along the row of the matrix, when <code>keepdim=True</code> keeps the extra dimension. Refer to the <code>PyTorch</code> documentation and test out different parameters.</p> In\u00a0[14]: Copied! <pre>probs = bigrams/bigrams.sum(dim=1, keepdim=True)\nprobs.shape\n</pre> probs = bigrams/bigrams.sum(dim=1, keepdim=True) probs.shape Out[14]: <pre>torch.Size([28, 28])</pre> In\u00a0[15]: Copied! <pre>def sample_names(n=10):\n  names = ''\n  for i in range(n):\n    id = stoi['&lt;START&gt;']\n    while id != stoi['&lt;END&gt;']:\n      p = probs[id]\n      next_char = torch.multinomial(p, 1, replacement=True)\n      id = next_char.item()\n      names += itos[id]\n  return names.replace(\"&lt;END&gt;\", \"\\n\")\n</pre> def sample_names(n=10):   names = ''   for i in range(n):     id = stoi['']     while id != stoi['']:       p = probs[id]       next_char = torch.multinomial(p, 1, replacement=True)       id = next_char.item()       names += itos[id]   return names.replace(\"\", \"\\n\") In\u00a0[21]: Copied! <pre>print(sample_names())\n</pre> print(sample_names()) <pre>tarios\ncaly\nkelaherthrwa\ndyaronn\nzenel\nbrid\nc\nsh\nve\nminica\n\n</pre> <p>We can evaluate our model and determine the loss function with likelihood. Note that our prediction probabilities are generated by simply counting bigram frequencies.</p> In\u00a0[22]: Copied! <pre>n = 1\nfor w in words[:n]:\n  w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    print(f'{ch1, ch2}: {p.item():.4f}')\n</pre> n = 1 for w in words[:n]:   w = [''] + list(w) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     print(f'{ch1, ch2}: {p.item():.4f}') <pre>('&lt;START&gt;', 'e'): 0.0478\n('e', 'm'): 0.0377\n('m', 'm'): 0.0253\n('m', 'a'): 0.3899\n('a', '&lt;END&gt;'): 0.1960\n</pre> <p>The logic of bigram model is that the probability of rare characters coming together in names (e.g. <code>xy</code>) will be much smaller than the more common cases (e.g. <code>na</code>). A better training corpus captures more realistic character transitions and assigns higher probabilities to frequently seen patterns.</p> <p>Since the model assumes that each character depends only on the previous one (Markov assumption), the joint probability of a sequence is the product of all conditional probabilities:</p> <p>$P(c_1, c_2, \\ldots, c_n) = P(c_1) \\cdot P(c_2 \\mid c_1) \\cdot P(c_3 \\mid c_2) \\cdots P(c_n \\mid c_{n-1})$</p> <p>Likelihood estimates this quality for our model by multiplying all prediction probabilities. Higher is the joint probability, the better is model's prediction quality. However, direct multiplication may have the following issue:</p> In\u00a0[24]: Copied! <pre>n = 5\nfor word in words[:n]:\n  likelihood = 1.0\n  w = ['&lt;START&gt;'] + list(word) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    likelihood *= p\n  print(f'Model predicts {word} is {likelihood:.9f} likely')\n</pre> n = 5 for word in words[:n]:   likelihood = 1.0   w = [''] + list(word) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     likelihood *= p   print(f'Model predicts {word} is {likelihood:.9f} likely') <pre>Model predicts emma is 0.000003478 likely\nModel predicts olivia is 0.000000025 likely\nModel predicts ava is 0.000165674 likely\nModel predicts isabella is 0.000000000 likely\nModel predicts sophia is 0.000000026 likely\n</pre> <p>Question: How to fix the issue above?</p> <p>As can be seen, the result of chained multiplication is a very small number (somewhat resembling vanishing gradient problem). To resolve this issue, individual probabilities between <code>0</code> and <code>1</code> are mapped to a <code>log</code> function domain (-$\\infty$, 0]. Logarithm function is monotonic (preserves order): maximum probability is mapped to <code>0</code>, smaller probabilities are mapped to bigger negative values.</p> In\u00a0[25]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\np = np.linspace(0.001, 1, 200)\nlog_p = np.log(p)\n\nplt.figure(figsize=(6, 4))\nplt.plot(p, log_p)\nplt.title(\"Natural Log Function\")\nplt.grid(True)\n</pre> import numpy as np import matplotlib.pyplot as plt  p = np.linspace(0.001, 1, 200) log_p = np.log(p)  plt.figure(figsize=(6, 4)) plt.plot(p, log_p) plt.title(\"Natural Log Function\") plt.grid(True) In\u00a0[26]: Copied! <pre>n = 1\nfor w in words[:n]:\n  w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    log_p = torch.log(p)\n    print(f'{ch1, ch2}: {p.item():.4f} | {log_p.item():.4f}')\n</pre> n = 1 for w in words[:n]:   w = [''] + list(w) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     log_p = torch.log(p)     print(f'{ch1, ch2}: {p.item():.4f} | {log_p.item():.4f}') <pre>('&lt;START&gt;', 'e'): 0.0478 | -3.0408\n('e', 'm'): 0.0377 | -3.2793\n('m', 'm'): 0.0253 | -3.6772\n('m', 'a'): 0.3899 | -0.9418\n('a', '&lt;END&gt;'): 0.1960 | -1.6299\n</pre> <p>Log-likelihood also has an advantage of making calculations and hence optimization (calculation of gradients) faster due to the product rule:</p> <p>$\\log P(c_1, c_2, \\ldots, c_n) = \\log P(c_1) + \\log P(c_2 \\mid c_1) + \\log P(c_3 \\mid c_2) + \\cdots + \\log P(c_n \\mid c_{n-1})$</p> In\u00a0[27]: Copied! <pre>n = 5\nfor word in words[:n]:\n  log_likelihood = 0.0\n  w = ['&lt;START&gt;'] + list(word) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    log_p = torch.log(p)\n    log_likelihood += log_p\n  print(f'Model predicts {word} is {log_likelihood} likely')\n</pre> n = 5 for word in words[:n]:   log_likelihood = 0.0   w = [''] + list(word) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     log_p = torch.log(p)     log_likelihood += log_p   print(f'Model predicts {word} is {log_likelihood} likely') <pre>Model predicts emma is -12.568990707397461 likely\nModel predicts olivia is -17.511159896850586 likely\nModel predicts ava is -8.705486297607422 likely\nModel predicts isabella is -21.5141544342041 likely\nModel predicts sophia is -17.468196868896484 likely\n</pre> <p>As optimization algorithms usually strive for minimizing the loss, it makes sense to invert the negative values of the log-likelihood to be positive. We can generate a single loss value by averaging negative log-likelihoods across all the samples.</p> In\u00a0[30]: Copied! <pre>log_likelihood = 0.0\nfor word in words:\n  w = ['&lt;START&gt;'] + list(word) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    log_p = torch.log(p)\n    log_likelihood += log_p\nloss = -log_likelihood / len(words)\nprint(f'Loss: {loss}')\n</pre> log_likelihood = 0.0 for word in words:   w = [''] + list(word) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     log_p = torch.log(p)     log_likelihood += log_p loss = -log_likelihood / len(words) print(f'Loss: {loss}') <pre>Loss: 17.478591918945312\n</pre> <p>As we know, logarithmic function is undefined at <code>0</code>, which we need to take into consideration. Consider the case when a character combination has never occured in our training data.</p> In\u00a0[31]: Copied! <pre>for word in ['jq']:\n  log_likelihood = 0.0\n  w = ['&lt;START&gt;'] + list(word) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    log_p = torch.log(p)\n    log_likelihood += log_p\n  print(f'Model predicts {word} is {log_likelihood} likely')\n</pre> for word in ['jq']:   log_likelihood = 0.0   w = [''] + list(word) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     log_p = torch.log(p)     log_likelihood += log_p   print(f'Model predicts {word} is {log_likelihood} likely') <pre>Model predicts jq is -inf likely\n</pre> <p>In case any character sequence in string will return infinite likelihood, it will lead to infinite loss as well, which is undesirable.</p> <p>Question: How to avoid infinite loss?</p> <p>Model-smoothing is a simple technique, which aims to assign a minimal non-zero probability to cases leading to infinite likelihood. Run the next cell and replicate the experiments above to see the outcome.</p> In\u00a0[32]: Copied! <pre>bigrams = bigrams + 1  # model smoothing avoids zero probabilities\nprobs = bigrams/bigrams.sum(dim=1, keepdim=True)\n</pre> bigrams = bigrams + 1  # model smoothing avoids zero probabilities probs = bigrams/bigrams.sum(dim=1, keepdim=True) <p>Our frequency-based bigram model didn't perform well due its simplicity. We will now build a neural network-based bigram model with the aim of increasing individual bigram prediction probabilities (recall that likelihood was calculated by multiplying conditional probabilities). Instead of counting bigrams in our training set, we will learn parameters leading to reduced loss. We will now rewrite our <code>get_bigrams()</code> function to suit the training of neural network model, where the label of each character will be the next character.</p> In\u00a0[33]: Copied! <pre>def get_bigrams(n):\n  X, Y = [], []\n  for w in words[:n]:\n    w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n    for ch1, ch2 in zip(w, w[1:]):\n      X.append(stoi[ch1])\n      Y.append(stoi[ch2])\n  return torch.tensor(X), torch.tensor(Y)\n</pre> def get_bigrams(n):   X, Y = [], []   for w in words[:n]:     w = [''] + list(w) + ['']     for ch1, ch2 in zip(w, w[1:]):       X.append(stoi[ch1])       Y.append(stoi[ch2])   return torch.tensor(X), torch.tensor(Y) In\u00a0[34]: Copied! <pre>X, Y = get_bigrams(1)\nX, Y\n</pre> X, Y = get_bigrams(1) X, Y Out[34]: <pre>(tensor([26,  4, 12, 12,  0]), tensor([ 4, 12, 12,  0, 27]))</pre> In\u00a0[35]: Copied! <pre>[itos[x.item()] for x in X], [itos[y.item()] for y in Y]\n</pre> [itos[x.item()] for x in X], [itos[y.item()] for y in Y] Out[35]: <pre>(['&lt;START&gt;', 'e', 'm', 'm', 'a'], ['e', 'm', 'm', 'a', '&lt;END&gt;'])</pre> <p>We will one-hot encode our data with the <code>torch.nn.functional</code> module function, in order to not inject unnecessary numerical pattern to our data.</p> In\u00a0[38]: Copied! <pre>import torch.nn.functional as F\n\nX_train = F.one_hot(X, num_classes=SIZE).float()\ny_train = F.one_hot(Y, num_classes=SIZE).float()\n</pre> import torch.nn.functional as F  X_train = F.one_hot(X, num_classes=SIZE).float() y_train = F.one_hot(Y, num_classes=SIZE).float() In\u00a0[39]: Copied! <pre>plt.imshow(X_train);\n</pre> plt.imshow(X_train); <p>We will now generate weights for each character and find their linear transformation.</p> In\u00a0[41]: Copied! <pre>W = torch.randn((SIZE, 1))\nX_train @ W\n</pre> W = torch.randn((SIZE, 1)) X_train @ W Out[41]: <pre>tensor([[ 0.5013],\n        [-0.8258],\n        [ 1.0634],\n        [ 1.0634],\n        [ 0.9002]])</pre> <p>For each input character, our goal is to predict not a single probability, but probabilities for all possible output characters. Hence, we will update our weight matrix to correspond to both input and output. We will set <code>requires_grad=True</code> for future gradient calculation.</p> In\u00a0[42]: Copied! <pre>W = torch.randn((SIZE, SIZE), requires_grad=True)\n(X_train @ W).shape\n</pre> W = torch.randn((SIZE, SIZE), requires_grad=True) (X_train @ W).shape Out[42]: <pre>torch.Size([5, 28])</pre> <p>Note that <code>torch.randn()</code> function is generating weight values corresponding to Guassian distribution. Our goal is to map these values to all be positive, so that we can interpret the ouput as probabilities later on. The idea is similar to <code>log()</code> function previously. Here, the output values lower than zero will be mapped to be below <code>1</code> approaching <code>0</code>, when positive values will grow towards infinity.</p> In\u00a0[43]: Copied! <pre>x = np.linspace(-4, 4, 200)\nexp_x = np.exp(x)\n\nplt.figure(figsize=(6, 4))\nplt.plot(x, exp_x)\nplt.title(\"Exponentiation Function\")\nplt.grid(True)\n</pre> x = np.linspace(-4, 4, 200) exp_x = np.exp(x)  plt.figure(figsize=(6, 4)) plt.plot(x, exp_x) plt.title(\"Exponentiation Function\") plt.grid(True) <p>We call the raw scores we obtain after a linear transformation logits (log-counts). These values are real numbers, not constrained to be positive or normalized. To interpret them as prediction probabilities, we apply the exponential function to map them to the positive domain. The output of exponentiation can be interpreted as unnormalized character frequencies or relative likelihoods. When we normalize these exponentials (i.e. divide by their total sum), we obtain values that sum to <code>1.0</code>, forming a proper probability distribution. This is exactly what we want. Note that all these functions are differentiable and applying exponentiation and normalization is exactly what softmax function does:</p> <p>$\\text{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)}$</p> In\u00a0[44]: Copied! <pre>logits = X_train @ W\n\n# softmax\ncounts = torch.exp(logits)\nprobs = counts / counts.sum(dim=1, keepdim=True)\n\nprobs[0], probs[0].sum()\n</pre> logits = X_train @ W  # softmax counts = torch.exp(logits) probs = counts / counts.sum(dim=1, keepdim=True)  probs[0], probs[0].sum() Out[44]: <pre>(tensor([0.0471, 0.0249, 0.0381, 0.0411, 0.0384, 0.0678, 0.0478, 0.0210, 0.0155,\n         0.0191, 0.0920, 0.0500, 0.0138, 0.0090, 0.0042, 0.0120, 0.1256, 0.0500,\n         0.0855, 0.0199, 0.0043, 0.0082, 0.0215, 0.0744, 0.0232, 0.0054, 0.0234,\n         0.0168], grad_fn=&lt;SelectBackward0&gt;),\n tensor(1.0000, grad_fn=&lt;SumBackward0&gt;))</pre> <p>Exercise: Calculate the average negative log-likelihood (loss) of the model.</p> In\u00a0[45]: Copied! <pre>pred_probs = probs[torch.arange(len(Y)), Y]\nloss = -pred_probs.log().mean()\nloss\n</pre> pred_probs = probs[torch.arange(len(Y)), Y] loss = -pred_probs.log().mean() loss Out[45]: <pre>tensor(3.5038, grad_fn=&lt;NegBackward0&gt;)</pre> <p>We know how to call backward pass and optimize our model from previous lectures. We will combine all the steps, train our model on the whole dataset, and backpropagate through our network.</p> In\u00a0[46]: Copied! <pre>X, Y = get_bigrams(len(words))\n\nX_train = F.one_hot(X, num_classes=SIZE).float()\ny_train = F.one_hot(Y, num_classes=SIZE).float()\n\nW = torch.randn((SIZE, SIZE), requires_grad=True)\n</pre> X, Y = get_bigrams(len(words))  X_train = F.one_hot(X, num_classes=SIZE).float() y_train = F.one_hot(Y, num_classes=SIZE).float()  W = torch.randn((SIZE, SIZE), requires_grad=True) In\u00a0[47]: Copied! <pre>num_epochs = 100\nlearning_rate = 1\nlambda_ = 0.01\n\nfor epoch in range(num_epochs):\n  # forward pass\n  logits = X_train @ W\n  counts = torch.exp(logits)\n  probs = counts / counts.sum(dim=1, keepdim=True)\n  pred_probs = probs[torch.arange(len(Y)), Y]\n\n  l2 = (W**2).sum() # regularization\n  loss = -pred_probs.log().mean() + lambda_ * l2.sum()\n\n  # backward pass\n  W.grad = None\n  loss.backward()\n\n  # optimization\n  W.data -= learning_rate * W.grad\n\n  if (epoch + 1) % 10 == 0:\n    print(f'{epoch+1}/{num_epochs}, loss: {loss}')\n</pre> num_epochs = 100 learning_rate = 1 lambda_ = 0.01  for epoch in range(num_epochs):   # forward pass   logits = X_train @ W   counts = torch.exp(logits)   probs = counts / counts.sum(dim=1, keepdim=True)   pred_probs = probs[torch.arange(len(Y)), Y]    l2 = (W**2).sum() # regularization   loss = -pred_probs.log().mean() + lambda_ * l2.sum()    # backward pass   W.grad = None   loss.backward()    # optimization   W.data -= learning_rate * W.grad    if (epoch + 1) % 10 == 0:     print(f'{epoch+1}/{num_epochs}, loss: {loss}') <pre>10/100, loss: 9.035313606262207\n20/100, loss: 7.005525588989258\n30/100, loss: 5.685214042663574\n40/100, loss: 4.8257904052734375\n50/100, loss: 4.2659807205200195\n60/100, loss: 3.901088237762451\n70/100, loss: 3.6630942821502686\n80/100, loss: 3.5077712535858154\n90/100, loss: 3.406341552734375\n100/100, loss: 3.3400678634643555\n</pre> <p>Follows the implementation of Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Janvin: A neural probabilistic language model, based on Andrej Karpathy's building makemore: part 2.</p> <p>We will modify the <code>get_bigrams()</code> function to include custom <code>block_size</code>, which simply implies <code>N-1</code> of <code>N-gram</code> (bigram has the block size of <code>1</code>).</p> In\u00a0[48]: Copied! <pre>def get_ngrams(end, start=0, block_size=3):\n  X, Y = [], []\n  for w in words[start:end]:\n    context = ['&lt;START&gt;'] * block_size\n    for ch in list(w) + ['&lt;END&gt;']:\n      X.append([stoi[c] for c in context])\n      Y.append(stoi[ch])\n      context = context[1:] + [ch]\n  return torch.tensor(X), torch.tensor(Y)\n</pre> def get_ngrams(end, start=0, block_size=3):   X, Y = [], []   for w in words[start:end]:     context = [''] * block_size     for ch in list(w) + ['']:       X.append([stoi[c] for c in context])       Y.append(stoi[ch])       context = context[1:] + [ch]   return torch.tensor(X), torch.tensor(Y) In\u00a0[49]: Copied! <pre>X, Y = get_ngrams(1, block_size=3) # try different block sizes\n</pre> X, Y = get_ngrams(1, block_size=3) # try different block sizes In\u00a0[50]: Copied! <pre>for x, y in zip(X, Y):\n  context = [itos[i.item()] for i in x]\n  target = itos[y.item()]\n  print(f\"{context}: {target}\")\n</pre> for x, y in zip(X, Y):   context = [itos[i.item()] for i in x]   target = itos[y.item()]   print(f\"{context}: {target}\") <pre>['&lt;START&gt;', '&lt;START&gt;', '&lt;START&gt;']: e\n['&lt;START&gt;', '&lt;START&gt;', 'e']: m\n['&lt;START&gt;', 'e', 'm']: m\n['e', 'm', 'm']: a\n['m', 'm', 'a']: &lt;END&gt;\n</pre> <p>It is managable to convert <code>28</code> character indices by one hot encoding them to suit a neural network. However, this approach becomes innefficient when the vocabulary size increases. What if we have <code>10,000</code> words and our goal is to predict the next word? We would have to create large vectors with lots of zeros, not only wasting resources, but also losing similarity information among tokens (dot product of any two vectors will always be <code>0</code> due to orthogonality).</p> <p>A different approach is to get some smaller dimensional embedding of an index. Initially, these embeddings (parameters) are random, but over the course of training, model updates them to reflect the actual usage of the words in a context. When vectors of two different characters are learned to be close to each other, then we can conclude that, in our data, these characters were in a similar context. If two characters often show up in the same positions relative to surrounding characters, their embeddings get pulled toward each other. Consider the simple case below:</p> <pre>context1 = ['b', 'a', 'd', 'a']\ncontext2 = ['m', 'a', 'd', 'a']\ntarget = 'm'\n</pre> <p>If the training data consists of these contexts, then, in order to predict <code>m</code> for both contexts, their learned embeddings should be similar in value.</p> <p>For our case, <code>2</code> dimensional embeddings will be enough. For small datasets, having high embedding dimensionality may cause overfitting. When dataset is bigger, increasing dimensions helps to learn more nuanced relationships in the data, albeit at a higher computational cost.</p> In\u00a0[51]: Copied! <pre>X\n</pre> X Out[51]: <pre>tensor([[26, 26, 26],\n        [26, 26,  4],\n        [26,  4, 12],\n        [ 4, 12, 12],\n        [12, 12,  0]])</pre> In\u00a0[52]: Copied! <pre>C = torch.randn((SIZE, 2))\nemb = C[X]\nemb\n</pre> C = torch.randn((SIZE, 2)) emb = C[X] emb Out[52]: <pre>tensor([[[-0.6723,  0.3140],\n         [-0.6723,  0.3140],\n         [-0.6723,  0.3140]],\n\n        [[-0.6723,  0.3140],\n         [-0.6723,  0.3140],\n         [-1.4631, -1.0324]],\n\n        [[-0.6723,  0.3140],\n         [-1.4631, -1.0324],\n         [ 0.6817,  1.5840]],\n\n        [[-1.4631, -1.0324],\n         [ 0.6817,  1.5840],\n         [ 0.6817,  1.5840]],\n\n        [[ 0.6817,  1.5840],\n         [ 0.6817,  1.5840],\n         [-0.8068,  0.7403]]])</pre> <p>Now we will initilaize weights and biases by considering correct shape. In order to be able to use matrix multiplication, we will have to flatten embeddings as well.</p> In\u00a0[53]: Copied! <pre>emb.shape\n</pre> emb.shape Out[53]: <pre>torch.Size([5, 3, 2])</pre> <p>Question: What do embedding dimensions correspond to?</p> <p>Exercise: Pass embeddings through a single neuron.</p> In\u00a0[57]: Copied! <pre>layer_size = 100\nin_features = emb.shape[1] * emb.shape[2]\n\nW1 = torch.randn((in_features, layer_size))\nb1 = torch.randn(layer_size)\n\nout = emb.view(-1, in_features) @ W1 + b1\nact = torch.tanh(out)\n\nW1.shape, b1.shape, out.shape\n</pre> layer_size = 100 in_features = emb.shape[1] * emb.shape[2]  W1 = torch.randn((in_features, layer_size)) b1 = torch.randn(layer_size)  out = emb.view(-1, in_features) @ W1 + b1 act = torch.tanh(out)  W1.shape, b1.shape, out.shape Out[57]: <pre>(torch.Size([6, 100]), torch.Size([100]), torch.Size([5, 100]))</pre> In\u00a0[58]: Copied! <pre>emb.view(-1, in_features)\n</pre> emb.view(-1, in_features) Out[58]: <pre>tensor([[-0.6723,  0.3140, -0.6723,  0.3140, -0.6723,  0.3140],\n        [-0.6723,  0.3140, -0.6723,  0.3140, -1.4631, -1.0324],\n        [-0.6723,  0.3140, -1.4631, -1.0324,  0.6817,  1.5840],\n        [-1.4631, -1.0324,  0.6817,  1.5840,  0.6817,  1.5840],\n        [ 0.6817,  1.5840,  0.6817,  1.5840, -0.8068,  0.7403]])</pre> <p>Exercise: Create the next (final) layer.</p> In\u00a0[59]: Copied! <pre>W2 = torch.randn((layer_size, SIZE))\nb2 = torch.randn(SIZE)\n\nlogits = act @ W2 + b2\nlogits.shape\n</pre> W2 = torch.randn((layer_size, SIZE)) b2 = torch.randn(SIZE)  logits = act @ W2 + b2 logits.shape Out[59]: <pre>torch.Size([5, 28])</pre> <p>Exercise: Calculate loss.</p> In\u00a0[60]: Copied! <pre>counts = logits.exp()\nprob = counts / counts.sum(dim=1, keepdim=True)\nprob.shape\n</pre> counts = logits.exp() prob = counts / counts.sum(dim=1, keepdim=True) prob.shape Out[60]: <pre>torch.Size([5, 28])</pre> In\u00a0[65]: Copied! <pre>loss = -prob[torch.arange(prob.shape[0]), Y].log().mean()\nloss\n</pre> loss = -prob[torch.arange(prob.shape[0]), Y].log().mean() loss Out[65]: <pre>tensor(16.2439)</pre> <p>Average negative log-likelihood loss can be calculated with built-in <code>PyTorch</code> functions as well. Cross-entropy is simply <code>Softmax + Negative Log-Likelihood</code>.</p> In\u00a0[66]: Copied! <pre># loss = F.nll_loss(torch.log(prob), Y)\nloss = F.cross_entropy(logits, Y)\nloss\n</pre> # loss = F.nll_loss(torch.log(prob), Y) loss = F.cross_entropy(logits, Y) loss Out[66]: <pre>tensor(16.2439)</pre> <p>Exercise: Let's combine everything together and train on whole data.</p> In\u00a0[96]: Copied! <pre>X, Y = get_ngrams(len(words))\n</pre> X, Y = get_ngrams(len(words)) In\u00a0[97]: Copied! <pre>C = torch.randn((SIZE, 2), requires_grad=True)\nemb = C[X]\n\nlayer_size = 100\nin_features = emb.shape[1] * emb.shape[2]\n\nW1 = torch.randn((in_features, layer_size), requires_grad=True)\nb1 = torch.randn(layer_size, requires_grad=True)\n\nW2 = torch.randn((layer_size, SIZE), requires_grad=True)\nb2 = torch.randn(SIZE, requires_grad=True)\n\nparams = [C, W1, W2, b1, b2]\n</pre> C = torch.randn((SIZE, 2), requires_grad=True) emb = C[X]  layer_size = 100 in_features = emb.shape[1] * emb.shape[2]  W1 = torch.randn((in_features, layer_size), requires_grad=True) b1 = torch.randn(layer_size, requires_grad=True)  W2 = torch.randn((layer_size, SIZE), requires_grad=True) b2 = torch.randn(SIZE, requires_grad=True)  params = [C, W1, W2, b1, b2] In\u00a0[100]: Copied! <pre>def train(X, Y, params, num_epochs=10, learning_rate=0.01):\n  C, W1, W2, b1, b2 = params\n  for epoch in range(num_epochs):\n    # forward pass\n    emb = C[X]\n    in_features = emb.shape[1] * emb.shape[2]\n    out = emb.view(-1, in_features) @ W1 + b1\n    act = torch.tanh(out)\n    logits = act @ W2 + b2\n    loss = F.cross_entropy(logits, Y)\n    print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss}')\n    # backward pass\n    for p in params:\n      p.grad = None\n    loss.backward()\n    # optimization\n    for p in params:\n      p.data -= learning_rate * p.grad\n</pre> def train(X, Y, params, num_epochs=10, learning_rate=0.01):   C, W1, W2, b1, b2 = params   for epoch in range(num_epochs):     # forward pass     emb = C[X]     in_features = emb.shape[1] * emb.shape[2]     out = emb.view(-1, in_features) @ W1 + b1     act = torch.tanh(out)     logits = act @ W2 + b2     loss = F.cross_entropy(logits, Y)     print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss}')     # backward pass     for p in params:       p.grad = None     loss.backward()     # optimization     for p in params:       p.data -= learning_rate * p.grad In\u00a0[101]: Copied! <pre>train(X, Y, params)\n</pre> train(X, Y, params) <pre>Epoch: 1/10, Loss: 14.651238441467285\nEpoch: 2/10, Loss: 14.548416137695312\nEpoch: 3/10, Loss: 14.447701454162598\nEpoch: 4/10, Loss: 14.349052429199219\nEpoch: 5/10, Loss: 14.252448081970215\nEpoch: 6/10, Loss: 14.157876968383789\nEpoch: 7/10, Loss: 14.065322875976562\nEpoch: 8/10, Loss: 13.974783897399902\nEpoch: 9/10, Loss: 13.886248588562012\nEpoch: 10/10, Loss: 13.799703598022461\n</pre> <p>Exercise: Notice how slow it is to train on the whole data. Find out why. Can you also train on mini-batches? See the hint below:</p> In\u00a0[102]: Copied! <pre>batch_size = 4\nidx = torch.randint(0, X.shape[0], (batch_size,))\nidx, X[idx]\n</pre> batch_size = 4 idx = torch.randint(0, X.shape[0], (batch_size,)) idx, X[idx] Out[102]: <pre>(tensor([114762,  40173,  66663,  87063]),\n tensor([[ 7, 11,  4],\n         [18,  4, 24],\n         [ 0, 13,  4],\n         [12,  8, 11]]))</pre> In\u00a0[103]: Copied! <pre>def run(X, Y, params, num_epochs, lr=0.1, batch_size=None):\n  C, W1, W2, b1, b2 = params\n\n  for epoch in range(1, num_epochs+1):\n    if batch_size:\n      idx = torch.randint(0, X.size(0), (batch_size,))\n      batch_X, batch_Y = X[idx], Y[idx]\n    else:\n      batch_X, batch_Y = X, Y\n\n    emb = C[batch_X]\n    in_features = emb.shape[1] * emb.shape[2]\n    out = emb.view(-1, in_features) @ W1 + b1\n    act = torch.tanh(out)\n    logits = act @ W2 + b2\n    loss = F.cross_entropy(logits, batch_Y)\n\n    if epoch % (100 if batch_size else 1) == 0:\n      print(f'Epoch {epoch}, Loss {loss.item()}')\n\n    for p in params:\n      p.grad = None\n    loss.backward()\n\n    with torch.no_grad():\n      for p in params:\n        p.data -= lr * p.grad\n</pre> def run(X, Y, params, num_epochs, lr=0.1, batch_size=None):   C, W1, W2, b1, b2 = params    for epoch in range(1, num_epochs+1):     if batch_size:       idx = torch.randint(0, X.size(0), (batch_size,))       batch_X, batch_Y = X[idx], Y[idx]     else:       batch_X, batch_Y = X, Y      emb = C[batch_X]     in_features = emb.shape[1] * emb.shape[2]     out = emb.view(-1, in_features) @ W1 + b1     act = torch.tanh(out)     logits = act @ W2 + b2     loss = F.cross_entropy(logits, batch_Y)      if epoch % (100 if batch_size else 1) == 0:       print(f'Epoch {epoch}, Loss {loss.item()}')      for p in params:       p.grad = None     loss.backward()      with torch.no_grad():       for p in params:         p.data -= lr * p.grad In\u00a0[104]: Copied! <pre>run(X, Y, params, num_epochs=10, batch_size=None)\n</pre> run(X, Y, params, num_epochs=10, batch_size=None) <pre>Epoch 1, Loss 13.715128898620605\nEpoch 2, Loss 12.94245719909668\nEpoch 3, Loss 12.326409339904785\nEpoch 4, Loss 11.779574394226074\nEpoch 5, Loss 11.269842147827148\nEpoch 6, Loss 10.805395126342773\nEpoch 7, Loss 10.449189186096191\nEpoch 8, Loss 10.27437686920166\nEpoch 9, Loss 9.558894157409668\nEpoch 10, Loss 9.190631866455078\n</pre> In\u00a0[105]: Copied! <pre>run(X, Y, params, num_epochs=1000, batch_size=64)\n</pre> run(X, Y, params, num_epochs=1000, batch_size=64) <pre>Epoch 100, Loss 2.8726632595062256\nEpoch 200, Loss 2.904355525970459\nEpoch 300, Loss 3.114861488342285\nEpoch 400, Loss 2.7828195095062256\nEpoch 500, Loss 2.580034017562866\nEpoch 600, Loss 2.742938995361328\nEpoch 700, Loss 2.582310676574707\nEpoch 800, Loss 2.6041131019592285\nEpoch 900, Loss 2.6011006832122803\nEpoch 1000, Loss 2.5459964275360107\n</pre>"},{"location":"Main_Content/06_nn_ngram/#06-python-code-for-neural-network-n-gram-model","title":"06 Python Code for Neural Network N-Gram Model\u00b6","text":"<p>Note: The notebook is highly based on Andrej Karpathy's makemore. See the original video.</p>"},{"location":"Main_Content/06_nn_ngram/#bigram-model","title":"Bigram Model\u00b6","text":""},{"location":"Main_Content/06_nn_ngram/#average-negative-log-likelihood","title":"Average Negative Log-Likelihood\u00b6","text":""},{"location":"Main_Content/06_nn_ngram/#neural-network-bigram-model","title":"Neural Network Bigram Model\u00b6","text":""},{"location":"Main_Content/06_nn_ngram/#neural-network-n-gram-model","title":"Neural Network N-gram Model\u00b6","text":""},{"location":"Main_Content/06_nn_ngram/#training-on-mini-batches","title":"Training on Mini-Batches\u00b6","text":""},{"location":"Mathematics_of_Deep_Learning/la/","title":"Linear Algebra","text":""}]}