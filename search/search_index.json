{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to CSCI 4701!","text":"<p>This is the website of the CSCI 4701: Deep Learning course taught at ADA University. </p> <p>Deep Learning focuses on artificial neural networks and how they are trained and optimized. This course covers core concepts starting with backpropagation, including regularization and optimization techniques. Students will gain practical skills using the PyTorch framework and learn neural architectures used in both Computer Vision (CV) and Natural Language Processing (NLP), including Convolutional Neural Networks (CNNs) and Transformer-based models. The course introduces generative modeling with Variational Autoencoders (VAEs) and briefly covers current developments in deep learning, such as Latent Diffusion Models (LDMs) and Large Language Models (LLMs). </p> <p>You can navigate through the course starting from the introductory overview or directly check the practical notebooks via the navigation bar.</p>"},{"location":"advanced/","title":"Advanced Material","text":"<p>Important</p> <p>     The page is currently under development.   </p> <p>This page presents advanced topics that go beyond the scope of the course. The material is optional and intended for students who wish to explore deeper or more current developments independently.</p>"},{"location":"course/spring-2026/syllabus/","title":"Syllabus","text":"<p>Important</p> <p>     The content of this syllabus is subject to change. Please consistently check the course page on Blackboard and the ADA University Academic Calendar for modifications. The last day of the add/drop period, holidays, and other academic deadlines are noted in the calendar.   </p> <p>Info</p> <p>    Square brackets in the Assessment / Notes column indicate the range of classes whose material is covered by the assessment. For example, Quiz 1 [1\u20133] means that the quiz assesses material covered in classes 1 through 3.   </p> Week Topic Learning Outcomes Assessment / Notes 1 Deep Learning (DL) Overview / Course Structure Describe the scope of DL and the course syllabus. Fulfill technological requirements. 2 Mathematics of DL: Linear Algebra / Calculus Work with vectors, matrices, and tensors; apply norms and inner products. Compute partial derivatives and apply the chain rule. Optional: intuition behind eigenvectors and SVD. 3 Gradient Descent / Backpropagation I Compute gradients on computational graphs. Perform forward and backward passes. Understand gradient descent updates and automatic differentiation (PyTorch autograd, micrograd). 4 Gradient Descent / Backpropagation II Implement full backpropagation. Feb 3: Quiz 1 [1\u20133]Last day to submit team member details 5 Activation Functions / Neuron Implement activation functions and understand non-linearity. Backpropagate over an N-dimensional neuron. 6 Multilayer Perceptron (MLP) Construct an MLP from stacked neurons. Train a simple MLP classifier on a small dataset. Feb 10: Project proposal deadline 7 Images as Tensors / MLP on MNIST / Batching &amp; Cross-Entropy Understand image representations, tensor shapes, and batching. Use torchvision datasets and dataloaders. Train an MLP on MNIST with SGD + cross-entropy. Feb 12: Quiz 2 [5\u20136] 8 Convolutional Neural Networks (CNN) Define and implement 2D cross-correlation (convolution) and pooling with kernels, including padding and stride. Train a LeNet-style CNN on MNIST. Compare MLP with CNN. 9 Mathematics of DL: Probability Theory Describe random variables; distinguish discrete and continuous distributions; work with PMF/PDF. Compute expectation, variance, and covariance. Use conditional probability, independence, and Bayes\u2019 rule. Recognize common distributions. Feb 19: Quiz 3 [7\u20138] 10 Regularization Apply weight decay and dropout. Handle exploding and vanishing gradients. Use Xavier and He initialization. Distinguish local minima from saddle points in training dynamics. 11 Optimization Adjust learning rate and apply schedules. Use SGD with momentum. Apply RMSProp and Adam. Compare optimizers based on convergence behavior and practical performance. 12 Regularization / Optimization Train a regularized CNN on CIFAR-10 using optimizers. Apply hyperparameter tuning. Mar 3: Quiz 4 [10\u201311] 13 Paper: AlexNet Discuss AlexNet, its key ideas, what is outdated, and the paper structure. 14 Bigram Model / Negative Log-Likelihood / Softmax Build a character-level bigram model and sample from it. Distinguish probability vs likelihood. Compute average negative log-likelihood as a loss. Explain the purpose of softmax. 15 Neural Network N-gram Model / Mini-Batch Training Construct a neural N-gram model. Train the model with mini-batch updates. Mar 12: Quiz 5 [14]Project milestone 1 deadline 16 Midterm Exam \u2014 Tuesday, Mar 17: Midterm Exam [1\u201315] 17 Midterm Exam Review Half-semester overview. \u2014 Holidays \u2014 Mar 20\u201330 18 Batch Normalization / Layer Normalization Explain why normalization helps training deep networks. Implement batch normalization and understand training vs evaluation behavior. Understand batch-size effects and when to prefer layer normalization. 19 Residual Blocks / Residual Network for NLP Understand residual (skip) connections. Add a residual block to a feed-forward N-gram model with correct dimensions. Connect residuals to vanishing gradients and regularization. 20 Sequence Modeling: Autoregressive Models and RNN/LSTM Explain autoregressive modeling beyond fixed context windows. Describe how RNNs maintain state. Identify limitations of RNN/LSTM/GRU. Apr 7: Quiz 6 [18\u201319] 21 Attention Mechanism Understand attention as weighted information selection. Derive queries, keys, and values at the tensor level. Implement attention with matrix operations and verify shapes and normalization. 22 Transformer Architecture / Self-Attention Explain self-attention and Transformer blocks. Explain how Transformers scale. Apr 14: Quiz 7 [20\u201321] 23 Transformer Blocks Assemble a Transformer block from self-attention and feed-forward sublayers. Trace signal flow. Analyze training stability and sensitivity to initialization and learning rate. 24 Paper Reading: Transformer, Vision Transformer, Swin Transformer Extract core architectural ideas and compare attention for sequences vs images. Discuss scalability and efficiency constraints. Apr 21: Quiz 8 [22\u201323] 25 Mathematics of DL: Information Theory and Probabilistic Modeling Compute entropy, cross-entropy, and KL divergence. Derive cross-entropy loss from maximum likelihood. Interpret common losses as probabilistic objectives. 26 Variational Autoencoders I Introduce latent-variable generative models. Explain latent representations and probabilistic encoders/decoders. Explain approximate inference and why variational methods are needed. Apr 28: Project milestone 2 deadline 27 Variational Autoencoders II Understand the VAE objective (ELBO). Implement a VAE. Interpret reconstruction and regularization terms and their trade-off. 28 Generative Adversarial Networks (GAN) / Diffusion Models / Score Matching Explain adversarial training between generator and discriminator. Describe common failure modes (mode collapse, instability) and stabilization techniques. Formulate diffusion via forward noising and learned reverse denoising. Interpret the training objective as denoising score matching. Explain sampling as iterative probabilistic inference. May 5: Quiz 9 [25\u201327] 29 Foundation Models and Modern Trends Explain large-scale pretraining and transfer learning. Examine GPT, BERT, CLIP, and latent diffusion models (LDMs). Discuss scaling behavior and limitations. \u2014 Final Exam \u2014 Tuesday, May 12: Final Exam [1\u201329]"},{"location":"introduction/01_overview/","title":"Deep Learning Overview","text":"18 Jan 2026 \u00b7   9 min <p>Artificial Intelligence (AI) is the broad field concerned with building systems that perform tasks requiring intelligence. Machine Learning (ML) is a subfield of AI that enables systems to learn patterns and make decisions from data rather than explicit rules. Deep Learning (DL) is a subfield of ML that uses multi-layer neural networks to learn complex representations from large datasets.</p> <p>Info</p> <p>     The following sources were used in preparing this text:   </p> <ul> <li>       Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016).       Deep Learning. MIT Press.        </li> <li>       Schmidhuber, J. (2015).       Deep Learning in Neural Networks: An Overview.       Neural Networks.     </li> <li>       Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J.      Dive into Deep Learning. d2l.ai."},{"location":"introduction/01_overview/#ai-ml-dl","title":"AI / ML / DL","text":"<p>AI initially focused on what is often called the knowledge-based approach, where intelligence was treated as something that could be explicitly written down. Researchers attempted to encode reasoning as rules, symbols, and logical statements. If a human expert knew how to solve a problem, the reasoning steps would be formalized and executed by a machine.</p> <p>This approach failed when faced with the ambiguity and variability of the real world. Tasks that humans perform effortlessly, such as recognizing faces or understanding speech, are precisely the tasks that are hardest to describe step by step. Human expertise in these domains is largely implicit rather than explicit. Rule-based systems therefore became brittle, difficult to scale, and expensive to maintain. Small changes in the environment often required rewriting large portions of the system, making progress slow and fragile.</p>      Deep Learning and AI ~ Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.     <p>ML offered a different perspective. Instead of programming intelligence directly, machines were allowed to learn patterns from data. Classical ML algorithms such as linear models, logistic regression, na\u00efve Bayes, and decision trees achieved real success in applications like medical decision support, spam filtering, and credit scoring. However, these methods relied heavily on hand-crafted features. Human designers had to decide in advance which properties of the data were relevant, and performance depended more on feature design than on the learning algorithm itself.</p> <p>This reliance on features became a serious limitation as data grew more complex. Images, audio signals, and language live in very high-dimensional spaces. In such spaces, intuition breaks down, a phenomenon often referred to as the curse of dimensionality. As dimensionality increases, data becomes sparse, distances lose their meaning, and small modeling assumptions can cause large failures. Feature engineering becomes brittle and does not scale to the richness of real-world data.</p> <p>The natural response to this problem was representation learning. Instead of manually defining features, the model learns useful representations directly from raw data. Early methods such as Principal Component Analysis (PCA), kernel methods, sparse coding, and shallow neural networks pursued this idea. They demonstrated that learning intermediate representations could significantly improve performance and reduce reliance on handcrafted features. However, these approaches were typically shallow, consisting of only one or two layers of transformation. As a result, they struggled to capture the hierarchical structure present in real-world data.</p> <p>Many perceptual tasks are inherently compositional. Images are composed of edges, edges form textures and parts, parts form objects, and objects form scenes. Speech and language exhibit similar hierarchies. Shallow models can learn simple transformations, but they cannot efficiently represent such multi-level abstractions. Attempting to do so requires an exponential number of features or parameters, making learning unstable and data-inefficient. In practice, representation learning without depth hit a ceiling: it reduced feature engineering, but it could not scale to the complexity of vision, speech, and language.</p> <p>DL extends representation learning by stacking many layers of nonlinear transformations. Each layer learns to represent the data at a higher level of abstraction, allowing complex structures to be built incrementally. </p> <p>At a fundamental level, both classical ML and DL do the same thing: they learn a function from data. The difference is not in what is learned, but in how much of the function is learned automatically. In all cases, learning amounts to selecting parameters so that a function best approximates the desired input\u2013output relationship under a given objective.</p> <p>Interestingly, DL did not introduce fundamentally new mathematical ideas. Many concepts, such as multi-layer neural networks, backpropagation, gradient-based optimization, and even convolutional architectures were known decades earlier. </p>"},{"location":"introduction/01_overview/#biological-and-artificial-neurons","title":"Biological and Artificial Neurons","text":"<p>DL is not an attempt to simulate the brain. Artificial neural networks are inspired by biological neurons, but the resemblance is conceptual rather than literal. </p>        Structure of a typical neuron with Schwann cells in the peripheral nervous system ~ \"Anatomy and Physiology\" by the US National Cancer Institute's Surveillance | CC BY-SA 3.0 | Wikimedia Commons <p>A biological neuron is a living cell designed for communication in a noisy, energy-constrained environment. It receives signals through dendrites, integrates them in the soma (cell body), and, if a threshold is reached, sends an electrical pulse along the axon to other neurons through synapses. Learning occurs locally by strengthening or weakening synaptic connections through repeated interaction with the environment.</p>        Artificial Neuron ~ Funcs, Own work | CC0 | Wikimedia Commons <p>An artificial neuron is a mathematical function that combines numerical inputs and produces a numerical output. Much like how airplanes were inspired by birds but rely on entirely different aerodynamic mechanisms, the success of DL does not come from biological realism. Biological systems served primarily as inspiration.</p>"},{"location":"introduction/01_overview/#evolution-of-deep-learning","title":"Evolution of Deep Learning","text":"<p>Learning from data predates computers. The mathematical backbone of modern deep learning is the chain rule, formalized by Gottfried Wilhelm Leibniz and later exploited by backpropagation algorithms. Carl Friedrich Gauss and Adrien-Marie Legendre used linear regression in the early nineteenth century, a method mathematically equivalent to a shallow neural network. In the mid-twentieth century, researchers such as Warren McCulloch and Walter Pitts, Frank Rosenblatt, and Bernard Widrow explored learning machines inspired by biological neurons. These early systems were limited\u2014often linear or single-layer\u2014and constrained by the theory and hardware of their time.</p> <p>Multi-layer learning systems already existed by the 1960s and 1970s. Alexey Ivakhnenko and Valentin Lapa trained models with adaptive hidden layers, while Kunihiko Fukushima introduced the Neocognitron, a hierarchical, convolution-like architecture that anticipated modern convolutional networks.</p> <p>But why did DL become popular only after the 2010s? The obstacle was never the lack of a correct algorithm. It was the lack of data and the cost of computation. DL worked because three forces aligned. Data became abundant because digital life produces it automatically. Computation became affordable because parallel hardware matured. And (less critically) software matured enough to make experimentation fast and scalable.</p>"},{"location":"introduction/01_overview/#data","title":"Data","text":"<p>The modern era began when data stopped being rare. This shift was driven by broader technological changes. Digital sensors replaced analog ones, smartphones placed cameras and microphones in billions of pockets, and the internet enabled continuous sharing of images, text, audio, and video. Companies began logging user interactions by default, storage became cheap, and bandwidth increased dramatically. Data was no longer collected deliberately, it was generated automatically as a byproduct of everyday life.</p> <p>Before large-scale datasets became feasible, progress relied on small, carefully curated benchmarks. The famous MNIST dataset was collected by the National Institute of Standards and Technology (NIST), and later was modified (hence the M before NIST) for simpler usage of ML algorithms<sup>2</sup>. MNIST is a simple dataset of handwritten digits that allowed researchers to isolate questions about optimization, architectures, and learning dynamics without the confounding effects of scale and noise. </p>      MNIST inputs ~ Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.     <p>A symbolic moment was the creation of ImageNet (Deng et al.). ImageNet contained roughly 14 million labeled images, with about 1.2 million training images across 1,000 categories used in its main benchmark. This scale exposed the limitations of hand-crafted features. Models that performed well on small datasets failed to generalize, while systems capable of learning representations directly from data improved reliably.</p> <p>In 2012, AlexNet (Krizhevsky et al.) won the ImageNet competition by a large margin. The model was unusually large and computationally demanding, and training it required GPUs rather than CPUs. This detail is crucial. DL did not succeed merely because sufficient data became available, it succeeded because the models finally fit within the limits of available hardware.<sup>1</sup></p>     \"Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5).\" ~ Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems (NeurIPS)      <p>As of 2016, a rough rule of thumb is that a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category and will match or exceed human performance when trained with a dataset containing at least 10 million labeled examples.  </p> <p>Deep Learning  (Chapter I) ~ Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). MIT Press.</p>"},{"location":"introduction/01_overview/#hardware","title":"Hardware","text":"<p>Training neural networks is dominated by large-scale numerical operations repeated many times. CPUs are optimized for general-purpose tasks and complex control flow, but they are inefficient for massive parallel arithmetic. GPUs, originally designed for rendering images, apply the same operation to many data points simultaneously. This made them a natural fit for neural network training.</p> <p>NVIDIA became central to DL because it invested early in programmable GPUs and the software needed to exploit them. Although originally developed for video games, GPUs are fundamentally optimized for massively parallel linear algebra, especially matrix and tensor operations. The introduction of CUDA exposed this capability to researchers, making large-scale matrix multiplications\u2014the core computational workload of neural networks\u2014efficient and accessible. As a result, models that once took weeks to train on CPUs could be trained in days or hours. Later accelerators such as Tensor Processing Unit (TPU) followed the same principle: DL scales when hardware is designed around dense linear algebra, high memory bandwidth, and parallel computation.</p>"},{"location":"introduction/01_overview/#software","title":"Software","text":"<p>The relevant software emerged in parallel with hardware. Python became the dominant language for ML because it allowed researchers to write clear, concise code while delegating computationally intensive operations to highly optimized numerical libraries implemented in C, C++, and CUDA. This separation between high-level model logic and low-level performance-critical kernels proved decisive. Researchers could focus on ideas rather than infrastructure, iterating rapidly while still benefiting from efficient linear algebra routines running on GPUs.</p> <p>Modern DL frameworks such as PyTorch and TensorFlow made it possible to automate differentiation, memory management, and efficient parallel execution. As a result, experiments that once required weeks of careful implementation could be expressed in hundreds of lines of code and tested within days.</p> <p>PyTorch is primarily a tool for research and experimentation. It is designed to feel like ordinary Python code, which makes models easy to write, modify, and debug. Tools such as PyTorch Lightning build on this flexibility by handling routine tasks like training loops and logging, allowing users to keep their focus on the model itself.</p> <p>TensorFlow, on the other hand, is more strongly oriented toward engineering and deployment. It was built to support large systems that need to run reliably across different machines and environments. With the addition of Keras, TensorFlow offers a high-level interface that makes it easy to define standard models and training pipelines in a consistent way. This structure is well suited to production settings, where models must be maintained, scaled, and deployed efficiently over long periods of time.</p>"},{"location":"introduction/01_overview/#transformers-and-beyond","title":"Transformers and Beyond","text":"<p>Computer Vision (CV) and Natural Language Processing (NLP) are the two main perception-oriented branches of modern DL. Both aim to convert raw, high-dimensional signals into structured representations that machines can reason over, but they operate on different data modalities and evolved under different constraints.</p> <p>CV focuses on visual data such as images and videos. Early progress was driven by convolutional neural networks (CNN). NLP deals with sequential, symbolic data such as text and speech. While early neural NLP relied on recurrent models (RNN), a major conceptual shift occurred with the introduction of the Transformer architecture (Vaswani et al., 2017), which replaced sequential recurrence with attention-based information routing. This change enabled massive parallelism, better long-range dependency modeling, and effective scaling with data and compute. The same architecture was later adapted to images via Vision Transformers (Dosovitskiy et al., 2020), revealing that vision and language could share a common computational backbone despite their different input structures.</p> <p>DL also extended beyond perception into decision-making. The combination of deep learning and reinforcement learning became widely visible through AlphaGo and later AlphaZero (Silver et al., 2016; 2018), which learned complex games through self-play without human examples.</p> <p>Building on the Transformer architecture, Large Language Models (LLM) such as Generative Pre-trained Transformer (GPT) marked a shift from task-specific NLP systems to general-purpose foundation models. By training a model on massive text corpora, GPT-style models learn broad linguistic, semantic, and world-level regularities that can be reused across tasks. Their success demonstrated that scale\u2014data, parameters, and compute\u2014can replace handcrafted linguistic structure, and that a single architecture can support a wide range of capabilities, including translation, summarization, reasoning, and code generation, without explicit task-specific design.</p> <ol> <li> <p>Even then, Alex Krizhevsky had to distribute training across two NVIDIA GeForce GTX 580 GPUs, each with 3 GB of memory (best at the moment), because the network did not fit on a single GPU.\u00a0\u21a9</p> </li> <li> <p>Geoffrey Hinton called this dataset \"the drosophila of ML\", a fruit fly extensively used in genetic research labs.\u00a0\u21a9</p> </li> </ol>"},{"location":"introduction/02_materials/","title":"Study Materials","text":"<p>Important</p> <p>     The page is currently under development.   </p>"},{"location":"mathematics/","title":"Mathematics of Deep Learning","text":"18 Jan 2026 \u00b7   6 min <p>Deep Learning (DL) relies on mathematics, but not on all of mathematics equally. Many topics that are common in standard mathematics curricula play little or no role in the practice of DL. The purpose of this section is to explain which parts of mathematics matter for DL and what role they play. </p> <p>In preparing this material, two widely used resources were consulted and found to be highly valuable: Deep Learning (Goodfellow et al., 2016) and Dive into Deep Learning (Zhang et al., online).</p> <p>Deep Learning presents the mathematics in a concise and rigorous form. Its strength lies in precision and breadth, but this compact style can make it difficult for readers to develop intuition, especially when encountering these ideas for the first time. Key concepts are often introduced quickly, with limited space for informal explanation or gradual buildup.</p> <p>Dive into Deep Learning takes a different approach, tightly integrating mathematical ideas with executable code. This makes experimentation accessible and practical, but it can also blur the boundary between mathematical concepts and their implementation. The mathematical knowledge is not always presented in a clearly systematized form.</p> <p>The goal of the present material is to combine the strengths of both approaches while addressing their limitations. Mathematical ideas are introduced carefully and explained in simple language, with implementation details separated whenever possible. Each concept is included because it plays a clear role in DL, not because it belongs to a traditional mathematics curriculum. The aim is to provide a conceptual foundation that supports both practical experimentation and deeper theoretical study.</p> <p>More detailed overviews can be found in the separate pages dedicated to Calculus, Linear Algebra, Probability Theory, and Information Theory. Below is a summary of the main mathematical concepts required for DL.</p>"},{"location":"mathematics/#calculus","title":"Calculus","text":"<p>Within calculus, the central idea for DL is the rate of change: if we change some model parameters slightly, how does the output change? In DL, the output of interest is usually a single number called the loss, which measures how bad the model's prediction is. A derivative tells us how much the loss changes when we slightly change one parameter. This makes derivatives a practical tool for learning, since they indicate the direction in which parameters should be adjusted to reduce the loss.</p> <p>Partial derivatives are essential because a model typically has many parameters. A partial derivative measures how sensitive the loss is to one parameter while all other parameters are kept fixed. The gradient simply collects all these sensitivities into a single array. The chain rule explains how sensitivities propagate through a model that is built from many smaller operations, and backpropagation is the algorithm that applies the chain rule efficiently to compute gradients.</p> <p>All of this relies on an important assumption: the loss changes smoothly with respect to the parameters. This means that small changes in parameters lead to small, predictable changes in the loss, making derivatives reliable guides for optimization.</p> <p>Integration and the Fundamental Theorem of Calculus appear more quietly in the background. They underlie concepts such as expectations and averages, which are central to training objectives. In DL, integrals are rarely computed by hand; understanding what integration represents is more important than learning how to calculate it.</p>"},{"location":"mathematics/#linear-algebra","title":"Linear Algebra","text":"<p>Linear algebra is the language in which DL models are written. Data points, parameters, and gradients are represented as vectors. Linear layers are represented as matrix\u2013vector or matrix\u2013matrix multiplications.</p> <p>What matters most is intuition. Vectors should be understood as ordered collections of numbers. Matrices should be understood as operations that transform vectors by scaling, rotating, or mixing their components. These ideas explain how information flows through a network and why many computations can be done efficiently in parallel.</p> <p>Linear algebra also explains why gradients have the same shape as parameters, why batching works, and why modern hardware such as GPUs is effective for DL. Model parameters are stored as vectors and matrices, and gradients are derivatives with respect to those parameters, so they naturally have the same structure. This one-to-one correspondence makes parameter updates straightforward: each parameter is adjusted using its matching gradient entry.</p> <p>Modern DL frameworks are built almost entirely on linear algebra operations. Matrix multiplication, matrix addition, and vectorized nonlinear functions form the core of both the forward and backward passes. Most performance optimizations are handled automatically by numerical libraries, allowing users to express models at a high level while relying on efficient low-level implementations. </p> <p>While matrix factorizations such as Singular Value Decomposition (SVD) are rarely invoked explicitly during training, they are used internally in optimized linear solvers, low-rank approximations, spectral normalization, and in estimating matrix norms or conditioning. Through these mechanisms, factorization-based ideas influence numerical stability, efficiency, and scaling behavior in DL systems without appearing directly in model code.</p> <p>Batching works because linear algebra operations naturally extend from single vectors to collections of vectors stacked into matrices or higher-dimensional tensors. Processing many data points at once is not a special trick, but a direct consequence of writing models in matrix form. GPUs are effective for DL for the same reason: linear algebra operations consist of many simple arithmetic operations that can be carried out in parallel. As a result, DL benefits directly from both the mathematical structure of linear algebra and the hardware designed to execute it efficiently.</p>"},{"location":"mathematics/#probability-theory","title":"Probability Theory","text":"<p>DL models are trained on data that is noisy, incomplete, and often ambiguous. Probability provides the language for describing this uncertainty and for turning learning into a well-defined mathematical problem. In DL, models are often best understood not as systems that produce a single \"correct\" output, but as systems that assign probabilities to possible outcomes.</p> <p>From this perspective, a model defines a probability distribution, either explicitly or implicitly. Training the model means adjusting its parameters so that the observed data becomes more probable under this distribution. Many commonly used loss functions arise directly from this idea. Minimizing such losses is equivalent to maximizing likelihood.</p> <p>Expectations play a central role because learning is not based on a single data point, but on averages over data drawn from an underlying distribution. Training objectives are typically expectations of a loss over the data distribution, which in practice are approximated using finite datasets and minibatches.</p> <p>DL does not require advanced probability theory, but it does require a clear understanding of what probabilistic models represent, how likelihood and expectation relate to loss functions, and why uncertainty is an essential part of learning from real data.</p>"},{"location":"mathematics/#information-theory","title":"Information Theory","text":"<p>Information theory enters DL when we want to measure how different two probability distributions are. Many DL models define a distribution over possible outputs rather than producing a single fixed prediction. Information-theoretic quantities provide a principled way to compare these predicted distributions to the true data distribution.</p> <p>Concepts such as entropy and cross-entropy arise naturally in this setting. Entropy measures uncertainty, while cross-entropy measures how well one distribution represents another. Minimizing cross-entropy encourages the model to assign high probability to the observed data.</p> <p>A closely related quantity is the Kullback\u2013Leibler (KL) divergence, which measures how much information is lost when one distribution is used to approximate another. Many common training objectives can be interpreted as minimizing a KL divergence, even when this connection is not stated explicitly.</p>"},{"location":"mathematics/#additional-mathematics","title":"Additional Mathematics","text":"<p>In addition to the core areas discussed above, several mathematical perspectives play an important role in DL. While they may not always appear as standalone topics or require extensive formal development, they shape how models are designed, trained, and evaluated. These ideas recur across many DL settings and deserve explicit attention, even when they are introduced briefly.</p> <p>Statistics enters DL through the fact that models are trained on finite samples rather than full data-generating processes. Concepts such as generalization, overfitting, and the bias\u2013variance tradeoff describe the structural limits of what can be learned from data and how model complexity interacts with sample size and noise. These considerations shape how results should be interpreted, how sensitive conclusions are to data variation, and how confidently performance can be expected to transfer beyond the observed sample.</p> <p>Optimization theory addresses a small set of practical questions that arise once a loss function is defined. Given a highly non-convex objective with millions of parameters, how can it be minimized efficiently, and why do simple gradient-based methods work at all? How do learning rates, momentum, adaptive updates, and noise from minibatching affect training behavior? Rather than providing exact convergence proofs, optimization theory offers guidance on training stability, speed, and failure modes.</p> <p>Geometry treats representations, parameters, and activations as points in high-dimensional spaces. Distances and angles define similarity, with measures such as cosine similarity capturing directional alignment between representations. Optimization itself is a geometric process, moving parameters across a loss surface whose local curvature influences learning speed and stability. Geometric intuition about distances, neighborhoods, and curvature helps explain why certain architectures, losses, and similarity measures are effective in practice.</p> <p>Graph theory becomes relevant whenever data is structured by relations rather than simple vectors. In graph neural networks (GNNs), data is represented as nodes and edges, and learning depends directly on graph connectivity. Related ideas also appear more broadly in message passing, relational reasoning, and attention mechanisms applied to structured inputs.</p> <p>Numerical computation and stability constrain how DL models are implemented. Because training relies on finite-precision arithmetic, issues such as overflow, underflow, and loss of precision directly influence model behavior. Many standard techniques in DL\u2014such as normalization layers, carefully designed loss functions, and specific activation choices\u2014exist primarily to ensure stable and reliable computation.</p> <p>Together, these perspectives complement the core mathematical foundations and connect them to practical modeling, training, and evaluation. They do not replace the core framework, but they shape how it is applied and understood in real-world deep learning systems.</p>"},{"location":"mathematics/01_calculus/","title":"Calculus","text":"18 Jan 2026 \u00b7   12 min <p>Calculus studies two closely related ideas: accumulation (integration) and change (differentiation). In DL, learning is defined by accumulating error across data, usually as an average loss. Training then proceeds by making small changes to model parameters in order to reduce this accumulated error. Calculus provides the language and structure for both. This section builds calculus concepts from fundamentals, with the goal of understanding how they support learning, optimization, and model behavior in deep learning.</p>"},{"location":"mathematics/01_calculus/#functions","title":"Functions","text":"<p>A function maps inputs to outputs. We write this as \\(y = f(x)\\). If the input \\(x\\) changes, the output \\(y\\) usually changes as well. Some functions change slowly, some change quickly, and some change differently depending on at which point of the function you are. Calculus begins by asking how these changes are related. </p> <p>Note</p> <p>For example, changing the brightness of an image slightly may barely affect a model's output in one case, but cause a large change in another.</p>"},{"location":"mathematics/01_calculus/#integration","title":"Integration","text":"<p>An integral such as \\(\\int_a^b f(x)\\,dx\\) represents the total accumulation of the values of \\(f(x)\\) as \\(x\\) moves from \\(a\\) to \\(b\\). It is simply the continuous analogue of a summation. You can think of an integral as summing many small contributions of \\(f(x)\\) over an interval. The exact techniques for computing integrals are less important in DL than the idea they represent.</p> <p>Conceptually, integration means breaking an interval into many small pieces. For each piece, we take the value of \\(f(x)\\) and multiply it by the width of the piece. Adding all these pieces together gives an approximation of the total accumulation. As the pieces become smaller and more numerous, this approximation approaches the integral. Mathematically, we represent this very small width as \\(dx\\).</p>      Riemann Integration and Darboux Lower Sums. By IkamusumeFan - Own work\u00a0This plot was created with Matplotlib., CC BY-SA 3.0, Link <p>Note</p> <p>In DL, training is never based on a single example. A model is evaluated by how it performs across many examples, so errors must be combined into one overall value. In practice, we only have access to a limited number of training examples. A common case in ML and DL is mean squared error (MSE), where for each training sample we compute a prediction error, square it (so negative and positive errors do not cancel, and larger mistakes are penalized), and then average these squared errors over the dataset. Conceptually, however, this dataset-level average is not the final goal. </p> <p>The dataset is usually treated as a small collection of examples drawn from a much larger source of data. Ideally, we would like to measure the model\u2019s average error over all possible data points it might encounter, not just the ones we happened to collect. The finite average used in training should therefore be understood as a practical approximation of a more general, ideal accumulated continuous quantity. For the values \\(g(x_1), g(x_2), \\dots, g(x_N)\\), we can write</p> \\[ \\mathbb{E}_{x \\sim p}[g(x)] \\approx \\frac{1}{N}\\sum_{i=1}^N g(x_i). \\] <p>Here, the right-hand side is what we compute from data, and the left-hand side represents the ideal quantity we are trying to approximate. This ideal accumulated quantity is written precisely using the concept of an expectation. When data is described by a probability distribution \\(p(x)\\), the average value of a quantity \\(g(x)\\) is written as</p> \\[ \\mathbb{E}_{x \\sim p}[g(x)] = \\int g(x)\\,p(x)\\,dx. \\] <p>You do not need a deep understanding of probability to read this expression. Conceptually, it means: consider all possible values of \\(x\\), weight each value by how common it is, and add everything up. Many DL loss functions can be understood this way, as average losses over all possible data. For discrete datasets, this expectation reduces to a finite sum, while for continuous variables it is written as an integral. The integral itself is not special\u2014it is simply the mathematical way to express an average over all possible inputs when the space of inputs is continuous.</p> <p>Tip</p> <p>If the idea of expectations or probability distributions feels unfamiliar, you may want to read the page dedicated to the Probability Theory alongside this section.</p>"},{"location":"mathematics/01_calculus/#differentiation","title":"Differentiation","text":"<p>Differentiation answers the question: if we change the input slightly, how much does the output change?  Suppose we start at \\(x\\) and then move a small step \\(h\\) to \\(x+h\\). The corresponding change in the output is \\(f(x+h) - f(x)\\). By itself, this number depends on how large \\(h\\) is. To describe change in a way that does not depend on the step size, we compare the output change to the input change by forming the ratio</p> \\[ \\frac{f(x+h) - f(x)}{h}. \\] <p>This ratio is called a difference quotient. It describes the average rate of change of the function over the small interval from \\(x\\) to \\(x+h\\). The derivative is defined as the limit of this ratio as the step size approaches zero:</p> \\[ f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}. \\] <p>This definition captures the idea of an \"instantaneous\" rate of change. Intuitively, the derivative tells us the slope of the function at the point \\(x\\): if \\(f'(x)\\) is large, a tiny change in \\(x\\) causes a large change in \\(f(x)\\), if \\(f'(x)\\) is close to zero, the function is locally flat. </p> <p>Note</p> <p>In DL, if increasing a model weight slightly increases the loss, then the derivative of the loss with respect to that weight is positive. That means decreasing the weight slightly should reduce the loss (at least locally).</p> <p>The derivative is not only a number; it also provides a practical approximation of how a function behaves near a given point. The key idea is that, over very small distances, a smooth function behaves almost like a straight line. If we start at a point \\(x\\) and move a small step \\(h\\), the derivative \\(f'(x)\\) tells us how steep the function is at \\(x\\). Using this slope, we can estimate how much the output will change. This leads to the approximation</p> \\[ f(x+h) \\approx f(x) + f'(x)\\,h. \\] <p>This formula should be read as a prediction: \"start from the current value \\(f(x)\\), then add the change suggested by the slope times the step size.\" The approximation becomes more accurate as the step \\(h\\) becomes smaller. Geometrically, this means that near the point \\(x\\), the function can be replaced by its tangent line. The tangent line touches the function at \\(x\\) and has the same slope there. Over a very small region, the curve and the tangent line are almost indistinguishable, which is why the linear approximation works.</p>      By Chorch - Own Work, Public Domain, Link <p>Note</p> <p>In DL, training works because, at each step, we treat the loss as locally almost linear in the parameters. The gradient (see below) gives the slope of this local linear approximation. By making small parameter updates in the direction opposite to the gradient, we can reliably reduce the loss step by step, even when the overall loss function is highly complex.</p>"},{"location":"mathematics/01_calculus/#partial-derivatives","title":"Partial derivatives","text":"<p>Deep learning models depend on many parameters at once. If the loss is written as</p> \\[ L = f(\\theta_1, \\theta_2, \\dots, \\theta_n), \\] <p>then each parameter has its own partial derivative \\(\\frac{\\partial L}{\\partial \\theta_i}.\\) A partial derivative measures how the loss changes when one parameter is varied while all others are held fixed.</p> <p>Note</p> <p>For example, changing a single weight in a neural network affects the loss while all other weights remain unchanged.</p>"},{"location":"mathematics/01_calculus/#gradients","title":"Gradients","text":"<p>The gradient collects all partial derivatives into a single vector:</p> \\[ \\nabla_{\\theta} L = \\left[ \\frac{\\partial L}{\\partial \\theta_1}, \\frac{\\partial L}{\\partial \\theta_2}, \\dots, \\frac{\\partial L}{\\partial \\theta_n} \\right]. \\] <p>The gradient points in the direction where the loss increases most rapidly. Moving in the opposite direction locally reduces the loss. Each component of the gradient corresponds to one parameter and tells us how that parameter influences the loss. Training typically consists of repeated updates of the form</p> \\[ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L, \\] <p>where \\(\\eta\\) is the learning rate. Each update makes a small change. Over many updates, these small changes accumulate and reduce the overall loss.</p> <p>Tip</p> <p>The learning rate update through backward pass is discussed in the notebook dedicated to backpropagation.</p>"},{"location":"mathematics/01_calculus/#jacobian","title":"Jacobian","text":"<p>The Jacobian is the general first-order derivative for functions with vector inputs and vector outputs. If a function maps an \\(n\\)-dimensional input vector to an \\(m\\)-dimensional output vector, \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m,\\) its Jacobian is an \\(m \\times n\\) matrix defined as</p> \\[ J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\dots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\dots &amp; \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} &amp; \\frac{\\partial f_m}{\\partial x_2} &amp; \\dots &amp; \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}. \\] <p>Each entry measures how one component of the output changes when one component of the input is varied. The Jacobian therefore captures all first-order sensitivities between inputs and outputs.</p> <p>Note</p> <p>In DL, layers often map vectors to vectors. Although Jacobians are rarely written explicitly, they are the objects through which changes propagate from one layer to the next. When the output is a scalar loss, the Jacobian reduces to a row vector. Conceptually, the gradient introduced earlier is simply the Jacobian of a scalar-valued function. Backpropagation avoids forming full Jacobian matrices explicitly. Instead, it efficiently computes vector\u2013Jacobian products, which is why gradients can be computed for models with millions of parameters at reasonable cost.</p> <p>Tip</p> <p>Jacobians are best understood through linear algebra. If matrices and vector transformations feel unfamiliar, you may want to read the Linear Algebra page alongside this section.</p>"},{"location":"mathematics/01_calculus/#chain-rule","title":"Chain Rule","text":"<p>DL models are built by composing functions. Instead of a single operation, a model applies many transformations one after another. Each transformation takes the output of the previous one as its input. To understand how changes propagate through such a model, consider a simple composition:</p> \\[ y = g(x), \\qquad L = f(y). \\] <p>Here, \\(x\\) influences \\(L\\) indirectly, through the intermediate variable \\(y\\). If we change \\(x\\) slightly, \\(y\\) will change, and that change in \\(y\\) will in turn affect \\(L\\). The chain rule formalizes this dependency.</p> <p>The chain rule states that the sensitivity of \\(L\\) with respect to \\(x\\) is the product of two sensitivities:</p> \\[ \\frac{dL}{dx} = \\frac{dL}{dy} \\cdot \\frac{dy}{dx}. \\] <p>This equation should be read step by step. First, \\(\\frac{dy}{dx}\\) tells us how a small change in \\(x\\) affects \\(y\\). Second, \\(\\frac{dL}{dy}\\) tells us how a small change in \\(y\\) affects the loss. Multiplying them gives the total effect of changing \\(x\\) on \\(L\\).</p> <p>This idea extends naturally to longer chains. If a model applies many functions in sequence, the chain rule is applied repeatedly, multiplying together the local sensitivities at each step. Each operation contributes a small piece to the overall gradient.</p>"},{"location":"mathematics/01_calculus/#taylor-expansion","title":"Taylor Expansion","text":"<p>Taylor series provides a systematic way to describe how a function behaves near a given point. It expresses a function as a sum of terms built from its derivatives at that point. Each term captures progressively finer details of how the function changes.</p> <p>For a function \\(f(x)\\) expanded around a point \\(x\\), the Taylor series in one dimension is</p> \\[ f(x+h) = f(x) + f'(x)h + \\tfrac{1}{2}f''(x)h^2 + \\tfrac{1}{6}f'''(x)h^3 + \\dots \\] <p>This expression says that the value of the function at \\(x+h\\) can be predicted by starting from the value at \\(x\\) and then adding corrections based on information about how the function changes at \\(x\\).</p> <p>In practice, we rarely use the full infinite series. Instead, we keep only the first few terms. This truncated version is called a Taylor expansion and is used as a local approximation.</p> <p>Keeping only the first-order term gives the linear approximation already used in gradient-based learning:</p> \\[ f(x+h) \\approx f(x) + f'(x)h. \\] <p>This approximation assumes that, for small updates, the function behaves almost like a straight line near the current point. It explains why gradients provide useful guidance for optimization.</p> <p>This local linear approximation relies on an important assumption: the function must be smooth enough near the point of expansion. Smoothness means that small changes in the input lead to small, predictable changes in the output, and that derivatives do not change abruptly.</p> <p>Note</p> <p>In DL, loss functions are often not perfectly smooth everywhere, but they are typically piecewise smooth. This is sufficient. Taylor expansions and gradient-based updates only rely on local behavior along the training trajectory, not on global smoothness of the loss surface. A common example is the ReLU activation, which is not differentiable at zero but is differentiable almost everywhere else. Gradient-based methods rely on this local behavior and use subgradients at nondifferentiable points.</p> <p>Keeping second-order terms reveals that this linear behavior is only approximate. These higher-order terms explain why the slope itself can change as we move, motivating the need to understand second-order structure.</p> <p>Note</p> <p>In DL, gradient-based learning relies on first-order Taylor approximations. Understanding why and when this approximation breaks down requires looking at second-order effects, which are captured by the Hessian.</p>"},{"location":"mathematics/01_calculus/#hessian","title":"Hessian","text":"<p>While the Jacobian describes first-order behavior\u2014how the loss changes under small parameter changes\u2014the Hessian describes second-order behavior.<sup>1</sup> It captures how these first-order sensitivities themselves change as we move in parameter space. The Hessian of \\(L\\) with respect to the parameter vector \\(\\theta\\) is a matrix of second-order partial derivatives:</p> \\[ H = \\begin{bmatrix} \\frac{\\partial^2 L}{\\partial \\theta_1^2} &amp; \\frac{\\partial^2 L}{\\partial \\theta_1 \\partial \\theta_2} &amp; \\dots &amp; \\frac{\\partial^2 L}{\\partial \\theta_1 \\partial \\theta_n} \\\\[0.5em] \\frac{\\partial^2 L}{\\partial \\theta_2 \\partial \\theta_1} &amp; \\frac{\\partial^2 L}{\\partial \\theta_2^2} &amp; \\dots &amp; \\frac{\\partial^2 L}{\\partial \\theta_2 \\partial \\theta_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 L}{\\partial \\theta_n \\partial \\theta_1} &amp; \\frac{\\partial^2 L}{\\partial \\theta_n \\partial \\theta_2} &amp; \\dots &amp; \\frac{\\partial^2 L}{\\partial \\theta_n^2} \\end{bmatrix}. \\] <p>Each entry tells us how the sensitivity with respect to one parameter changes when another parameter is varied. In this sense, the Hessian measures curvature: how the loss surface bends in different directions. Consider a simple two-parameter loss \\(L(\\theta_1, \\theta_2)\\). The diagonal entries of the Hessian describe how sharply the loss curves when we move along each parameter direction individually. The off-diagonal entries describe how changes in one parameter affect the sensitivity with respect to another parameter.</p> <p>Note</p> <p>In DL, this information explains important optimization behavior. Directions with strong positive curvature correspond to narrow valleys, where large updates can easily overshoot. Directions with weak curvature correspond to flat regions, where progress can be slow. Negative curvature indicates directions where the loss bends downward, which is typical near saddle points. Although full Hessians are rarely computed explicitly in DL due to their size and cost, their effects are always present. Learning rate selection, optimization stability, and the behavior of training near minima and saddle points are all influenced by second-order structure.</p> <p>Tip</p> <p>Like the Jacobian, the Hessian is a linear algebra object\u2014a matrix encoding directional behavior. If matrices, eigenvalues, or curvature interpretations feel unfamiliar, you may want to read the Linear Algebra page alongside this section.</p>"},{"location":"mathematics/01_calculus/#minima-saddle-points-and-convexity","title":"Minima, saddle points, and convexity","text":"<p>A minimum is a point where small changes in any direction increase the loss. At such a point, the gradient is zero and the surrounding curvature points upward.</p> <p>A saddle point is also a point where the gradient is zero, but the behavior is mixed: the loss increases in some directions and decreases in others. This means the point is neither a true minimum nor a maximum. The distinction between minima and saddle points is determined by the local curvature described by the Hessian.</p> <p>Note</p> <p>In high-dimensional DL models, saddle points are far more common than poor local minima. Gradient-based methods can often escape saddle points because curvature creates unstable directions, and stochastic noise from minibatches helps push parameters away from them.</p> <p>In classical optimization, convex loss functions play a special role. For a convex function, any point where the gradient is zero is guaranteed to be a global minimum. There are no saddle points and no spurious local minima.</p> <p>Note</p> <p>Most DL loss functions are not convex. As a result, global guarantees do not apply. Instead, training relies on local information provided by gradients and curvature. Despite the lack of convexity, gradient-based methods work well in practice due to overparameterization, stochastic gradients, and the structure induced by modern architectures, even though no global guarantees apply.</p> <p>Gradient-based learning does not require global convexity. What matters is that, locally, the loss behaves smoothly enough for gradients and Taylor approximations to provide reliable guidance along the training trajectory.</p>"},{"location":"mathematics/01_calculus/#fundamental-theorem-of-calculus","title":"Fundamental Theorem of Calculus","text":"<p>The Fundamental Theorem of Calculus explains the precise relationship between accumulation and change. If we define an accumulated quantity</p> \\[ F(x) = \\int_a^x f(t)\\,dt, \\] <p>then \\(F(x)\\) is differentiable and</p> \\[ \\frac{d}{dx} F(x) = f(x). \\] <p>This means that differentiation recovers the rate at which accumulation occurs. Conversely, if \\(F(x)\\) is any antiderivative of \\(f(x)\\), then total accumulation over an interval can be computed as</p> \\[ \\int_a^b f(x)\\,dx = F(b) - F(a). \\] <p>Together, these statements show that local change and total accumulation are two sides of the same idea.</p> <p>Note</p> <p>In DL, losses are defined as accumulated quantities, while gradients describe local change. Training works because following local gradients causes a consistent reduction in the accumulated loss over time.</p> <ol> <li> <p>For a further DL\u2013oriented treatment of gradients, Jacobians, Hessians, and numerical aspects of optimization, see Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press. Chapter 4: Numerical Computation.\u00a0\u21a9</p> </li> </ol>"},{"location":"mathematics/02_linear_algebra/","title":"Linear Algebra","text":"<p>Important</p> <p>     The page is currently under development.   </p> 19 Jan 2026 \u00b7   22 min <p>Linear Algebra is the branch of mathematics that studies vector spaces and the linear mappings between them. In DL, almost all computation is formulated in the language of linear algebra: data, model parameters, activations, gradients are represented as vectors or matrices. A clear understanding of what these objects represent\u2014and how they behave under linear operations\u2014is necessary not only for correct implementation, but for reasoning about model structure, learning dynamics, and numerical behavior.</p>"},{"location":"mathematics/02_linear_algebra/#scalars-and-vectors","title":"Scalars and Vectors","text":"<p>A scalar is a single number (often real-valued). It is more challenging to define a vector. Different fields use the same object with different mental models and all are legitimate because they rely on the same linear rules.</p> <p>From a mathematician's point of view, a vector is an element of a vector space: something you can add and scale while satisfying certain axioms (closure, associativity, distributivity, etc.). The axioms exist to guarantee that linear combinations behave predictably. From this requirement of linearity through addition and scalar scaling (homogeinity), you get a powerful combined statement: $$ f(\\alpha x + \\beta y) = \\alpha f(x) + \\beta f(y) $$</p> <p>Note</p> <p>Raw audio signals satisfy the linearity properties to a very good approximation. If two sounds are played at the same time, the resulting waveform is (approximately) the sum of the individual waveforms. If the volume of a sound is increased or decreased, its waveform is scaled by a constant factor. Because audio combines by superposition and scales linearly with amplitude, it can be naturally represented as a vector and manipulated using linear algebra.</p> <p>From a physicist's point of view, a vector represents a quantity with direction and magnitude (velocity, force). You add forces, scale forces, decompose into components. The vector predicts physical behavior. From a computer scientist's point of view, a vector is simply an array of numbers. It can represent pixel values of an image, coordinates of a point, words in a document, etc.</p> <p>Warning</p> <p>In many ways, ML/DL borrows terminology from mathematics and uses it rather freely. Terms like vector, dimension, space, metric, manifold, and even linear are frequently misused. For example, by \"dimension\" one could assume \"tensor axis length\". This is convenient shorthand, but it can break intuition if you don't keep in mind the underlying differences between DL and mathemtics which often use the same tools for different purposes.</p> <p>A vector is often written explicitly as a column of numbers. For example, a vector with \\(n\\) real-valued components can be written as</p> \\[ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} \\in \\mathbb{R}^n . \\] <p>In DL, such a vector is typically understood operationally: it is stored as a contiguous array of \\(n\\) real numbers and is mathematically an element of \\(\\mathbb{R}^n\\), the Cartesian product of \\(\\mathbb{R}\\) with itself \\(n\\) times. In this context, its \"dimension\" refers simply to its length \\(n\\). When \\(n = 2\\) or \\(n = 3\\), the vector can be visualized geometrically as a point or an arrow. When \\(n\\) is large, direct visualization is no longer possible, but the same algebraic operations\u2014addition, scalar multiplication, dot products, and linear transformations\u2014still apply. </p> <p>Note</p> <p>Depending on context, vectors can be visualized in different ways. In geometry and physics, they are often drawn as arrows representing magnitude and direction. In other settings, a vector can be viewed as a function that assigns a value to each index or coordinate. These visualizations are useful for building intuition, especially in low dimensions, but they do not alter the underlying algebraic definition of a vector. Linear algebra itself does not rely on geometric interpretation. It is fundamentally an algebraic theory of vector spaces and linear maps, and all definitions and results are independent of visualization. Geometry serves only as an intuitive aid not as a prerequisite. Beyond three dimensions, geometry in the literal, visual sense becomes unusable. Since most representations in DL live in very high-dimensional spaces, geometric visualization is generally not available and plays no direct role in practice. What remains meaningful are algebraic and analytical notions\u2014such as inner products, norms, projections, and linear maps\u2014rather than pictures or spatial intuition.</p>"},{"location":"mathematics/03_probability/","title":"Probability Theory","text":"<p>Important</p> <p>     The page is currently under development.   </p>"},{"location":"mathematics/04_information/","title":"Information Theory","text":"<p>Important</p> <p>     The page is currently under development.   </p>"},{"location":"notebooks/01_backprop/","title":"01. From Derivatives to Backpropagation","text":"<p>We will go from illustrating differentiation and finding derivatives in Python, all the way down till the implementation of the backpropagation algorithm. Even if mathematically the idea of derivatives is very familiar, it still makes sense to see its various visualization plots via code.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[2]: Copied! <pre>def f(x):\n  return x**2\n</pre> def f(x):   return x**2 In\u00a0[3]: Copied! <pre>x = 3.0\nfor h in [10, 1, 0.1, 0]:\n  print(f\"If we shift input by {h}, output becomes {f(x+h)}\")\n</pre> x = 3.0 for h in [10, 1, 0.1, 0]:   print(f\"If we shift input by {h}, output becomes {f(x+h)}\") <pre>If we shift input by 10, output becomes 169.0\nIf we shift input by 1, output becomes 16.0\nIf we shift input by 0.1, output becomes 9.610000000000001\nIf we shift input by 0, output becomes 9.0\n</pre> <p>The well-known equation for the change $ \\Delta y = f(x + \\Delta x) - f(x)$ we can code and  visualize for various cases as follows. Again, feel free to modify the values and functions to observe the behavior of change.</p> In\u00a0[4]: Copied! <pre>h = 1.0\n\ndx = h\ndy = f(x+h) - f(x)\n\nprint(f\"\u0394x: {dx}\")\nprint(f\"\u0394y: {dy}\")\nprint(f\"When you change x by {dx} unit, y changes by {dy} units.\")\n</pre> h = 1.0  dx = h dy = f(x+h) - f(x)  print(f\"\u0394x: {dx}\") print(f\"\u0394y: {dy}\") print(f\"When you change x by {dx} unit, y changes by {dy} units.\") <pre>\u0394x: 1.0\n\u0394y: 7.0\nWhen you change x by 1.0 unit, y changes by 7.0 units.\n</pre> In\u00a0[5]: Copied! <pre>def plot_delta(x, h, start=-4, stop=4, num=30):\n  # `np.linspace` returns an array of num inputs within a range.\n  x_all = np.linspace(start, stop, num)\n  y_all = f(x_all)\n\n  plt.figure(figsize=(4, 4))\n  plt.plot(x_all, y_all)\n\n  # dx &amp; dy\n  plt.plot([x, x + h], [f(x), f(x)], color='r')\n  plt.plot([x + h, x + h], [f(x), f(x + h)], color='r')\n</pre> def plot_delta(x, h, start=-4, stop=4, num=30):   # `np.linspace` returns an array of num inputs within a range.   x_all = np.linspace(start, stop, num)   y_all = f(x_all)    plt.figure(figsize=(4, 4))   plt.plot(x_all, y_all)    # dx &amp; dy   plt.plot([x, x + h], [f(x), f(x)], color='r')   plt.plot([x + h, x + h], [f(x), f(x + h)], color='r') In\u00a0[6]: Copied! <pre>plot_delta(x=2, h=1)\n</pre> plot_delta(x=2, h=1) <p>How to find if the ouput changes significantly when we change the input by some amount $h$? We should be familiar with the rate of change ratio when we choose a small step size $h$:</p> <p>$$ \\dfrac{\\Delta y}{\\Delta x} = \\dfrac{f(x + h) - f(x)}{h}.$$</p> In\u00a0[7]: Copied! <pre>def plot_roc(x, h):\n  dx = h\n  dy = f(x + h) - f(x)\n\n  plot_delta(x, h)\n  print(f\"Rate of change is {dy / dx}\")\n</pre> def plot_roc(x, h):   dx = h   dy = f(x + h) - f(x)    plot_delta(x, h)   print(f\"Rate of change is {dy / dx}\") In\u00a0[8]: Copied! <pre>plot_roc(3, 1)\n</pre> plot_roc(3, 1) <pre>Rate of change is 7.0\n</pre> In\u00a0[9]: Copied! <pre>plot_roc(3, 0.5)\n</pre> plot_roc(3, 0.5) <pre>Rate of change is 6.5\n</pre> In\u00a0[10]: Copied! <pre>plot_roc(1, 1)\n</pre> plot_roc(1, 1) <pre>Rate of change is 3.0\n</pre> In\u00a0[11]: Copied! <pre>plot_roc(-2, 0.5)\n</pre> plot_roc(-2, 0.5) <pre>Rate of change is -3.5\n</pre> <p>The rate of change for different values of $h$ are different at the same point $x$. We would like to come up with a single value that would tell how significantly $y$ changes at a given point $x$ within the function:</p> <p>$$ L = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} $$</p> <p>Simply, this limit tells us how much the value of the output will change when we change the input by just a very small amount.</p> <p>Note</p> <p>         Essentially, derivative is a function: it assigns to each input $ x $ the rate at which the original function changes at that point. Meaning, for each input, the limit above produces a value, and as $ x $ changes, this value changes as well.     </p> In\u00a0[12]: Copied! <pre>x = 3\nh = 0.000001 # The limit of h approaching 0\nd = (f(x + h) - f(x)) / h\nf\"The value of derivative function is {d}\"\n</pre> x = 3 h = 0.000001 # The limit of h approaching 0 d = (f(x + h) - f(x)) / h f\"The value of derivative function is {d}\" Out[12]: <pre>'The value of derivative function is 6.000001000927568'</pre> <p>When we extend this idea to multivariable calculus, we use partial derivatives. A partial derivative with respect to (wrt) a given variable measures how much the output changes when we nudge that variable alone by a very small amount, while keeping all other variables fixed. Below we will see how it behaves during addition $f(x, y)=x+y$.</p> In\u00a0[13]: Copied! <pre>f = lambda x, y: x + y\n\nx = 2\ny = 3\n\nf(x, y)\n</pre> f = lambda x, y: x + y  x = 2 y = 3  f(x, y) Out[13]: <pre>5</pre> <p>Partial derivatives w.r.t $x$ an $y$ will be as follows:</p> In\u00a0[14]: Copied! <pre>h = 0.000001\n(f(x + h, y) - f(x, y)) / h\n</pre> h = 0.000001 (f(x + h, y) - f(x, y)) / h Out[14]: <pre>1.000000000139778</pre> In\u00a0[15]: Copied! <pre>h = 0.000001\n(f(x, y+h) - f(x, y)) / h\n</pre> h = 0.000001 (f(x, y+h) - f(x, y)) / h Out[15]: <pre>1.000000000139778</pre> <p>No matter what the input values are, the expression will always approach $1.0$ for addition, because adding a constant increases the output by the same amount as the input change.</p> In\u00a0[16]: Copied! <pre>for x, y in zip([-20, 2, 3], [300, 75, 10]):\n  print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}')\n</pre> for x, y in zip([-20, 2, 3], [300, 75, 10]):   print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}') <pre>x=-20, y=300: 0.9999999974752427\nx=2, y=75: 0.9999999974752427\nx=3, y=10: 1.0000000010279564\n</pre> <p>Indeed, if we have a simple addition $x + y$, then increasing $x$ or $y$ by some amount will increase the result by the exact same amount. Assertion will work for any number $h$ gets.</p> In\u00a0[17]: Copied! <pre>h = 10\nassert f(x+h, y) - f(x, y) == h\nassert f(x, y+h) - f(x, y) == h\n</pre> h = 10 assert f(x+h, y) - f(x, y) == h assert f(x, y+h) - f(x, y) == h <p>Let's now see the case for multiplication $f(x, y)=x * y$.</p> In\u00a0[18]: Copied! <pre>f = lambda x, y: x * y\n</pre> f = lambda x, y: x * y In\u00a0[19]: Copied! <pre>x = 2\ny = 3\nh = 1e-5 # scientific notation for 0.00001\n(f(x + h, y) - f(x, y)) / h # wrt x\n</pre> x = 2 y = 3 h = 1e-5 # scientific notation for 0.00001 (f(x + h, y) - f(x, y)) / h # wrt x Out[19]: <pre>3.000000000064062</pre> In\u00a0[20]: Copied! <pre>for x in [-20, 2, 3]:\n  print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}')\n</pre> for x in [-20, 2, 3]:   print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}') <pre>x=-20, y=3: 2.999999999531155\nx=2, y=3: 3.000000000064062\nx=3, y=3: 3.000000000064062\n</pre> In\u00a0[21]: Copied! <pre>x = 10\nh = 5\npdx = (f(x+h, y) - f(x, y)) / h\nprint(pdx, y)\nassert round(pdx, 2) == round(y, 2)\n</pre> x = 10 h = 5 pdx = (f(x+h, y) - f(x, y)) / h print(pdx, y) assert round(pdx, 2) == round(y, 2) <pre>3.0 3\n</pre> <p>Finally, we will consider a complex function with three variables $f(x, y, z)=x^2+y^3-z$.</p> In\u00a0[22]: Copied! <pre>def f(x, y, z):\n  return x**2 + y**3 - z\n</pre> def f(x, y, z):   return x**2 + y**3 - z In\u00a0[23]: Copied! <pre>x = 2\ny = 3\nz = 4\n\nf(x, y, z)\n</pre> x = 2 y = 3 z = 4  f(x, y, z) Out[23]: <pre>27</pre> In\u00a0[24]: Copied! <pre>h = 1\n\nf(x + h, y, z)\n</pre> h = 1  f(x + h, y, z) Out[24]: <pre>32</pre> In\u00a0[25]: Copied! <pre>f(x + h, y, z) - f(x, y, z)\n</pre> f(x + h, y, z) - f(x, y, z) Out[25]: <pre>5</pre> In\u00a0[26]: Copied! <pre>(f(x + h, y, z) - f(x, y, z)) / h\n</pre> (f(x + h, y, z) - f(x, y, z)) / h Out[26]: <pre>5.0</pre> In\u00a0[27]: Copied! <pre>h = 0.00001 # change in limit\npdx = (f(x + h, y, z) - f(x, y, z)) / h\npdx\n</pre> h = 0.00001 # change in limit pdx = (f(x + h, y, z) - f(x, y, z)) / h pdx Out[27]: <pre>4.000010000027032</pre> In\u00a0[28]: Copied! <pre>assert 2*x == round(pdx)\n</pre> assert 2*x == round(pdx) <p>Partial derivative w.r.t $x$ is $2x$ (by the power rule), and for $x=2$ indeed we get $4$.</p> <p>Execise</p> <p>     Code the partial derivative w.r.t $y$ and $z$ and verify if the result correct.   </p> <p>Tip</p> <p>     It is recommended to run this notebook on Google Colab, where Graphviz is available by default. For local development, graph visualization requires both the Python wrapper (<code>pip install graphviz</code>) and the Graphviz system executable to be installed and available on the system PATH.   </p> In\u00a0[29]: Copied! <pre># This is a graph visualization code from micrograd, no need to understand the details\n# https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb\nfrom graphviz import Digraph\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n\n    for n in nodes:\n        dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n\n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> # This is a graph visualization code from micrograd, no need to understand the details # https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb from graphviz import Digraph  def trace(root):     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root, format='svg', rankdir='LR'):     \"\"\"     format: png | svg | ...     rankdir: TB (top to bottom graph) | LR (left to right)     \"\"\"     assert rankdir in ['LR', 'TB']     nodes, edges = trace(root)     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})      for n in nodes:         dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')         if n._op:             dot.node(name=str(id(n)) + n._op, label=n._op)             dot.edge(str(id(n)) + n._op, str(id(n)))      for n1, n2 in edges:         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot <p>The purpose of the <code>Value</code> class below is to store both numerical values and the history of operations that produced them. Each instance represents a node in a [computation graph](computation graph). When a <code>Value</code> object is created, it stores:</p> <ul> <li>the numerical result of a computation (<code>data</code>),</li> <li>references to the input values that produced it (<code>_prev</code>),</li> <li>the operation that combined those inputs (<code>_op</code>),</li> <li>label provided by us in order to see it on the computation graph (<code>label</code>),</li> <li>gradient of the final output w.r.t this value (<code>grad</code>).</li> </ul> <p>Arithmetic operations such as addition and multiplication are overloaded so that, instead of returning plain numbers, they return new <code>Value</code> objects. These new objects remember which values were combined and how they were combined. As computations are chained together, this process builds a directed graph, later used to propagate gradients.</p> In\u00a0[30]: Copied! <pre># Value class stores a number and \"remembers\" information about its origins\nclass Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self.grad = 0\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+')\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), \"*\")\n    return out\n\n  def __repr__(self):\n    return f\"Value(data={self.data}, grad={self.grad})\"\n</pre> # Value class stores a number and \"remembers\" information about its origins class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self.grad = 0    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+')     return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), \"*\")     return out    def __repr__(self):     return f\"Value(data={self.data}, grad={self.grad})\" <p>Below is the example of a simple expression built with the help of <code>Value</code> class: $L=c*d$ where $c=a+b$.</p> In\u00a0[31]: Copied! <pre>a = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b; c.label = 'c'\nd = Value(10, label='d')\nL = c * d; L.label = 'L'\n</pre> a = Value(5, label='a') b = Value(3, label='b') c = a + b; c.label = 'c' d = Value(10, label='d') L = c * d; L.label = 'L' In\u00a0[32]: Copied! <pre>print(a, a._prev)\nprint(L, L._prev)\n</pre> print(a, a._prev) print(L, L._prev) <pre>Value(data=5, grad=0) ()\nValue(data=80, grad=0) (Value(data=8, grad=0), Value(data=10, grad=0))\n</pre> <p>We can use the helper function built with <code>graphviz</code> above to plot our computation graph as a nice visualization.</p> In\u00a0[33]: Copied! <pre>draw_dot(c)\n</pre> draw_dot(c) Out[33]: In\u00a0[34]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[34]: <p>Gradient is vector of partial derivatives. We want to know how much changing each variable will affect the output of <code>L</code> (loss). We will store those partial derivatives inside each <code>grad</code> variable of each <code>Value</code> object. We start by noting that the derivative of a variable with respect to itself is $1$ (you get the same $dx/dy$).</p> In\u00a0[35]: Copied! <pre>L.grad = 1.0\n\nf = lambda x: x\nh = 1e-5\npdx = (f(x + h) - f(x)) / h\nassert round(pdx) == 1\n</pre> L.grad = 1.0  f = lambda x: x h = 1e-5 pdx = (f(x + h) - f(x)) / h assert round(pdx) == 1 <p>Now let's see how changing other variables will affect the eventual result.</p> In\u00a0[36]: Copied! <pre># extended version of the function we saw previously\ndef f(ha=0, hb=0, hc=0, hd=0):\n  a = Value(5 + ha, label='a')\n  b = Value(3 + hb, label='b')\n  c = a + b + Value(hc); c.label = 'c'\n  d = Value(10 + hd, label='d')\n  L = c * d; L.label = 'L'\n  return L.data\n</pre> # extended version of the function we saw previously def f(ha=0, hb=0, hc=0, hd=0):   a = Value(5 + ha, label='a')   b = Value(3 + hb, label='b')   c = a + b + Value(hc); c.label = 'c'   d = Value(10 + hd, label='d')   L = c * d; L.label = 'L'   return L.data In\u00a0[37]: Copied! <pre>h = 1e-5\n(f(hd=h) - f()) / h\n</pre> h = 1e-5 (f(hd=h) - f()) / h Out[37]: <pre>7.999999999697137</pre> <p>From the computational graph we can also know that $L=c*d$. When we change the value of $d$ just a little bit (derivative of $L$ wrt $d$) the value of $L$ will change by the amount of $c$, which is $8.0$. We saw it above in the partial derivative of a multiplication.</p> In\u00a0[38]: Copied! <pre>d.grad = c.data\nc.grad = d.data\n</pre> d.grad = c.data c.grad = d.data <p>With the same logic, the derivative of $L$ wrt $c$ will be the value of $d$, which is $10.0$ in our specific case. We can verify it.</p> In\u00a0[39]: Copied! <pre>(f(hc=h) - f()) / h\n</pre> (f(hc=h) - f()) / h Out[39]: <pre>10.000000000331966</pre> <p>To determine how much changing earlier variables in the computation graph will affect the loss, we can apply the chain rule. Simply, the derivative of $L$ wrt $a$ is the derivative of $c$ wrt $a$ multiplied by the derivative of $L$ wrt $c$:</p> <p>$$ \\frac{dL}{da} = \\frac{dL}{dc} \\cdot \\frac{dc}{da}. $$</p> <p>The derivate of $c$ both wrt $a$ and $b$ is $1.0$ due to the property of addition we had seen previously. From here:</p> In\u00a0[40]: Copied! <pre>a.grad = 1.0 * c.grad\nb.grad = 1.0 * c.grad\n\na.grad, b.grad\n</pre> a.grad = 1.0 * c.grad b.grad = 1.0 * c.grad  a.grad, b.grad Out[40]: <pre>(10.0, 10.0)</pre> <p>We can verify it as well. Let's see how much $L$ gets affected, when we shift $a$ or $b$ by a very small amount.</p> In\u00a0[41]: Copied! <pre>(f(ha=h) - f()) / h\n</pre> (f(ha=h) - f()) / h Out[41]: <pre>10.000000000331966</pre> In\u00a0[42]: Copied! <pre>(f(hb=h) - f()) / h\n</pre> (f(hb=h) - f()) / h Out[42]: <pre>10.000000000331966</pre> <p>We will finally redraw the manually updated computation graph.</p> In\u00a0[43]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[43]: <p>It basically implies that, for example, changing the value of $a$ by $1.0$ unit (from $5$ to $6$) will increase the value of $L$ by $10$ units (from $80$ to $90$).</p> In\u00a0[44]: Copied! <pre>f(ha=1), f(hb=1), f(hc=1), f(hd=1)\n</pre> f(ha=1), f(hb=1), f(hc=1), f(hd=1) Out[44]: <pre>(90, 90, 90, 88)</pre> <p>What we saw above was one backward pass done manually. We are mainly interested in the signs of partial derivatives to know if they are positively or negatively influencing the eventual loss $L$ of our model. In our case, all the derivatives are positive and influence loss positively.  We have to simply nudge the values in the opposite direction of the gradient to bring the loss down. This is known as gradient descent.</p> <p>$$ \\theta \\leftarrow \\theta - \\eta \\, \\nabla_\\theta L $$</p> <p>Here, $ \\theta $ denotes a model parameter, $ \\nabla_\\theta L $ is the gradient of the loss wrt that parameter, and $ \\eta $ is the learning rate. The learning rate controls how large each update step is during gradient descent. If the learning rate is too small, the parameters change very slowly and training takes a long time. If the learning rate is too large, the updates can overshoot the minimum and cause the loss to increase or oscillate. We will discuss learning rate in a greater detail in the future when discussing optimization.</p> In\u00a0[45]: Copied! <pre>lr = 0.01\n\na.data -= lr * a.grad\nb.data -= lr * b.grad\nd.data -= lr * d.grad\n\n# we skip c which is controlled by the values of a and b\n# pay attention that the rest are leaf nodes in the computation graph\n</pre> lr = 0.01  a.data -= lr * a.grad b.data -= lr * b.grad d.data -= lr * d.grad  # we skip c which is controlled by the values of a and b # pay attention that the rest are leaf nodes in the computation graph <p>Note</p> <p>     In case the loss is a negative value (not common), we will need to gradient ascend the loss upwards towards zero and change the sign to <code>+=</code> from <code>-=</code>. Note that the values of parameters (a, b, d) can decrease or increase depending on the sign of <code>grad</code>.   </p> <p>We will now do a single forward pass to see if loss has been decreased. Recall that the previous loss was <code>80</code>.</p> In\u00a0[46]: Copied! <pre>c = a + b\nL = c * d\n\nL.data\n</pre> c = a + b L = c * d  L.data Out[46]: <pre>77.376</pre> <p>It seems like we optimized our values and brought down the loss.</p> <p>Manually calculating gradient is good only for educational purposes. We should implement automatic backward pass which will calculate gradients. To support this, we will rewrite our <code>Value</code> class to store a <code>_backward</code> function. This function will enforce the local gradient associated with the operation that produced the value. Initially, <code>_backward</code> does nothing.</p> <p>For addition, the local derivatives are: $ \\frac{\\partial c}{\\partial a} = 1 $ and $ \\frac{\\partial c}{\\partial b} = 1 $. Hence, the gradient flowing into $c$ (stored in <code>out.grad</code>) is passed unchanged to both inputs:</p> <ul> <li><code>a.grad += out.grad</code></li> <li><code>b.grad += out.grad</code></li> </ul> <p>This is why, in the <code>_backward</code> function for addition, both operands receive the same gradient contribution.</p> <p>For multiplication, the local derivatives are: $ \\frac{\\partial c}{\\partial a} = b $ and $ \\frac{\\partial c}{\\partial b} = a $. During backpropagation, the incoming gradient is scaled by the opposite operand:</p> <ul> <li><code>a.grad += b.data * out.grad</code></li> <li><code>b.grad += a.data * out.grad</code></li> </ul> <p>This reflects the chain rule: each variable\u2019s gradient depends on how the output changes with respect to that variable. By storing a <code>_backward</code> function at every node and calling these functions in reverse topological order, the full gradient of the loss with respect to all intermediate values can be computed automatically.</p> In\u00a0[47]: Copied! <pre>class Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self.grad = 0.0\n    self._backward = lambda: None # initially it is a function which does nothing\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+')\n\n    def _backward():\n      self.grad = 1.0 * out.grad\n      other.grad = 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), \"*\")\n\n    def _backward():\n      self.grad = other.data * out.grad\n      other.grad = self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __repr__(self):\n    return f\"Value(data={self.data}, grad={self.grad})\"\n</pre> class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self.grad = 0.0     self._backward = lambda: None # initially it is a function which does nothing    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+')      def _backward():       self.grad = 1.0 * out.grad       other.grad = 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), \"*\")      def _backward():       self.grad = other.data * out.grad       other.grad = self.data * out.grad     out._backward = _backward      return out    def __repr__(self):     return f\"Value(data={self.data}, grad={self.grad})\" In\u00a0[48]: Copied! <pre># Recreating the same function\na = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b; c.label = 'c'\nd = Value(10, label='d')\nL = c * d; L.label = 'L'\n</pre> # Recreating the same function a = Value(5, label='a') b = Value(3, label='b') c = a + b; c.label = 'c' d = Value(10, label='d') L = c * d; L.label = 'L' In\u00a0[49]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[49]: <p>We should not forget to initialize the gradient of the loss to be $1.0$ and then call <code>backward</code> function. With correct implementation, we will get the same results which we manually calculated previously.</p> In\u00a0[50]: Copied! <pre>L.grad = 1.0\nL._backward()\nc._backward()\n</pre> L.grad = 1.0 L._backward() c._backward() In\u00a0[51]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[51]: <p>Exercise</p> <p>     Make sure that all operations and their partial derivatives can be calculated (e.g. division, power).   </p> In\u00a0[52]: Copied! <pre># optimization\nlr = 0.05\na.data -= lr * a.grad\nb.data -= lr * b.grad\nd.data -= lr * d.grad\n\n# forward pass\nc = a + b\nL = c * d\n\n# backward pass\nL.grad = 1.0\nL._backward()\nc._backward()\n\nL.data # loss\n</pre> # optimization lr = 0.05 a.data -= lr * a.grad b.data -= lr * b.grad d.data -= lr * d.grad  # forward pass c = a + b L = c * d  # backward pass L.grad = 1.0 L._backward() c._backward()  L.data # loss Out[52]: <pre>67.2</pre> <p>We have now trained the model for a single epoch. Even though what we do is oversimplistic and not precise, the main intuition and concepts behind training a neural network will be the same. We will now train the model for multiple epochs until we reduce the loss down to zero.</p> In\u00a0[53]: Copied! <pre>while True:\n  # optimization\n  a.data -= lr * a.grad\n  b.data -= lr * b.grad\n  d.data -= lr * d.grad\n\n  # forward pass\n  c = a + b\n  L = c * d\n\n  # backward pass\n  L.grad = 1.0\n  L._backward()\n  c._backward()\n\n  if L.data &lt; 0:\n    break\n\n  print(f'Loss: {round(L.data,2)}')\n</pre> while True:   # optimization   a.data -= lr * a.grad   b.data -= lr * b.grad   d.data -= lr * d.grad    # forward pass   c = a + b   L = c * d    # backward pass   L.grad = 1.0   L._backward()   c._backward()    if L.data &lt; 0:     break    print(f'Loss: {round(L.data,2)}') <pre>Loss: 55.87\nLoss: 45.77\nLoss: 36.68\nLoss: 28.42\nLoss: 20.81\nLoss: 13.69\nLoss: 6.91\nLoss: 0.34\n</pre> In\u00a0[54]: Copied! <pre>import torch\n\na = torch.tensor(5.0);    a.requires_grad = True\nb = torch.tensor(3.0);    b.requires_grad = True\nc = a + b\nd = torch.tensor(10.0);   d.requires_grad = True\nL = c * d\n</pre> import torch  a = torch.tensor(5.0);    a.requires_grad = True b = torch.tensor(3.0);    b.requires_grad = True c = a + b d = torch.tensor(10.0);   d.requires_grad = True L = c * d In\u00a0[55]: Copied! <pre>L.backward()\n</pre> L.backward() In\u00a0[56]: Copied! <pre>a.grad, b.grad, d.grad\n</pre> a.grad, b.grad, d.grad Out[56]: <pre>(tensor(10.), tensor(10.), tensor(8.))</pre> <p>This notebook introduced the core ideas behind gradient-based learning. It began with differentiation and extended the concept to partial derivatives for multivariable functions. Computation graphs were then introduced to represent how values depend on one another and to make gradient flow explicit. Using this structure, the gradient and the chain rule were used to explain how gradients were propagated backward from the loss to earlier variables. Gradient descent was presented as the mechanism for updating parameters in the direction that reduces the loss. The roles of the forward pass and backward pass were clarified and combined to describe model training with backpropagation. Finally, these ideas were demonstrated through a PyTorch implementation using automatic differentiation. In the next notebook, a neural network will be trained using a more advanced training engine.</p>"},{"location":"notebooks/01_backprop/#01-from-derivatives-to-backpropagation","title":"01. From Derivatives to Backpropagation\u00b6","text":"25 Jan 2025 \u00b7   19 Jan 2026 \u00b7   18 min <p>Tip</p> <p>      It will be helpful to revise the necessary Calculus needed for DL for understanding the material that follows.   </p>"},{"location":"notebooks/01_backprop/#differentiation","title":"Differentiation\u00b6","text":"<p>The code below shows the function $f(x) = x^2$ in action. Our goal is to see how much the output changes as we modify the input by some value h. You can modify and see behavior for more values on other functions as well.</p>"},{"location":"notebooks/01_backprop/#partial-derivatives","title":"Partial Derivatives\u00b6","text":""},{"location":"notebooks/01_backprop/#computation-graph","title":"Computation Graph\u00b6","text":"<p>Info</p> <p>The following source was used in preparing this material: Andrej Karpathy's lecture on Micrograd.</p> <p>Micrograd is an educational library that demonstrates the core ideas behind automatic differentiation. The idea behind it is the same one used in major DL frameworks. <code>Micrograd</code> closely mirrors the logic of the PyTorch autograd engine. In this and the following notebooks, we will modify <code>micrograd</code> further to improve correspondence with PyTorch and make the explanations clearer.</p>"},{"location":"notebooks/01_backprop/#gradient","title":"Gradient\u00b6","text":""},{"location":"notebooks/01_backprop/#chain-rule","title":"Chain Rule\u00b6","text":""},{"location":"notebooks/01_backprop/#gradient-descent","title":"Gradient Descent\u00b6","text":""},{"location":"notebooks/01_backprop/#forward-pass","title":"Forward Pass\u00b6","text":""},{"location":"notebooks/01_backprop/#backward-pass","title":"Backward Pass\u00b6","text":""},{"location":"notebooks/01_backprop/#model-training-with-backpropagation","title":"Model Training with Backpropagation\u00b6","text":"<p>We can now train the model by repeatedly performing a forward pass, a backward pass, and an optimization (gradient descent) step to reduce the loss. The backward pass, which computes gradients by propagating them through the whole computation graph, is called backpropagation.</p>"},{"location":"notebooks/01_backprop/#pytorch-implementation","title":"PyTorch Implementation\u00b6","text":"<p>Tip</p> <p> PyTorch is preinstalled and configured for Google Colab. For local development, PyTorch can be installed via <code>pip install torch</code>. If you plan to use a GPU, make sure the installed PyTorch version matches your CUDA setup.   </p> <p>As mentioned in the beginning, our manual implementation is built into PyTorch. We will do a forward and backward pass with the help of autograd engine and check if the gradients are what we had previosuly calculated. As gradients need not to be calculated for, say, leaf nodes, to optimizate calculation further, <code>requires_grad</code> is set to <code>False</code> by default, which we need to update.</p>"},{"location":"notebooks/02_neural_network/","title":"02. From Neuron to Neural Network","text":"In\u00a0[1]: Copied! <pre># This is a graph visualization code from micrograd, no need to understand the details\n# https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb\nfrom graphviz import Digraph\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n\n    for n in nodes:\n        dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n\n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> # This is a graph visualization code from micrograd, no need to understand the details # https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb from graphviz import Digraph  def trace(root):     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root, format='svg', rankdir='LR'):     \"\"\"     format: png | svg | ...     rankdir: TB (top to bottom graph) | LR (left to right)     \"\"\"     assert rankdir in ['LR', 'TB']     nodes, edges = trace(root)     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})      for n in nodes:         dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')         if n._op:             dot.node(name=str(id(n)) + n._op, label=n._op)             dot.edge(str(id(n)) + n._op, str(id(n)))      for n1, n2 in edges:         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot In\u00a0[2]: Copied! <pre># This is the code from previous lecture\n# It is important to understand _backward function\nclass Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self._backward = lambda: None\n    self.grad = 0.0\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other),'+')\n\n    def _backward():\n      self.grad = 1.0 * out.grad\n      other.grad = 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other),'*')\n\n    def _backward():\n      self.grad = other.data * out.grad\n      other.grad = self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __sub__(self, other):\n    return self + (Value(-1) * other) # self + (-other)\n\n  def __repr__(self):\n    return f'{self.label}: {self.data}'\n</pre> # This is the code from previous lecture # It is important to understand _backward function class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self._backward = lambda: None     self.grad = 0.0    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other),'+')      def _backward():       self.grad = 1.0 * out.grad       other.grad = 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other),'*')      def _backward():       self.grad = other.data * out.grad       other.grad = self.data * out.grad     out._backward = _backward      return out    def __sub__(self, other):     return self + (Value(-1) * other) # self + (-other)    def __repr__(self):     return f'{self.label}: {self.data}' In\u00a0[3]: Copied! <pre>a = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b;   c.label = 'c'\nd = Value(10, label='d')\nL = c * d;   L.label = 'L'\n</pre> a = Value(5, label='a') b = Value(3, label='b') c = a + b;   c.label = 'c' d = Value(10, label='d') L = c * d;   L.label = 'L' In\u00a0[4]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[4]: In\u00a0[5]: Copied! <pre>epochs = 10\nlearning_rate = 0.01\n\nfor _ in range(epochs):\n  L.grad = 1.0\n\n  # backward pass\n  L._backward()\n  c._backward()\n\n  # optimization (gradient descent)\n  a.data -= learning_rate * a.grad\n  b.data -= learning_rate * b.grad\n  d.data -= learning_rate * d.grad\n\n  # forward pass\n  c = a + b\n  L = c * d\n\n  print(f'Loss: {L.data:.2f}')\n</pre> epochs = 10 learning_rate = 0.01  for _ in range(epochs):   L.grad = 1.0    # backward pass   L._backward()   c._backward()    # optimization (gradient descent)   a.data -= learning_rate * a.grad   b.data -= learning_rate * b.grad   d.data -= learning_rate * d.grad    # forward pass   c = a + b   L = c * d    print(f'Loss: {L.data:.2f}') <pre>Loss: 77.38\nLoss: 74.81\nLoss: 72.31\nLoss: 69.87\nLoss: 67.49\nLoss: 65.16\nLoss: 62.88\nLoss: 60.66\nLoss: 58.48\nLoss: 56.35\n</pre> In\u00a0[6]: Copied! <pre># Equivalent implementation in PyTorch\n# pay attention to requires_grad, no_grad() and zero_()\n\nimport torch\n\na = torch.tensor(5.0, requires_grad = True);\nb = torch.tensor(3.0, requires_grad = True);\nc = a + b\nd = torch.tensor(10.0,requires_grad = True);\nL = c * d\n\nfor _ in range(epochs):\n  # backward pass\n  L.backward()\n\n  # optimization (gradient descent)\n  with torch.no_grad():\n    a -= learning_rate * a.grad\n    b -= learning_rate * b.grad\n    d -= learning_rate * d.grad\n\n  # avoids accumulating gradients\n  # comment this out to see how it affects the learning\n  a.grad.zero_()\n  b.grad.zero_()\n  d.grad.zero_()\n\n  # forward pass\n  c = a + b\n  L = c * d\n\n  print(f'Loss: {L.data:.2f}')\n</pre> # Equivalent implementation in PyTorch # pay attention to requires_grad, no_grad() and zero_()  import torch  a = torch.tensor(5.0, requires_grad = True); b = torch.tensor(3.0, requires_grad = True); c = a + b d = torch.tensor(10.0,requires_grad = True); L = c * d  for _ in range(epochs):   # backward pass   L.backward()    # optimization (gradient descent)   with torch.no_grad():     a -= learning_rate * a.grad     b -= learning_rate * b.grad     d -= learning_rate * d.grad    # avoids accumulating gradients   # comment this out to see how it affects the learning   a.grad.zero_()   b.grad.zero_()   d.grad.zero_()    # forward pass   c = a + b   L = c * d    print(f'Loss: {L.data:.2f}') <pre>Loss: 77.38\nLoss: 74.81\nLoss: 72.31\nLoss: 69.87\nLoss: 67.49\nLoss: 65.16\nLoss: 62.88\nLoss: 60.66\nLoss: 58.48\nLoss: 56.35\n</pre> <p>The function <code>f(x) = x * w</code> is a linear function always passing from origin. The real world data, however, will be much more complex, and in order to describe a pattern in the data our Machine Learning model should return a more flexible function. For that, we will do two things: add bias <code>b</code> and bring non-linearity with an activation function. We can choose different non-linear activation functions with the condition that it should be differentiable (otherwise we won't be able to calculate gradients for backpropagation). We will implement <code>sigmoid</code>(logistic) activation function which has the following formula:</p> <p></p> <p>Sigmoid function not only makes a linear function non-linear and continuous, but also maps any value of <code>x</code> to be between 0 and 1. It may be useful when we want to predict probabilities for different output classes.</p> In\u00a0[7]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[8]: Copied! <pre>def sigmoid(x):\n  return 1.0 / (1 + np.exp(-x)) # see formula above\n</pre> def sigmoid(x):   return 1.0 / (1 + np.exp(-x)) # see formula above In\u00a0[9]: Copied! <pre># a simple linear function where activation will be applied\ndef f(x, w=0.5, b=10, activation=None):\n  out = x * w + b\n  return activation(out) if activation else out\n</pre> # a simple linear function where activation will be applied def f(x, w=0.5, b=10, activation=None):   out = x * w + b   return activation(out) if activation else out In\u00a0[10]: Copied! <pre>def plot(f, x, activation=None):\n  plt.figure(figsize=(4, 4))\n  x_all = np.linspace(-50, 50, 100)\n  y_all = f(x_all, activation=activation)\n  plt.plot(x_all, y_all)\n  plt.scatter(x, f(x, activation=activation), color='r')\n  plt.show()\n</pre> def plot(f, x, activation=None):   plt.figure(figsize=(4, 4))   x_all = np.linspace(-50, 50, 100)   y_all = f(x_all, activation=activation)   plt.plot(x_all, y_all)   plt.scatter(x, f(x, activation=activation), color='r')   plt.show() In\u00a0[11]: Copied! <pre>x = -20\nplot(f, x)\n</pre> x = -20 plot(f, x) <p>Now we will plot the exact same point mapped into the non-linear function between <code>0</code> and <code>1</code>. Try out different <code>x</code> values and see the plots.</p> In\u00a0[12]: Copied! <pre>plot(f, x, sigmoid)\n</pre> plot(f, x, sigmoid) <p>Exercise: Implement other activation functions (e.g. <code>tanh</code>, <code>relu</code>) and see the plot.</p> <p>Exercise: What could be the distadvantage of using sigmoid activation function?</p> <p>Funcs - Own work (CC0 | Wikimedia Commons)</p> <p></p> <p>An artificial neuron is simply a linear function passing through an activation function  (e.g. <code>sigmoid(x * w + b)</code>). The illustration above describes an N-dimensional neuron, accepting inputs between <code>x<sub>1</sub> ... x<sub>n</sub></code>. The function <code>f</code> we had above is a very simple neuron with 1-dimensional input.</p> <p>Question: What could be input values for predicting the probability of a customer cancelling their subscription?</p> <p>Before creating our neuron, we will first make some updates to the <code>Value</code> class.</p> <p>Not only value class should have a <code>sigmoid(x)</code> function, but also it should be able to calculate a derivative for it.</p> <p>Exercise: Find the derivative of the <code>sigmoid</code> function:</p> <p></p> <p>The <code>requires_grad</code> flag (similar to PyTorch) will tell which parameters are trainable and requires gradient calculation and update (Note that this feature is not implemented in <code>micrograd</code>).</p> <p>For example, it doesn't make sense to modify the real-life training inputs <code>x1</code> and <code>x2</code> for our neuron. We shouldn't spend resources for calculating unnecessary gradients. Our goal is to nudge only the weight and bias (i.e. parameter) values, as well as the nodes dependent on them, in order to minimize the eventual loss.</p> In\u00a0[13]: Copied! <pre>class Value:\n  def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self._backward = lambda: None\n    self.grad = 0.0\n    self.requires_grad = requires_grad\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad = 1.0 * out.grad\n      if other.requires_grad:\n        other.grad = 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad = other.data * out.grad\n      if other.requires_grad:\n        other.grad = self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __sub__(self, other):\n    return self + (Value(-1) * other) # self + (-other)\n\n  def sigmoid(self):\n    s = 1.0 / (1 + np.exp(-self.data))\n    out = Value(s, (self, ), 'sigmoid', self.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad = s * (1.0 - s) * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __repr__(self):\n    return f'Value({self.data:.4f})'\n</pre> class Value:   def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self._backward = lambda: None     self.grad = 0.0     self.requires_grad = requires_grad    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad = 1.0 * out.grad       if other.requires_grad:         other.grad = 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad = other.data * out.grad       if other.requires_grad:         other.grad = self.data * out.grad     out._backward = _backward      return out    def __sub__(self, other):     return self + (Value(-1) * other) # self + (-other)    def sigmoid(self):     s = 1.0 / (1 + np.exp(-self.data))     out = Value(s, (self, ), 'sigmoid', self.requires_grad)      def _backward():       if self.requires_grad:         self.grad = s * (1.0 - s) * out.grad     out._backward = _backward      return out    def __repr__(self):     return f'Value({self.data:.4f})' <p>We will initially implement a simple <code>Neuron</code> class in 3D (2-dimensional input values and an output value). The function will have two inputs <code>x1</code> and <code>x2</code>, which will become <code>Value</code> objects. Their weights <code>w1</code> and <code>w2</code> will determine how much input (e.g. age of a customer) influences outcome.</p> In\u00a0[14]: Copied! <pre>from mpl_toolkits.mplot3d import Axes3D\n\nclass Neuron:\n  def __init__(self):\n    self.w1 = Value(np.random.uniform(-1, 1), label='w1', requires_grad=True)\n    self.w2 = Value(np.random.uniform(-1, 1), label='w2', requires_grad=True)\n    self.b = Value(0, label='b', requires_grad=True)\n\n  def __call__(self, x1, x2):\n    out = x1 * self.w1 + x2 * self.w2 + self.b\n    return out.sigmoid()\n\n  # this code here is for plotting, no need to understand, works for only 3D\n  def plot(self):\n    x1_vals = np.linspace(-5, 5, 100)\n    x2_vals = np.linspace(-5, 5, 100)\n    X1, X2 = np.meshgrid(x1_vals, x2_vals)\n    Z = np.zeros_like(X1)\n\n    for i in range(X1.shape[0]):\n      for j in range(X1.shape[1]):\n        x1 = Value(X1[i, j])\n        x2 = Value(X2[i, j])\n        output = self(x1, x2)\n        Z[i, j] = output.data\n\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot_surface(X1, X2, Z, cmap='viridis')\n    ax.set_title(f'Neuron Output with Sigmoid Activation)')\n    plt.show()\n</pre> from mpl_toolkits.mplot3d import Axes3D  class Neuron:   def __init__(self):     self.w1 = Value(np.random.uniform(-1, 1), label='w1', requires_grad=True)     self.w2 = Value(np.random.uniform(-1, 1), label='w2', requires_grad=True)     self.b = Value(0, label='b', requires_grad=True)    def __call__(self, x1, x2):     out = x1 * self.w1 + x2 * self.w2 + self.b     return out.sigmoid()    # this code here is for plotting, no need to understand, works for only 3D   def plot(self):     x1_vals = np.linspace(-5, 5, 100)     x2_vals = np.linspace(-5, 5, 100)     X1, X2 = np.meshgrid(x1_vals, x2_vals)     Z = np.zeros_like(X1)      for i in range(X1.shape[0]):       for j in range(X1.shape[1]):         x1 = Value(X1[i, j])         x2 = Value(X2[i, j])         output = self(x1, x2)         Z[i, j] = output.data      fig = plt.figure(figsize=(10, 8))     ax = fig.add_subplot(111, projection='3d')     ax.plot_surface(X1, X2, Z, cmap='viridis')     ax.set_title(f'Neuron Output with Sigmoid Activation)')     plt.show() <p>Now we can initialize our inputs and neuron to see our computation graph. Our loss will be simple: the ground truth label <code>y</code> minus the predicted probability. Let's assume that, our input values <code>x1</code> and <code>x2</code> correspond to a customer who made the purchase (<code>y = 1</code>). We will try out both activation functions and see their plots.</p> In\u00a0[15]: Copied! <pre>x1 = Value(2, label='x1')\nx2 = Value(3, label='x2')\ny  = Value(1, label= 'y')\n\nn = Neuron()\n\npred = n(x1, x2);      pred.label = 'pred'\nL = y - pred;          L.label = 'loss'\n</pre> x1 = Value(2, label='x1') x2 = Value(3, label='x2') y  = Value(1, label= 'y')  n = Neuron()  pred = n(x1, x2);      pred.label = 'pred' L = y - pred;          L.label = 'loss' In\u00a0[16]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[16]: In\u00a0[17]: Copied! <pre>n.plot()\n</pre> n.plot() <p>The ground truth label <code>1</code> tells us that we should push our probability towards <code>1.0</code>. In other words, as our loss <code>L</code> here is a simple error value corresponding to <code>1 - prob</code>, we should try to minimize the loss down to zero with backpropagation. However, our computation graph is bigger than how it was before. The <code>_backward</code> pass function we call manually on each node is not scalable. Ideally, we should have a single function <code>backward()</code> to calculate all the gradients, which we previously saw in the PyTorch implementation. For that, we will need to sort the nodes of the computation graph (in this case, from input/weight nodes until the probability node). We can achieve that with a topological sort function implemented for micrograd.</p> In\u00a0[18]: Copied! <pre>topo = []\nvisited = set()\ndef build_topo(v):\n  if v not in visited:\n    visited.add(v)\n    for child in v._prev:\n      build_topo(child)\n    topo.append(v)\nbuild_topo(pred)\ntopo\n</pre> topo = [] visited = set() def build_topo(v):   if v not in visited:     visited.add(v)     for child in v._prev:       build_topo(child)     topo.append(v) build_topo(pred) topo Out[18]: <pre>[Value(2.0000),\n Value(-0.7886),\n Value(-1.5772),\n Value(3.0000),\n Value(0.0694),\n Value(0.2081),\n Value(-1.3691),\n Value(0.0000),\n Value(-1.3691),\n Value(0.2028)]</pre> <p>We will integrate topological sort into our <code>Value</code> object and implement complete backward pass. We can also add a simple gradient descent function <code>optimize()</code> which will use this topology. Finally, instead of overriding gradients (<code>=</code>), we will accumulate them (<code>+=</code>) to avoid gradient update bugs when using the same node more than once in an operation. And as a consequence, we will have to reset gradients with <code>zero_()</code> (similar to PyTorch) so that the gradients of different backward passes will not affect each other (it does the exact same thing as <code>self.grad = 0.0</code> was doing before gradient accumulation). Although, to be precise, <code>zero_()</code> function should reset only the gradient of <code>self</code>, and it is actually a function called <code>zero_grad()</code> of <code>optimizer</code> in PyTorch which resets gradients accross all nodes.</p> In\u00a0[\u00a0]: Copied! <pre>class Value:\n  def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self._backward = lambda: None\n    self.grad = 0.0\n    self.requires_grad = requires_grad\n    self.topo = self.build_topo()\n    self.params = [node for node in self.topo if node.requires_grad]\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad += 1.0 * out.grad\n      if other.requires_grad:\n        other.grad += 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad += other.data * out.grad\n      if other.requires_grad:\n        other.grad += self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __sub__(self, other):\n    return self + (Value(-1) * other) # self + (-other)\n\n  def sigmoid(self):\n    s = 1.0 / (1 + np.exp(-self.data))\n    out = Value(s, (self, ), 'sigmoid', self.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad += s * (1.0 - s) * out.grad\n    out._backward = _backward\n\n    return out\n\n  def build_topo(self):\n    # topological order all of the children in the graph\n    topo = []\n    visited = set()\n\n    def _build_topo(node):\n      if node not in visited:\n        visited.add(node)\n        for child in node._prev:\n          _build_topo(child)\n        topo.append(node)\n    _build_topo(self)\n\n    return topo\n\n  def backward(self):\n    if self.requires_grad:\n      self.grad = 1.0\n      for node in reversed(self.params):\n        node._backward()\n\n  def optimize(self, learning_rate=0.01):\n    for node in self.params:\n      node.data -= learning_rate * node.grad\n\n  def zero_(self):\n    self.grad = 0.0\n\n  def zero_grad(self):\n    for node in self.params:\n      node.grad = 0.0\n\n  def __repr__(self):\n    return f'Value({self.data})'\n</pre> class Value:   def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self._backward = lambda: None     self.grad = 0.0     self.requires_grad = requires_grad     self.topo = self.build_topo()     self.params = [node for node in self.topo if node.requires_grad]    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad += 1.0 * out.grad       if other.requires_grad:         other.grad += 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad += other.data * out.grad       if other.requires_grad:         other.grad += self.data * out.grad     out._backward = _backward      return out    def __sub__(self, other):     return self + (Value(-1) * other) # self + (-other)    def sigmoid(self):     s = 1.0 / (1 + np.exp(-self.data))     out = Value(s, (self, ), 'sigmoid', self.requires_grad)      def _backward():       if self.requires_grad:         self.grad += s * (1.0 - s) * out.grad     out._backward = _backward      return out    def build_topo(self):     # topological order all of the children in the graph     topo = []     visited = set()      def _build_topo(node):       if node not in visited:         visited.add(node)         for child in node._prev:           _build_topo(child)         topo.append(node)     _build_topo(self)      return topo    def backward(self):     if self.requires_grad:       self.grad = 1.0       for node in reversed(self.params):         node._backward()    def optimize(self, learning_rate=0.01):     for node in self.params:       node.data -= learning_rate * node.grad    def zero_(self):     self.grad = 0.0    def zero_grad(self):     for node in self.params:       node.grad = 0.0    def __repr__(self):     return f'Value({self.data})' <p>In addition to the <code>_backward()</code> function which calculated the derivatives for only immediate previous nodes, we now have the <code>backward()</code> function which calculates derivates for all the nodes (we also won't forget to set the gradient to <code>1.0</code> in the beginning). Once we plot the graph, pay attention that the input and leaf node gradients which we have no control over are not calculated, thanks to <code>requires_grad</code>.</p> In\u00a0[20]: Copied! <pre>x1 = Value(2, label='x1')\nx2 = Value(5, label='x2')\ny  = Value(1, label= 'y')\n\nn = Neuron()\n\npred = n(x1, x2);      pred.label = 'pred'\nL = y - pred;          L.label = 'loss'\n</pre> x1 = Value(2, label='x1') x2 = Value(5, label='x2') y  = Value(1, label= 'y')  n = Neuron()  pred = n(x1, x2);      pred.label = 'pred' L = y - pred;          L.label = 'loss' In\u00a0[21]: Copied! <pre>L.backward()\ndraw_dot(L)\n</pre> L.backward() draw_dot(L) Out[21]: <p>We can finally implement complete backpropogatation with the goal of increasing the final probability to <code>1.0</code> (decreasing the loss down to zero).</p> In\u00a0[22]: Copied! <pre># gradient descent\nL.optimize()\n\n# forward pass\npred = n(x1, x2);      pred.label = 'pred'\nL = y - pred;          L.label = 'loss'\n\ndraw_dot(L)\n</pre> # gradient descent L.optimize()  # forward pass pred = n(x1, x2);      pred.label = 'pred' L = y - pred;          L.label = 'loss'  draw_dot(L) Out[22]: <p>Let's repeat the backpropagation in multiple epochs until we achieve a minimal loss. We will also print the parameters to see when our neuron function returns a maximum probability for the given input values. And we will make sure to not forget to reset the gradients.</p> In\u00a0[\u00a0]: Copied! <pre>while True:\n  L.zero_grad()\n\n  # backward pass\n  L.backward()\n\n  # gradient descent\n  L.optimize()\n\n  # forward pass\n  pred = n(x1, x2);\n  L = y - pred;\n\n  print(f'Loss {L.data:.4f}')\n\n  if L.data &lt; 0.01:\n    print(f'\\nInputs: {x1} {x2}')\n    print(f'Parameters: {n.w1} {n.w2} {n.b}')\n    print(f'Prediction Probability: {pred.data}')\n    break\n</pre> while True:   L.zero_grad()    # backward pass   L.backward()    # gradient descent   L.optimize()    # forward pass   pred = n(x1, x2);   L = y - pred;    print(f'Loss {L.data:.4f}')    if L.data &lt; 0.01:     print(f'\\nInputs: {x1} {x2}')     print(f'Parameters: {n.w1} {n.w2} {n.b}')     print(f'Prediction Probability: {pred.data}')     break <pre>Loss 0.0148\nLoss 0.0148\nLoss 0.0147\nLoss 0.0147\nLoss 0.0146\nLoss 0.0145\nLoss 0.0145\nLoss 0.0144\nLoss 0.0144\nLoss 0.0143\nLoss 0.0142\nLoss 0.0142\nLoss 0.0141\nLoss 0.0141\nLoss 0.0140\nLoss 0.0139\nLoss 0.0139\nLoss 0.0138\nLoss 0.0138\nLoss 0.0137\nLoss 0.0137\nLoss 0.0136\nLoss 0.0136\nLoss 0.0135\nLoss 0.0134\nLoss 0.0134\nLoss 0.0133\nLoss 0.0133\nLoss 0.0132\nLoss 0.0132\nLoss 0.0131\nLoss 0.0131\nLoss 0.0130\nLoss 0.0130\nLoss 0.0129\nLoss 0.0129\nLoss 0.0128\nLoss 0.0128\nLoss 0.0127\nLoss 0.0127\nLoss 0.0127\nLoss 0.0126\nLoss 0.0126\nLoss 0.0125\nLoss 0.0125\nLoss 0.0124\nLoss 0.0124\nLoss 0.0123\nLoss 0.0123\nLoss 0.0122\nLoss 0.0122\nLoss 0.0122\nLoss 0.0121\nLoss 0.0121\nLoss 0.0120\nLoss 0.0120\nLoss 0.0119\nLoss 0.0119\nLoss 0.0119\nLoss 0.0118\nLoss 0.0118\nLoss 0.0117\nLoss 0.0117\nLoss 0.0117\nLoss 0.0116\nLoss 0.0116\nLoss 0.0115\nLoss 0.0115\nLoss 0.0115\nLoss 0.0114\nLoss 0.0114\nLoss 0.0113\nLoss 0.0113\nLoss 0.0113\nLoss 0.0112\nLoss 0.0112\nLoss 0.0112\nLoss 0.0111\nLoss 0.0111\nLoss 0.0111\nLoss 0.0110\nLoss 0.0110\nLoss 0.0109\nLoss 0.0109\nLoss 0.0109\nLoss 0.0108\nLoss 0.0108\nLoss 0.0108\nLoss 0.0107\nLoss 0.0107\nLoss 0.0107\nLoss 0.0106\nLoss 0.0106\nLoss 0.0106\nLoss 0.0105\nLoss 0.0105\nLoss 0.0105\nLoss 0.0104\nLoss 0.0104\nLoss 0.0104\nLoss 0.0103\nLoss 0.0103\nLoss 0.0103\nLoss 0.0103\nLoss 0.0102\nLoss 0.0102\nLoss 0.0102\nLoss 0.0101\nLoss 0.0101\nLoss 0.0101\nLoss 0.0100\nLoss 0.0100\nLoss 0.0100\n\nInputs: Value(2) Value(5)\nParameters: Value(0.15289672859342332) Value(0.8555124993367387) Value(0.013701999952082168)\nPrediction Probability: 0.9900191690162561\n</pre> <p>We have just now trained our 2-dimensional input neuron to find suitable parameter values for achieving a maximum probability for input values <code>x1</code> and <code>x2</code>. Now we would like to create N-dimensional neuron which will accept much more inputs, similar to what we saw in the illustration of artificial neuron: <code>x<sub>1</sub> ... x<sub>n</sub></code>. As a consequence, our neuron will have to learn the parameter values for N-dimensional weights <code>w<sub>1</sub> ... w<sub>n</sub></code>.</p> In\u00a0[24]: Copied! <pre>class Neuron:\n  def __init__(self, N):\n    self.W = [Value(np.random.uniform(-1, 1), label=f'w{i}', requires_grad=True) for i in range(N)]\n    self.b = Value(0, label='b', requires_grad=True)\n\n  def __call__(self, X):\n    out = sum((x * w for x, w in zip(X, self.W)), self.b)\n    return out.sigmoid()\n</pre> class Neuron:   def __init__(self, N):     self.W = [Value(np.random.uniform(-1, 1), label=f'w{i}', requires_grad=True) for i in range(N)]     self.b = Value(0, label='b', requires_grad=True)    def __call__(self, X):     out = sum((x * w for x, w in zip(X, self.W)), self.b)     return out.sigmoid() <p>We will now see the training output of our N-dimensional neuron which will accept N <code>Value</code> inputs as a list. Note that our <code>Neuron</code> which implements <code>sigmoid</code> (logistic) activation is known as Logistic Regression.</p> In\u00a0[25]: Copied! <pre>X = [Value(x, label=f'x{i}') for i, x in enumerate([5, 0.4, -1, -2])]\n\nn = Neuron(len(X))\n\npred = n(X);           pred.label = 'pred'\nL = y - pred;          L.label = 'loss'\n</pre> X = [Value(x, label=f'x{i}') for i, x in enumerate([5, 0.4, -1, -2])]  n = Neuron(len(X))  pred = n(X);           pred.label = 'pred' L = y - pred;          L.label = 'loss' In\u00a0[26]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[26]: In\u00a0[\u00a0]: Copied! <pre>while True:\n  L.zero_grad()\n\n  # backward pass\n  L.backward()\n\n  # gradient descent\n  L.optimize()\n\n  # forward pass\n  pred = n(X)\n  L = y - pred\n\n  print(f'Loss {L.data:.4f}')\n\n  if L.data &lt; 0.01:\n    print(f'\\nInputs: {X}')\n    print(f'Parameters: {n.W} {n.b}')\n    print(f'Prediction Probability: {pred.data}')\n    break\n</pre> while True:   L.zero_grad()    # backward pass   L.backward()    # gradient descent   L.optimize()    # forward pass   pred = n(X)   L = y - pred    print(f'Loss {L.data:.4f}')    if L.data &lt; 0.01:     print(f'\\nInputs: {X}')     print(f'Parameters: {n.W} {n.b}')     print(f'Prediction Probability: {pred.data}')     break <pre>Loss 0.1273\nLoss 0.1235\nLoss 0.1199\nLoss 0.1165\nLoss 0.1132\nLoss 0.1101\nLoss 0.1072\nLoss 0.1043\nLoss 0.1016\nLoss 0.0991\nLoss 0.0966\nLoss 0.0943\nLoss 0.0920\nLoss 0.0899\nLoss 0.0878\nLoss 0.0858\nLoss 0.0839\nLoss 0.0821\nLoss 0.0804\nLoss 0.0787\nLoss 0.0770\nLoss 0.0755\nLoss 0.0740\nLoss 0.0725\nLoss 0.0711\nLoss 0.0698\nLoss 0.0685\nLoss 0.0672\nLoss 0.0660\nLoss 0.0648\nLoss 0.0637\nLoss 0.0626\nLoss 0.0615\nLoss 0.0605\nLoss 0.0595\nLoss 0.0585\nLoss 0.0576\nLoss 0.0567\nLoss 0.0558\nLoss 0.0549\nLoss 0.0541\nLoss 0.0533\nLoss 0.0525\nLoss 0.0517\nLoss 0.0510\nLoss 0.0503\nLoss 0.0496\nLoss 0.0489\nLoss 0.0482\nLoss 0.0476\nLoss 0.0469\nLoss 0.0463\nLoss 0.0457\nLoss 0.0451\nLoss 0.0445\nLoss 0.0440\nLoss 0.0434\nLoss 0.0429\nLoss 0.0424\nLoss 0.0419\nLoss 0.0414\nLoss 0.0409\nLoss 0.0404\nLoss 0.0399\nLoss 0.0395\nLoss 0.0390\nLoss 0.0386\nLoss 0.0382\nLoss 0.0378\nLoss 0.0373\nLoss 0.0369\nLoss 0.0366\nLoss 0.0362\nLoss 0.0358\nLoss 0.0354\nLoss 0.0351\nLoss 0.0347\nLoss 0.0344\nLoss 0.0340\nLoss 0.0337\nLoss 0.0334\nLoss 0.0330\nLoss 0.0327\nLoss 0.0324\nLoss 0.0321\nLoss 0.0318\nLoss 0.0315\nLoss 0.0312\nLoss 0.0309\nLoss 0.0307\nLoss 0.0304\nLoss 0.0301\nLoss 0.0298\nLoss 0.0296\nLoss 0.0293\nLoss 0.0291\nLoss 0.0288\nLoss 0.0286\nLoss 0.0283\nLoss 0.0281\nLoss 0.0279\nLoss 0.0277\nLoss 0.0274\nLoss 0.0272\nLoss 0.0270\nLoss 0.0268\nLoss 0.0266\nLoss 0.0264\nLoss 0.0262\nLoss 0.0260\nLoss 0.0258\nLoss 0.0256\nLoss 0.0254\nLoss 0.0252\nLoss 0.0250\nLoss 0.0248\nLoss 0.0246\nLoss 0.0244\nLoss 0.0243\nLoss 0.0241\nLoss 0.0239\nLoss 0.0238\nLoss 0.0236\nLoss 0.0234\nLoss 0.0233\nLoss 0.0231\nLoss 0.0229\nLoss 0.0228\nLoss 0.0226\nLoss 0.0225\nLoss 0.0223\nLoss 0.0222\nLoss 0.0220\nLoss 0.0219\nLoss 0.0217\nLoss 0.0216\nLoss 0.0215\nLoss 0.0213\nLoss 0.0212\nLoss 0.0211\nLoss 0.0209\nLoss 0.0208\nLoss 0.0207\nLoss 0.0205\nLoss 0.0204\nLoss 0.0203\nLoss 0.0202\nLoss 0.0200\nLoss 0.0199\nLoss 0.0198\nLoss 0.0197\nLoss 0.0196\nLoss 0.0195\nLoss 0.0193\nLoss 0.0192\nLoss 0.0191\nLoss 0.0190\nLoss 0.0189\nLoss 0.0188\nLoss 0.0187\nLoss 0.0186\nLoss 0.0185\nLoss 0.0184\nLoss 0.0183\nLoss 0.0182\nLoss 0.0181\nLoss 0.0180\nLoss 0.0179\nLoss 0.0178\nLoss 0.0177\nLoss 0.0176\nLoss 0.0175\nLoss 0.0174\nLoss 0.0173\nLoss 0.0172\nLoss 0.0172\nLoss 0.0171\nLoss 0.0170\nLoss 0.0169\nLoss 0.0168\nLoss 0.0167\nLoss 0.0166\nLoss 0.0166\nLoss 0.0165\nLoss 0.0164\nLoss 0.0163\nLoss 0.0162\nLoss 0.0161\nLoss 0.0161\nLoss 0.0160\nLoss 0.0159\nLoss 0.0158\nLoss 0.0158\nLoss 0.0157\nLoss 0.0156\nLoss 0.0155\nLoss 0.0155\nLoss 0.0154\nLoss 0.0153\nLoss 0.0153\nLoss 0.0152\nLoss 0.0151\nLoss 0.0150\nLoss 0.0150\nLoss 0.0149\nLoss 0.0148\nLoss 0.0148\nLoss 0.0147\nLoss 0.0146\nLoss 0.0146\nLoss 0.0145\nLoss 0.0145\nLoss 0.0144\nLoss 0.0143\nLoss 0.0143\nLoss 0.0142\nLoss 0.0141\nLoss 0.0141\nLoss 0.0140\nLoss 0.0140\nLoss 0.0139\nLoss 0.0138\nLoss 0.0138\nLoss 0.0137\nLoss 0.0137\nLoss 0.0136\nLoss 0.0136\nLoss 0.0135\nLoss 0.0134\nLoss 0.0134\nLoss 0.0133\nLoss 0.0133\nLoss 0.0132\nLoss 0.0132\nLoss 0.0131\nLoss 0.0131\nLoss 0.0130\nLoss 0.0130\nLoss 0.0129\nLoss 0.0129\nLoss 0.0128\nLoss 0.0128\nLoss 0.0127\nLoss 0.0127\nLoss 0.0126\nLoss 0.0126\nLoss 0.0125\nLoss 0.0125\nLoss 0.0124\nLoss 0.0124\nLoss 0.0123\nLoss 0.0123\nLoss 0.0122\nLoss 0.0122\nLoss 0.0122\nLoss 0.0121\nLoss 0.0121\nLoss 0.0120\nLoss 0.0120\nLoss 0.0119\nLoss 0.0119\nLoss 0.0118\nLoss 0.0118\nLoss 0.0118\nLoss 0.0117\nLoss 0.0117\nLoss 0.0116\nLoss 0.0116\nLoss 0.0116\nLoss 0.0115\nLoss 0.0115\nLoss 0.0114\nLoss 0.0114\nLoss 0.0114\nLoss 0.0113\nLoss 0.0113\nLoss 0.0112\nLoss 0.0112\nLoss 0.0112\nLoss 0.0111\nLoss 0.0111\nLoss 0.0110\nLoss 0.0110\nLoss 0.0110\nLoss 0.0109\nLoss 0.0109\nLoss 0.0109\nLoss 0.0108\nLoss 0.0108\nLoss 0.0108\nLoss 0.0107\nLoss 0.0107\nLoss 0.0107\nLoss 0.0106\nLoss 0.0106\nLoss 0.0106\nLoss 0.0105\nLoss 0.0105\nLoss 0.0104\nLoss 0.0104\nLoss 0.0104\nLoss 0.0104\nLoss 0.0103\nLoss 0.0103\nLoss 0.0103\nLoss 0.0102\nLoss 0.0102\nLoss 0.0102\nLoss 0.0101\nLoss 0.0101\nLoss 0.0101\nLoss 0.0100\nLoss 0.0100\nLoss 0.0100\n\nInputs: [Value(5), Value(0.4), Value(-1), Value(-2)]\nParameters: [Value(0.6195076510193583), Value(0.988462556700058), Value(0.6814715991439656), Value(-0.8498024629867303)] Value(0.08692061974641961)\nPrediction Probability: 0.9900282484345291\n</pre> <p>Glosser.ca - Own work, Derivative of Artificial neural network.svg (CC BY-SA 3.0 | Wikimedia Commons)</p> <p></p> <p>We managed to train our single neuron to learn a function for our input values. In reality, however, data is much more complex and we need to learn more complication functions. How to achieve that? By chaining many neurons together, similar to biological neuron. Each neuron will basically learn some portion of the overall function.</p> <p>What we see above is an illustration of an artificial neural network. In the input layer we have three neurons, each separately accepting N-dimensional input values. The output values of each neuron are then fully connected, as inputs to the hidden layer with four neurons (note that there can be more than one hidden layer). And finally, the output of hidden layer neurons are passed as inputs to the output layer, which may, for example, predict probability scores for two classes.</p> <p>We will now try to implement a fully connected feedforward neural network, which is often referred to as Multi-Layer Perceptron (MLP).</p> In\u00a0[28]: Copied! <pre>class Layer:\n  def __init__(self, N, count):\n    self.neurons = [Neuron(N) for _ in range(count)]\n\n  def __call__(self, X):\n    outs = [n(X) for n in self.neurons]\n    return outs[0] if len(outs) == 1 else outs # flattening dimension if a single element\n</pre> class Layer:   def __init__(self, N, count):     self.neurons = [Neuron(N) for _ in range(count)]    def __call__(self, X):     outs = [n(X) for n in self.neurons]     return outs[0] if len(outs) == 1 else outs # flattening dimension if a single element <p>The code above creates a list of <code>count</code> number of neurons, each accepting <code>N</code> dimensional input. Let's build our layers shown in the illustration above and connect them. Note that the input dimension of the next layer is the amount of neurons in the previous layer.</p> In\u00a0[29]: Copied! <pre># input data and its dimension\nX = [Value(x, label=f'x{i}') for i, x in enumerate([1, 4, -3, -2, 3])]\nN = len(X)\n</pre> # input data and its dimension X = [Value(x, label=f'x{i}') for i, x in enumerate([1, 4, -3, -2, 3])] N = len(X) In\u00a0[30]: Copied! <pre># creating layers\nin_layer = Layer(N, 3)\nhid_layer = Layer(3, 4)\nout_layer = Layer(4, 2)\n</pre> # creating layers in_layer = Layer(N, 3) hid_layer = Layer(3, 4) out_layer = Layer(4, 2) In\u00a0[31]: Copied! <pre># output of each layer is input to the next\nX_hidden = in_layer(X)\nX_output = hid_layer(X_hidden)\nout = out_layer(X_output)\n</pre> # output of each layer is input to the next X_hidden = in_layer(X) X_output = hid_layer(X_hidden) out = out_layer(X_output) In\u00a0[32]: Copied! <pre># let's plot either one of the outputs\ndraw_dot(out[0])\n</pre> # let's plot either one of the outputs draw_dot(out[0]) Out[32]: <p>We will further abstract away the neuron and layer creation inside the <code>MLP</code> class. We will then reimplement the exact same network.</p> In\u00a0[33]: Copied! <pre>class MLP:\n  def __init__(self, N, counts):\n    dims = [N] + counts # concatenates dimensions\n    self.layers = [Layer(dims[i], dims[i+1]) for i in range(len(dims)-1)]\n\n  def __call__(self, X):\n    out = X\n    for layer in self.layers:\n      out = layer(out)\n    return out\n</pre> class MLP:   def __init__(self, N, counts):     dims = [N] + counts # concatenates dimensions     self.layers = [Layer(dims[i], dims[i+1]) for i in range(len(dims)-1)]    def __call__(self, X):     out = X     for layer in self.layers:       out = layer(out)     return out In\u00a0[34]: Copied! <pre>nn = MLP(N, [3, 4, 2])\nout = nn(X)\ndraw_dot(out[0]) # out[1] will return the second output\n</pre> nn = MLP(N, [3, 4, 2]) out = nn(X) draw_dot(out[0]) # out[1] will return the second output Out[34]: <p>It is time to judge our network by applying it to the real dataset. Even though applying neural networks to Fisher's Iris dataset is a little overkill (as the dataset is simple), it will be a nice demonstration of our MLP's capacity.</p> <p>Iris dataset has 4-dimensional input samples with three possible output classes. We should be able to predict the class of the Iris flower based on the width and height values of its two elements. We will load our dataset and split it into train and test sets.</p> In\u00a0[35]: Copied! <pre>from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\n\nX = iris.data  # 50x3 4-dimensional samples\ny = iris.target # 3 classes (0, 1, 2)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nprint(f'Train data shape: {X_train.shape}, {y_train.shape}')\nprint(f'Test data shape: {X_test.shape}, {y_test.shape}')\nprint(f'Input Samples:\\n {X_train[:5]}')\nprint(f'Labels:\\n {y_train[:5]}')\n</pre> from sklearn import datasets from sklearn.model_selection import train_test_split  iris = datasets.load_iris()  X = iris.data  # 50x3 4-dimensional samples y = iris.target # 3 classes (0, 1, 2)  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  print(f'Train data shape: {X_train.shape}, {y_train.shape}') print(f'Test data shape: {X_test.shape}, {y_test.shape}') print(f'Input Samples:\\n {X_train[:5]}') print(f'Labels:\\n {y_train[:5]}') <pre>Train data shape: (120, 4), (120,)\nTest data shape: (30, 4), (30,)\nInput Samples:\n [[4.4 2.9 1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [6.5 3.  5.5 1.8]\n [6.4 3.1 5.5 1.8]\n [6.3 2.5 5.  1.9]]\nLabels:\n [0 0 2 2 2]\n</pre> <p>We should then convert each element to be a <code>Value</code> object. But before that, let's try out two ready scikit-learn classifiers, <code>LogisticRegression</code> and <code>MLPClassifier</code>, the latter of which can be seen as the extension of the former, which we will implement in its simplistic form. As we have already discussed, <code>LogisticRegression</code> is basically our <code>Neuron</code> class which uses <code>sigmoid</code> (logistic) function as its activation. And in fact, Logistic Regression will simply be our MLP with the layer size for just a single neuron. Thanks to <code>numpy</code> vectorization and other optimizations, the <code>sklearn</code> implementations will be extremely quick.</p> In\u00a0[36]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\naccuracy = accuracy_score(y_test, preds)\nprint(f\"Logistic Regression Accuracy: {accuracy:.2f}\")\n\nmodel = MLPClassifier()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\naccuracy = accuracy_score(y_test, preds)\nprint(f\"MLP Classifier Accuracy: {accuracy:.2f}\")\n</pre> from sklearn.linear_model import LogisticRegression from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score  model = LogisticRegression() model.fit(X_train, y_train) preds = model.predict(X_test) accuracy = accuracy_score(y_test, preds) print(f\"Logistic Regression Accuracy: {accuracy:.2f}\")  model = MLPClassifier() model.fit(X_train, y_train) preds = model.predict(X_test) accuracy = accuracy_score(y_test, preds) print(f\"MLP Classifier Accuracy: {accuracy:.2f}\") <pre>Logistic Regression Accuracy: 1.00\nMLP Classifier Accuracy: 1.00\n</pre> <pre>/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n</pre> <p>When converting labels to <code>Value</code> objects, we can technically scale them down to be between <code>0</code> and <code>1</code> (by multiplying them to <code>0.5</code>). Then we can apply the simple Mean Squared Error (MSE). Because there are only three labels and the dataset is small, it will work, yet it will penalize ordinal labels unnecessarily. For example, our loss should give the same penalty if we predict <code>0</code> instead of <code>1</code> or <code>2</code>  (it is either one type of flower or another). MSE, however, will give more penalty when predicting <code>0</code> instead of <code>2</code> and less penalty when predicting <code>0</code> instead of <code>1</code> because of the imaginary distances between numbers, which do not exist among the real labels.</p> In\u00a0[37]: Copied! <pre># converting numpy float arrays into Value lists\n# scaling labels to be between 0 and 1 to simplify our code\n# again, it is not a right way to treat the labels\nX_train = [[Value(x) for x in X] for X in X_train]\nX_test = [[Value(x) for x in X] for X in X_test]\ny_train = [Value(y) * Value(0.5) for y in y_train]\ny_test = [Value(y) * Value(0.5) for y in y_test]\n</pre> # converting numpy float arrays into Value lists # scaling labels to be between 0 and 1 to simplify our code # again, it is not a right way to treat the labels X_train = [[Value(x) for x in X] for X in X_train] X_test = [[Value(x) for x in X] for X in X_test] y_train = [Value(y) * Value(0.5) for y in y_train] y_test = [Value(y) * Value(0.5) for y in y_test] <p>We will now create a class for our model, in the style of <code>scikit-learn</code> models, more specifically MLPClassifier.</p> <p>As noted above, what we do is simple and will work in this case, but is not right. Ideally, we should initially one hot encode the ground truth labels to describe them with only zeros and ones. Our final layer for multiclass classification, instead of <code>sigmoid</code> activation, should output <code>logits</code> (unprocessed predictions) passing through <code>softmax</code> function. The loss in this case should be Cross Entropy instead of MSE.</p> <p>Exercise (Advanced): Write code for the correct implementation noted above. See Josh Starmer's (StatQuest) videos which explain its theory, including the Iris dataset, argmax and softmax functions, as well as Cross Entropy.</p> In\u00a0[\u00a0]: Copied! <pre>class Classifier:\n  def __init__(self, layer_sizes=[2, 3, 1]):\n    self.layer_sizes = layer_sizes\n    self.nn = None\n    self.L = None\n    self.iterations = 0\n\n  def forward(self, Xs):\n    out = [self.nn(X) for X in Xs]\n    return out\n\n  def predict(self, X_test):\n    return self.forward(X_test)\n\n  def train(self, X_train, y_train, learning_rate=0.01):\n    preds = self.forward(X_train)\n    self.L = self.mean_squared_error(y_train, preds)\n    self.L.zero_grad()\n    self.L.backward()\n    self.L.optimize(learning_rate=learning_rate)\n    print(f'Loss: {self.L.data:.4f}')\n\n  def fit(self, X_train, y_train, learning_rate=0.01, num_epochs=50):\n    if not self.nn: # in order to not restart training if nn exists\n      self.nn = MLP(len(X_train), self.layer_sizes)\n    for i in range(num_epochs):\n      print(f'Training epoch {self.iterations + i + 1}')\n      self.train(X_train, y_train, learning_rate)\n    self.iterations += i + 1\n\n  def mean_squared_error(self, y_train, preds):\n    return sum([(y-y_hat)*(y-y_hat) for y, y_hat in zip(y_train, preds)], Value(0))\n\n  def score(self, y_test, preds):\n    return self.mean_squared_error(y_test, preds).data / len(y_test)\n\n  def accuracy_score(self, y_test, preds):\n    # due to incorrect handling of the labels\n    # we need to scale y_test and preds values back\n    y_test = [y * Value(2) for y in y_test]\n    preds = [y_hat * Value(2) for y_hat in preds]\n    correct = sum(1 for y, y_hat in zip(y_test, preds) if round(y_hat.data) == y.data)\n    total = len(y_test)\n    return (correct/total)\n</pre> class Classifier:   def __init__(self, layer_sizes=[2, 3, 1]):     self.layer_sizes = layer_sizes     self.nn = None     self.L = None     self.iterations = 0    def forward(self, Xs):     out = [self.nn(X) for X in Xs]     return out    def predict(self, X_test):     return self.forward(X_test)    def train(self, X_train, y_train, learning_rate=0.01):     preds = self.forward(X_train)     self.L = self.mean_squared_error(y_train, preds)     self.L.zero_grad()     self.L.backward()     self.L.optimize(learning_rate=learning_rate)     print(f'Loss: {self.L.data:.4f}')    def fit(self, X_train, y_train, learning_rate=0.01, num_epochs=50):     if not self.nn: # in order to not restart training if nn exists       self.nn = MLP(len(X_train), self.layer_sizes)     for i in range(num_epochs):       print(f'Training epoch {self.iterations + i + 1}')       self.train(X_train, y_train, learning_rate)     self.iterations += i + 1    def mean_squared_error(self, y_train, preds):     return sum([(y-y_hat)*(y-y_hat) for y, y_hat in zip(y_train, preds)], Value(0))    def score(self, y_test, preds):     return self.mean_squared_error(y_test, preds).data / len(y_test)    def accuracy_score(self, y_test, preds):     # due to incorrect handling of the labels     # we need to scale y_test and preds values back     y_test = [y * Value(2) for y in y_test]     preds = [y_hat * Value(2) for y_hat in preds]     correct = sum(1 for y, y_hat in zip(y_test, preds) if round(y_hat.data) == y.data)     total = len(y_test)     return (correct/total) <p>Our naive classifer is ready and we can now train our model and note the accuracy. However, unlike the optimized classifiers of the <code>sklearn</code> library, it will be much slower and inefficient. Try out experiments with different layer sizes and learning rates, and notice how it affects the training process and loss. As we have mentioned before, we can implement Logistic Regression by simply passing <code>layer_sizes=[1]</code> to our MLP classifier.</p> In\u00a0[45]: Copied! <pre>model = Classifier([1])\n# model = Classifier([4, 1])\n</pre> model = Classifier([1]) # model = Classifier([4, 1]) In\u00a0[48]: Copied! <pre>model.fit(X_train, y_train, learning_rate=0.002, num_epochs=30)\n</pre> model.fit(X_train, y_train, learning_rate=0.002, num_epochs=30) <pre>Training epoch 61\nLoss: 1.3948\nTraining epoch 62\nLoss: 1.3938\nTraining epoch 63\nLoss: 1.3929\nTraining epoch 64\nLoss: 1.3920\nTraining epoch 65\nLoss: 1.3911\nTraining epoch 66\nLoss: 1.3901\nTraining epoch 67\nLoss: 1.3892\nTraining epoch 68\nLoss: 1.3883\nTraining epoch 69\nLoss: 1.3874\nTraining epoch 70\nLoss: 1.3865\nTraining epoch 71\nLoss: 1.3856\nTraining epoch 72\nLoss: 1.3847\nTraining epoch 73\nLoss: 1.3838\nTraining epoch 74\nLoss: 1.3830\nTraining epoch 75\nLoss: 1.3821\nTraining epoch 76\nLoss: 1.3812\nTraining epoch 77\nLoss: 1.3803\nTraining epoch 78\nLoss: 1.3795\nTraining epoch 79\nLoss: 1.3786\nTraining epoch 80\nLoss: 1.3777\nTraining epoch 81\nLoss: 1.3769\nTraining epoch 82\nLoss: 1.3760\nTraining epoch 83\nLoss: 1.3752\nTraining epoch 84\nLoss: 1.3743\nTraining epoch 85\nLoss: 1.3735\nTraining epoch 86\nLoss: 1.3726\nTraining epoch 87\nLoss: 1.3718\nTraining epoch 88\nLoss: 1.3710\nTraining epoch 89\nLoss: 1.3701\nTraining epoch 90\nLoss: 1.3693\n</pre> In\u00a0[49]: Copied! <pre>preds = model.predict(X_train)\nprint(f'Custom MLP classifier accuracy on train Data: {model.accuracy_score(y_train, preds):.2f}')\n</pre> preds = model.predict(X_train) print(f'Custom MLP classifier accuracy on train Data: {model.accuracy_score(y_train, preds):.2f}') <pre>Custom MLP classifier accuracy on train Data: 0.97\n</pre> In\u00a0[50]: Copied! <pre>preds = model.predict(X_test)\nprint(f'Custom MLP classifier accuracy on test Data: {model.accuracy_score(y_test, preds):.2f}')\n</pre> preds = model.predict(X_test) print(f'Custom MLP classifier accuracy on test Data: {model.accuracy_score(y_test, preds):.2f}') <pre>Custom MLP classifier accuracy on test Data: 0.97\n</pre> In\u00a0[51]: Copied! <pre>for i in range(len(y_test)):\n  print(y_test[i].data * 2, preds[i].data * 2)\n</pre> for i in range(len(y_test)):   print(y_test[i].data * 2, preds[i].data * 2) <pre>2.0 1.7378279677947062\n1.0 1.3786491598121862\n2.0 1.7797233124612888\n2.0 1.5546211532302696\n1.0 0.7808593322180695\n1.0 1.1323777346177633\n0.0 0.03179964376674919\n2.0 1.6900839613983492\n2.0 1.818651271834627\n2.0 1.5712929781499119\n2.0 1.9175600703929185\n2.0 1.8478802623012778\n2.0 1.8502101169387906\n0.0 0.024109593550568645\n1.0 1.6944624014543954\n1.0 1.1691956406120396\n2.0 1.6617236866912894\n2.0 1.7796042897673732\n1.0 1.186491156964572\n2.0 1.7267232886353379\n1.0 1.197352283588582\n0.0 0.01104177713999268\n2.0 1.860650974045892\n0.0 0.029735232803150373\n1.0 1.021880542808286\n1.0 0.7892688298891827\n0.0 0.026875629893556307\n1.0 1.219772374852777\n0.0 0.015342452967957023\n2.0 1.5336924121298994\n</pre>"},{"location":"notebooks/02_neural_network/#02-from-neuron-to-neural-network","title":"02. From Neuron to Neural Network\u00b6","text":"8 Feb 2025 \u00b7    <p>Important</p> <p>     The notebook is currently under revision.   </p> <p>Note</p> <p>The notebook is mainly based on Andrej Karpathy's lecture on Micrograd</p>"},{"location":"notebooks/02_neural_network/#recall-backpropagation","title":"Recall: Backpropagation\u00b6","text":""},{"location":"notebooks/02_neural_network/#activation-function","title":"Activation Function\u00b6","text":""},{"location":"notebooks/02_neural_network/#artificial-neuron","title":"Artificial Neuron\u00b6","text":""},{"location":"notebooks/02_neural_network/#n-dimensional-neuron","title":"N-dimensional Neuron\u00b6","text":""},{"location":"notebooks/02_neural_network/#artificial-neural-network","title":"Artificial Neural Network\u00b6","text":""},{"location":"notebooks/02_neural_network/#iris-dataset","title":"Iris Dataset\u00b6","text":""},{"location":"notebooks/02_neural_network/#training-custom-mlp-classifier","title":"Training Custom MLP Classifier\u00b6","text":""},{"location":"notebooks/03_cnn_torch/","title":"03. From Kernel to Convolutional Neural Network","text":"<p>In previous lectures, we learned how to build a neural network. We will come back to discuss many optimization and regularization techniques (e.g. parameter initialization, momentum, weight decay, dropout, batch normalization, etc.) to improve the efficiency of our training. But before coming back to these important notions, we will discuss Convolutional Neural Networks (CNN) which are used in image processing. And even before that, we wil learn what an image is.</p> <p>A grayscale image is simply a 2D array holding pixel values, often ranging between <code>0</code> and <code>1</code> (normalized) or <code>0</code> and <code>255</code> (8-bit image), representing the range of intensities between black and white. A colored image often is represented by the RGB color model, where we have 3 sets (channels) of 2D arrays for Red, Green, and Blue. Combinations of three values represent additional colors to black and white (e.g. <code>[255, 0, 0]</code> is red).</p> <p>We will create a toy image to get a feeling of how images are stored in the memory. We will also delibarately use <code>torch</code> library to familiarize ourselves with <code>PyTorch</code> functions. In <code>PyTorch</code>, images are stored in <code>(C, H, W)</code> format, corresponding to channel, height, and width. Note that batch size can also be included <code>(B, C, H, W)</code>. Batch is simply a subset of a dataset. We  should process our dataset in batches in order to not load all the images at once to the RAM memory, which is usually limited to several <code>Gigabytes</code> (see the top right corner of a <code>Colab</code> notebook for RAM information). Tensor is the main data abstraction in PyTorch, similar to <code>numpy</code> arrays. Tensors are optimized for GPU and can calculate gradients with its autograd engine (i.e. advanced micrograd), which we saw in the previous lecture.</p> In\u00a0[1]: Copied! <pre>import torch\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import torch from torchvision import datasets, transforms  import matplotlib.pyplot as plt %matplotlib inline In\u00a0[2]: Copied! <pre># this is for plotting, skip for now\ndef plot(img_tensors, titles=None):\n  if not isinstance(img_tensors, list):\n    img_tensors = [img_tensors]\n  if not isinstance(titles, list):\n    titles = [titles] * len(img_tensors)\n\n  fig, axes = plt.subplots(1, len(img_tensors), figsize=(len(img_tensors) * 4, 4))\n  if len(img_tensors) == 1:\n    axes = [axes]\n\n  for ax, img_tensor, title in zip(axes, img_tensors, titles):\n\n    img_tensor = img_tensor.cpu()\n    img = img_tensor.permute(1, 2, 0).numpy()\n    ax.imshow(img, cmap='gray')\n    ax.set_title(title)\n    ax.axis(\"off\")\n\n  plt.show()\n</pre> # this is for plotting, skip for now def plot(img_tensors, titles=None):   if not isinstance(img_tensors, list):     img_tensors = [img_tensors]   if not isinstance(titles, list):     titles = [titles] * len(img_tensors)    fig, axes = plt.subplots(1, len(img_tensors), figsize=(len(img_tensors) * 4, 4))   if len(img_tensors) == 1:     axes = [axes]    for ax, img_tensor, title in zip(axes, img_tensors, titles):      img_tensor = img_tensor.cpu()     img = img_tensor.permute(1, 2, 0).numpy()     ax.imshow(img, cmap='gray')     ax.set_title(title)     ax.axis(\"off\")    plt.show() In\u00a0[3]: Copied! <pre>color = ['gray','rgb'][0] # change 0 to 1 for rgb\n\nH = W = 8\nC = 3 if color == 'rgb' else 1\n\nimg = torch.rand((C, H, W))\nplot(img)\n</pre> color = ['gray','rgb'][0] # change 0 to 1 for rgb  H = W = 8 C = 3 if color == 'rgb' else 1  img = torch.rand((C, H, W)) plot(img) In\u00a0[4]: Copied! <pre>img\n</pre> img Out[4]: <pre>tensor([[[0.6681, 0.9885, 0.3858, 0.6401, 0.8704, 0.5164, 0.6299, 0.8198],\n         [0.5352, 0.3675, 0.8261, 0.6846, 0.7454, 0.7086, 0.6866, 0.0161],\n         [0.5431, 0.9798, 0.7918, 0.1760, 0.0203, 0.2780, 0.8877, 0.1514],\n         [0.4076, 0.0945, 0.1907, 0.3847, 0.2569, 0.0359, 0.3500, 0.5701],\n         [0.0235, 0.6880, 0.2567, 0.5896, 0.3844, 0.9977, 0.2146, 0.1763],\n         [0.3725, 0.7660, 0.5657, 0.1217, 0.5217, 0.2664, 0.6946, 0.1770],\n         [0.1950, 0.0631, 0.5228, 0.1239, 0.7185, 0.7971, 0.2741, 0.3123],\n         [0.8518, 0.9311, 0.3031, 0.2274, 0.1595, 0.2751, 0.4300, 0.4698]]])</pre> <p>Technically, if we have a label for the image, and enough images in the dataset, we can already train a neural network to make predictions. For that, we can turn our 2D (grayscale) or 3D (RGB) tensors into vectors and feed in to our model. For example, and image with input dimensions (3,32,32), which corresponds to a 32x32 pixel RGB image, can be reshaped into a single dimensional vector of size (3 * 32 * 32) which will be the input dimension of our network.</p> <p>Question: What will be the shape of 8x8 grayscale image after reshaping?</p> In\u00a0[5]: Copied! <pre>img.view(-1)\n</pre> img.view(-1) Out[5]: <pre>tensor([0.6681, 0.9885, 0.3858, 0.6401, 0.8704, 0.5164, 0.6299, 0.8198, 0.5352,\n        0.3675, 0.8261, 0.6846, 0.7454, 0.7086, 0.6866, 0.0161, 0.5431, 0.9798,\n        0.7918, 0.1760, 0.0203, 0.2780, 0.8877, 0.1514, 0.4076, 0.0945, 0.1907,\n        0.3847, 0.2569, 0.0359, 0.3500, 0.5701, 0.0235, 0.6880, 0.2567, 0.5896,\n        0.3844, 0.9977, 0.2146, 0.1763, 0.3725, 0.7660, 0.5657, 0.1217, 0.5217,\n        0.2664, 0.6946, 0.1770, 0.1950, 0.0631, 0.5228, 0.1239, 0.7185, 0.7971,\n        0.2741, 0.3123, 0.8518, 0.9311, 0.3031, 0.2274, 0.1595, 0.2751, 0.4300,\n        0.4698])</pre> <p>We will download and train our model on the MNIST dataset, which has 70,000 grayscale 28x28 images of hand-written digits with their labels. Our main goal is to familiarize ourselves with <code>PyTorch</code>. Make sure to go through the Datasets and DataLoaders tutorial. Even though it won't be essential for understanding this notebook, try to answer the following questions: What other transforms can we apply to our dataset and why? Why do we shuffle the <code>train</code> data but not the <code>test</code> data? What are <code>num_workers</code> and <code>batch_size</code>?</p> In\u00a0[7]: Copied! <pre>train_data = datasets.MNIST(root='./data', train=True,  transform=transforms.ToTensor(), download=True)\ntest_data  = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n</pre> train_data = datasets.MNIST(root='./data', train=True,  transform=transforms.ToTensor(), download=True) test_data  = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True) In\u00a0[9]: Copied! <pre>train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, num_workers=2, shuffle=True)\ntest_loader  = torch.utils.data.DataLoader(test_data,  batch_size=64, num_workers=2, shuffle=False)\n</pre> train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, num_workers=2, shuffle=True) test_loader  = torch.utils.data.DataLoader(test_data,  batch_size=64, num_workers=2, shuffle=False) In\u00a0[15]: Copied! <pre>X_train, y_train = next(iter(train_loader)) # gets the images of the batch\nplot(X_train[0], f'Label: {y_train[0].item()} | {X_train[0].shape}')\n</pre> X_train, y_train = next(iter(train_loader)) # gets the images of the batch plot(X_train[0], f'Label: {y_train[0].item()} | {X_train[0].shape}') <p>There are many ways to collapse our multi-dimensional image tensor into a single dimension. It is worth understanding the internals of PyTorch and the workings of the equivalent methods below, especially the difference of memory management between <code>reshape</code> and <code>view</code> functions. We can also see the number of elements with <code>numel</code>.</p> In\u00a0[16]: Copied! <pre># X_train[0].view(-1).shape[0]\n# X_train[0].reshape(-1).shape[0]\n# X_train[0].flatten().shape[0]\nX_train[0].numel() # 1x28x28\n</pre> # X_train[0].view(-1).shape[0] # X_train[0].reshape(-1).shape[0] # X_train[0].flatten().shape[0] X_train[0].numel() # 1x28x28 Out[16]: <pre>784</pre> <p>We will now build a simple neural network and train our model on MNIST, but not from scratch :). When we build our custom network, we need to inherit from <code>torch.nn.Module</code> which will handle many useful operations (e.g. autograd). It will also enforce us to define our <code>forward</code> pass function. Our activation will be <code>ReLU</code> function. Fully Connected (FC) layer (also known as Dense Layer) is simply a network layer where all the neurons are connected to the previous layer neurons (recall: MLP). When applying forward pass, we should reshape our input dimensions from <code>(B, C, H, W)</code> to <code>(B, C*H*W)</code>, so that we can pass the input from <code>train_loader</code> to our FC layer.</p> <p>Exercise: Implement different ways of reshaping tensor <code>(B, C, H, W)</code> to <code>(B, C*H*W)</code>.</p> In\u00a0[17]: Copied! <pre>import torch.nn as nn\nimport torch.optim as optim\n\nclass MLP(nn.Module):\n  def __init__(self, input_size, hidden_size, output_size):\n    super(MLP, self).__init__()\n    self.fc1 = nn.Linear(input_size, hidden_size)\n    self.fc2 = nn.Linear(hidden_size, output_size)\n    self.relu = nn.ReLU()\n\n  def forward(self, X):\n    X = X.view(X.shape[0], -1)\n    return self.fc2(self.relu(self.fc1(X)))\n</pre> import torch.nn as nn import torch.optim as optim  class MLP(nn.Module):   def __init__(self, input_size, hidden_size, output_size):     super(MLP, self).__init__()     self.fc1 = nn.Linear(input_size, hidden_size)     self.fc2 = nn.Linear(hidden_size, output_size)     self.relu = nn.ReLU()    def forward(self, X):     X = X.view(X.shape[0], -1)     return self.fc2(self.relu(self.fc1(X))) <p>We will now initialize our model, define our loss and optimizer. Stochastic Gradient Descent approximates gradient descent accross data batches to achieve faster performance. We will discuss <code>Adam</code> optimizer in the future as well.</p> <p>Question: What should be the input and output layer sizes of our model?</p> <p>Question: What should be our loss function and why?</p> <p>Exercise: Change <code>SGD</code> optimizer to <code>Adam</code> and retrain the model and note the training performance and inference accuracy.</p> In\u00a0[18]: Copied! <pre>model = MLP(784, 128, 10)\ncel = nn.CrossEntropyLoss() # for multiclass classification\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n</pre> model = MLP(784, 128, 10) cel = nn.CrossEntropyLoss() # for multiclass classification optimizer = optim.SGD(model.parameters(), lr=0.001) <p>It is possible to run faster training calculations on GPU with <code>cuda</code>. It is possible to change the runtime in Google Colab through the menu <code>Runtime -&gt; Change runtime type</code>. We will move our model's parameters to the available device with <code>.to()</code>.</p> In\u00a0[19]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model.to(device) Out[19]: <pre>MLP(\n  (fc1): Linear(in_features=784, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n  (relu): ReLU()\n)</pre> <p>As <code>PyTorch</code> is highly optimized and the dataset images are small in size, training will be quick, especially with GPU. We should move our data in batches to the GPU memory as well in order to make it compatible with the model. Backpropagation steps we have repeatedly discussed in the previous lectures. We will train out model for only three epochs to quickly see interesting mistakes of our model, but feel free to train the model for a longer period to achieve a higher accuracy.</p> In\u00a0[20]: Copied! <pre>num_epochs = 3\nfor epoch in range(num_epochs):\n  loss = 0.0\n  for X_train, y_train in train_loader:\n    X_train, y_train = X_train.to(device), y_train.to(device)\n    optimizer.zero_grad()\n    preds = model(X_train)\n    batch_loss = cel(preds, y_train)\n    batch_loss.backward()\n    optimizer.step()\n    loss += batch_loss.item()\n  print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}\")\n</pre> num_epochs = 3 for epoch in range(num_epochs):   loss = 0.0   for X_train, y_train in train_loader:     X_train, y_train = X_train.to(device), y_train.to(device)     optimizer.zero_grad()     preds = model(X_train)     batch_loss = cel(preds, y_train)     batch_loss.backward()     optimizer.step()     loss += batch_loss.item()   print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}\") <pre>Epoch: 1/3, Loss: 2.2294\nEpoch: 2/3, Loss: 2.0287\nEpoch: 3/3, Loss: 1.7497\n</pre> <p>Forward passing unseen data through a trained network to measure how well the model is doing is called inference. In this stage, we do not need to calculate gradients (<code>torch.no_grad()</code>). We will also call <code>model.eval()</code> which is not necessary for our simple network, yet is a good practice to follow. As <code>y_test</code> is a tensor of batch size and the output dimension <code>(B, 10)</code>, we will find top prediction labels on each batch (<code>dim=1</code>) and store them for future visualization. We will then sum the correct predictions and divide by the image count in the test dataset to get the final accuracy score.</p> In\u00a0[21]: Copied! <pre>model.eval()\n\ncorrect = 0\nall_preds = []\nwith torch.no_grad():\n  for X_test, y_test in test_loader:\n    X_test, y_test = X_test.to(device), y_test.to(device)\n    preds = model(X_test)\n    _, top_preds = torch.max(preds, dim=1)\n    correct += (top_preds == y_test).sum().item()\n    all_preds.append(top_preds)\n\naccuracy = 100 * correct / len(test_loader.dataset)\nprint(f\"Accuracy on test data: {accuracy:.2f}%\")\n</pre> model.eval()  correct = 0 all_preds = [] with torch.no_grad():   for X_test, y_test in test_loader:     X_test, y_test = X_test.to(device), y_test.to(device)     preds = model(X_test)     _, top_preds = torch.max(preds, dim=1)     correct += (top_preds == y_test).sum().item()     all_preds.append(top_preds)  accuracy = 100 * correct / len(test_loader.dataset) print(f\"Accuracy on test data: {accuracy:.2f}%\") <pre>Accuracy on test data: 76.14%\n</pre> <p>Let's visualize some of our predictions in the last test batch. Pay attention to the false predictions of our model. Can you guess why the model could have made those mistakes?</p> In\u00a0[25]: Copied! <pre>plot(\n    [X_test[i] for i in range(4)],\n    [f\"Pred: {all_preds[-1][i]}, True: {y_test[i]}\" for i in range(4)]\n)\n</pre> plot(     [X_test[i] for i in range(4)],     [f\"Pred: {all_preds[-1][i]}, True: {y_test[i]}\" for i in range(4)] ) <p>Our simple MLP with two FC layers acheived a pretty high accuracy. But that was mainly due to the simplicity of the dataset. Processing bigger datasets, also with a more challenging goal in mind (e.g. object detection, segmentation, etc), in addition to huge computational resources, requires from a model to understand spatial relationship in the images. When we reshape our image pixels into a single dimension two major things happen: 1) input size of our neurons drastically increases (a 224x224 pixel RGB image is more than 150,000 input dimension for a FC neuron), and 2) we lose important information about the pixels: their spatial location. It would make sense, if we could somehow also train our model to learn, for example, which pixels are close to each other. Most probably a combination of pixels (superpixels) make up a wheel of a car, or an ear of an animal. It would be great to look at pixels not as distinct and unrelated items (which FC layer does), but as connected and spatially related items.</p> <p>The word \"convolution\" is actually a misnomer, and \"cross-correlation\" would be a more precise name for the operation which we will describe now. The image below is <code>figure 7.2.1</code> from the Dive into Deep Learning book. As can be seen, we have a 2D input tensor (image) and a kernel of size <code>2x2</code>. We can put full kernel onto four different locations in the image (top left, which we see in the image, top right, bottom left and bottom right) and calculate the output. And the output is calculated by a simple dot product: all the overlapping values are multiplied to each other and summed up. The output in the image is calculated as <code>0*0+1*1+3*2+4*3=19</code>.</p> <p>Question: Given the image dimensions and the kernel size, how many times can we move kernel over the image?</p> <p></p> In\u00a0[26]: Copied! <pre>img = torch.arange(9).reshape((1,3,3))\nkernel = torch.arange(4).reshape((1,2,2))\nplot([img, kernel], [f'{img}', f'{kernel}'])\n</pre> img = torch.arange(9).reshape((1,3,3)) kernel = torch.arange(4).reshape((1,2,2)) plot([img, kernel], [f'{img}', f'{kernel}']) <p>If the kernel width is the same as image width, we can put kernel only once on each row of the image. If kernel width is <code>1</code> pixel less than the image width dimension, we can put kernel twice on the image row. The same rule applies to height and vertical movement of the kernel over the image. From that, we can determine the number of steps kernel will move over the image.</p> In\u00a0[27]: Copied! <pre>horizontal_steps = img.shape[1] - kernel.shape[1] + 1 # height\nvertical_steps   = img.shape[2] - kernel.shape[2] + 1 # width\n</pre> horizontal_steps = img.shape[1] - kernel.shape[1] + 1 # height vertical_steps   = img.shape[2] - kernel.shape[2] + 1 # width <p>We will now use for loops for calculating cross-correlation operation, but note that we do it for simplicity, and using loops isn't an efficient choice. Whenever we can, we should use vectorized operations provided by <code>Pytroch</code>. Finally, pay attention how <code>squeeze/unsqueeze</code> functions take care of the channel dimension below.</p> In\u00a0[28]: Copied! <pre>out = torch.zeros((horizontal_steps, vertical_steps))\nfor i in range(horizontal_steps):\n  for j in range(vertical_steps):\n    patch = img.squeeze()[i:kernel.shape[1]+i, j:kernel.shape[2]+j]\n    out[i, j] = torch.sum(kernel.squeeze() * patch)\nout = out.unsqueeze(0)\n\nplot(out, f'{out}')\n</pre> out = torch.zeros((horizontal_steps, vertical_steps)) for i in range(horizontal_steps):   for j in range(vertical_steps):     patch = img.squeeze()[i:kernel.shape[1]+i, j:kernel.shape[2]+j]     out[i, j] = torch.sum(kernel.squeeze() * patch) out = out.unsqueeze(0)  plot(out, f'{out}') <p>The good news is that we do not have to implement complicated (and inefficient) cross-correlation operations in higher dimensions. <code>torch.nn.functional</code> provides functions for that, which, however accept arguments in 4-dimensions, the first dimension being batch size. Again, pay attention to <code>squeeze</code> and <code>unsqueeze</code> dimension values, which will add and remove the fourth (batch) dimension.</p> In\u00a0[29]: Copied! <pre>import torch.nn.functional as F\n\nout = F.conv2d(img.unsqueeze(0), kernel.unsqueeze(0)).squeeze(1)\nplot(out, f'{out}')\n</pre> import torch.nn.functional as F  out = F.conv2d(img.unsqueeze(0), kernel.unsqueeze(0)).squeeze(1) plot(out, f'{out}') <p>Kernels are capable of extracting relevant features from images. We can choose different kernels, depending on what we try to detect in the image. To see how kernel values influence the output of the cross-correlation operation, let's see the following edge detection example. Edges in the toy image below are the points where pixel values change extremely (from 0 to 1 or vice versa).</p> In\u00a0[30]: Copied! <pre>img = torch.zeros((1, 6, 8))\nimg[:, :, 2:6] = 1.0\nplot(img, f'{img}\\n{img.shape}')\n</pre> img = torch.zeros((1, 6, 8)) img[:, :, 2:6] = 1.0 plot(img, f'{img}\\n{img.shape}') <p>Question: What kind of kernel would be appropriate for detecting edges?</p> <p>Try to understand why the code below detects the edges. Hint: manually calculate a cross-correlation step.</p> In\u00a0[31]: Copied! <pre>kernel = torch.tensor([[[1.0, -1.0]]])\nout = F.conv2d(img.unsqueeze(0), kernel.unsqueeze(0)).squeeze(1)\nplot(out, f'{out}\\n{out.shape}')\n</pre> kernel = torch.tensor([[[1.0, -1.0]]]) out = F.conv2d(img.unsqueeze(0), kernel.unsqueeze(0)).squeeze(1) plot(out, f'{out}\\n{out.shape}') <p>There exists many ready kernels#Details) for getting different image features (e.g. sharpening, blurring, edge detection, etc). But what if we wanted to learn more complicated kernels, how to achieve that? Can we learn correct kernel values, say, for detecting an ear of a dog? It turns out backpropagation will help with that as well. Given image and the edge detected output, we can learn the kernel. Instead of MLP with linear (FC, Dense) layers, we will use a single convolutional layer provided by <code>PyTorch</code> to learn the kernel. <code>nn.Conv2d</code> requires from us to specify the input dimensions, when <code>nn.LazyConv2d</code> will determine the dimension dynamically, when we pass the input image to the model. Note that we are overfitting the model (layer) to a single example. Our goal is to learn a single kernel of size <code>(1,2)</code> for a grayscale image. What the convolutional layer does under the hood we will learn soon.</p> In\u00a0[32]: Copied! <pre>model = nn.LazyConv2d(out_channels=1, kernel_size=(1, 2)).to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nX_img = img.to(device).unsqueeze(0)\ny_out = out.to(device).unsqueeze(0)\n\nnum_epochs = 50\nfor i in range(num_epochs):\n  optimizer.zero_grad()\n  preds = model(X_img)\n  loss = torch.sum((preds - y_out) ** 2) # mse\n  loss.backward()\n  optimizer.step()\n\nprint(f'Epoch: {i+1}/{num_epochs}, Loss: {loss:.4f}')\nprint(f'Learned Kernel: {model.weight.data}')\n</pre> model = nn.LazyConv2d(out_channels=1, kernel_size=(1, 2)).to(device) optimizer = optim.SGD(model.parameters(), lr=0.01)  X_img = img.to(device).unsqueeze(0) y_out = out.to(device).unsqueeze(0)  num_epochs = 50 for i in range(num_epochs):   optimizer.zero_grad()   preds = model(X_img)   loss = torch.sum((preds - y_out) ** 2) # mse   loss.backward()   optimizer.step()  print(f'Epoch: {i+1}/{num_epochs}, Loss: {loss:.4f}') print(f'Learned Kernel: {model.weight.data}') <pre>Epoch: 50/50, Loss: 0.0000\nLearned Kernel: tensor([[[[ 0.9987, -0.9987]]]])\n</pre> <p>Just to see what kernels are capable of, you can try out different kernels and see their outputs on an image in an interactive visualization. But here, let's take 1) a simple blur kernel which averages pixels within its frame, and 2) a Gaussian blur kernel which is basically giving more weight to the pixels that are closer to the middle, by considering Gaussian (normal) distribution (see nice demonstration). We will apply them on a photo and see the output. Note that Gaussian blur is commonly used for detecting edges (increase the size of the kernel and see the output). 3Blue1Brown video called \"But what is a convolution?\" explains cross-correlation and kernel operations in a nice visual way.</p> In\u00a0[34]: Copied! <pre>from skimage import data\n\ntest_img = data.astronaut()\ntest_img = torch.tensor(test_img, dtype=torch.float32).permute(2,0,1) # C, H, W\ntest_img = test_img.mean(0).unsqueeze(0) # RGB to grayscale\n\nH = W = 5 # try out bigger kernel sizes\nmean, std = 0.0, 1.0 # mean and standard deviation of Gaussian distribution\n\nblur_kernel = torch.ones((1,H,W)) * 1/H*W # averaging pixels\ngaussian_blur_kernel = torch.normal(mean, std, size=(1,H,W)) # pixels closer to middle get more weight\n\nblurred  = F.conv2d(test_img.unsqueeze(0), blur_kernel.unsqueeze(0)).squeeze(1)\ngaussian = F.conv2d(test_img.unsqueeze(0), gaussian_blur_kernel.unsqueeze(0)).squeeze(1)\n\nplot([test_img, blurred, gaussian], ['original', 'blurred', 'gaussian'])\n</pre> from skimage import data  test_img = data.astronaut() test_img = torch.tensor(test_img, dtype=torch.float32).permute(2,0,1) # C, H, W test_img = test_img.mean(0).unsqueeze(0) # RGB to grayscale  H = W = 5 # try out bigger kernel sizes mean, std = 0.0, 1.0 # mean and standard deviation of Gaussian distribution  blur_kernel = torch.ones((1,H,W)) * 1/H*W # averaging pixels gaussian_blur_kernel = torch.normal(mean, std, size=(1,H,W)) # pixels closer to middle get more weight  blurred  = F.conv2d(test_img.unsqueeze(0), blur_kernel.unsqueeze(0)).squeeze(1) gaussian = F.conv2d(test_img.unsqueeze(0), gaussian_blur_kernel.unsqueeze(0)).squeeze(1)  plot([test_img, blurred, gaussian], ['original', 'blurred', 'gaussian']) <p>By Michael Plotke - Own work (CC BY-SA 3.0 / Wikimedia Commons)</p> <p></p> <p>When we find <code>valid</code> cross-correlation, we move kernel within the constraints of the image. As a consequence, we pass through boundaries only once, losing some spatial information (pixels on the corners are barely used). <code>Padding</code> resolves this issue.</p> <p>By default, we step with kernel one row/column at a time (as in the image above). <code>Striding</code> determines the step size of our kernel, which is used when we want to reduce the output dimensionality after cross-correlation even further. It may be useful when the kernel size is big, we need faster computation, etc.</p> <p>From this point on, we will replace the more accurate word \"cross-correlation\" with the accepted terminology \"convolution\". Simply put, convolution operation is equivalent to the 90 degrees rotated cross-correlation operation.</p> In\u00a0[35]: Copied! <pre>img = torch.arange(1., 10).reshape((1,3,3))\npadded_img = F.pad(img, (1,1,1,1)) # left right top bottom\nplot([img, padded_img], [f'{img}', f'{padded_img}'])\n</pre> img = torch.arange(1., 10).reshape((1,3,3)) padded_img = F.pad(img, (1,1,1,1)) # left right top bottom plot([img, padded_img], [f'{img}', f'{padded_img}']) In\u00a0[36]: Copied! <pre>kernel = torch.ones((1,1,2,2))\n\nmanual_padding = F.conv2d(padded_img.unsqueeze(0), kernel).squeeze(1)\nconv2d_padding = F.conv2d(img.unsqueeze(0), kernel, padding=1).squeeze(1)\nconv2d_stride  = F.conv2d(img.unsqueeze(0), kernel, padding=1, stride=2).squeeze(1)\n\nplot([manual_padding, conv2d_padding, conv2d_stride],\n     [f'{manual_padding}', f'{conv2d_padding}', f'{conv2d_stride}'])\n</pre> kernel = torch.ones((1,1,2,2))  manual_padding = F.conv2d(padded_img.unsqueeze(0), kernel).squeeze(1) conv2d_padding = F.conv2d(img.unsqueeze(0), kernel, padding=1).squeeze(1) conv2d_stride  = F.conv2d(img.unsqueeze(0), kernel, padding=1, stride=2).squeeze(1)  plot([manual_padding, conv2d_padding, conv2d_stride],      [f'{manual_padding}', f'{conv2d_padding}', f'{conv2d_stride}']) <p>Exercise: Manually calculate the strided convolution above.</p> <p>Exercise: Create 100 random images (colored) and calculate the output dimension after the convolution operation. Calculate and print out how many features will be passed to the next layer after flattening the feature maps for all the cases:</p> Image Size Kernel Size Stride Padding 30x30 3x3 1 0 224x224 5x5 2 1 528x528 7x7 3 2 In\u00a0[37]: Copied! <pre>img = torch.cat((torch.arange(9.), torch.arange(1.,10))).reshape((1,2,3,3))\nimg\n</pre> img = torch.cat((torch.arange(9.), torch.arange(1.,10))).reshape((1,2,3,3)) img Out[37]: <pre>tensor([[[[0., 1., 2.],\n          [3., 4., 5.],\n          [6., 7., 8.]],\n\n         [[1., 2., 3.],\n          [4., 5., 6.],\n          [7., 8., 9.]]]])</pre> In\u00a0[38]: Copied! <pre>kernels = torch.cat((torch.arange(4.), torch.arange(1.,5))).reshape((1,2,2,2))\nkernels\n</pre> kernels = torch.cat((torch.arange(4.), torch.arange(1.,5))).reshape((1,2,2,2)) kernels Out[38]: <pre>tensor([[[[0., 1.],\n          [2., 3.]],\n\n         [[1., 2.],\n          [3., 4.]]]])</pre> In\u00a0[39]: Copied! <pre>conv1 = F.conv2d(img[:,0:1,:,:], kernels[:,0:1,:,:])\nconv2 = F.conv2d(img[:,1:2,:,:], kernels[:,1:2,:,:])\n\nconv1 + conv2\n</pre> conv1 = F.conv2d(img[:,0:1,:,:], kernels[:,0:1,:,:]) conv2 = F.conv2d(img[:,1:2,:,:], kernels[:,1:2,:,:])  conv1 + conv2 Out[39]: <pre>tensor([[[[ 56.,  72.],\n          [104., 120.]]]])</pre> In\u00a0[40]: Copied! <pre># or equivalently\nF.conv2d(img, kernels)\n</pre> # or equivalently F.conv2d(img, kernels) Out[40]: <pre>tensor([[[[ 56.,  72.],\n          [104., 120.]]]])</pre> <p>We may want to not only deal with multiple channels, but also output multiple channels. The simplistic reasoning for it could be that each output channel may hold different feature information. For that, we will apply convolution of each kernel set to the whole image (all dimensions) and eventually concatenate the results along the channel axis.</p> In\u00a0[41]: Copied! <pre>torch.cat([F.conv2d(img, kernels+i) for i in range(3)], dim=0)\n</pre> torch.cat([F.conv2d(img, kernels+i) for i in range(3)], dim=0) Out[41]: <pre>tensor([[[[ 56.,  72.],\n          [104., 120.]]],\n\n\n        [[[ 76., 100.],\n          [148., 172.]]],\n\n\n        [[[ 96., 128.],\n          [192., 224.]]]])</pre> <p>As we know, the advantage of convolutional layers is that they take into account spatial information about the pixels (their locations, neighbors, etc). But how can we make sure that our predictions will not be very sensitive to small changes in pixel locations? Because a cat image where the cat is streching is still a cat image, and our model should correctly classify it, even if it hasn't seen a stretching cat during training. As we will see, applying a <code>pooling</code> layer will help with spatial invariance, as well as downsample the representations of the previous layer.</p> <p>The image below (figure 7.5.1) describes maximum pooling operation, which looks like a kernel, but instead of convolution, simply takes the maximum pixel value within that range. Similarly, an average pooling operation, averages all the corresponding pixels and is equivalent to the simple blur kernel which we saw previously.</p> <p></p> In\u00a0[42]: Copied! <pre>img_channel = img.unbind(1)[0] # unbinds from the channel dimension and takes the first channel\nimg_channel\n</pre> img_channel = img.unbind(1)[0] # unbinds from the channel dimension and takes the first channel img_channel Out[42]: <pre>tensor([[[0., 1., 2.],\n         [3., 4., 5.],\n         [6., 7., 8.]]])</pre> In\u00a0[43]: Copied! <pre>operation = ['avg','max'][1] # change to 0 for average poolig\n\n# Compare the code below with the convolution code\nkernel_height = kernel_width = 2\n\nhorizontal_steps = img_channel.shape[1] - kernel_height + 1\nvertical_steps   = img_channel.shape[2] - kernel_width  + 1\n\nout = torch.zeros((horizontal_steps, vertical_steps))\nfor i in range(horizontal_steps):\n  for j in range(vertical_steps):\n    patch = img_channel.squeeze(0)[i:i+kernel_height, j:j+kernel_width]\n    out[i, j] = patch.mean() if operation == 'avg' else patch.max()\n\nout.unsqueeze(0)\n</pre> operation = ['avg','max'][1] # change to 0 for average poolig  # Compare the code below with the convolution code kernel_height = kernel_width = 2  horizontal_steps = img_channel.shape[1] - kernel_height + 1 vertical_steps   = img_channel.shape[2] - kernel_width  + 1  out = torch.zeros((horizontal_steps, vertical_steps)) for i in range(horizontal_steps):   for j in range(vertical_steps):     patch = img_channel.squeeze(0)[i:i+kernel_height, j:j+kernel_width]     out[i, j] = patch.mean() if operation == 'avg' else patch.max()  out.unsqueeze(0) Out[43]: <pre>tensor([[[4., 5.],\n         [7., 8.]]])</pre> In\u00a0[44]: Copied! <pre># or equivalently\nnn.MaxPool2d(kernel_size=(2,2), stride=1, padding=0)(img_channel)\n</pre> # or equivalently nn.MaxPool2d(kernel_size=(2,2), stride=1, padding=0)(img_channel) Out[44]: <pre>tensor([[[4., 5.],\n         [7., 8.]]])</pre> In\u00a0[45]: Copied! <pre>nn.AvgPool2d(kernel_size=(2,2), stride=1, padding=0)(img_channel)\n</pre> nn.AvgPool2d(kernel_size=(2,2), stride=1, padding=0)(img_channel) Out[45]: <pre>tensor([[[2., 3.],\n         [5., 6.]]])</pre> <p>In multiple dimensions, instead of summing channel outputs of each kernel, we will concatenate them after the pooling operation, which makes sense, as we have to retain channel information. Notice that the output channel is 2 dimensions, as we pass both image channels to <code>nn.MaxPool2d()</code>.</p> In\u00a0[46]: Copied! <pre>nn.MaxPool2d(kernel_size=2, stride=1, padding=0)(img)\n</pre> nn.MaxPool2d(kernel_size=2, stride=1, padding=0)(img) Out[46]: <pre>tensor([[[[4., 5.],\n          [7., 8.]],\n\n         [[5., 6.],\n          [8., 9.]]]])</pre> <p>The origins of Convolutional Neural Networks (CNN) goes back to the 1980 paper called Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position by Kunihiko Fukushima. As the computational power increased and the area developed, infamous CNN implementation by Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner was introduced in the 1998 paper called Gradient-Based Learning Applied to Document Recognition. The convolutional network architecture proposed for detecting handwritten digits (and more) was named after its first author LeNet.</p> <p></p> <p>The figure 7.6.1 describes the LeNet architecture. Convolutional block consists of convolution layer, sigmoid activation function (convolution is a linear function), and average pooling layer. Note that the superiority of max-pooling over average pooling, as well as ReLU activation over Sigmoid activation function weren't yet discovered. Kernel size of convolutional layers is <code>5x5</code> (with initial padding of <code>2</code>). Two convolutional layers output <code>6</code> and <code>16</code> feature maps respectively. Average pooling kernel size is <code>2x2</code> (with stride <code>2</code>).</p> <p>Once we extract the relevant features with convolutional blocks, it is time to make predictions with FC (Dense) layers. For that, we must flatten the feature maps similar to what we did when using our simple <code>MLP</code>, which was directly accepting the image as its inputs, instead of convolutional feature maps. FC layers sizes proposed in the paper are <code>120, 84, 10</code>, classifying 10 handwritten digits.</p> <p>Exercise: Code the model architecture by using the knowledge of <code>MLP class</code>.</p> <p>We will now code and train a modern implementation for <code>LeNet</code>, similar to <code>MLP</code>. We will replace average pool with max-pool layer, Sigmoid with ReLU activation, and the original Gaussian decoder with softmax function. We will use <code>Lazy</code> versions of the layers in order to not hard-code input dimensions.</p> In\u00a0[47]: Copied! <pre>class LeNet(nn.Module):\n  def __init__(self):\n    super(LeNet, self).__init__()\n    self.conv1 = nn.LazyConv2d(out_channels=6, kernel_size=(5,5), padding=2)\n    self.conv2 = nn.LazyConv2d(out_channels=16, kernel_size=(5,5))\n    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n    self.fc1 = nn.LazyLinear(out_features=84)\n    self.fc2 = nn.LazyLinear(out_features=10)\n    self.act = nn.ReLU()\n\n  def forward(self, X):\n    block1 = self.pool(self.act(self.conv1(X)))\n    block2 = self.pool(self.act(self.conv2(block1)))\n    flatten = block2.view(block2.shape[0], -1)\n    logits = self.fc2(self.act(self.fc1(flatten)))\n    return nn.Softmax(dim=-1)(logits)\n</pre> class LeNet(nn.Module):   def __init__(self):     super(LeNet, self).__init__()     self.conv1 = nn.LazyConv2d(out_channels=6, kernel_size=(5,5), padding=2)     self.conv2 = nn.LazyConv2d(out_channels=16, kernel_size=(5,5))     self.pool = nn.MaxPool2d(kernel_size=2, stride=2)     self.fc1 = nn.LazyLinear(out_features=84)     self.fc2 = nn.LazyLinear(out_features=10)     self.act = nn.ReLU()    def forward(self, X):     block1 = self.pool(self.act(self.conv1(X)))     block2 = self.pool(self.act(self.conv2(block1)))     flatten = block2.view(block2.shape[0], -1)     logits = self.fc2(self.act(self.fc1(flatten)))     return nn.Softmax(dim=-1)(logits) In\u00a0[48]: Copied! <pre>mlp = nn.Sequential(\n    nn.Flatten(),\n    nn.LazyLinear(out_features=128),\n    nn.ReLU(),\n    nn.LazyLinear(out_features=10),\n    nn.Softmax(dim=-1)\n)\n\nlenet = nn.Sequential(\n    nn.LazyConv2d(out_channels=6, kernel_size=5, padding=2),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.LazyConv2d(out_channels=16, kernel_size=5),\n    nn.ReLU(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Flatten(),\n    nn.LazyLinear(out_features=84),\n    nn.ReLU(),\n    nn.LazyLinear(out_features=10),\n    nn.Softmax(dim=-1)\n)\n</pre> mlp = nn.Sequential(     nn.Flatten(),     nn.LazyLinear(out_features=128),     nn.ReLU(),     nn.LazyLinear(out_features=10),     nn.Softmax(dim=-1) )  lenet = nn.Sequential(     nn.LazyConv2d(out_channels=6, kernel_size=5, padding=2),     nn.ReLU(),     nn.MaxPool2d(kernel_size=2, stride=2),     nn.LazyConv2d(out_channels=16, kernel_size=5),     nn.ReLU(),     nn.AvgPool2d(kernel_size=2, stride=2),     nn.Flatten(),     nn.LazyLinear(out_features=84),     nn.ReLU(),     nn.LazyLinear(out_features=10),     nn.Softmax(dim=-1) ) In\u00a0[49]: Copied! <pre>class Classifier:\n  def __init__(self, model, lr=0.01):\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.model = model.to(device)\n    self.optimizer = optim.SGD(model.parameters(), lr=lr)\n    self.loss_fn = nn.CrossEntropyLoss()\n\n  def fit(self, train_loader, num_epochs=10):\n    self.model.train()\n    for epoch in range(num_epochs):\n      loss = 0.0\n      for X_train, y_train in train_loader:\n        X_train, y_train = X_train.to(device), y_train.to(device)\n        self.optimizer.zero_grad()\n        preds = self.model(X_train)\n        batch_loss = self.loss_fn(preds, y_train)\n        batch_loss.backward()\n        loss += batch_loss.item()\n        self.optimizer.step()\n      print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}\")\n\n  def inference(self, test_loader):\n    self.model.eval()\n    correct = 0\n    all_preds = []\n    with torch.no_grad():\n      for X_test, y_test in test_loader:\n        X_test, y_test = X_test.to(device), y_test.to(device)\n        preds = self.model(X_test)\n        _, top_preds = torch.max(preds, dim=1)\n        correct += (top_preds == y_test).sum().item()\n        all_preds.append(top_preds)\n    accuracy = 100 * correct / len(test_loader.dataset)\n    print(f\"Accuracy on test data: {accuracy:.2f}%\")\n</pre> class Classifier:   def __init__(self, model, lr=0.01):     self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     self.model = model.to(device)     self.optimizer = optim.SGD(model.parameters(), lr=lr)     self.loss_fn = nn.CrossEntropyLoss()    def fit(self, train_loader, num_epochs=10):     self.model.train()     for epoch in range(num_epochs):       loss = 0.0       for X_train, y_train in train_loader:         X_train, y_train = X_train.to(device), y_train.to(device)         self.optimizer.zero_grad()         preds = self.model(X_train)         batch_loss = self.loss_fn(preds, y_train)         batch_loss.backward()         loss += batch_loss.item()         self.optimizer.step()       print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}\")    def inference(self, test_loader):     self.model.eval()     correct = 0     all_preds = []     with torch.no_grad():       for X_test, y_test in test_loader:         X_test, y_test = X_test.to(device), y_test.to(device)         preds = self.model(X_test)         _, top_preds = torch.max(preds, dim=1)         correct += (top_preds == y_test).sum().item()         all_preds.append(top_preds)     accuracy = 100 * correct / len(test_loader.dataset)     print(f\"Accuracy on test data: {accuracy:.2f}%\") In\u00a0[50]: Copied! <pre>clf_mlp = Classifier(mlp)\nclf_mlp.fit(train_loader)\nclf_mlp.inference(test_loader)\n</pre> clf_mlp = Classifier(mlp) clf_mlp.fit(train_loader) clf_mlp.inference(test_loader) <pre>Epoch: 1/10, Loss: 2.2903\nEpoch: 2/10, Loss: 2.2232\nEpoch: 3/10, Loss: 2.0272\nEpoch: 4/10, Loss: 1.8475\nEpoch: 5/10, Loss: 1.7862\nEpoch: 6/10, Loss: 1.7610\nEpoch: 7/10, Loss: 1.7387\nEpoch: 8/10, Loss: 1.7022\nEpoch: 9/10, Loss: 1.6840\nEpoch: 10/10, Loss: 1.6726\nAccuracy on test data: 83.36%\n</pre> In\u00a0[51]: Copied! <pre>clf_lenet = Classifier(lenet)\nclf_lenet.fit(train_loader)\nclf_lenet.inference(test_loader)\n</pre> clf_lenet = Classifier(lenet) clf_lenet.fit(train_loader) clf_lenet.inference(test_loader) <pre>Epoch: 1/10, Loss: 2.3026\nEpoch: 2/10, Loss: 2.3020\nEpoch: 3/10, Loss: 2.3011\nEpoch: 4/10, Loss: 2.2996\nEpoch: 5/10, Loss: 2.2953\nEpoch: 6/10, Loss: 2.2337\nEpoch: 7/10, Loss: 1.8479\nEpoch: 8/10, Loss: 1.6735\nEpoch: 9/10, Loss: 1.6107\nEpoch: 10/10, Loss: 1.5920\nAccuracy on test data: 88.74%\n</pre> <p>Finally, it is important to note that convolutions are expensive matrix operations, when GPUs can come in aid. Also, do not rush to compare <code>MLP</code> with <code>LeNet</code>, as our <code>MNIST</code> dataset is very simple. In the next lecture, we will discuss many optimization and regularization techniques of artificial neural networks.</p>"},{"location":"notebooks/03_cnn_torch/#03-from-kernel-to-convolutional-neural-network","title":"03. From Kernel to Convolutional Neural Network\u00b6","text":"15 Feb 2025 \u00b7    <p>Important</p> <p>     The notebook is currently under revision.   </p>"},{"location":"notebooks/03_cnn_torch/#image-dimensions","title":"Image Dimensions\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#mnist-dataset","title":"MNIST Dataset\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#training-custom-pytorch-model-on-mnist","title":"Training Custom PyTorch Model on MNIST\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#cross-correlation-operation","title":"Cross-Correlation Operation\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#kernels","title":"Kernels\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#padding-stride","title":"Padding &amp; Stride\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#increasing-dimensions","title":"Increasing Dimensions\u00b6","text":"<p>So far we have been working with grayscale images. When we increase the dimension of channels <code>C</code>, we will simply have <code>C</code> number of kernels. We will apply convolution of each kernel to their corresponding channels and eventually sum them up. Below is figure 7.4.1.</p> <p></p>"},{"location":"notebooks/03_cnn_torch/#maximum-average-pooling","title":"Maximum &amp; Average Pooling\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#convolutional-neural-network","title":"Convolutional Neural Network\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#sequential-module-in-pytorch","title":"Sequential Module in PyTorch\u00b6","text":"<p>Before testing our convnet, we will rewrite our model classes in order to demonstrate the power and clarity of <code>nn.Sequential</code> module in <code>PyTorch</code>. The code below is self-explanatory and is equivalent to <code>MLP</code> and <code>LeNet</code> classes we had initilaized previously. Notice how simple it is.</p>"},{"location":"notebooks/03_cnn_torch/#training-inference","title":"Training &amp; Inference\u00b6","text":"<p>We will abstract away our model initalization, train and inference codes before testing our models.</p>"},{"location":"notebooks/04_regul_optim/","title":"04. Regularization and Optimization","text":"<p>Our artificial neural networks which we built from scratch in previous classes were in their simplistic form and very inefficient, causing slow and less accurate training. In this notebook we will introduce many regularization/optimization techniques which will improve our model performance.</p> <p>The prerequisite of this course, Machine Learning, should have introduced the problems of underfitting and overfitting (See StatQuest video and MLU vizualization on bias and variance). Regularization) techniques like weight decay tackle high variance (overfitting) without getting more high-quality data, which is often costly.</p> <p>Loss function $L$ (e.g. MSE) can be regularized by adding to it regularizer $R$, where \u03bb is the regularization hyperparameter that controls the strength of the regularization function (we saw <code>learning rate</code> hyperparameter previously).</p> <p>$$ L(w, b) + \\lambda R(w) $$</p> <p>Even if there are many different regularizers for loss, the most common and practically efficient one is $l_2$ (ridge) regularization, which is called weight decay in the context of Deep Learning. It has the following formula:</p> <p>$$ L(w, b) + \\frac{\\lambda}{2} \\sum_{i=1}^{d} w_i^2 $$</p> <p>As can be seen, the eventual loss increases for higher weight values. Hence, backpropagation will not only reduce the chosen loss function (e.g. MSE) but will also strive for smaller weights. You can imagine that in the limit weights approach zero, reducing the impact of the corresponding neuron on the outcome. Less neuron impact means getting simpler function to avoid overfitting.</p> <p>When $\u03bb$ is zero we restore the original loss function, when $\u03bb$ is large it forces shifts the attention from the original loss function to $R$, constraining $w$ further. We divide by $2$ so that when the derivative of the square will be found, it will get cancelled out with it, simplifying our derivative expression to $ \u03bbw_i $. Recall linear algebra that we are finding the square of the euclidean norm of $d$-dimensional vector, again to ease computation: we remove the burden of finding the square root in $l_2$.</p> In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import torch import torch.nn as nn import torch.optim as optim  import numpy as np import matplotlib.pyplot as plt %matplotlib inline <p>We will now implement the simplified and clearer version of the example noted in the d2l book. We will generate random data corresponding to an N-dimensional linear function, where each data point will be shifted by some amount of noise.</p> In\u00a0[3]: Copied! <pre>def generate_data(samples=100, N=100, test_size=0.5, b=0.05, scale=0.01):\n  X = torch.randn(samples, N)\n  w = torch.randn(N, 1) * scale # scaling will ease training\n  n = torch.randn(samples, 1) * scale # noise is not a bias\n  y = torch.matmul(X, w) + n + b\n\n  size = int(samples * test_size)\n  X_train, X_test = X[:-size], X[-size:]\n  y_train, y_test = y[:-size], y[-size:]\n\n  return X_train, X_test, y_train, y_test\n</pre> def generate_data(samples=100, N=100, test_size=0.5, b=0.05, scale=0.01):   X = torch.randn(samples, N)   w = torch.randn(N, 1) * scale # scaling will ease training   n = torch.randn(samples, 1) * scale # noise is not a bias   y = torch.matmul(X, w) + n + b    size = int(samples * test_size)   X_train, X_test = X[:-size], X[-size:]   y_train, y_test = y[:-size], y[-size:]    return X_train, X_test, y_train, y_test In\u00a0[4]: Copied! <pre>X_train, _, y_train, _ = generate_data(100, 1) # 1D input for plotting\nplt.scatter(X_train, y_train);\n</pre> X_train, _, y_train, _ = generate_data(100, 1) # 1D input for plotting plt.scatter(X_train, y_train); <p>We will now generate a bigger dataset (both in the number of samples and dimensions), a good deal of which will be used for testing (so that we can illustrate overfitting). We will then create simple <code>Linear Regression</code> model and train it. When training we will implement the $l_2$ regularization discussed above to see how it affects the test loss.</p> In\u00a0[5]: Copied! <pre>X_train, X_test, y_train, y_test = generate_data(samples=100, N=200, test_size=0.5, scale=0.01)\n</pre> X_train, X_test, y_train, y_test = generate_data(samples=100, N=200, test_size=0.5, scale=0.01) In\u00a0[6]: Copied! <pre>def train(num_epochs=1000, lambda_=0.1):\n  model = nn.LazyLinear(1)\n  optimizer = optim.SGD(model.parameters(), lr=0.001)\n  mse = nn.MSELoss()\n\n  # needed for plotting\n  train_losses = []\n  test_losses = []\n\n  for epoch in range(num_epochs):\n    optimizer.zero_grad()\n    pred = model(X_train)\n    # regularization\n    l2 = sum(p.pow(2).sum() for p in model.parameters())/2\n    loss = mse(pred, y_train) + lambda_ * l2 # see formula above\n    loss.backward()\n    optimizer.step()\n\n    # inference during training\n    with torch.no_grad():\n      test_pred = model(X_test)\n      test_loss = mse(test_pred, y_test)\n\n    # needed for plotting\n    train_losses.append(loss.item())\n    test_losses.append(test_loss.item())\n\n    if epoch % 200 == 0:\n      print(f'Epoch {epoch}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n\n  # plotting\n  plt.figure(figsize=(6, 4))\n  plt.plot(range(num_epochs), train_losses, label='Train Loss')\n  plt.plot(range(num_epochs), test_losses, label='Test Loss')\n  plt.xlabel('Epochs')\n  plt.ylabel('Loss')\n</pre> def train(num_epochs=1000, lambda_=0.1):   model = nn.LazyLinear(1)   optimizer = optim.SGD(model.parameters(), lr=0.001)   mse = nn.MSELoss()    # needed for plotting   train_losses = []   test_losses = []    for epoch in range(num_epochs):     optimizer.zero_grad()     pred = model(X_train)     # regularization     l2 = sum(p.pow(2).sum() for p in model.parameters())/2     loss = mse(pred, y_train) + lambda_ * l2 # see formula above     loss.backward()     optimizer.step()      # inference during training     with torch.no_grad():       test_pred = model(X_test)       test_loss = mse(test_pred, y_test)      # needed for plotting     train_losses.append(loss.item())     test_losses.append(test_loss.item())      if epoch % 200 == 0:       print(f'Epoch {epoch}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')    # plotting   plt.figure(figsize=(6, 4))   plt.plot(range(num_epochs), train_losses, label='Train Loss')   plt.plot(range(num_epochs), test_losses, label='Test Loss')   plt.xlabel('Epochs')   plt.ylabel('Loss') <p>Try to understand why the line which calculates regularization function's output has two <code>sum</code> functions: <code>l2 = sum(p.pow(2).sum() for p in model.parameters())/2</code>. We have plotting logic in the function above for demonstration purposes, and we will give different $\u03bb$ values to customize our regularizer $R$ ($\u03bb$=0 implies no regularization).</p> In\u00a0[7]: Copied! <pre>train(lambda_=0.0) # no regularization\n</pre> train(lambda_=0.0) # no regularization <pre>Epoch 0, Train Loss: 0.3230, Test Loss: 0.4413\nEpoch 200, Train Loss: 0.0165, Test Loss: 0.3265\nEpoch 400, Train Loss: 0.0023, Test Loss: 0.3051\nEpoch 600, Train Loss: 0.0005, Test Loss: 0.2979\nEpoch 800, Train Loss: 0.0002, Test Loss: 0.2950\n</pre> In\u00a0[8]: Copied! <pre>train(lambda_=5.0) # with regularization\n</pre> train(lambda_=5.0) # with regularization <pre>Epoch 0, Train Loss: 1.1423, Test Loss: 0.2795\nEpoch 200, Train Loss: 0.1009, Test Loss: 0.0429\nEpoch 400, Train Loss: 0.0218, Test Loss: 0.0185\nEpoch 600, Train Loss: 0.0116, Test Loss: 0.0153\nEpoch 800, Train Loss: 0.0103, Test Loss: 0.0150\n</pre> In\u00a0[9]: Copied! <pre>X = torch.arange(1,7)\nprint(\"Initial layer activations:\", X)\n\nprob = 0.5 # try out different values between 0 and 1\nmask = (torch.rand(X.shape) &gt; prob ).float()\nprint(f\"Random mask with dropout probability {prob}: {mask}\")\n\nX_dropout = X * mask\nprint(f\"Layer activations after dropout {X_dropout}\")\n\nX_dropout_scaled = X_dropout / (1.0 - prob + 1e-9)\nprint(f\"Scaled activations after dropout {X_dropout_scaled}\")\n</pre> X = torch.arange(1,7) print(\"Initial layer activations:\", X)  prob = 0.5 # try out different values between 0 and 1 mask = (torch.rand(X.shape) &gt; prob ).float() print(f\"Random mask with dropout probability {prob}: {mask}\")  X_dropout = X * mask print(f\"Layer activations after dropout {X_dropout}\")  X_dropout_scaled = X_dropout / (1.0 - prob + 1e-9) print(f\"Scaled activations after dropout {X_dropout_scaled}\") <pre>Initial layer activations: tensor([1, 2, 3, 4, 5, 6])\nRandom mask with dropout probability 0.5: tensor([0., 0., 0., 0., 1., 1.])\nLayer activations after dropout tensor([0., 0., 0., 0., 5., 6.])\nScaled activations after dropout tensor([ 0.,  0.,  0.,  0., 10., 12.])\n</pre> <p>The idea behind dropout is that it injects noise to the network during training. If a model can achieve good accuracy with noise, that probably implies that it has learned a more generalizable function. Dropping out certain proportion of neurons in each iteration basically guides the model to train a smaller network. Scaling is done to so that the expected sum of activations will roughly remain the same to compensate for missing activations. See Andrew Ng's explanation to get a better intuition of dropout. While watching the video, note that in other framework implementations <code>keep_prob</code> can be used for dropout, which keeps all the nodes in case of being set to <code>1.0</code>. In the <code>PyTorch</code> implementation, the same is achieved by doing the opposite and setting the probability to <code>0.0</code>. Although a useful technique, dropout should be used carefully in order to not hurt the overall performance of the model.</p> <p>We will now implement a <code>Dropout</code> class (an equivalent of <code>nn.Dropout</code> module of <code>PyTorch</code>) and build MLP. Dropout probability is usually set higher for bigger layers and layer closer to the input (but not the input layer, as it doesn't make sense to drop out the input features). Once the training is over, dropout is usually turned off. However, it can also be used in the test time as a mean of estimating how uncertain the neural network is.</p> In\u00a0[10]: Copied! <pre>class Dropout(nn.Module):\n  def __init__(self, prob):\n    super().__init__()\n    assert 0 &lt;= prob &lt;= 1\n    self.prob = prob\n    self.epsilon = 1e-8\n\n  def forward(self, X):\n    if self.training: # model.eval() will turn it off\n      mask = (torch.rand_like(X) &gt; self.prob).float()\n      X = X * mask / (1.0 - self.prob + self.epsilon)\n    return X\n</pre> class Dropout(nn.Module):   def __init__(self, prob):     super().__init__()     assert 0 &lt;= prob &lt;= 1     self.prob = prob     self.epsilon = 1e-8    def forward(self, X):     if self.training: # model.eval() will turn it off       mask = (torch.rand_like(X) &gt; self.prob).float()       X = X * mask / (1.0 - self.prob + self.epsilon)     return X In\u00a0[11]: Copied! <pre>model = nn.Sequential(\n    nn.Flatten(),\n    nn.LazyLinear(256),\n    nn.ReLU(),\n    Dropout(0.5), # nn.Dropout(0.5)\n    nn.LazyLinear(128),\n    nn.ReLU(),\n    Dropout(0.3), # nn.Dropout(0.3)\n    nn.LazyLinear(10)\n)\n</pre> model = nn.Sequential(     nn.Flatten(),     nn.LazyLinear(256),     nn.ReLU(),     Dropout(0.5), # nn.Dropout(0.5)     nn.LazyLinear(128),     nn.ReLU(),     Dropout(0.3), # nn.Dropout(0.3)     nn.LazyLinear(10) ) <p>So far we have looked at two regularization techniques: $l_2$ regularization and dropout. There is also a technique called early stopping which simply stops the training when validation loss stops improving. Regulization methods had the goal of reducing overfitting, and thus increasing the generalization ability of our model. Additionally, we would like to optimize our models to efficiently minimize the loss and converge to a good solution.</p> In\u00a0[12]: Copied! <pre>import scipy.stats as stats\n\nW = torch.normal(mean=0, std=1, size=(10000,))\nx = np.linspace(-4, 4, 1000)\npdf = stats.norm.pdf(x, 0, 1)\n\nplt.figure(figsize=(6, 4))\nplt.hist(W.numpy(), bins=50, density=True, alpha=0.6)\nplt.plot(x, pdf, linewidth=2)\nplt.title('Random Normal (Gaussian) Distribution');\n</pre> import scipy.stats as stats  W = torch.normal(mean=0, std=1, size=(10000,)) x = np.linspace(-4, 4, 1000) pdf = stats.norm.pdf(x, 0, 1)  plt.figure(figsize=(6, 4)) plt.hist(W.numpy(), bins=50, density=True, alpha=0.6) plt.plot(x, pdf, linewidth=2) plt.title('Random Normal (Gaussian) Distribution'); <p>By chain rule, we know that the node gradients are multiplied. Hence, multiplying many gradients that have big value along our neural network will explode the final gradient, resulting in unnessarily big jumps and missing the minimum of the function and thus not converging. The opposite is also true: if the gradients are too small then multiplying lost of small values will result in a number that is almost zero, causing the vanishing gradient problem. With gradients almost zero, values during backpropagation will not get updated and the learning will stop. Below are some examples from the textbook.</p> In\u00a0[13]: Copied! <pre>W = torch.normal(mean=0, std=1, size=(3, 3)) # Gaussian distribution matrix\n\nnum_epochs = 20\nfor _ in range(num_epochs):\n  W = W @ torch.normal(mean=0, std=1, size=(3, 3)) # matrix multiplication of N such matrices\nprint('Exploding gradients:\\n', W)\n</pre> W = torch.normal(mean=0, std=1, size=(3, 3)) # Gaussian distribution matrix  num_epochs = 20 for _ in range(num_epochs):   W = W @ torch.normal(mean=0, std=1, size=(3, 3)) # matrix multiplication of N such matrices print('Exploding gradients:\\n', W) <pre>Exploding gradients:\n tensor([[-2657.0173,   939.6129,  3719.5845],\n        [-2549.2896,   842.5388,  3360.0208],\n        [ -697.6809,   168.5173,   699.8742]])\n</pre> <p>We will now take three activation functions (<code>sigmoid</code>, <code>relu</code>, <code>leaky_relu</code>) and plot their functions together with the graident values. Try to interpret what the plots mean.</p> In\u00a0[14]: Copied! <pre>plt.figure(figsize=(14,3))\n\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny_sigmoid = torch.sigmoid(x)\ny_sigmoid.backward(torch.ones_like(x))\n\nplt.subplot(1, 3, 1)\nplt.plot(x.detach().numpy(), y_sigmoid.detach().numpy(), label='Sigmoid')\nplt.plot(x.detach().numpy(), x.grad.numpy(), label='Gradient')\nplt.legend()\n\nx.grad.zero_()\ny_relu = torch.relu(x)\ny_relu.backward(torch.ones_like(x))\n\nplt.subplot(1, 3, 2)\nplt.plot(x.detach().numpy(), y_relu.detach().numpy(), label='ReLU')\nplt.plot(x.detach().numpy(), x.grad.numpy(), label='Gradient')\nplt.legend()\n\nx.grad.zero_()\ny_leaky_relu = torch.nn.functional.leaky_relu(x, 0.2)\ny_leaky_relu.backward(torch.ones_like(x))\n\nplt.subplot(1, 3, 3)\nplt.plot(x.detach().numpy(), y_leaky_relu.detach().numpy(), label='Leaky ReLU')\nplt.plot(x.detach().numpy(), x.grad.numpy(), label='Gradient')\nplt.legend();\n</pre> plt.figure(figsize=(14,3))  x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True) y_sigmoid = torch.sigmoid(x) y_sigmoid.backward(torch.ones_like(x))  plt.subplot(1, 3, 1) plt.plot(x.detach().numpy(), y_sigmoid.detach().numpy(), label='Sigmoid') plt.plot(x.detach().numpy(), x.grad.numpy(), label='Gradient') plt.legend()  x.grad.zero_() y_relu = torch.relu(x) y_relu.backward(torch.ones_like(x))  plt.subplot(1, 3, 2) plt.plot(x.detach().numpy(), y_relu.detach().numpy(), label='ReLU') plt.plot(x.detach().numpy(), x.grad.numpy(), label='Gradient') plt.legend()  x.grad.zero_() y_leaky_relu = torch.nn.functional.leaky_relu(x, 0.2) y_leaky_relu.backward(torch.ones_like(x))  plt.subplot(1, 3, 3) plt.plot(x.detach().numpy(), y_leaky_relu.detach().numpy(), label='Leaky ReLU') plt.plot(x.detach().numpy(), x.grad.numpy(), label='Gradient') plt.legend(); <p>As can be seen, for the sigmoid function, the gradients are almost zero for low and high ends of the function (saddle points) due to its gradient being $\u03c3(x)(1\u2212\u03c3(x))$. Since the AlexNet paper, for the reason of vanishing gradient problem, <code>relu</code> activation is usually preferred over <code>sigmoid</code>. Note that when the output is mapped to the negative values on the <code>relu</code>, derivates become zero, still causing so-called dying relu problem, which can be solved to some degree with the help of <code>leaky_relu</code> where the gradients do not become exactly zero when the output is mapped to the negative value. Still, in practice, <code>relu</code> is often robust.</p> <p>For simplicity, let's take a simple neural network layer without activation and bias. We know that the next layer's values will be affected by the following sum: $o_{i} = \\sum_{j=1}^{n_\\textrm{in}} w_{ij} x_j$. Let's also assume that the weights are initializated with the Guassian distribution with zero mean. Then we can calculate the Expectation $E$ (mean). Recall that $E$ of a random variable represents the average value you would expect if you repeated experiment many times.</p> <p>\\begin{split}\\begin{aligned}     E[o_i] &amp; = \\sum_{j=1}^{n_\\textrm{in}} E[w_{ij} x_j] = \\sum_{j=1}^{n_\\textrm{in}} E[w_{ij}] E[x_j] = \\sum_{j=1}^{n_\\textrm{in}} 0 \\cdot E[x_j] = 0 \\end{aligned}\\end{split}</p> <p>Recall that variance measures the spread of data around its mean. Squaring is applied so that negative and positive values will not cancel each others out. Considering that weights and inputs are independent:</p> <p>\\begin{split}\\begin{aligned}     \\textrm{Var}[o_i] &amp; = E[o_i^2] - (E[o_i])^2 = \\sum_{j=1}^{n_\\textrm{in}} E[w^2_{ij} x^2_j] - 0  = \\sum_{j=1}^{n_\\textrm{in}} E[w^2_{ij}] E[x^2_j] = n_\\textrm{in} \\sigma^2 \\gamma^2 \\end{aligned}\\end{split}</p> <p>It implies that the variance of the output will be proportional to the number of input neurons.</p> In\u00a0[\u00a0]: Copied! <pre>n_out = 10\nfor n_in in [10**i for i in range(5)]:\n  X = torch.randn(n_in)\n  W = torch.normal(mean=0, std=1, size=(n_in, n_out))\n  O = X @ W\n  print(f'n_in: {n_in}, var(o): {O.var().item()}')\n</pre> n_out = 10 for n_in in [10**i for i in range(5)]:   X = torch.randn(n_in)   W = torch.normal(mean=0, std=1, size=(n_in, n_out))   O = X @ W   print(f'n_in: {n_in}, var(o): {O.var().item()}') <pre>n_in: 1, var(o): 0.6714984774589539\nn_in: 10, var(o): 2.69901967048645\nn_in: 100, var(o): 45.740028381347656\nn_in: 1000, var(o): 1667.195068359375\nn_in: 10000, var(o): 12416.2021484375\n</pre> <p>To mitigate the exploding gradient problem, an obvious trick is to We can achieve the stability of variance by ensuring that $n_\\textrm{in} \\sigma^2 = 1$. Hence we retrieve the standard deviation to be $\\sigma = \\sqrt{\\frac{1}{n_\\textrm{in}}}$. The paper called Understanding the difficulty of training deep feedforward neural networks by Xavier Glorot and Yoshua Bengio propose this solution together with considering the case for backpropagation. With the same logic, we can realize that to keep the gradient variance consistent, we should ensure $n_\\textrm{out} \\sigma^2 = 1$, where $n_\\textrm{out}$ is the number of output neurons. To satisfy both cases, we simply average the variance scaling: $(n_\\textrm{in} + n_\\textrm{out}) \\sigma^2 = 1$ or equivalently $\\sigma = \\sqrt{\\frac{2}{n_\\textrm{in} + n_\\textrm{out}}}$.</p> <p>Xavier initialization is mainly used for <code>sigmoid</code> and <code>tanh</code> activations. The paper Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification addresses rectified non-linearities. As <code>relu</code> function doesn't struggle with the vanishing gradient problem, the backpropagation doesn't have much effect on the variance. Hence Kaiming He initialization gets the form of $\\sigma = \\sqrt{\\frac{2}{n_\\textrm{in}}}$.</p> In\u00a0[\u00a0]: Copied! <pre>n_out = 10\nfor n_in in [10**i for i in range(5)]:\n  X = torch.randn(n_in)\n  W = torch.normal(mean=0, std=np.sqrt(2.0/n_in), size=(n_in, n_out))\n  O = X @ W\n  print(f'n_in: {n_in}, var(o): {O.var().item()}')\n</pre> n_out = 10 for n_in in [10**i for i in range(5)]:   X = torch.randn(n_in)   W = torch.normal(mean=0, std=np.sqrt(2.0/n_in), size=(n_in, n_out))   O = X @ W   print(f'n_in: {n_in}, var(o): {O.var().item()}') <pre>n_in: 1, var(o): 0.12620338797569275\nn_in: 10, var(o): 2.6602299213409424\nn_in: 100, var(o): 2.959010124206543\nn_in: 1000, var(o): 2.462198257446289\nn_in: 10000, var(o): 2.8828887939453125\n</pre> <p>When minimizing the loss with gradient descent, we often deal with complicated functions with local and global minimums. When we are in local minima, we need certain noise to kick our parameter out of it so that we can find global minimum value for the loss. It is also possible to have a function where the gradients almost disappear (vanish), yet it is not a local or global minima. Optimization will not improve when we are at that point of the function due to very small gradients. We will plot both of the mentioned cases in 2D, but they can be extended to higher dimensions. For example, the function $f(x,y)=x^2-y^2$ indeed looks like a horse saddle when plotted, with its saddle point at $(0,0,0)$, hence the terminology. Saddle points are more frequent than local minima.</p> In\u00a0[\u00a0]: Copied! <pre>f1 = lambda x: x * np.cos(np.pi * x)\nf2 = lambda x: x**3\n\nx1 = np.linspace(-1, 2, 100)\ny1 = f1(x1)\n\nlocal_min = (-0.3, f1(-0.3))\nglobal_min = (1.1, f1(1.1))\n\nx2 = np.linspace(-2, 2, 100)\ny2 = f2(x2)\n\nsaddle_point = (0.0, f2(0.0))\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 4))\n\naxs[0].plot(x1, y1, label=r'$f(x) = x \\cos(\\pi x)$', color='b')\naxs[0].scatter(*local_min, color='r', label=\"Local Minimum\")\naxs[0].scatter(*global_min, color='g', label=\"Global Minimum\")\naxs[0].legend()\naxs[0].set_title(\"Local &amp; Global Minima\")\n\naxs[1].plot(x2, y2, label=r'$f(x) = x^3$', color='m')\naxs[1].scatter(*saddle_point, color='orange', label=\"Saddle Point\", zorder=3)\naxs[1].legend()\naxs[1].set_title(\"Saddle Point\");\n</pre> f1 = lambda x: x * np.cos(np.pi * x) f2 = lambda x: x**3  x1 = np.linspace(-1, 2, 100) y1 = f1(x1)  local_min = (-0.3, f1(-0.3)) global_min = (1.1, f1(1.1))  x2 = np.linspace(-2, 2, 100) y2 = f2(x2)  saddle_point = (0.0, f2(0.0))  fig, axs = plt.subplots(1, 2, figsize=(10, 4))  axs[0].plot(x1, y1, label=r'$f(x) = x \\cos(\\pi x)$', color='b') axs[0].scatter(*local_min, color='r', label=\"Local Minimum\") axs[0].scatter(*global_min, color='g', label=\"Global Minimum\") axs[0].legend() axs[0].set_title(\"Local &amp; Global Minima\")  axs[1].plot(x2, y2, label=r'$f(x) = x^3$', color='m') axs[1].scatter(*saddle_point, color='orange', label=\"Saddle Point\", zorder=3) axs[1].legend() axs[1].set_title(\"Saddle Point\"); In\u00a0[\u00a0]: Copied! <pre>def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps):\n  path = [initial_point]\n  x = initial_point\n  for _ in range(num_steps):\n    grad = grad_f(x)\n    x -= learning_rate * grad\n    path.append(x)\n  return np.array(path)\n\ndef plot_descent(f, grad_f, point, steps, start, end, learning_rates=[0.1, 0.3, 0.9, 1.0], figsize=(14,4)):\n  x_vals = np.linspace(start, end, 100)\n  y_vals = f(x_vals)\n\n  fig, axs = plt.subplots(1, len(learning_rates), figsize=figsize)\n  for i, lr in enumerate(learning_rates):\n    path = gradient_descent(f, grad_f, lr, point, steps)\n    axs[i].plot(x_vals, y_vals, color='b')\n    axs[i].plot(path, f(path), marker='o', label=f'LR = {lr}', color=[1.0, 0.5, 0.5])\n    axs[i].set_title(f'LR = {lr}')\n</pre> def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps):   path = [initial_point]   x = initial_point   for _ in range(num_steps):     grad = grad_f(x)     x -= learning_rate * grad     path.append(x)   return np.array(path)  def plot_descent(f, grad_f, point, steps, start, end, learning_rates=[0.1, 0.3, 0.9, 1.0], figsize=(14,4)):   x_vals = np.linspace(start, end, 100)   y_vals = f(x_vals)    fig, axs = plt.subplots(1, len(learning_rates), figsize=figsize)   for i, lr in enumerate(learning_rates):     path = gradient_descent(f, grad_f, lr, point, steps)     axs[i].plot(x_vals, y_vals, color='b')     axs[i].plot(path, f(path), marker='o', label=f'LR = {lr}', color=[1.0, 0.5, 0.5])     axs[i].set_title(f'LR = {lr}') In\u00a0[\u00a0]: Copied! <pre>f = lambda x: x**2\ngrad_f = lambda x: 2*x\n\nplot_descent(f, grad_f, 1.5, 10, -2, 2)\n</pre> f = lambda x: x**2 grad_f = lambda x: 2*x  plot_descent(f, grad_f, 1.5, 10, -2, 2) In\u00a0[\u00a0]: Copied! <pre>c = np.array(0.15 * np.pi)\nf = lambda x: x * np.cos(c * x)\ngrad_f = lambda x: np.cos(c * x) - c * x * np.sin(c * x)\n</pre> c = np.array(0.15 * np.pi) f = lambda x: x * np.cos(c * x) grad_f = lambda x: np.cos(c * x) - c * x * np.sin(c * x) In\u00a0[\u00a0]: Copied! <pre>plot_descent(f, grad_f, point=10, steps=10, start=-13, end=13, learning_rates=[0.3, 1.3, 2.0])\n</pre> plot_descent(f, grad_f, point=10, steps=10, start=-13, end=13, learning_rates=[0.3, 1.3, 2.0]) In\u00a0[\u00a0]: Copied! <pre>plot_descent(f, grad_f, point=-7, steps=5, start=-13, end=13, learning_rates=[0.5, 2.7, 3.5])\n</pre> plot_descent(f, grad_f, point=-7, steps=5, start=-13, end=13, learning_rates=[0.5, 2.7, 3.5]) In\u00a0[\u00a0]: Copied! <pre>def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, momentum=0.9):\n  assert momentum &lt;= 1 and momentum &gt;= 0\n  path = [initial_point]\n  x = initial_point\n\n  beta = momentum\n  v = 0.0\n  for _ in range(num_steps):\n    grad = grad_f(x)\n    v = beta * v + (1 - beta) * grad\n    x -= learning_rate * v\n    path.append(x)\n  return np.array(path)\n\ndef plot_descent(f, grad_f, point, steps, start, end, learning_rate, momentums=[0.1, 0.5, 0.9], figsize=(14,4)):\n  x_vals = np.linspace(start, end, 100)\n  y_vals = f(x_vals)\n\n  fig, axs = plt.subplots(1, len(momentums), figsize=figsize)\n  for i, momentum in enumerate(momentums):\n    path = gradient_descent(f, grad_f, learning_rate, point, steps, momentum)\n    axs[i].plot(x_vals, y_vals, color='b')\n    axs[i].plot(path, f(path), marker='o', label=f'M = {momentum}', color=[1.0, 0.5, 0.5])\n    axs[i].set_title(f'M = {momentum}')\n</pre> def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, momentum=0.9):   assert momentum &lt;= 1 and momentum &gt;= 0   path = [initial_point]   x = initial_point    beta = momentum   v = 0.0   for _ in range(num_steps):     grad = grad_f(x)     v = beta * v + (1 - beta) * grad     x -= learning_rate * v     path.append(x)   return np.array(path)  def plot_descent(f, grad_f, point, steps, start, end, learning_rate, momentums=[0.1, 0.5, 0.9], figsize=(14,4)):   x_vals = np.linspace(start, end, 100)   y_vals = f(x_vals)    fig, axs = plt.subplots(1, len(momentums), figsize=figsize)   for i, momentum in enumerate(momentums):     path = gradient_descent(f, grad_f, learning_rate, point, steps, momentum)     axs[i].plot(x_vals, y_vals, color='b')     axs[i].plot(path, f(path), marker='o', label=f'M = {momentum}', color=[1.0, 0.5, 0.5])     axs[i].set_title(f'M = {momentum}') In\u00a0[\u00a0]: Copied! <pre>plot_descent(f, grad_f, point=-7, steps=35, start=-13, end=13, learning_rate=1.1, momentums=[0.0, 0.6, 0.9])\n</pre> plot_descent(f, grad_f, point=-7, steps=35, start=-13, end=13, learning_rate=1.1, momentums=[0.0, 0.6, 0.9]) In\u00a0[\u00a0]: Copied! <pre>def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, gamma=0.9):\n  assert gamma &lt;= 1 and gamma &gt;= 0\n  path = [initial_point]\n  x = initial_point\n\n  s = 0.0\n  epsilon=1e-8\n  for _ in range(num_steps):\n    grad = grad_f(x)\n    s = gamma * s + (1 - gamma) * grad**2\n    x -= learning_rate * grad / (np.sqrt(s) + epsilon)\n    path.append(x)\n  return np.array(path)\n\ndef plot_descent(f, grad_f, point, steps, start, end, learning_rate, gammas=[0.9, 0.99, 0.999], figsize=(14,4)):\n  x_vals = np.linspace(start, end, 100)\n  y_vals = f(x_vals)\n\n  fig, axs = plt.subplots(1, len(gammas), figsize=figsize)\n  for i, gamma in enumerate(gammas):\n    path = gradient_descent(f, grad_f, learning_rate, point, steps, gamma)\n    axs[i].plot(x_vals, y_vals, color='b')\n    axs[i].plot(path, f(path), marker='o', label=f'G = {gamma}', color=[1.0, 0.5, 0.5])\n    axs[i].set_title(f'G = {gamma}')\n</pre> def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, gamma=0.9):   assert gamma &lt;= 1 and gamma &gt;= 0   path = [initial_point]   x = initial_point    s = 0.0   epsilon=1e-8   for _ in range(num_steps):     grad = grad_f(x)     s = gamma * s + (1 - gamma) * grad**2     x -= learning_rate * grad / (np.sqrt(s) + epsilon)     path.append(x)   return np.array(path)  def plot_descent(f, grad_f, point, steps, start, end, learning_rate, gammas=[0.9, 0.99, 0.999], figsize=(14,4)):   x_vals = np.linspace(start, end, 100)   y_vals = f(x_vals)    fig, axs = plt.subplots(1, len(gammas), figsize=figsize)   for i, gamma in enumerate(gammas):     path = gradient_descent(f, grad_f, learning_rate, point, steps, gamma)     axs[i].plot(x_vals, y_vals, color='b')     axs[i].plot(path, f(path), marker='o', label=f'G = {gamma}', color=[1.0, 0.5, 0.5])     axs[i].set_title(f'G = {gamma}') In\u00a0[\u00a0]: Copied! <pre>plot_descent(f, grad_f, point=12, steps=10, start=-13, end=13, learning_rate=0.5, gammas=[0, 0.7, 0.9])\n</pre> plot_descent(f, grad_f, point=12, steps=10, start=-13, end=13, learning_rate=0.5, gammas=[0, 0.7, 0.9]) In\u00a0[\u00a0]: Copied! <pre>def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, b1=0.9, b2=0.999, normalize=False):\n  assert b1 &lt;= 1 and b1 &gt;= 0\n  assert b2 &lt;= 1 and b2 &gt;= 0\n  path = [initial_point]\n  x = initial_point\n\n  v = 0.0\n  s = 0.0\n  epsilon=1e-8\n  for t in range(1, num_steps + 1):\n    grad = grad_f(x)\n    v = b1 * v + (1 - b1) * grad\n    s = b2 * s + (1 - b2) * grad**2\n    # as our function and plot are simple, we will keep\n    # unnormalized option as well for demonstration purposes\n    v_hat = v / (1 - b1**t) if normalize else v\n    s_hat = s / (1 - b2**t) if normalize else s\n    x -= learning_rate * v_hat / (np.sqrt(s_hat) + epsilon)\n    path.append(x)\n  return np.array(path)\n\ndef plot_descent(f, grad_f, point, steps, start, end, learning_rate, b1_vals=[0.9, 0.99, 0.999], b2_vals=[0.1, 0.99, 0.999], figsize=(14,4)):\n  x_vals = np.linspace(start, end, 100)\n  y_vals = f(x_vals)\n\n  fig, axs = plt.subplots(len(b1_vals), len(b2_vals), figsize=figsize)\n  for i, b1 in enumerate(b1_vals):\n    for j, b2 in enumerate(b2_vals):\n      path = gradient_descent(f, grad_f, learning_rate, point, steps, b1, b2)\n      axs[i, j].plot(x_vals, y_vals, color='b')\n      axs[i, j].plot(path, f(path), marker='o', label=f'B1 = {b1}, B2 = {b2}', color=[1.0, 0.5, 0.5])\n      axs[i, j].set_title(f'B1 = {b1}, B2 = {b2}')\n</pre> def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, b1=0.9, b2=0.999, normalize=False):   assert b1 &lt;= 1 and b1 &gt;= 0   assert b2 &lt;= 1 and b2 &gt;= 0   path = [initial_point]   x = initial_point    v = 0.0   s = 0.0   epsilon=1e-8   for t in range(1, num_steps + 1):     grad = grad_f(x)     v = b1 * v + (1 - b1) * grad     s = b2 * s + (1 - b2) * grad**2     # as our function and plot are simple, we will keep     # unnormalized option as well for demonstration purposes     v_hat = v / (1 - b1**t) if normalize else v     s_hat = s / (1 - b2**t) if normalize else s     x -= learning_rate * v_hat / (np.sqrt(s_hat) + epsilon)     path.append(x)   return np.array(path)  def plot_descent(f, grad_f, point, steps, start, end, learning_rate, b1_vals=[0.9, 0.99, 0.999], b2_vals=[0.1, 0.99, 0.999], figsize=(14,4)):   x_vals = np.linspace(start, end, 100)   y_vals = f(x_vals)    fig, axs = plt.subplots(len(b1_vals), len(b2_vals), figsize=figsize)   for i, b1 in enumerate(b1_vals):     for j, b2 in enumerate(b2_vals):       path = gradient_descent(f, grad_f, learning_rate, point, steps, b1, b2)       axs[i, j].plot(x_vals, y_vals, color='b')       axs[i, j].plot(path, f(path), marker='o', label=f'B1 = {b1}, B2 = {b2}', color=[1.0, 0.5, 0.5])       axs[i, j].set_title(f'B1 = {b1}, B2 = {b2}') In\u00a0[\u00a0]: Copied! <pre>plot_descent(f, grad_f, point=15, steps=20, start=-15, end=25, learning_rate=0.7, b1_vals=[0, 0.6, 0.9], b2_vals=[0, 0.6, 0.9], figsize=(12, 12))\n</pre> plot_descent(f, grad_f, point=15, steps=20, start=-15, end=25, learning_rate=0.7, b1_vals=[0, 0.6, 0.9], b2_vals=[0, 0.6, 0.9], figsize=(12, 12)) <p>Instead of simple grayscale MNIST dataset, we will choose CIFAR-10 this time to train our model on. It consists of <code>50,000</code> training and <code>10,000</code> test images of size <code>32x32</code> divided into 10 classes. An alternative version of the dataset called CIFAR-100 has 100 classes. Unlike MNIST, images have three channels (RGB) which makes computation more expensive and the task of the model more difficult.</p> In\u00a0[\u00a0]: Copied! <pre>from torchvision import datasets, transforms\n\ntrain_data = datasets.CIFAR10(root='./data', train=True,  download=True, transform=transforms.ToTensor())\ntest_data  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</pre> from torchvision import datasets, transforms  train_data = datasets.CIFAR10(root='./data', train=True,  download=True, transform=transforms.ToTensor()) test_data  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor()) <pre>Files already downloaded and verified\nFiles already downloaded and verified\n</pre> In\u00a0[\u00a0]: Copied! <pre>from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_data, batch_size=64, pin_memory=True, shuffle=True)\ntest_loader  = DataLoader(test_data,  batch_size=64, pin_memory=True, shuffle=False)\n</pre> from torch.utils.data import DataLoader  train_loader = DataLoader(train_data, batch_size=64, pin_memory=True, shuffle=True) test_loader  = DataLoader(test_data,  batch_size=64, pin_memory=True, shuffle=False) In\u00a0[\u00a0]: Copied! <pre>idx_to_class = {v: k for k, v in train_data.class_to_idx.items()}\nidx_to_class\n</pre> idx_to_class = {v: k for k, v in train_data.class_to_idx.items()} idx_to_class Out[\u00a0]: <pre>{0: 'airplane',\n 1: 'automobile',\n 2: 'bird',\n 3: 'cat',\n 4: 'deer',\n 5: 'dog',\n 6: 'frog',\n 7: 'horse',\n 8: 'ship',\n 9: 'truck'}</pre> In\u00a0[\u00a0]: Copied! <pre>X_train, y_train = next(iter(train_loader))\nplt.imshow(X_train[0].permute(1, 2, 0))\nplt.title(idx_to_class[y_train[0].item()]);\n</pre> X_train, y_train = next(iter(train_loader)) plt.imshow(X_train[0].permute(1, 2, 0)) plt.title(idx_to_class[y_train[0].item()]); In\u00a0[\u00a0]: Copied! <pre>model = nn.Sequential(\n    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Flatten(),\n    nn.Linear(128 * 4 * 4, 256),\n    nn.ReLU(),\n    nn.Dropout(0.5), # Dropout\n    nn.Linear(256, 10)\n)\n</pre> model = nn.Sequential(     nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),     nn.ReLU(),     nn.MaxPool2d(kernel_size=2, stride=2),     nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),     nn.ReLU(),     nn.MaxPool2d(kernel_size=2, stride=2),     nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),     nn.ReLU(),     nn.MaxPool2d(kernel_size=2, stride=2),     nn.Flatten(),     nn.Linear(128 * 4 * 4, 256),     nn.ReLU(),     nn.Dropout(0.5), # Dropout     nn.Linear(256, 10) ) In\u00a0[\u00a0]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model.to(device) Out[\u00a0]: <pre>Sequential(\n  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (4): ReLU()\n  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (7): ReLU()\n  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (9): Flatten(start_dim=1, end_dim=-1)\n  (10): Linear(in_features=2048, out_features=256, bias=True)\n  (11): ReLU()\n  (12): Dropout(p=0.5, inplace=False)\n  (13): Linear(in_features=256, out_features=10, bias=True)\n)</pre> In\u00a0[\u00a0]: Copied! <pre># Moment weigths are by default 0.9 and 0.999 for Adam optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.005, betas=(0.9, 0.999), weight_decay=1e-4)\nloss_fn = nn.CrossEntropyLoss()\n</pre> # Moment weigths are by default 0.9 and 0.999 for Adam optimizer optimizer = optim.Adam(model.parameters(), lr=0.005, betas=(0.9, 0.999), weight_decay=1e-4) loss_fn = nn.CrossEntropyLoss() In\u00a0[\u00a0]: Copied! <pre># Kaiming He Initialization\nfor layer in model:\n  if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n    nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n    if layer.bias is not None:\n      nn.init.zeros_(layer.bias)\n</pre> # Kaiming He Initialization for layer in model:   if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):     nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')     if layer.bias is not None:       nn.init.zeros_(layer.bias) In\u00a0[\u00a0]: Copied! <pre>model.train()\n\nnum_epochs = 50\nfor epoch in range(num_epochs):\n  loss = 0.0\n  for X_train, y_train in train_loader:\n    X_train, y_train = X_train.to(device), y_train.to(device)\n    optimizer.zero_grad()\n    preds = model(X_train)\n    batch_loss = loss_fn(preds, y_train)\n    batch_loss.backward()\n    loss += batch_loss.item()\n    optimizer.step()\n  print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}')\n</pre> model.train()  num_epochs = 50 for epoch in range(num_epochs):   loss = 0.0   for X_train, y_train in train_loader:     X_train, y_train = X_train.to(device), y_train.to(device)     optimizer.zero_grad()     preds = model(X_train)     batch_loss = loss_fn(preds, y_train)     batch_loss.backward()     loss += batch_loss.item()     optimizer.step()   print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}') <pre>Epoch: 1/50, Loss: 1.6063\nEpoch: 2/50, Loss: 1.3142\nEpoch: 3/50, Loss: 1.2205\nEpoch: 4/50, Loss: 1.1668\nEpoch: 5/50, Loss: 1.1335\nEpoch: 6/50, Loss: 1.0996\nEpoch: 7/50, Loss: 1.0810\nEpoch: 8/50, Loss: 1.0594\nEpoch: 9/50, Loss: 1.0465\nEpoch: 10/50, Loss: 1.0295\nEpoch: 11/50, Loss: 1.0175\nEpoch: 12/50, Loss: 1.0196\nEpoch: 13/50, Loss: 0.9891\nEpoch: 14/50, Loss: 0.9799\nEpoch: 15/50, Loss: 0.9781\nEpoch: 16/50, Loss: 0.9729\nEpoch: 17/50, Loss: 0.9789\nEpoch: 18/50, Loss: 0.9651\nEpoch: 19/50, Loss: 0.9611\nEpoch: 20/50, Loss: 0.9605\nEpoch: 21/50, Loss: 0.9545\nEpoch: 22/50, Loss: 0.9485\nEpoch: 23/50, Loss: 0.9362\nEpoch: 24/50, Loss: 0.9436\nEpoch: 25/50, Loss: 0.9309\nEpoch: 26/50, Loss: 0.9296\nEpoch: 27/50, Loss: 0.9240\nEpoch: 28/50, Loss: 0.9205\nEpoch: 29/50, Loss: 0.9164\nEpoch: 30/50, Loss: 0.9104\nEpoch: 31/50, Loss: 0.9124\nEpoch: 32/50, Loss: 0.9160\nEpoch: 33/50, Loss: 0.9211\nEpoch: 34/50, Loss: 0.9067\nEpoch: 35/50, Loss: 0.9168\nEpoch: 36/50, Loss: 0.9151\nEpoch: 37/50, Loss: 0.9065\nEpoch: 38/50, Loss: 0.9048\nEpoch: 39/50, Loss: 0.9024\nEpoch: 40/50, Loss: 0.8971\nEpoch: 41/50, Loss: 0.9015\nEpoch: 42/50, Loss: 0.9055\nEpoch: 43/50, Loss: 0.8923\nEpoch: 44/50, Loss: 0.8804\nEpoch: 45/50, Loss: 0.8855\nEpoch: 46/50, Loss: 0.8931\nEpoch: 47/50, Loss: 0.8802\nEpoch: 48/50, Loss: 0.8820\nEpoch: 49/50, Loss: 0.8748\nEpoch: 50/50, Loss: 0.8847\n</pre> In\u00a0[\u00a0]: Copied! <pre>model.eval()\n\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n  for X_test, y_test in test_loader:\n    X_test, y_test = X_test.to(device), y_test.to(device)\n    preds = model(X_test)\n    _, predicted = torch.max(preds, 1)\n    total += y_test.size(0)\n    correct += (predicted == y_test).sum().item()\n\naccuracy = 100 * correct / total\nprint(f'Accuracy on the test set: {accuracy:.2f}%')\n</pre> model.eval()  correct = 0 total = 0 with torch.no_grad():   for X_test, y_test in test_loader:     X_test, y_test = X_test.to(device), y_test.to(device)     preds = model(X_test)     _, predicted = torch.max(preds, 1)     total += y_test.size(0)     correct += (predicted == y_test).sum().item()  accuracy = 100 * correct / total print(f'Accuracy on the test set: {accuracy:.2f}%') <pre>Accuracy on the test set: 68.98%\n</pre>"},{"location":"notebooks/04_regul_optim/#04-regularization-and-optimization","title":"04. Regularization and Optimization\u00b6","text":"28 Feb 2025 \u00b7    <p>Important</p> <p>     The notebook is currently under revision.   </p> <p>Note</p> <p> The notebook is highly based on the examples given in different chapters of the Dive Deep into Deep Learning book.</p>"},{"location":"notebooks/04_regul_optim/#weight-decay","title":"Weight Decay\u00b6","text":""},{"location":"notebooks/04_regul_optim/#dropout","title":"Dropout\u00b6","text":"<p>The more complicated the model is, the more chance it has to overfit to the train data. In order to improve generalization ability of our model to the unseen data, we may aim for a simpler model. Regularization with weight decay achieved that to some degree but it's not the only solution. Srivastava et al. building on top of the previous idea developed by Bishop came up with the concept called Dropout. Dropout literally drops out some neurons during training by setting their activations to zero, which is illustrated in the figure 5.6.1 of d2l book below.</p> <p></p>"},{"location":"notebooks/04_regul_optim/#exploding-vanishing-gradients","title":"Exploding &amp; Vanishing Gradients\u00b6","text":"<p>When building neural network from scratch, we have been initilializing our parameters randomly. By following a certain guideline when initializing weights, however, it is possible to influence the speed and convergence of our training. Poor parameter initialization can cause problems like exploding and vanishing gradients.</p>"},{"location":"notebooks/04_regul_optim/#xavier-he-parameter-initialization","title":"Xavier / He Parameter Initialization\u00b6","text":""},{"location":"notebooks/04_regul_optim/#local-minima-saddle-point","title":"Local Minima &amp; Saddle Point\u00b6","text":""},{"location":"notebooks/04_regul_optim/#learning-rate","title":"Learning Rate\u00b6","text":"<p>We have seen from previous experiments how learning rate can affect the training process. When the learning rate is low, parameters take smaller steps towards the minimum, and training becomes slow. When the learning rate is high, it can overshoot the local minima and may even never converge. Therefore, it is common to use a learning rate scheduler which schedules the learning rate values to initially take larger steps to speed up the training, but as the parameter values approach local minima, the scheduler gradually reduces the learning rate to avoid overshooting. For example, we can reduce the learning rate by a factor of <code>0.1</code> or so every <code>N</code> epochs. We can simulate the gradient descent in a 2D plot to illustrate the impact of different learning rates.</p>"},{"location":"notebooks/04_regul_optim/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)\u00b6","text":"<p>As mentioned before, it is possible for the parameters to get stuck in the local minima. A slightly modified version of the gradient descent, the minibatch Stochastic Gradient Descent (SGD) has the ability to avoid local minima, as the variation of gradients in the minibatches bring noise and affect the parameter update. In SGD, instead of averaging the gradients of all data (batch) samples, we randomly choose only a single sample to approximate the average gradient. In case of minibatch SGD, we choose a small data subset (minibatch) instead of a single data point (SGD) or all data points (BGD) and average the minibatch gradients to approximate the average of full batch gradients.</p> <p>In the examples below, we can see how different values of learning rates may postiviely or negatively influence the outcome depending on whether we are close to the local or global minima.</p>"},{"location":"notebooks/04_regul_optim/#momentum","title":"Momentum\u00b6","text":"<p>We have observed that the learning rate is sensitive to small changes in its values. We can look at the direction of the gradient and encourage the learning rate to move smoothly in that direction (e.g. instead of oscillating a lot in a zig-zag pattern). It can be achieved with momentum, which adds a velocity term with acceleration to parameter updates. It behaves similar to how a ball would roll down a hill. Momentum builds speed as we move in the direction of the gradient, helping the training converge faster and more smoothly. This explanation, however, is oversimplistic. If you have a good mathematical background, you can find a nice demo and better explanation of momentum in this distill article.</p> <p>Recall that gradient descent for parameter $\\theta$ had the following formula where $\\nabla_{\\theta} L(\\theta_t)$ is gradient of loss and $\\eta $ is learning rate:</p> <p>$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} L(\\theta_t)$$</p> <p>Momentum has a velocity term $v$ which accumulates the values of previous gradients. It is also scaled by a momentum hyperparamter $\\beta$ where $\\beta \\in (0, 1)$. Unlike  $\\eta$ (learning rate hyperparamter), $\\beta$ is usually set to $0.9$. Gradient update instead of pointing towards the direction of steepest descent on a single instance, becomes the direction of a weighted average of previous gradients. Momentum will play the role of an accelerator, From this point on, and parameters will be updated by the accumulated gradient value.</p> <p>$$ v_t = \\beta v_{t-1} +\\nabla_{\\theta} L(\\theta_t) \\\\ \\theta_{t+1} = \\theta_t - \\eta v_t $$</p> <p>You will usually see a slighly modified version of the velocity, which scales the current gradient by $(1-\\beta)$:</p> <p>$$ v_t = \\beta v_{t-1} + (1 - \\beta)\\nabla_{\\theta} L(\\theta_t)$$</p> <p>You can imagine $\\beta$ value to be an averaging factor. As the gradients accumulate, we may want to give less weight to the current gradient, which $1-\\beta$ achieves. We can also notice that $\\beta=0.0$ will turn off the momentum update and restore the original gradient descent, when $\\beta=1.0$ would completely remove the effect of the gradient update. For this reason, you will see values for the momentum hyperparameter to be $0.99$ or even $0.999$ but never $1.0$.</p>"},{"location":"notebooks/04_regul_optim/#rmsprop","title":"RMSProp\u00b6","text":"<p>We can further optimize learning with RMSProp algorithm. RMSProp adjusts the parameter step size based on the historical magnitude of the parameter gradients. For that, step size (learning rate) is divided by a running average of squared gradients. This helps to avoid large updates in regions where gradients are large, while allowing for bigger steps in flatter regions of the function. Squaring and then desquaring (finding the square root) is a common technique (similar to finding MSE or standard deviation) which deals with negative values. RMSProp has the common abbreviation as MSE: the algorithm's full title is Root Mean Squared Propagation. The state $s$ in the formula below (which should seem familiar) holds the running average of squared gradients, which is then used to scale the learning rate for each parameter:</p> <p>$$s_t = \\gamma s_{t-1} + (1 - \\gamma) \\nabla_{\\theta} L(\\theta_t)^2$$</p> <p>The updated parameter $\\theta$ is then divided by the square root of this state vector. To avoid division by zero and achieve numerical stability $\u03f5$ (a very small number approaching zero) is added. The term $\\sqrt{s_t}$ represents the root of the running average of squared gradients, normalizing the gradient for each parameter.</p> <p>$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{s_t} + \\epsilon} \\nabla_{\\theta} L(\\theta_t)$$</p> <p>RMSprop often performs well when dealing with noisy or sparse gradients, and the parameter $\\gamma$ typically ranges between $0.9$ and $0.99$. Again note that RMSProp updates each parameter individually. The formula and all may seem complicated, yet the reasoning is very simple: you adjust the learning rate of each parameter by its past gradient. If the gradient was big, learning rate for that parameter will get smaller and vice versa.</p> <p>In our example below, we will work with optimizing a single paramter to visualize it in 2D which should give a feeling on how RMSProp works. But it is important to note that, just like many concepts, the idea can be extended to higher dimensions. Learning rates for each parameter may get different values as a result.</p>"},{"location":"notebooks/04_regul_optim/#adam","title":"Adam\u00b6","text":"<p>You might have noted certain similarities in momentum and RSMProp formulas. But can't we simply make the best out of both worlds and integrate them into a single gradient descent function? It turns out we can. Adam stands for Adaptive Moment Estimation. Adam: A Method for Stochastic Optimization does exactly that.</p> <p>$$ \\begin{aligned} v_t &amp;= \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla_{\\theta} L(\\theta_t) \\\\ s_t &amp;= \\beta_2 s_{t-1} + (1 - \\beta_2) \\nabla_{\\theta} L(\\theta_t)^2 \\end{aligned} $$</p> <p>We should note a couple of more things. The common values for weights are $\\beta_1=0.9$ and $\\beta_2=0.999$ (why not $\\beta_2=1.0$ we discussed above), as variance estimate $s_t$ is much slower than the momentum. As we initilize it together with velocity to zero ($v=s=0$) we initially get bias towards smaller values. It can be fixed with normalization:</p> <p>$$\\hat{v_t} = \\frac{v_t}{1 - \\beta_1^t}, \\quad \\hat{s_t} = \\frac{s_t}{1 - \\beta_2^t}$$</p> <p>Here $v_t$ and $s_t$ are called moments. Let's see how normalized moments get affected after the first two iterations:</p> <p>$$ \\beta_1 = 0.9^1 = 0.9 \\quad \\Rightarrow \\quad \\frac{1}{1 - \\beta_1^1} = \\frac{1}{0.1} = 10 \\\\ \\beta_1^2 = 0.9^2 = 0.81 \\quad \\Rightarrow \\quad \\frac{1}{1 - \\beta_1^2} = \\frac{1}{0.19} \\approx 5.26 \\\\ \\beta_2^1 = 0.999^1 = 0.999 \\quad \\Rightarrow \\quad \\frac{1}{1 - \\beta_2^1} = \\frac{1}{0.001} = 1000 \\\\ \\beta_2^2 = 0.999^2 = 0.998001 \\quad \\Rightarrow \\quad \\frac{1}{1 - \\beta_2^2} = \\frac{1}{0.001999} \\approx 500 $$</p> <p>Finally, we can integrate the accumulated values into our gradient descent formula to get the final step size:</p> <p>$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{s_t}} + \\epsilon} \\hat{v_t}$$</p>"},{"location":"notebooks/04_regul_optim/#cifar-10-dataset","title":"CIFAR-10 Dataset\u00b6","text":""},{"location":"notebooks/04_regul_optim/#training-efficient-model","title":"Training Efficient Model\u00b6","text":"<p>We will now train our custom CNN model (see LeNet model in the previous lecture) by applying regularization / optimization methods we have discussed above. Pay attention to the architecture, try to understand the input dimensions. We have included a <code>Dropout</code> layer. In order to initialize our weights non-randomly, we will not use <code>nn.Lazy</code> modules.</p>"},{"location":"notebooks/04_regul_optim/#hyperparameter-tuning","title":"Hyperparameter Tuning\u00b6","text":"<p>When building our model architecture we should consider activation function, number of layers and number of neurons per layer, kernel size, stride, padding. We have also come across to many different hyperparameters up until now: learning rate, batch size, initialization method, number of epochs, optimizer type (e.g. SGD, Adam), momentum (beta values for Adam), weight decay, dropout rate, etc. It is often not very clear which values are the best for our model architecture and dataset. We have to try out many different values for our hyperparameters to find the most efficient training conditions.</p> <p>Hyperparameter tuning is the process of finding the best set of hyperparameters. Grid search, random search, Bayesian optimization help us to explore different hyperparameter combinations. It is recommended to start with common default values, and gradually adjust one or more hyperparameters during experiments. You can use cross-validation to evaluate model performance for each hypermeter combination.</p> <p>Learning rate and optimizer affect the convergence of training. Batch size and number of epochs affect the training time and model accuracy. Regularization  (dropout, weight decay) parameters should aid with preventing overfitting.</p> <p>Exercise: Tune hyperparameters for the model and note down the results. Which hyperparameter set achieved the best accuracy on test data?</p>"},{"location":"notebooks/05_nn_ngram/","title":"05. Neural Network N-Gram Model","text":"In\u00a0[1]: Copied! <pre>import requests\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\nresponse = requests.get(url)\ndata = response.text\nwords = data.splitlines()\n</pre> import requests  url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\" response = requests.get(url) data = response.text words = data.splitlines() In\u00a0[2]: Copied! <pre>print('Length:', len(words))\nprint(words[:5] )\n</pre> print('Length:', len(words)) print(words[:5] ) <pre>Length: 32033\n['emma', 'olivia', 'ava', 'isabella', 'sophia']\n</pre> <p>This data provides information about names. For example, by having five examples <code>['emma', 'olivia', 'ava', 'isabella', 'sophia']</code> we may conclude that the probability of <code>'a'</code> being the last letter is <code>1.0</code> after which the word will certainly end, or that the letter <code>'o'</code> is more likely to be at the beginning of the name. Our goal will be to to predict the most probable next character. A common technique is to take track of bigrams of characters (there can be N-grams of words, etc., which have both advantages and disadvantages).</p> In\u00a0[3]: Copied! <pre>def get_bigrams(n):\n  bigrams = {}\n  for w in words[:n]:\n    w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n    for ch1, ch2 in zip(w, w[1:]):\n      b = (ch1, ch2)\n      bigrams[b] = bigrams.get(b, 0) + 1\n  return bigrams\n</pre> def get_bigrams(n):   bigrams = {}   for w in words[:n]:     w = [''] + list(w) + ['']     for ch1, ch2 in zip(w, w[1:]):       b = (ch1, ch2)       bigrams[b] = bigrams.get(b, 0) + 1   return bigrams <p>Change <code>n</code> up to 5 and take note of the bigram counts.</p> In\u00a0[4]: Copied! <pre>get_bigrams(n=2)\n</pre> get_bigrams(n=2) Out[4]: <pre>{('&lt;START&gt;', 'e'): 1,\n ('e', 'm'): 1,\n ('m', 'm'): 1,\n ('m', 'a'): 1,\n ('a', '&lt;END&gt;'): 2,\n ('&lt;START&gt;', 'o'): 1,\n ('o', 'l'): 1,\n ('l', 'i'): 1,\n ('i', 'v'): 1,\n ('v', 'i'): 1,\n ('i', 'a'): 1}</pre> In\u00a0[5]: Copied! <pre>from collections import Counter\n\nbigrams = get_bigrams(len(words))\nCounter(bigrams).most_common()\n</pre> from collections import Counter  bigrams = get_bigrams(len(words)) Counter(bigrams).most_common() Out[5]: <pre>[(('n', '&lt;END&gt;'), 6763),\n (('a', '&lt;END&gt;'), 6640),\n (('a', 'n'), 5438),\n (('&lt;START&gt;', 'a'), 4410),\n (('e', '&lt;END&gt;'), 3983),\n (('a', 'r'), 3264),\n (('e', 'l'), 3248),\n (('r', 'i'), 3033),\n (('n', 'a'), 2977),\n (('&lt;START&gt;', 'k'), 2963),\n (('l', 'e'), 2921),\n (('e', 'n'), 2675),\n (('l', 'a'), 2623),\n (('m', 'a'), 2590),\n (('&lt;START&gt;', 'm'), 2538),\n (('a', 'l'), 2528),\n (('i', '&lt;END&gt;'), 2489),\n (('l', 'i'), 2480),\n (('i', 'a'), 2445),\n (('&lt;START&gt;', 'j'), 2422),\n (('o', 'n'), 2411),\n (('h', '&lt;END&gt;'), 2409),\n (('r', 'a'), 2356),\n (('a', 'h'), 2332),\n (('h', 'a'), 2244),\n (('y', 'a'), 2143),\n (('i', 'n'), 2126),\n (('&lt;START&gt;', 's'), 2055),\n (('a', 'y'), 2050),\n (('y', '&lt;END&gt;'), 2007),\n (('e', 'r'), 1958),\n (('n', 'n'), 1906),\n (('y', 'n'), 1826),\n (('k', 'a'), 1731),\n (('n', 'i'), 1725),\n (('r', 'e'), 1697),\n (('&lt;START&gt;', 'd'), 1690),\n (('i', 'e'), 1653),\n (('a', 'i'), 1650),\n (('&lt;START&gt;', 'r'), 1639),\n (('a', 'm'), 1634),\n (('l', 'y'), 1588),\n (('&lt;START&gt;', 'l'), 1572),\n (('&lt;START&gt;', 'c'), 1542),\n (('&lt;START&gt;', 'e'), 1531),\n (('j', 'a'), 1473),\n (('r', '&lt;END&gt;'), 1377),\n (('n', 'e'), 1359),\n (('l', 'l'), 1345),\n (('i', 'l'), 1345),\n (('i', 's'), 1316),\n (('l', '&lt;END&gt;'), 1314),\n (('&lt;START&gt;', 't'), 1308),\n (('&lt;START&gt;', 'b'), 1306),\n (('d', 'a'), 1303),\n (('s', 'h'), 1285),\n (('d', 'e'), 1283),\n (('e', 'e'), 1271),\n (('m', 'i'), 1256),\n (('s', 'a'), 1201),\n (('s', '&lt;END&gt;'), 1169),\n (('&lt;START&gt;', 'n'), 1146),\n (('a', 's'), 1118),\n (('y', 'l'), 1104),\n (('e', 'y'), 1070),\n (('o', 'r'), 1059),\n (('a', 'd'), 1042),\n (('t', 'a'), 1027),\n (('&lt;START&gt;', 'z'), 929),\n (('v', 'i'), 911),\n (('k', 'e'), 895),\n (('s', 'e'), 884),\n (('&lt;START&gt;', 'h'), 874),\n (('r', 'o'), 869),\n (('e', 's'), 861),\n (('z', 'a'), 860),\n (('o', '&lt;END&gt;'), 855),\n (('i', 'r'), 849),\n (('b', 'r'), 842),\n (('a', 'v'), 834),\n (('m', 'e'), 818),\n (('e', 'i'), 818),\n (('c', 'a'), 815),\n (('i', 'y'), 779),\n (('r', 'y'), 773),\n (('e', 'm'), 769),\n (('s', 't'), 765),\n (('h', 'i'), 729),\n (('t', 'e'), 716),\n (('n', 'd'), 704),\n (('l', 'o'), 692),\n (('a', 'e'), 692),\n (('a', 't'), 687),\n (('s', 'i'), 684),\n (('e', 'a'), 679),\n (('d', 'i'), 674),\n (('h', 'e'), 674),\n (('&lt;START&gt;', 'g'), 669),\n (('t', 'o'), 667),\n (('c', 'h'), 664),\n (('b', 'e'), 655),\n (('t', 'h'), 647),\n (('v', 'a'), 642),\n (('o', 'l'), 619),\n (('&lt;START&gt;', 'i'), 591),\n (('i', 'o'), 588),\n (('e', 't'), 580),\n (('v', 'e'), 568),\n (('a', 'k'), 568),\n (('a', 'a'), 556),\n (('c', 'e'), 551),\n (('a', 'b'), 541),\n (('i', 't'), 541),\n (('&lt;START&gt;', 'y'), 535),\n (('t', 'i'), 532),\n (('s', 'o'), 531),\n (('m', '&lt;END&gt;'), 516),\n (('d', '&lt;END&gt;'), 516),\n (('&lt;START&gt;', 'p'), 515),\n (('i', 'c'), 509),\n (('k', 'i'), 509),\n (('o', 's'), 504),\n (('n', 'o'), 496),\n (('t', '&lt;END&gt;'), 483),\n (('j', 'o'), 479),\n (('u', 's'), 474),\n (('a', 'c'), 470),\n (('n', 'y'), 465),\n (('e', 'v'), 463),\n (('s', 's'), 461),\n (('m', 'o'), 452),\n (('i', 'k'), 445),\n (('n', 't'), 443),\n (('i', 'd'), 440),\n (('j', 'e'), 440),\n (('a', 'z'), 435),\n (('i', 'g'), 428),\n (('i', 'm'), 427),\n (('r', 'r'), 425),\n (('d', 'r'), 424),\n (('&lt;START&gt;', 'f'), 417),\n (('u', 'r'), 414),\n (('r', 'l'), 413),\n (('y', 's'), 401),\n (('&lt;START&gt;', 'o'), 394),\n (('e', 'd'), 384),\n (('a', 'u'), 381),\n (('c', 'o'), 380),\n (('k', 'y'), 379),\n (('d', 'o'), 378),\n (('&lt;START&gt;', 'v'), 376),\n (('t', 't'), 374),\n (('z', 'e'), 373),\n (('z', 'i'), 364),\n (('k', '&lt;END&gt;'), 363),\n (('g', 'h'), 360),\n (('t', 'r'), 352),\n (('k', 'o'), 344),\n (('t', 'y'), 341),\n (('g', 'e'), 334),\n (('g', 'a'), 330),\n (('l', 'u'), 324),\n (('b', 'a'), 321),\n (('d', 'y'), 317),\n (('c', 'k'), 316),\n (('&lt;START&gt;', 'w'), 307),\n (('k', 'h'), 307),\n (('u', 'l'), 301),\n (('y', 'e'), 301),\n (('y', 'r'), 291),\n (('m', 'y'), 287),\n (('h', 'o'), 287),\n (('w', 'a'), 280),\n (('s', 'l'), 279),\n (('n', 's'), 278),\n (('i', 'z'), 277),\n (('u', 'n'), 275),\n (('o', 'u'), 275),\n (('n', 'g'), 273),\n (('y', 'd'), 272),\n (('c', 'i'), 271),\n (('y', 'o'), 271),\n (('i', 'v'), 269),\n (('e', 'o'), 269),\n (('o', 'm'), 261),\n (('r', 'u'), 252),\n (('f', 'a'), 242),\n (('b', 'i'), 217),\n (('s', 'y'), 215),\n (('n', 'c'), 213),\n (('h', 'y'), 213),\n (('p', 'a'), 209),\n (('r', 't'), 208),\n (('q', 'u'), 206),\n (('p', 'h'), 204),\n (('h', 'r'), 204),\n (('j', 'u'), 202),\n (('g', 'r'), 201),\n (('p', 'e'), 197),\n (('n', 'l'), 195),\n (('y', 'i'), 192),\n (('g', 'i'), 190),\n (('o', 'd'), 190),\n (('r', 's'), 190),\n (('r', 'd'), 187),\n (('h', 'l'), 185),\n (('s', 'u'), 185),\n (('a', 'x'), 182),\n (('e', 'z'), 181),\n (('e', 'k'), 178),\n (('o', 'v'), 176),\n (('a', 'j'), 175),\n (('o', 'h'), 171),\n (('u', 'e'), 169),\n (('m', 'm'), 168),\n (('a', 'g'), 168),\n (('h', 'u'), 166),\n (('x', '&lt;END&gt;'), 164),\n (('u', 'a'), 163),\n (('r', 'm'), 162),\n (('a', 'w'), 161),\n (('f', 'i'), 160),\n (('z', '&lt;END&gt;'), 160),\n (('u', '&lt;END&gt;'), 155),\n (('u', 'm'), 154),\n (('e', 'c'), 153),\n (('v', 'o'), 153),\n (('e', 'h'), 152),\n (('p', 'r'), 151),\n (('d', 'd'), 149),\n (('o', 'a'), 149),\n (('w', 'e'), 149),\n (('w', 'i'), 148),\n (('y', 'm'), 148),\n (('z', 'y'), 147),\n (('n', 'z'), 145),\n (('y', 'u'), 141),\n (('r', 'n'), 140),\n (('o', 'b'), 140),\n (('k', 'l'), 139),\n (('m', 'u'), 139),\n (('l', 'd'), 138),\n (('h', 'n'), 138),\n (('u', 'd'), 136),\n (('&lt;START&gt;', 'x'), 134),\n (('t', 'l'), 134),\n (('a', 'f'), 134),\n (('o', 'e'), 132),\n (('e', 'x'), 132),\n (('e', 'g'), 125),\n (('f', 'e'), 123),\n (('z', 'l'), 123),\n (('u', 'i'), 121),\n (('v', 'y'), 121),\n (('e', 'b'), 121),\n (('r', 'h'), 121),\n (('j', 'i'), 119),\n (('o', 't'), 118),\n (('d', 'h'), 118),\n (('h', 'm'), 117),\n (('c', 'l'), 116),\n (('o', 'o'), 115),\n (('y', 'c'), 115),\n (('o', 'w'), 114),\n (('o', 'c'), 114),\n (('f', 'r'), 114),\n (('b', '&lt;END&gt;'), 114),\n (('m', 'b'), 112),\n (('z', 'o'), 110),\n (('i', 'b'), 110),\n (('i', 'u'), 109),\n (('k', 'r'), 109),\n (('g', '&lt;END&gt;'), 108),\n (('y', 'v'), 106),\n (('t', 'z'), 105),\n (('b', 'o'), 105),\n (('c', 'y'), 104),\n (('y', 't'), 104),\n (('u', 'b'), 103),\n (('u', 'c'), 103),\n (('x', 'a'), 103),\n (('b', 'l'), 103),\n (('o', 'y'), 103),\n (('x', 'i'), 102),\n (('i', 'f'), 101),\n (('r', 'c'), 99),\n (('c', '&lt;END&gt;'), 97),\n (('m', 'r'), 97),\n (('n', 'u'), 96),\n (('o', 'p'), 95),\n (('i', 'h'), 95),\n (('k', 's'), 95),\n (('l', 's'), 94),\n (('u', 'k'), 93),\n (('&lt;START&gt;', 'q'), 92),\n (('d', 'u'), 92),\n (('s', 'm'), 90),\n (('r', 'k'), 90),\n (('i', 'x'), 89),\n (('v', '&lt;END&gt;'), 88),\n (('y', 'k'), 86),\n (('u', 'w'), 86),\n (('g', 'u'), 85),\n (('b', 'y'), 83),\n (('e', 'p'), 83),\n (('g', 'o'), 83),\n (('s', 'k'), 82),\n (('u', 't'), 82),\n (('a', 'p'), 82),\n (('e', 'f'), 82),\n (('i', 'i'), 82),\n (('r', 'v'), 80),\n (('f', '&lt;END&gt;'), 80),\n (('t', 'u'), 78),\n (('y', 'z'), 78),\n (('&lt;START&gt;', 'u'), 78),\n (('l', 't'), 77),\n (('r', 'g'), 76),\n (('c', 'r'), 76),\n (('i', 'j'), 76),\n (('w', 'y'), 73),\n (('z', 'u'), 73),\n (('l', 'v'), 72),\n (('h', 't'), 71),\n (('j', '&lt;END&gt;'), 71),\n (('x', 't'), 70),\n (('o', 'i'), 69),\n (('e', 'u'), 69),\n (('o', 'k'), 68),\n (('b', 'd'), 65),\n (('a', 'o'), 63),\n (('p', 'i'), 61),\n (('s', 'c'), 60),\n (('d', 'l'), 60),\n (('l', 'm'), 60),\n (('a', 'q'), 60),\n (('f', 'o'), 60),\n (('p', 'o'), 59),\n (('n', 'k'), 58),\n (('w', 'n'), 58),\n (('u', 'h'), 58),\n (('e', 'j'), 55),\n (('n', 'v'), 55),\n (('s', 'r'), 55),\n (('o', 'z'), 54),\n (('i', 'p'), 53),\n (('l', 'b'), 52),\n (('i', 'q'), 52),\n (('w', '&lt;END&gt;'), 51),\n (('m', 'c'), 51),\n (('s', 'p'), 51),\n (('e', 'w'), 50),\n (('k', 'u'), 50),\n (('v', 'r'), 48),\n (('u', 'g'), 47),\n (('o', 'x'), 45),\n (('u', 'z'), 45),\n (('z', 'z'), 45),\n (('j', 'h'), 45),\n (('b', 'u'), 45),\n (('o', 'g'), 44),\n (('n', 'r'), 44),\n (('f', 'f'), 44),\n (('n', 'j'), 44),\n (('z', 'h'), 43),\n (('c', 'c'), 42),\n (('r', 'b'), 41),\n (('x', 'o'), 41),\n (('b', 'h'), 41),\n (('p', 'p'), 39),\n (('x', 'l'), 39),\n (('h', 'v'), 39),\n (('b', 'b'), 38),\n (('m', 'p'), 38),\n (('x', 'x'), 38),\n (('u', 'v'), 37),\n (('x', 'e'), 36),\n (('w', 'o'), 36),\n (('c', 't'), 35),\n (('z', 'm'), 35),\n (('t', 's'), 35),\n (('m', 's'), 35),\n (('c', 'u'), 35),\n (('o', 'f'), 34),\n (('u', 'x'), 34),\n (('k', 'w'), 34),\n (('p', '&lt;END&gt;'), 33),\n (('g', 'l'), 32),\n (('z', 'r'), 32),\n (('d', 'n'), 31),\n (('g', 't'), 31),\n (('g', 'y'), 31),\n (('h', 's'), 31),\n (('x', 's'), 31),\n (('g', 's'), 30),\n (('x', 'y'), 30),\n (('y', 'g'), 30),\n (('d', 'm'), 30),\n (('d', 's'), 29),\n (('h', 'k'), 29),\n (('y', 'x'), 28),\n (('q', '&lt;END&gt;'), 28),\n (('g', 'n'), 27),\n (('y', 'b'), 27),\n (('g', 'w'), 26),\n (('n', 'h'), 26),\n (('k', 'n'), 26),\n (('g', 'g'), 25),\n (('d', 'g'), 25),\n (('l', 'c'), 25),\n (('r', 'j'), 25),\n (('w', 'u'), 25),\n (('l', 'k'), 24),\n (('m', 'd'), 24),\n (('s', 'w'), 24),\n (('s', 'n'), 24),\n (('h', 'd'), 24),\n (('w', 'h'), 23),\n (('y', 'j'), 23),\n (('y', 'y'), 23),\n (('r', 'z'), 23),\n (('d', 'w'), 23),\n (('w', 'r'), 22),\n (('t', 'n'), 22),\n (('l', 'f'), 22),\n (('y', 'h'), 22),\n (('r', 'w'), 21),\n (('s', 'b'), 21),\n (('m', 'n'), 20),\n (('f', 'l'), 20),\n (('w', 's'), 20),\n (('k', 'k'), 20),\n (('h', 'z'), 20),\n (('g', 'd'), 19),\n (('l', 'h'), 19),\n (('n', 'm'), 19),\n (('x', 'z'), 19),\n (('u', 'f'), 19),\n (('f', 't'), 18),\n (('l', 'r'), 18),\n (('p', 't'), 17),\n (('t', 'c'), 17),\n (('k', 't'), 17),\n (('d', 'v'), 17),\n (('u', 'p'), 16),\n (('p', 'l'), 16),\n (('l', 'w'), 16),\n (('p', 's'), 16),\n (('o', 'j'), 16),\n (('r', 'q'), 16),\n (('y', 'p'), 15),\n (('l', 'p'), 15),\n (('t', 'v'), 15),\n (('r', 'p'), 14),\n (('l', 'n'), 14),\n (('e', 'q'), 14),\n (('f', 'y'), 14),\n (('s', 'v'), 14),\n (('u', 'j'), 14),\n (('v', 'l'), 14),\n (('q', 'a'), 13),\n (('u', 'y'), 13),\n (('q', 'i'), 13),\n (('w', 'l'), 13),\n (('p', 'y'), 12),\n (('y', 'f'), 12),\n (('c', 'q'), 11),\n (('j', 'r'), 11),\n (('n', 'w'), 11),\n (('n', 'f'), 11),\n (('t', 'w'), 11),\n (('m', 'z'), 11),\n (('u', 'o'), 10),\n (('f', 'u'), 10),\n (('l', 'z'), 10),\n (('h', 'w'), 10),\n (('u', 'q'), 10),\n (('j', 'y'), 10),\n (('s', 'z'), 10),\n (('s', 'd'), 9),\n (('j', 'l'), 9),\n (('d', 'j'), 9),\n (('k', 'm'), 9),\n (('r', 'f'), 9),\n (('h', 'j'), 9),\n (('v', 'n'), 8),\n (('n', 'b'), 8),\n (('i', 'w'), 8),\n (('h', 'b'), 8),\n (('b', 's'), 8),\n (('w', 't'), 8),\n (('w', 'd'), 8),\n (('v', 'v'), 7),\n (('v', 'u'), 7),\n (('j', 's'), 7),\n (('m', 'j'), 7),\n (('f', 's'), 6),\n (('l', 'g'), 6),\n (('l', 'j'), 6),\n (('j', 'w'), 6),\n (('n', 'x'), 6),\n (('y', 'q'), 6),\n (('w', 'k'), 6),\n (('g', 'm'), 6),\n (('x', 'u'), 5),\n (('m', 'h'), 5),\n (('m', 'l'), 5),\n (('j', 'm'), 5),\n (('c', 's'), 5),\n (('j', 'v'), 5),\n (('n', 'p'), 5),\n (('d', 'f'), 5),\n (('x', 'd'), 5),\n (('z', 'b'), 4),\n (('f', 'n'), 4),\n (('x', 'c'), 4),\n (('m', 't'), 4),\n (('t', 'm'), 4),\n (('z', 'n'), 4),\n (('z', 't'), 4),\n (('p', 'u'), 4),\n (('c', 'z'), 4),\n (('b', 'n'), 4),\n (('z', 's'), 4),\n (('f', 'w'), 4),\n (('d', 't'), 4),\n (('j', 'd'), 4),\n (('j', 'c'), 4),\n (('y', 'w'), 4),\n (('v', 'k'), 3),\n (('x', 'w'), 3),\n (('t', 'j'), 3),\n (('c', 'j'), 3),\n (('q', 'w'), 3),\n (('g', 'b'), 3),\n (('o', 'q'), 3),\n (('r', 'x'), 3),\n (('d', 'c'), 3),\n (('g', 'j'), 3),\n (('x', 'f'), 3),\n (('z', 'w'), 3),\n (('d', 'k'), 3),\n (('u', 'u'), 3),\n (('m', 'v'), 3),\n (('c', 'x'), 3),\n (('l', 'q'), 3),\n (('p', 'b'), 2),\n (('t', 'g'), 2),\n (('q', 's'), 2),\n (('t', 'x'), 2),\n (('f', 'k'), 2),\n (('b', 't'), 2),\n (('j', 'n'), 2),\n (('k', 'c'), 2),\n (('z', 'k'), 2),\n (('s', 'j'), 2),\n (('s', 'f'), 2),\n (('z', 'j'), 2),\n (('n', 'q'), 2),\n (('f', 'z'), 2),\n (('h', 'g'), 2),\n (('w', 'w'), 2),\n (('k', 'j'), 2),\n (('j', 'k'), 2),\n (('w', 'm'), 2),\n (('z', 'c'), 2),\n (('z', 'v'), 2),\n (('w', 'f'), 2),\n (('q', 'm'), 2),\n (('k', 'z'), 2),\n (('j', 'j'), 2),\n (('z', 'p'), 2),\n (('j', 't'), 2),\n (('k', 'b'), 2),\n (('m', 'w'), 2),\n (('h', 'f'), 2),\n (('c', 'g'), 2),\n (('t', 'f'), 2),\n (('h', 'c'), 2),\n (('q', 'o'), 2),\n (('k', 'd'), 2),\n (('k', 'v'), 2),\n (('s', 'g'), 2),\n (('z', 'd'), 2),\n (('q', 'r'), 1),\n (('d', 'z'), 1),\n (('p', 'j'), 1),\n (('q', 'l'), 1),\n (('p', 'f'), 1),\n (('q', 'e'), 1),\n (('b', 'c'), 1),\n (('c', 'd'), 1),\n (('m', 'f'), 1),\n (('p', 'n'), 1),\n (('w', 'b'), 1),\n (('p', 'c'), 1),\n (('h', 'p'), 1),\n (('f', 'h'), 1),\n (('b', 'j'), 1),\n (('f', 'g'), 1),\n (('z', 'g'), 1),\n (('c', 'p'), 1),\n (('p', 'k'), 1),\n (('p', 'm'), 1),\n (('x', 'n'), 1),\n (('s', 'q'), 1),\n (('k', 'f'), 1),\n (('m', 'k'), 1),\n (('x', 'h'), 1),\n (('g', 'f'), 1),\n (('v', 'b'), 1),\n (('j', 'p'), 1),\n (('g', 'z'), 1),\n (('v', 'd'), 1),\n (('d', 'b'), 1),\n (('v', 'h'), 1),\n (('h', 'h'), 1),\n (('g', 'v'), 1),\n (('d', 'q'), 1),\n (('x', 'b'), 1),\n (('w', 'z'), 1),\n (('h', 'q'), 1),\n (('j', 'b'), 1),\n (('x', 'm'), 1),\n (('w', 'g'), 1),\n (('t', 'b'), 1),\n (('z', 'x'), 1)]</pre> <p>We will store bigrams in a <code>PyTorch</code> tensor instead of a dictionary. Each character will be mapped to an id.</p> In\u00a0[7]: Copied! <pre>import string\n\n# character mapping from string to integer\nchars = list(string.ascii_lowercase)\nstoi = {ch: i for i, ch in enumerate(chars)}\nstoi['&lt;START&gt;'] = 26\nstoi['&lt;END&gt;'] = 27\nstoi\n</pre> import string  # character mapping from string to integer chars = list(string.ascii_lowercase) stoi = {ch: i for i, ch in enumerate(chars)} stoi[''] = 26 stoi[''] = 27 stoi Out[7]: <pre>{'a': 0,\n 'b': 1,\n 'c': 2,\n 'd': 3,\n 'e': 4,\n 'f': 5,\n 'g': 6,\n 'h': 7,\n 'i': 8,\n 'j': 9,\n 'k': 10,\n 'l': 11,\n 'm': 12,\n 'n': 13,\n 'o': 14,\n 'p': 15,\n 'q': 16,\n 'r': 17,\n 's': 18,\n 't': 19,\n 'u': 20,\n 'v': 21,\n 'w': 22,\n 'x': 23,\n 'y': 24,\n 'z': 25,\n '&lt;START&gt;': 26,\n '&lt;END&gt;': 27}</pre> In\u00a0[8]: Copied! <pre>import torch\n\nSIZE = len(stoi)\n\ndef get_bigrams(n):\n  bigrams = torch.zeros((SIZE, SIZE))\n  for w in words[:n]:\n    w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n    for ch1, ch2 in zip(w, w[1:]):\n      bigrams[stoi[ch1], stoi[ch2]] += 1\n  return bigrams\n</pre> import torch  SIZE = len(stoi)  def get_bigrams(n):   bigrams = torch.zeros((SIZE, SIZE))   for w in words[:n]:     w = [''] + list(w) + ['']     for ch1, ch2 in zip(w, w[1:]):       bigrams[stoi[ch1], stoi[ch2]] += 1   return bigrams In\u00a0[9]: Copied! <pre>bigrams = get_bigrams(len(words))\n</pre> bigrams = get_bigrams(len(words)) <p>We can refer to bigrams by indices and slicing. Modify <code>i</code> and <code>j</code> and run the code to get a sense of the table.</p> In\u00a0[10]: Copied! <pre>itos = {ch: i for i, ch in stoi.items()} # reverse stoi\n\ni, j = 0, 1\ncount = bigrams[i, j]\nprint(f'({itos[i]}, {itos[j]}): {count}')\n</pre> itos = {ch: i for i, ch in stoi.items()} # reverse stoi  i, j = 0, 1 count = bigrams[i, j] print(f'({itos[i]}, {itos[j]}): {count}') <pre>(a, b): 541.0\n</pre> <p>Exercise: Find the probability of a character (e.g. <code>b</code>) being the first character (hint: it will follow <code>&lt;START&gt;</code>).</p> In\u00a0[11]: Copied! <pre>counts = bigrams[stoi['&lt;START&gt;']]\nprobs = counts / counts.sum()\nprobs[stoi['b']]\n</pre> counts = bigrams[stoi['']] probs = counts / counts.sum() probs[stoi['b']] Out[11]: <pre>tensor(0.0408)</pre> <p>To generate some output we need to understand <code>torch.multinomial</code>. Let's have a simpler probability distribution for three classes (<code>0</code>, <code>1</code>, <code>2</code>). Our goal is to generate <code>n</code> samples according to the given probabilities. Setting <code>replacement=True</code> means the same class index can be picked multiple times. The higher a class' probability, the more often it is likely to appear in the samples. Rerun the cell below and notice how probabilities are related to the generated samples.</p> In\u00a0[12]: Copied! <pre>p = torch.rand(3)\np /= p.sum()\nsamples = torch.multinomial(p, num_samples=10, replacement=True)\n\nprint(p)\nprint(samples)\n</pre> p = torch.rand(3) p /= p.sum() samples = torch.multinomial(p, num_samples=10, replacement=True)  print(p) print(samples) <pre>tensor([0.5375, 0.1891, 0.2735])\ntensor([0, 0, 0, 2, 0, 1, 0, 0, 1, 0])\n</pre> <p>Once we understand the logic of <code>torch.multinomial</code>, we will randomly pick a next character based on our probability distribution. The higher is the frequency of the bigram, the more likely is that the random sampler will return us that character.</p> In\u00a0[13]: Copied! <pre>next_char = torch.multinomial(probs, num_samples=1, replacement=True)\nnext_char, itos[next_char.item()]\n</pre> next_char = torch.multinomial(probs, num_samples=1, replacement=True) next_char, itos[next_char.item()] Out[13]: <pre>(tensor([17]), 'r')</pre> <p>We will start with bigrams of <code>&lt;START&gt;</code>. Once we randomly generate the next character based on its probability distribution, we will start looking for bigrams starting with that generated character. This process will continue until we our sampling returns <code>&lt;END&gt;</code>.</p> <p>We will work with the probability matrix from now on, instead of the frequency matrix. Below, <code>dim=1</code> ensures that we sum along the row of the matrix, when <code>keepdim=True</code> keeps the extra dimension. Refer to the <code>PyTorch</code> documentation and test out different parameters.</p> In\u00a0[14]: Copied! <pre>probs = bigrams/bigrams.sum(dim=1, keepdim=True)\nprobs.shape\n</pre> probs = bigrams/bigrams.sum(dim=1, keepdim=True) probs.shape Out[14]: <pre>torch.Size([28, 28])</pre> In\u00a0[15]: Copied! <pre>def sample_names(n=10):\n  names = ''\n  for i in range(n):\n    id = stoi['&lt;START&gt;']\n    while id != stoi['&lt;END&gt;']:\n      p = probs[id]\n      next_char = torch.multinomial(p, 1, replacement=True)\n      id = next_char.item()\n      names += itos[id]\n  return names.replace(\"&lt;END&gt;\", \"\\n\")\n</pre> def sample_names(n=10):   names = ''   for i in range(n):     id = stoi['']     while id != stoi['']:       p = probs[id]       next_char = torch.multinomial(p, 1, replacement=True)       id = next_char.item()       names += itos[id]   return names.replace(\"\", \"\\n\") In\u00a0[21]: Copied! <pre>print(sample_names())\n</pre> print(sample_names()) <pre>tarios\ncaly\nkelaherthrwa\ndyaronn\nzenel\nbrid\nc\nsh\nve\nminica\n\n</pre> <p>We can evaluate our model and determine the loss function with likelihood. Note that our prediction probabilities are generated by simply counting bigram frequencies.</p> In\u00a0[22]: Copied! <pre>n = 1\nfor w in words[:n]:\n  w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    print(f'{ch1, ch2}: {p.item():.4f}')\n</pre> n = 1 for w in words[:n]:   w = [''] + list(w) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     print(f'{ch1, ch2}: {p.item():.4f}') <pre>('&lt;START&gt;', 'e'): 0.0478\n('e', 'm'): 0.0377\n('m', 'm'): 0.0253\n('m', 'a'): 0.3899\n('a', '&lt;END&gt;'): 0.1960\n</pre> <p>The logic of bigram model is that the probability of rare characters coming together in names (e.g. <code>xy</code>) will be much smaller than the more common cases (e.g. <code>na</code>). A better training corpus captures more realistic character transitions and assigns higher probabilities to frequently seen patterns.</p> <p>Since the model assumes that each character depends only on the previous one (Markov assumption), the joint probability of a sequence is the product of all conditional probabilities:</p> <p>$P(c_1, c_2, \\ldots, c_n) = P(c_1) \\cdot P(c_2 \\mid c_1) \\cdot P(c_3 \\mid c_2) \\cdots P(c_n \\mid c_{n-1})$</p> <p>Likelihood estimates this quality for our model by multiplying all prediction probabilities. Higher is the joint probability, the better is model's prediction quality. However, direct multiplication may have the following issue:</p> In\u00a0[24]: Copied! <pre>n = 5\nfor word in words[:n]:\n  likelihood = 1.0\n  w = ['&lt;START&gt;'] + list(word) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    likelihood *= p\n  print(f'Model predicts {word} is {likelihood:.9f} likely')\n</pre> n = 5 for word in words[:n]:   likelihood = 1.0   w = [''] + list(word) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     likelihood *= p   print(f'Model predicts {word} is {likelihood:.9f} likely') <pre>Model predicts emma is 0.000003478 likely\nModel predicts olivia is 0.000000025 likely\nModel predicts ava is 0.000165674 likely\nModel predicts isabella is 0.000000000 likely\nModel predicts sophia is 0.000000026 likely\n</pre> <p>Question: How to fix the issue above?</p> <p>As can be seen, the result of chained multiplication is a very small number (somewhat resembling vanishing gradient problem). To resolve this issue, individual probabilities between <code>0</code> and <code>1</code> are mapped to a <code>log</code> function domain (-$\\infty$, 0]. Logarithm function is monotonic (preserves order): maximum probability is mapped to <code>0</code>, smaller probabilities are mapped to bigger negative values.</p> In\u00a0[25]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\np = np.linspace(0.001, 1, 200)\nlog_p = np.log(p)\n\nplt.figure(figsize=(6, 4))\nplt.plot(p, log_p)\nplt.title(\"Natural Log Function\")\nplt.grid(True)\n</pre> import numpy as np import matplotlib.pyplot as plt  p = np.linspace(0.001, 1, 200) log_p = np.log(p)  plt.figure(figsize=(6, 4)) plt.plot(p, log_p) plt.title(\"Natural Log Function\") plt.grid(True) In\u00a0[26]: Copied! <pre>n = 1\nfor w in words[:n]:\n  w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    log_p = torch.log(p)\n    print(f'{ch1, ch2}: {p.item():.4f} | {log_p.item():.4f}')\n</pre> n = 1 for w in words[:n]:   w = [''] + list(w) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     log_p = torch.log(p)     print(f'{ch1, ch2}: {p.item():.4f} | {log_p.item():.4f}') <pre>('&lt;START&gt;', 'e'): 0.0478 | -3.0408\n('e', 'm'): 0.0377 | -3.2793\n('m', 'm'): 0.0253 | -3.6772\n('m', 'a'): 0.3899 | -0.9418\n('a', '&lt;END&gt;'): 0.1960 | -1.6299\n</pre> <p>Log-likelihood also has an advantage of making calculations and hence optimization (calculation of gradients) faster due to the product rule:</p> <p>$\\log P(c_1, c_2, \\ldots, c_n) = \\log P(c_1) + \\log P(c_2 \\mid c_1) + \\log P(c_3 \\mid c_2) + \\cdots + \\log P(c_n \\mid c_{n-1})$</p> In\u00a0[27]: Copied! <pre>n = 5\nfor word in words[:n]:\n  log_likelihood = 0.0\n  w = ['&lt;START&gt;'] + list(word) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    log_p = torch.log(p)\n    log_likelihood += log_p\n  print(f'Model predicts {word} is {log_likelihood} likely')\n</pre> n = 5 for word in words[:n]:   log_likelihood = 0.0   w = [''] + list(word) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     log_p = torch.log(p)     log_likelihood += log_p   print(f'Model predicts {word} is {log_likelihood} likely') <pre>Model predicts emma is -12.568990707397461 likely\nModel predicts olivia is -17.511159896850586 likely\nModel predicts ava is -8.705486297607422 likely\nModel predicts isabella is -21.5141544342041 likely\nModel predicts sophia is -17.468196868896484 likely\n</pre> <p>As optimization algorithms usually strive for minimizing the loss, it makes sense to invert the negative values of the log-likelihood to be positive. We can generate a single loss value by averaging negative log-likelihoods across all the samples.</p> In\u00a0[30]: Copied! <pre>log_likelihood = 0.0\nfor word in words:\n  w = ['&lt;START&gt;'] + list(word) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    log_p = torch.log(p)\n    log_likelihood += log_p\nloss = -log_likelihood / len(words)\nprint(f'Loss: {loss}')\n</pre> log_likelihood = 0.0 for word in words:   w = [''] + list(word) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     log_p = torch.log(p)     log_likelihood += log_p loss = -log_likelihood / len(words) print(f'Loss: {loss}') <pre>Loss: 17.478591918945312\n</pre> <p>As we know, logarithmic function is undefined at <code>0</code>, which we need to take into consideration. Consider the case when a character combination has never occured in our training data.</p> In\u00a0[31]: Copied! <pre>for word in ['jq']:\n  log_likelihood = 0.0\n  w = ['&lt;START&gt;'] + list(word) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    log_p = torch.log(p)\n    log_likelihood += log_p\n  print(f'Model predicts {word} is {log_likelihood} likely')\n</pre> for word in ['jq']:   log_likelihood = 0.0   w = [''] + list(word) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     log_p = torch.log(p)     log_likelihood += log_p   print(f'Model predicts {word} is {log_likelihood} likely') <pre>Model predicts jq is -inf likely\n</pre> <p>In case any character sequence in string will return infinite likelihood, it will lead to infinite loss as well, which is undesirable.</p> <p>Question: How to avoid infinite loss?</p> <p>Model-smoothing is a simple technique, which aims to assign a minimal non-zero probability to cases leading to infinite likelihood. Run the next cell and replicate the experiments above to see the outcome.</p> In\u00a0[32]: Copied! <pre>bigrams = bigrams + 1  # model smoothing avoids zero probabilities\nprobs = bigrams/bigrams.sum(dim=1, keepdim=True)\n</pre> bigrams = bigrams + 1  # model smoothing avoids zero probabilities probs = bigrams/bigrams.sum(dim=1, keepdim=True) <p>Our frequency-based bigram model didn't perform well due its simplicity. We will now build a neural network-based bigram model with the aim of increasing individual bigram prediction probabilities (recall that likelihood was calculated by multiplying conditional probabilities). Instead of counting bigrams in our training set, we will learn parameters leading to reduced loss. We will now rewrite our <code>get_bigrams()</code> function to suit the training of neural network model, where the label of each character will be the next character.</p> In\u00a0[33]: Copied! <pre>def get_bigrams(n):\n  X, Y = [], []\n  for w in words[:n]:\n    w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n    for ch1, ch2 in zip(w, w[1:]):\n      X.append(stoi[ch1])\n      Y.append(stoi[ch2])\n  return torch.tensor(X), torch.tensor(Y)\n</pre> def get_bigrams(n):   X, Y = [], []   for w in words[:n]:     w = [''] + list(w) + ['']     for ch1, ch2 in zip(w, w[1:]):       X.append(stoi[ch1])       Y.append(stoi[ch2])   return torch.tensor(X), torch.tensor(Y) In\u00a0[34]: Copied! <pre>X, Y = get_bigrams(1)\nX, Y\n</pre> X, Y = get_bigrams(1) X, Y Out[34]: <pre>(tensor([26,  4, 12, 12,  0]), tensor([ 4, 12, 12,  0, 27]))</pre> In\u00a0[35]: Copied! <pre>[itos[x.item()] for x in X], [itos[y.item()] for y in Y]\n</pre> [itos[x.item()] for x in X], [itos[y.item()] for y in Y] Out[35]: <pre>(['&lt;START&gt;', 'e', 'm', 'm', 'a'], ['e', 'm', 'm', 'a', '&lt;END&gt;'])</pre> <p>We will one-hot encode our data with the <code>torch.nn.functional</code> module function, in order to not inject unnecessary numerical pattern to our data.</p> In\u00a0[38]: Copied! <pre>import torch.nn.functional as F\n\nX_train = F.one_hot(X, num_classes=SIZE).float()\ny_train = F.one_hot(Y, num_classes=SIZE).float()\n</pre> import torch.nn.functional as F  X_train = F.one_hot(X, num_classes=SIZE).float() y_train = F.one_hot(Y, num_classes=SIZE).float() In\u00a0[39]: Copied! <pre>plt.imshow(X_train);\n</pre> plt.imshow(X_train); <p>We will now generate weights for each character and find their linear transformation.</p> In\u00a0[41]: Copied! <pre>W = torch.randn((SIZE, 1))\nX_train @ W\n</pre> W = torch.randn((SIZE, 1)) X_train @ W Out[41]: <pre>tensor([[ 0.5013],\n        [-0.8258],\n        [ 1.0634],\n        [ 1.0634],\n        [ 0.9002]])</pre> <p>For each input character, our goal is to predict not a single probability, but probabilities for all possible output characters. Hence, we will update our weight matrix to correspond to both input and output. We will set <code>requires_grad=True</code> for future gradient calculation.</p> In\u00a0[42]: Copied! <pre>W = torch.randn((SIZE, SIZE), requires_grad=True)\n(X_train @ W).shape\n</pre> W = torch.randn((SIZE, SIZE), requires_grad=True) (X_train @ W).shape Out[42]: <pre>torch.Size([5, 28])</pre> <p>Note that <code>torch.randn()</code> function is generating weight values corresponding to Guassian distribution. Our goal is to map these values to all be positive, so that we can interpret the ouput as probabilities later on. The idea is similar to <code>log()</code> function previously. Here, the output values lower than zero will be mapped to be below <code>1</code> approaching <code>0</code>, when positive values will grow towards infinity.</p> In\u00a0[43]: Copied! <pre>x = np.linspace(-4, 4, 200)\nexp_x = np.exp(x)\n\nplt.figure(figsize=(6, 4))\nplt.plot(x, exp_x)\nplt.title(\"Exponentiation Function\")\nplt.grid(True)\n</pre> x = np.linspace(-4, 4, 200) exp_x = np.exp(x)  plt.figure(figsize=(6, 4)) plt.plot(x, exp_x) plt.title(\"Exponentiation Function\") plt.grid(True) <p>We call the raw scores we obtain after a linear transformation logits (log-counts). These values are real numbers, not constrained to be positive or normalized. To interpret them as prediction probabilities, we apply the exponential function to map them to the positive domain. The output of exponentiation can be interpreted as unnormalized character frequencies or relative likelihoods. When we normalize these exponentials (i.e. divide by their total sum), we obtain values that sum to <code>1.0</code>, forming a proper probability distribution. This is exactly what we want. Note that all these functions are differentiable and applying exponentiation and normalization is exactly what softmax function does:</p> <p>$\\text{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)}$</p> In\u00a0[44]: Copied! <pre>logits = X_train @ W\n\n# softmax\ncounts = torch.exp(logits)\nprobs = counts / counts.sum(dim=1, keepdim=True)\n\nprobs[0], probs[0].sum()\n</pre> logits = X_train @ W  # softmax counts = torch.exp(logits) probs = counts / counts.sum(dim=1, keepdim=True)  probs[0], probs[0].sum() Out[44]: <pre>(tensor([0.0471, 0.0249, 0.0381, 0.0411, 0.0384, 0.0678, 0.0478, 0.0210, 0.0155,\n         0.0191, 0.0920, 0.0500, 0.0138, 0.0090, 0.0042, 0.0120, 0.1256, 0.0500,\n         0.0855, 0.0199, 0.0043, 0.0082, 0.0215, 0.0744, 0.0232, 0.0054, 0.0234,\n         0.0168], grad_fn=&lt;SelectBackward0&gt;),\n tensor(1.0000, grad_fn=&lt;SumBackward0&gt;))</pre> <p>Exercise: Calculate the average negative log-likelihood (loss) of the model.</p> In\u00a0[45]: Copied! <pre>pred_probs = probs[torch.arange(len(Y)), Y]\nloss = -pred_probs.log().mean()\nloss\n</pre> pred_probs = probs[torch.arange(len(Y)), Y] loss = -pred_probs.log().mean() loss Out[45]: <pre>tensor(3.5038, grad_fn=&lt;NegBackward0&gt;)</pre> <p>We know how to call backward pass and optimize our model from previous lectures. We will combine all the steps, train our model on the whole dataset, and backpropagate through our network.</p> In\u00a0[46]: Copied! <pre>X, Y = get_bigrams(len(words))\n\nX_train = F.one_hot(X, num_classes=SIZE).float()\ny_train = F.one_hot(Y, num_classes=SIZE).float()\n\nW = torch.randn((SIZE, SIZE), requires_grad=True)\n</pre> X, Y = get_bigrams(len(words))  X_train = F.one_hot(X, num_classes=SIZE).float() y_train = F.one_hot(Y, num_classes=SIZE).float()  W = torch.randn((SIZE, SIZE), requires_grad=True) In\u00a0[47]: Copied! <pre>num_epochs = 100\nlearning_rate = 1\nlambda_ = 0.01\n\nfor epoch in range(num_epochs):\n  # forward pass\n  logits = X_train @ W\n  counts = torch.exp(logits)\n  probs = counts / counts.sum(dim=1, keepdim=True)\n  pred_probs = probs[torch.arange(len(Y)), Y]\n\n  l2 = (W**2).sum() # regularization\n  loss = -pred_probs.log().mean() + lambda_ * l2.sum()\n\n  # backward pass\n  W.grad = None\n  loss.backward()\n\n  # optimization\n  W.data -= learning_rate * W.grad\n\n  if (epoch + 1) % 10 == 0:\n    print(f'{epoch+1}/{num_epochs}, loss: {loss}')\n</pre> num_epochs = 100 learning_rate = 1 lambda_ = 0.01  for epoch in range(num_epochs):   # forward pass   logits = X_train @ W   counts = torch.exp(logits)   probs = counts / counts.sum(dim=1, keepdim=True)   pred_probs = probs[torch.arange(len(Y)), Y]    l2 = (W**2).sum() # regularization   loss = -pred_probs.log().mean() + lambda_ * l2.sum()    # backward pass   W.grad = None   loss.backward()    # optimization   W.data -= learning_rate * W.grad    if (epoch + 1) % 10 == 0:     print(f'{epoch+1}/{num_epochs}, loss: {loss}') <pre>10/100, loss: 9.035313606262207\n20/100, loss: 7.005525588989258\n30/100, loss: 5.685214042663574\n40/100, loss: 4.8257904052734375\n50/100, loss: 4.2659807205200195\n60/100, loss: 3.901088237762451\n70/100, loss: 3.6630942821502686\n80/100, loss: 3.5077712535858154\n90/100, loss: 3.406341552734375\n100/100, loss: 3.3400678634643555\n</pre> <p>Follows the implementation of Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Janvin: A neural probabilistic language model, based on Andrej Karpathy's building makemore: part 2.</p> <p>We will modify the <code>get_bigrams()</code> function to include custom <code>block_size</code>, which simply implies <code>N-1</code> of <code>N-gram</code> (bigram has the block size of <code>1</code>).</p> In\u00a0[48]: Copied! <pre>def get_ngrams(end, start=0, block_size=3):\n  X, Y = [], []\n  for w in words[start:end]:\n    context = ['&lt;START&gt;'] * block_size\n    for ch in list(w) + ['&lt;END&gt;']:\n      X.append([stoi[c] for c in context])\n      Y.append(stoi[ch])\n      context = context[1:] + [ch]\n  return torch.tensor(X), torch.tensor(Y)\n</pre> def get_ngrams(end, start=0, block_size=3):   X, Y = [], []   for w in words[start:end]:     context = [''] * block_size     for ch in list(w) + ['']:       X.append([stoi[c] for c in context])       Y.append(stoi[ch])       context = context[1:] + [ch]   return torch.tensor(X), torch.tensor(Y) In\u00a0[49]: Copied! <pre>X, Y = get_ngrams(1, block_size=3) # try different block sizes\n</pre> X, Y = get_ngrams(1, block_size=3) # try different block sizes In\u00a0[50]: Copied! <pre>for x, y in zip(X, Y):\n  context = [itos[i.item()] for i in x]\n  target = itos[y.item()]\n  print(f\"{context}: {target}\")\n</pre> for x, y in zip(X, Y):   context = [itos[i.item()] for i in x]   target = itos[y.item()]   print(f\"{context}: {target}\") <pre>['&lt;START&gt;', '&lt;START&gt;', '&lt;START&gt;']: e\n['&lt;START&gt;', '&lt;START&gt;', 'e']: m\n['&lt;START&gt;', 'e', 'm']: m\n['e', 'm', 'm']: a\n['m', 'm', 'a']: &lt;END&gt;\n</pre> <p>It is managable to convert <code>28</code> character indices by one hot encoding them to suit a neural network. However, this approach becomes innefficient when the vocabulary size increases. What if we have <code>10,000</code> words and our goal is to predict the next word? We would have to create large vectors with lots of zeros, not only wasting resources, but also losing similarity information among tokens (dot product of any two vectors will always be <code>0</code> due to orthogonality).</p> <p>A different approach is to get some smaller dimensional embedding of an index. Initially, these embeddings (parameters) are random, but over the course of training, model updates them to reflect the actual usage of the words in a context. When vectors of two different characters are learned to be close to each other, then we can conclude that, in our data, these characters were in a similar context. If two characters often show up in the same positions relative to surrounding characters, their embeddings get pulled toward each other. Consider the simple case below:</p> <pre>context1 = ['b', 'a', 'd', 'a']\ncontext2 = ['m', 'a', 'd', 'a']\ntarget = 'm'\n</pre> <p>If the training data consists of these contexts, then, in order to predict <code>m</code> for both contexts, their learned embeddings should be similar in value.</p> <p>For our case, <code>2</code> dimensional embeddings will be enough. For small datasets, having high embedding dimensionality may cause overfitting. When dataset is bigger, increasing dimensions helps to learn more nuanced relationships in the data, albeit at a higher computational cost.</p> In\u00a0[51]: Copied! <pre>X\n</pre> X Out[51]: <pre>tensor([[26, 26, 26],\n        [26, 26,  4],\n        [26,  4, 12],\n        [ 4, 12, 12],\n        [12, 12,  0]])</pre> In\u00a0[52]: Copied! <pre>C = torch.randn((SIZE, 2))\nemb = C[X]\nemb\n</pre> C = torch.randn((SIZE, 2)) emb = C[X] emb Out[52]: <pre>tensor([[[-0.6723,  0.3140],\n         [-0.6723,  0.3140],\n         [-0.6723,  0.3140]],\n\n        [[-0.6723,  0.3140],\n         [-0.6723,  0.3140],\n         [-1.4631, -1.0324]],\n\n        [[-0.6723,  0.3140],\n         [-1.4631, -1.0324],\n         [ 0.6817,  1.5840]],\n\n        [[-1.4631, -1.0324],\n         [ 0.6817,  1.5840],\n         [ 0.6817,  1.5840]],\n\n        [[ 0.6817,  1.5840],\n         [ 0.6817,  1.5840],\n         [-0.8068,  0.7403]]])</pre> <p>Now we will initilaize weights and biases by considering correct shape. In order to be able to use matrix multiplication, we will have to flatten embeddings as well.</p> In\u00a0[53]: Copied! <pre>emb.shape\n</pre> emb.shape Out[53]: <pre>torch.Size([5, 3, 2])</pre> <p>Question: What do embedding dimensions correspond to?</p> <p>Exercise: Pass embeddings through a single neuron.</p> In\u00a0[57]: Copied! <pre>layer_size = 100\nin_features = emb.shape[1] * emb.shape[2]\n\nW1 = torch.randn((in_features, layer_size))\nb1 = torch.randn(layer_size)\n\nout = emb.view(-1, in_features) @ W1 + b1\nact = torch.tanh(out)\n\nW1.shape, b1.shape, out.shape\n</pre> layer_size = 100 in_features = emb.shape[1] * emb.shape[2]  W1 = torch.randn((in_features, layer_size)) b1 = torch.randn(layer_size)  out = emb.view(-1, in_features) @ W1 + b1 act = torch.tanh(out)  W1.shape, b1.shape, out.shape Out[57]: <pre>(torch.Size([6, 100]), torch.Size([100]), torch.Size([5, 100]))</pre> In\u00a0[58]: Copied! <pre>emb.view(-1, in_features)\n</pre> emb.view(-1, in_features) Out[58]: <pre>tensor([[-0.6723,  0.3140, -0.6723,  0.3140, -0.6723,  0.3140],\n        [-0.6723,  0.3140, -0.6723,  0.3140, -1.4631, -1.0324],\n        [-0.6723,  0.3140, -1.4631, -1.0324,  0.6817,  1.5840],\n        [-1.4631, -1.0324,  0.6817,  1.5840,  0.6817,  1.5840],\n        [ 0.6817,  1.5840,  0.6817,  1.5840, -0.8068,  0.7403]])</pre> <p>Exercise: Create the next (final) layer.</p> In\u00a0[59]: Copied! <pre>W2 = torch.randn((layer_size, SIZE))\nb2 = torch.randn(SIZE)\n\nlogits = act @ W2 + b2\nlogits.shape\n</pre> W2 = torch.randn((layer_size, SIZE)) b2 = torch.randn(SIZE)  logits = act @ W2 + b2 logits.shape Out[59]: <pre>torch.Size([5, 28])</pre> <p>Exercise: Calculate loss.</p> In\u00a0[60]: Copied! <pre>counts = logits.exp()\nprob = counts / counts.sum(dim=1, keepdim=True)\nprob.shape\n</pre> counts = logits.exp() prob = counts / counts.sum(dim=1, keepdim=True) prob.shape Out[60]: <pre>torch.Size([5, 28])</pre> In\u00a0[65]: Copied! <pre>loss = -prob[torch.arange(prob.shape[0]), Y].log().mean()\nloss\n</pre> loss = -prob[torch.arange(prob.shape[0]), Y].log().mean() loss Out[65]: <pre>tensor(16.2439)</pre> <p>Average negative log-likelihood loss can be calculated with built-in <code>PyTorch</code> functions as well. Cross-entropy is simply <code>Softmax + Negative Log-Likelihood</code>.</p> In\u00a0[66]: Copied! <pre># loss = F.nll_loss(torch.log(prob), Y)\nloss = F.cross_entropy(logits, Y)\nloss\n</pre> # loss = F.nll_loss(torch.log(prob), Y) loss = F.cross_entropy(logits, Y) loss Out[66]: <pre>tensor(16.2439)</pre> <p>Exercise: Let's combine everything together and train on whole data.</p> In\u00a0[96]: Copied! <pre>X, Y = get_ngrams(len(words))\n</pre> X, Y = get_ngrams(len(words)) In\u00a0[97]: Copied! <pre>C = torch.randn((SIZE, 2), requires_grad=True)\nemb = C[X]\n\nlayer_size = 100\nin_features = emb.shape[1] * emb.shape[2]\n\nW1 = torch.randn((in_features, layer_size), requires_grad=True)\nb1 = torch.randn(layer_size, requires_grad=True)\n\nW2 = torch.randn((layer_size, SIZE), requires_grad=True)\nb2 = torch.randn(SIZE, requires_grad=True)\n\nparams = [C, W1, W2, b1, b2]\n</pre> C = torch.randn((SIZE, 2), requires_grad=True) emb = C[X]  layer_size = 100 in_features = emb.shape[1] * emb.shape[2]  W1 = torch.randn((in_features, layer_size), requires_grad=True) b1 = torch.randn(layer_size, requires_grad=True)  W2 = torch.randn((layer_size, SIZE), requires_grad=True) b2 = torch.randn(SIZE, requires_grad=True)  params = [C, W1, W2, b1, b2] In\u00a0[100]: Copied! <pre>def train(X, Y, params, num_epochs=10, learning_rate=0.01):\n  C, W1, W2, b1, b2 = params\n  for epoch in range(num_epochs):\n    # forward pass\n    emb = C[X]\n    in_features = emb.shape[1] * emb.shape[2]\n    out = emb.view(-1, in_features) @ W1 + b1\n    act = torch.tanh(out)\n    logits = act @ W2 + b2\n    loss = F.cross_entropy(logits, Y)\n    print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss}')\n    # backward pass\n    for p in params:\n      p.grad = None\n    loss.backward()\n    # optimization\n    for p in params:\n      p.data -= learning_rate * p.grad\n</pre> def train(X, Y, params, num_epochs=10, learning_rate=0.01):   C, W1, W2, b1, b2 = params   for epoch in range(num_epochs):     # forward pass     emb = C[X]     in_features = emb.shape[1] * emb.shape[2]     out = emb.view(-1, in_features) @ W1 + b1     act = torch.tanh(out)     logits = act @ W2 + b2     loss = F.cross_entropy(logits, Y)     print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss}')     # backward pass     for p in params:       p.grad = None     loss.backward()     # optimization     for p in params:       p.data -= learning_rate * p.grad In\u00a0[101]: Copied! <pre>train(X, Y, params)\n</pre> train(X, Y, params) <pre>Epoch: 1/10, Loss: 14.651238441467285\nEpoch: 2/10, Loss: 14.548416137695312\nEpoch: 3/10, Loss: 14.447701454162598\nEpoch: 4/10, Loss: 14.349052429199219\nEpoch: 5/10, Loss: 14.252448081970215\nEpoch: 6/10, Loss: 14.157876968383789\nEpoch: 7/10, Loss: 14.065322875976562\nEpoch: 8/10, Loss: 13.974783897399902\nEpoch: 9/10, Loss: 13.886248588562012\nEpoch: 10/10, Loss: 13.799703598022461\n</pre> <p>Exercise: Notice how slow it is to train on the whole data. Find out why. Can you also train on mini-batches? See the hint below:</p> In\u00a0[102]: Copied! <pre>batch_size = 4\nidx = torch.randint(0, X.shape[0], (batch_size,))\nidx, X[idx]\n</pre> batch_size = 4 idx = torch.randint(0, X.shape[0], (batch_size,)) idx, X[idx] Out[102]: <pre>(tensor([114762,  40173,  66663,  87063]),\n tensor([[ 7, 11,  4],\n         [18,  4, 24],\n         [ 0, 13,  4],\n         [12,  8, 11]]))</pre> In\u00a0[103]: Copied! <pre>def run(X, Y, params, num_epochs, lr=0.1, batch_size=None):\n  C, W1, W2, b1, b2 = params\n\n  for epoch in range(1, num_epochs+1):\n    if batch_size:\n      idx = torch.randint(0, X.size(0), (batch_size,))\n      batch_X, batch_Y = X[idx], Y[idx]\n    else:\n      batch_X, batch_Y = X, Y\n\n    emb = C[batch_X]\n    in_features = emb.shape[1] * emb.shape[2]\n    out = emb.view(-1, in_features) @ W1 + b1\n    act = torch.tanh(out)\n    logits = act @ W2 + b2\n    loss = F.cross_entropy(logits, batch_Y)\n\n    if epoch % (100 if batch_size else 1) == 0:\n      print(f'Epoch {epoch}, Loss {loss.item()}')\n\n    for p in params:\n      p.grad = None\n    loss.backward()\n\n    with torch.no_grad():\n      for p in params:\n        p.data -= lr * p.grad\n</pre> def run(X, Y, params, num_epochs, lr=0.1, batch_size=None):   C, W1, W2, b1, b2 = params    for epoch in range(1, num_epochs+1):     if batch_size:       idx = torch.randint(0, X.size(0), (batch_size,))       batch_X, batch_Y = X[idx], Y[idx]     else:       batch_X, batch_Y = X, Y      emb = C[batch_X]     in_features = emb.shape[1] * emb.shape[2]     out = emb.view(-1, in_features) @ W1 + b1     act = torch.tanh(out)     logits = act @ W2 + b2     loss = F.cross_entropy(logits, batch_Y)      if epoch % (100 if batch_size else 1) == 0:       print(f'Epoch {epoch}, Loss {loss.item()}')      for p in params:       p.grad = None     loss.backward()      with torch.no_grad():       for p in params:         p.data -= lr * p.grad In\u00a0[104]: Copied! <pre>run(X, Y, params, num_epochs=10, batch_size=None)\n</pre> run(X, Y, params, num_epochs=10, batch_size=None) <pre>Epoch 1, Loss 13.715128898620605\nEpoch 2, Loss 12.94245719909668\nEpoch 3, Loss 12.326409339904785\nEpoch 4, Loss 11.779574394226074\nEpoch 5, Loss 11.269842147827148\nEpoch 6, Loss 10.805395126342773\nEpoch 7, Loss 10.449189186096191\nEpoch 8, Loss 10.27437686920166\nEpoch 9, Loss 9.558894157409668\nEpoch 10, Loss 9.190631866455078\n</pre> In\u00a0[105]: Copied! <pre>run(X, Y, params, num_epochs=1000, batch_size=64)\n</pre> run(X, Y, params, num_epochs=1000, batch_size=64) <pre>Epoch 100, Loss 2.8726632595062256\nEpoch 200, Loss 2.904355525970459\nEpoch 300, Loss 3.114861488342285\nEpoch 400, Loss 2.7828195095062256\nEpoch 500, Loss 2.580034017562866\nEpoch 600, Loss 2.742938995361328\nEpoch 700, Loss 2.582310676574707\nEpoch 800, Loss 2.6041131019592285\nEpoch 900, Loss 2.6011006832122803\nEpoch 1000, Loss 2.5459964275360107\n</pre>"},{"location":"notebooks/05_nn_ngram/#05-neural-network-n-gram-model","title":"05. Neural Network N-Gram Model\u00b6","text":"1 May 2025 \u00b7    <p>Important</p> <p>     The notebook is currently under revision.   </p> <p>Note</p> <p>The notebook is highly based on Andrej Karpathy's makemore See the original video.</p>"},{"location":"notebooks/05_nn_ngram/#bigram-model","title":"Bigram Model\u00b6","text":""},{"location":"notebooks/05_nn_ngram/#average-negative-log-likelihood","title":"Average Negative Log-Likelihood\u00b6","text":""},{"location":"notebooks/05_nn_ngram/#neural-network-bigram-model","title":"Neural Network Bigram Model\u00b6","text":""},{"location":"notebooks/05_nn_ngram/#neural-network-n-gram-model","title":"Neural Network N-gram Model\u00b6","text":""},{"location":"notebooks/05_nn_ngram/#training-on-mini-batches","title":"Training on Mini-Batches\u00b6","text":""},{"location":"notebooks/06_batchnorm_resnet/","title":"06. Batch Normalization and Residual Blocks","text":"<p>Increasing the number of layers in neural networks for learning more advanced functions is challenging due to issues like vanishing gradients. VGGNet partially addressed this problem by using repetitive blocks that stack multiple convolutional layers before downsampling with max-pooling. For instance, two consecutive <code>3x3</code> convolutional layers achieve the same receptive field as a single <code>5x5</code> convolution, while preserving a higher spatial resolution for the next layer. In simpler terms, repeating a smaller kernel allows the network to access the same input pixels while retaining more detail for subsequent processing. Larger kernels blur (downsample) the image more aggressively, which can lead to the loss of important details and force the network to reduce resolution earlier in the architecture and stop.</p> <p>Despite this breakthrough, VGGNet was still limited and showed diminishing returns beyond <code>19</code> layers (hence, <code>vgg19</code> architecture). Another architecture was introduced the same year with the paper titled Going Deeper with Convolutions. It was named Inception because of the internet meme from the infamous Inception movie. I am not joking. If you don't believe me, scroll down the paper for references section and check out the very first reference.</p> <p>Inception architecture, and its implementation, <code>GoogLeNet</code> model (a play on words: 1) was developed by Google researchers, and 2) pays homage to the LeNet architecture), significantly reduced parameter count and leveraged the advantages of the <code>1x1</code> convolution kernel (see the Network in Network paper which also introduced <code>Global Average Pooling (GAP)</code> layer). Despite enabling deeper networks with far fewer parameters, Inception did not fully resolve the core training and convergence problems faced by very deep models.</p> <p>Batch Normalization and Residual Networks emerged as two major solutions for efficiently training neural networks as deep as <code>100</code> layers and more. We will now set up the data environment and go on discussing the core ideas and implementations of both papers.</p> In\u00a0[1]: Copied! <pre>import requests\nimport random\nimport string\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n</pre> import requests import random import string import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import TensorDataset, DataLoader In\u00a0[2]: Copied! <pre>########## DATA SETUP ##########\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\nresponse = requests.get(url)\nwords = response.text.splitlines()\nrandom.shuffle(words)\n\nchars = list(string.ascii_lowercase)\nstoi = {ch: i for i, ch in enumerate(chars)}\nstoi['&lt;START&gt;'] = len(stoi)\nstoi['&lt;END&gt;'] = len(stoi)\nitos = {i: ch for ch, i in stoi.items()}\n\nBLOCK_SIZE = 3\nVOCAB_SIZE = len(stoi)\nEMBED_SIZE = 10\nLAYER_SIZE = 100\n\nlen(words), BLOCK_SIZE, VOCAB_SIZE, words[0]\n</pre> ########## DATA SETUP ##########  url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\" response = requests.get(url) words = response.text.splitlines() random.shuffle(words)  chars = list(string.ascii_lowercase) stoi = {ch: i for i, ch in enumerate(chars)} stoi[''] = len(stoi) stoi[''] = len(stoi) itos = {i: ch for ch, i in stoi.items()}  BLOCK_SIZE = 3 VOCAB_SIZE = len(stoi) EMBED_SIZE = 10 LAYER_SIZE = 100  len(words), BLOCK_SIZE, VOCAB_SIZE, words[0] Out[2]: <pre>(32033, 3, 28, 'wafi')</pre> <p>A quick sidenote: it is encouraged to split the data into training, validation (also called dev), and test sets. When the dataset is not large, an <code>80/10/10</code> split is a reasonable ratio for allocation. For larger datasets (e.g. with one million images), it is fine to allocate <code>90%</code> or more of your data for training. The training set is used to update the model's parameters. The validation set is used for tuning hyperparameters (e.g. testing different learning rates, regularization strengths, etc.). The test split should ideally be used only once to report the final performance of the selected model (e.g. for inclusion in a research paper).</p> In\u00a0[3]: Copied! <pre>########## DATA PREP ##########\n\ndef get_ngrams(start=0, end=None):\n  X, Y = [], []\n  for word in words[start:end]:\n    context = ['&lt;START&gt;'] * BLOCK_SIZE\n    for ch in list(word) + ['&lt;END&gt;']:\n      X.append([stoi[c] for c in context])\n      Y.append(stoi[ch])\n      context = context[1:] + [ch]\n  return torch.tensor(X), torch.tensor(Y)\n\ndef split_data(p=80):\n  train_end = int(p/100 * len(words))\n  remaining = len(words) - train_end\n  val_end = train_end + remaining // 2\n\n  X_train, Y_train = get_ngrams(end=train_end)\n  X_val, Y_val = get_ngrams(start=train_end, end=val_end)\n  X_test, Y_test = get_ngrams(start=val_end, end=len(words))\n\n  return {\n    'train': (X_train, Y_train),\n    'val':   (X_val, Y_val),\n    'test':  (X_test, Y_test),\n  }\n\ndata = split_data()\n\nX_train, Y_train = data['train']\nX_val, Y_val = data['val']\nX_test, Y_test = data['test']\n\nlen(X_train), len(X_val), len(X_test)\n</pre> ########## DATA PREP ##########  def get_ngrams(start=0, end=None):   X, Y = [], []   for word in words[start:end]:     context = [''] * BLOCK_SIZE     for ch in list(word) + ['']:       X.append([stoi[c] for c in context])       Y.append(stoi[ch])       context = context[1:] + [ch]   return torch.tensor(X), torch.tensor(Y)  def split_data(p=80):   train_end = int(p/100 * len(words))   remaining = len(words) - train_end   val_end = train_end + remaining // 2    X_train, Y_train = get_ngrams(end=train_end)   X_val, Y_val = get_ngrams(start=train_end, end=val_end)   X_test, Y_test = get_ngrams(start=val_end, end=len(words))    return {     'train': (X_train, Y_train),     'val':   (X_val, Y_val),     'test':  (X_test, Y_test),   }  data = split_data()  X_train, Y_train = data['train'] X_val, Y_val = data['val'] X_test, Y_test = data['test']  len(X_train), len(X_val), len(X_test) Out[3]: <pre>(182535, 22720, 22891)</pre> <p>Batch normalization normalizes the inputs within a mini-batch before passing them to the next layer. That is, for each input feature $x_i$, we subtract the batch mean and divide by the batch standard deviation. A small constant  $\\epsilon$ is commonly added for maintaining numerical stability (to avoid zero division):</p> <p>$$ \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} $$</p> <p>This standardization gives $\\hat{x}_i$ a mean close to 0 and a standard deviation close to 1 over the batch. This may limit the model's capacity if left unchanged. Therefore, we introduce learnable parameters $\\gamma$ (scale) and $\\beta$ (shift) for flexibility:</p> <p>$$ BN = \\gamma \\hat{x}_i + \\beta $$</p> <p>Batch normalization is typically applied after the affine transformation ($Wx + b$) and before the non-linearity (e.g., ReLU):</p> <p>$$ act = \\phi(\\textrm{BN}(Wx)) $$</p> <p>Pay attention that we omitted $b$ when using batch normalization. In practice, the bias $b$ becomes redundant, because the shifting role is already handled by $\\beta$. Recall that <code>PyTorch</code> has <code>bias=False</code> option  as well (e.g. in <code>nn.Conv2d()</code>).</p> <p>Batch normalization improves convergence in optimization and has regularization effect. The original paper by Ioffe and Szegedyattributes this to reducing internal covariate shift \u2014 i.e. the shift in the distribution of layer inputs during training as parameters in earlier layers change. But this intuition is challenged. You can read more about that in d2l book chapter dedicated to batch normalization.</p> In\u00a0[4]: Copied! <pre>########## PARAMETER SETUP ##########\n\ndef get_params(batch_norm=True):\n  C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)\n\n  W1 = torch.randn((BLOCK_SIZE * EMBED_SIZE, LAYER_SIZE), requires_grad = True)\n  b1 = torch.zeros(LAYER_SIZE, requires_grad=True)\n\n  W2 = torch.randn((LAYER_SIZE, VOCAB_SIZE), requires_grad = True)\n  b2 = torch.zeros(VOCAB_SIZE, requires_grad=True)\n\n  params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n\n  if batch_norm:\n    gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)\n    beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)\n    params['gamma'] = gamma\n    params['beta'] = beta\n    # we can add additional code for omitting b1 in case of using beta (BN bias)\n\n  return params\n</pre> ########## PARAMETER SETUP ##########  def get_params(batch_norm=True):   C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)    W1 = torch.randn((BLOCK_SIZE * EMBED_SIZE, LAYER_SIZE), requires_grad = True)   b1 = torch.zeros(LAYER_SIZE, requires_grad=True)    W2 = torch.randn((LAYER_SIZE, VOCAB_SIZE), requires_grad = True)   b2 = torch.zeros(VOCAB_SIZE, requires_grad=True)    params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}    if batch_norm:     gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)     beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)     params['gamma'] = gamma     params['beta'] = beta     # we can add additional code for omitting b1 in case of using beta (BN bias)    return params In\u00a0[5]: Copied! <pre>########## FORWARD PASS ##########\n\n@torch.no_grad() # applies \"with torch.no_grad()\" to the whole function\ndef get_bn_stats(X_train, params):\n  emb = params['C'][X_train]\n  out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']\n  mean, std = out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5\n  return mean, std\n\ndef forward(X, params, batch_norm=False, bn_stats=None):\n  emb = params['C'][X]\n  out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']\n\n  if batch_norm:\n    mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)\n    out = (out - mean) / std\n    out = params['gamma'] * out + params['beta']\n\n  act = torch.tanh(out)\n  logits = act @ params['W2'] + params['b2']\n  return logits\n</pre> ########## FORWARD PASS ##########  @torch.no_grad() # applies \"with torch.no_grad()\" to the whole function def get_bn_stats(X_train, params):   emb = params['C'][X_train]   out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']   mean, std = out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5   return mean, std  def forward(X, params, batch_norm=False, bn_stats=None):   emb = params['C'][X]   out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']    if batch_norm:     mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)     out = (out - mean) / std     out = params['gamma'] * out + params['beta']    act = torch.tanh(out)   logits = act @ params['W2'] + params['b2']   return logits In\u00a0[6]: Copied! <pre>########## TRAINING &amp; INFERENCE ##########\n\ndef train(X, Y, params, num_epochs=100, lr=0.1, batch_size=None, batch_norm=False):\n  for epoch in range(1, num_epochs+1):\n    if batch_size:\n      idx = torch.randint(0, X.size(0), (batch_size,))\n      batch_X, batch_Y = X[idx], Y[idx]\n    else:\n      batch_X, batch_Y = X, Y\n\n    logits = forward(batch_X, params, batch_norm)\n    loss = F.cross_entropy(logits, batch_Y)\n\n    for p in params.values():\n      if p.grad is not None:\n        p.grad.zero_()\n    loss.backward()\n\n    with torch.no_grad():\n      for p in params.values():\n        p.data -= lr * p.grad\n\n    if epoch % (1000 if batch_size else 10) == 0:\n      print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n@torch.no_grad()\ndef evaluate(X, Y, params, batch_norm=False, bn_stats=None):\n  logits = forward(X, params, batch_norm, bn_stats)\n  loss = F.cross_entropy(logits, Y)\n  print(f\"Loss: {loss.item():.4f}\")\n</pre> ########## TRAINING &amp; INFERENCE ##########  def train(X, Y, params, num_epochs=100, lr=0.1, batch_size=None, batch_norm=False):   for epoch in range(1, num_epochs+1):     if batch_size:       idx = torch.randint(0, X.size(0), (batch_size,))       batch_X, batch_Y = X[idx], Y[idx]     else:       batch_X, batch_Y = X, Y      logits = forward(batch_X, params, batch_norm)     loss = F.cross_entropy(logits, batch_Y)      for p in params.values():       if p.grad is not None:         p.grad.zero_()     loss.backward()      with torch.no_grad():       for p in params.values():         p.data -= lr * p.grad      if epoch % (1000 if batch_size else 10) == 0:       print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")  @torch.no_grad() def evaluate(X, Y, params, batch_norm=False, bn_stats=None):   logits = forward(X, params, batch_norm, bn_stats)   loss = F.cross_entropy(logits, Y)   print(f\"Loss: {loss.item():.4f}\") In\u00a0[7]: Copied! <pre>########## TEST ##########\n\nepochs = 10_000\nlr = 0.01\nbatch_size = 32\nbatch_norm = True\ninit = True\n</pre> ########## TEST ##########  epochs = 10_000 lr = 0.01 batch_size = 32 batch_norm = True init = True In\u00a0[8]: Copied! <pre>params = get_params(batch_norm=batch_norm)\n\n# xavier for tanh, kaiming for relu\nif init:\n  nn.init.xavier_uniform_(params['W1'])\n  nn.init.xavier_uniform_(params['W2'])\n</pre> params = get_params(batch_norm=batch_norm)  # xavier for tanh, kaiming for relu if init:   nn.init.xavier_uniform_(params['W1'])   nn.init.xavier_uniform_(params['W2']) In\u00a0[9]: Copied! <pre>train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm)\n</pre> train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm) <pre>Epoch 1000, Loss: 2.7862\nEpoch 2000, Loss: 2.5897\nEpoch 3000, Loss: 2.3340\nEpoch 4000, Loss: 2.4985\nEpoch 5000, Loss: 2.7501\nEpoch 6000, Loss: 2.2715\nEpoch 7000, Loss: 2.6008\nEpoch 8000, Loss: 2.0696\nEpoch 9000, Loss: 2.3910\nEpoch 10000, Loss: 2.4777\n</pre> In\u00a0[10]: Copied! <pre>bn_stats = get_bn_stats(X_train, params) if batch_norm else None\n\nprint('Train and Validation losses:')\nevaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats)\nevaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats)\n</pre> bn_stats = get_bn_stats(X_train, params) if batch_norm else None  print('Train and Validation losses:') evaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats) evaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats) <pre>Train and Validation losses:\nLoss: 2.3337\nLoss: 2.3345\n</pre> In\u00a0[11]: Copied! <pre>########## SAMPLING ##########\n\n# minor changes to what we had previously for adapting to new code\ndef sample(params, n=10, batch_norm=False, bn_stats=None):\n  names = []\n  for _ in range(n):\n    context = ['&lt;START&gt;'] * BLOCK_SIZE\n    name = ''\n    while True:\n      X = torch.tensor([[stoi[c] for c in context]])\n      logits = forward(X, params, batch_norm, bn_stats)\n      probs = F.softmax(logits, dim=1)\n      id = torch.multinomial(probs, num_samples=1).item()\n      char = itos[id]\n      if char == '&lt;END&gt;':\n        break\n      name += char\n      context = context[1:] + [char]\n    names.append(name)\n  return names\n</pre> ########## SAMPLING ##########  # minor changes to what we had previously for adapting to new code def sample(params, n=10, batch_norm=False, bn_stats=None):   names = []   for _ in range(n):     context = [''] * BLOCK_SIZE     name = ''     while True:       X = torch.tensor([[stoi[c] for c in context]])       logits = forward(X, params, batch_norm, bn_stats)       probs = F.softmax(logits, dim=1)       id = torch.multinomial(probs, num_samples=1).item()       char = itos[id]       if char == '':         break       name += char       context = context[1:] + [char]     names.append(name)   return names In\u00a0[12]: Copied! <pre>sample(params, batch_norm=batch_norm, bn_stats=bn_stats)\n</pre> sample(params, batch_norm=batch_norm, bn_stats=bn_stats) Out[12]: <pre>['khuc',\n 'boka',\n 'lyq',\n 'rasyanrith',\n 'onna',\n 'helia',\n 'brhaylanio',\n 'boleiklak',\n 'ekbnqron',\n 'aren']</pre> <p>Hence, the idea of the residual connection is very simple. Before the second activation function, we add the previous input to the affine transformation. You can imagine the simplified code as below:</p> In\u00a0[13]: Copied! <pre>def residual_block(X):\n  act = torch.relu(X @ params['W1'] + params['b1'])\n  out = act @ params['W2'] + params['b2']\n  return torch.relu(out + X)\n</pre> def residual_block(X):   act = torch.relu(X @ params['W1'] + params['b1'])   out = act @ params['W2'] + params['b2']   return torch.relu(out + X) <p>However, If we attempt to directly run the code above, we will see a shape mismatch, as our final layer returns a matrix of dimension <code>VOCAB_SIZE</code> which is not equal to the input dimension <code>BLOCK_SIZE * EMBED_SIZE</code>.</p> <p>Exercise: Modifying the <code>forward</code> function by adding a residual connection.</p> In\u00a0[14]: Copied! <pre>def forward(X, params, batch_norm=False, bn_stats=None, residual=True):\n  emb = params['C'][X]\n  out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']\n\n  if batch_norm:\n    mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)\n    out = (out - mean) / std\n    out = params['gamma'] * out + params['beta']\n\n  act = torch.tanh(out + emb) if residual else torch.tanh(out)\n  logits = act @ params['W2'] + params['b2']\n  return logits\n</pre> def forward(X, params, batch_norm=False, bn_stats=None, residual=True):   emb = params['C'][X]   out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']    if batch_norm:     mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)     out = (out - mean) / std     out = params['gamma'] * out + params['beta']    act = torch.tanh(out + emb) if residual else torch.tanh(out)   logits = act @ params['W2'] + params['b2']   return logits In\u00a0[15]: Copied! <pre>X = params['C'][X_train].view(X_train.shape[0], -1)\nX.shape\n</pre> X = params['C'][X_train].view(X_train.shape[0], -1) X.shape Out[15]: <pre>torch.Size([182535, 30])</pre> <p>What to do? For demonstration purposes we will have to add another layer.</p> <p>Exercise (Advanced): Train a three layer model with batch normalization and residual connections.</p> In\u00a0[16]: Copied! <pre>def get_params(batch_norm=True):\n  C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)\n\n  in_features = BLOCK_SIZE * EMBED_SIZE\n\n  W1 = torch.randn((in_features, LAYER_SIZE), requires_grad = True)\n  b1 = torch.zeros(LAYER_SIZE, requires_grad=True)\n\n  W2 = torch.randn((LAYER_SIZE, in_features), requires_grad = True)\n  b2 = torch.zeros(in_features, requires_grad=True)\n\n  W3 = torch.randn((in_features, VOCAB_SIZE), requires_grad = True)\n  b3 = torch.zeros(VOCAB_SIZE, requires_grad=True)\n\n  params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}\n\n  if batch_norm:\n    gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)\n    beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)\n    params['gamma'] = gamma\n    params['beta'] = beta\n\n  return params\n</pre> def get_params(batch_norm=True):   C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)    in_features = BLOCK_SIZE * EMBED_SIZE    W1 = torch.randn((in_features, LAYER_SIZE), requires_grad = True)   b1 = torch.zeros(LAYER_SIZE, requires_grad=True)    W2 = torch.randn((LAYER_SIZE, in_features), requires_grad = True)   b2 = torch.zeros(in_features, requires_grad=True)    W3 = torch.randn((in_features, VOCAB_SIZE), requires_grad = True)   b3 = torch.zeros(VOCAB_SIZE, requires_grad=True)    params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}    if batch_norm:     gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)     beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)     params['gamma'] = gamma     params['beta'] = beta    return params In\u00a0[17]: Copied! <pre>def forward(X, params, batch_norm=False, bn_stats=None, residual=True):\n  emb = params['C'][X].view(X.shape[0], -1)\n  out = emb @ params['W1'] + params['b1']\n\n  if batch_norm:\n    mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)\n    out = (out - mean) / std\n    out = params['gamma'] * out + params['beta']\n\n  act = torch.relu(out)\n  out2 = act @ params['W2'] + params['b2']\n\n  if residual:\n    out2 = out2 + emb\n\n  logits = torch.tanh(out2) @ params['W3'] + params['b3']\n  return logits\n</pre> def forward(X, params, batch_norm=False, bn_stats=None, residual=True):   emb = params['C'][X].view(X.shape[0], -1)   out = emb @ params['W1'] + params['b1']    if batch_norm:     mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)     out = (out - mean) / std     out = params['gamma'] * out + params['beta']    act = torch.relu(out)   out2 = act @ params['W2'] + params['b2']    if residual:     out2 = out2 + emb    logits = torch.tanh(out2) @ params['W3'] + params['b3']   return logits In\u00a0[18]: Copied! <pre>params = get_params()\nparams.keys()\n</pre> params = get_params() params.keys() Out[18]: <pre>dict_keys(['C', 'W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'gamma', 'beta'])</pre> In\u00a0[19]: Copied! <pre># we are using relu in intermediate layer\nif init:\n  nn.init.kaiming_uniform_(params['W1'])\n  nn.init.kaiming_uniform_(params['W2']);\n</pre> # we are using relu in intermediate layer if init:   nn.init.kaiming_uniform_(params['W1'])   nn.init.kaiming_uniform_(params['W2']); In\u00a0[20]: Copied! <pre>train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm)\n</pre> train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm) <pre>Epoch 1000, Loss: 2.5227\nEpoch 2000, Loss: 2.9970\nEpoch 3000, Loss: 2.5845\nEpoch 4000, Loss: 2.3321\nEpoch 5000, Loss: 2.2630\nEpoch 6000, Loss: 2.5062\nEpoch 7000, Loss: 2.8853\nEpoch 8000, Loss: 2.3080\nEpoch 9000, Loss: 2.7023\nEpoch 10000, Loss: 2.8854\n</pre> In\u00a0[21]: Copied! <pre>bn_stats = get_bn_stats(X_train, params) if batch_norm else None\n\nprint('Train and Validation losses:')\nevaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats)\nevaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats)\n</pre> bn_stats = get_bn_stats(X_train, params) if batch_norm else None  print('Train and Validation losses:') evaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats) evaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats) <pre>Train and Validation losses:\nLoss: 2.3690\nLoss: 2.3698\n</pre> In\u00a0[22]: Copied! <pre>class ResidualBlock(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.emb = nn.Embedding(VOCAB_SIZE, EMBED_SIZE)\n    self.fc1 = nn.Linear(in_features=EMBED_SIZE, out_features=LAYER_SIZE, bias=False)\n    self.fc2 = nn.Linear(in_features=LAYER_SIZE, out_features=EMBED_SIZE, bias=False)\n    self.fc3 = nn.Linear(in_features=EMBED_SIZE, out_features=VOCAB_SIZE, bias=True)\n    self.bn1 = nn.LazyBatchNorm1d()\n    self.bn2 = nn.LazyBatchNorm1d()\n    nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n\n  # nn.LazyBatchNorm1d in 3D input expects shape (batch, channels, length) = (B, C, T)\n  # it normalizes across the batch and time (token, block) dimensions for each channel, independently\n  # we need to move that dimension to the middle (axis 1) with transpose(1, 2)\n  def forward(self, X):\n    emb = self.emb(X)                     # (BATCH_SIZE, BLOCK_SIZE, EMBED_SIZE)\n    out = self.fc1(emb).transpose(1, 2)   # (BATCH_SIZE, LAYER_SIZE, BLOCK_SIZE) for BatchNorm1d\n    out = self.bn1(out).transpose(1, 2)   # back to our dimensions\n    act = F.relu(out)\n    out = self.fc2(act).transpose(1, 2)\n    out = self.bn2(out).transpose(1, 2)\n    out += emb                            # shortcut connection\n    logits = self.fc3(out)                # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)\n    return logits\n</pre> class ResidualBlock(nn.Module):   def __init__(self):     super().__init__()     self.emb = nn.Embedding(VOCAB_SIZE, EMBED_SIZE)     self.fc1 = nn.Linear(in_features=EMBED_SIZE, out_features=LAYER_SIZE, bias=False)     self.fc2 = nn.Linear(in_features=LAYER_SIZE, out_features=EMBED_SIZE, bias=False)     self.fc3 = nn.Linear(in_features=EMBED_SIZE, out_features=VOCAB_SIZE, bias=True)     self.bn1 = nn.LazyBatchNorm1d()     self.bn2 = nn.LazyBatchNorm1d()     nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')    # nn.LazyBatchNorm1d in 3D input expects shape (batch, channels, length) = (B, C, T)   # it normalizes across the batch and time (token, block) dimensions for each channel, independently   # we need to move that dimension to the middle (axis 1) with transpose(1, 2)   def forward(self, X):     emb = self.emb(X)                     # (BATCH_SIZE, BLOCK_SIZE, EMBED_SIZE)     out = self.fc1(emb).transpose(1, 2)   # (BATCH_SIZE, LAYER_SIZE, BLOCK_SIZE) for BatchNorm1d     out = self.bn1(out).transpose(1, 2)   # back to our dimensions     act = F.relu(out)     out = self.fc2(act).transpose(1, 2)     out = self.bn2(out).transpose(1, 2)     out += emb                            # shortcut connection     logits = self.fc3(out)                # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)     return logits In\u00a0[23]: Copied! <pre>model = ResidualBlock()\ncel = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel.train()\n</pre> model = ResidualBlock() cel = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.01) model.train() Out[23]: <pre>ResidualBlock(\n  (emb): Embedding(28, 10)\n  (fc1): Linear(in_features=10, out_features=100, bias=False)\n  (fc2): Linear(in_features=100, out_features=10, bias=False)\n  (fc3): Linear(in_features=10, out_features=28, bias=True)\n  (bn1): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bn2): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)</pre> In\u00a0[24]: Copied! <pre>num_epochs = 10_000\nbatch_size = 32\n\nfor epoch in range(1, num_epochs+1):\n  model.train()\n  idx = torch.randint(0, X_train.size(0), (batch_size,))\n  batch_X, batch_Y = X_train[idx], Y_train[idx]\n  optimizer.zero_grad()\n  logits = model(batch_X)     # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)\n  logits = logits[:, -1, :]   # (BATCH_SIZE, VOCAB_SIZE)\n  loss = cel(logits, batch_Y)\n  loss.backward()\n  optimizer.step()\n  if epoch % 1000 == 0 or epoch == 1:\n    print(f'Epoch {epoch}, Loss: {loss.item()}')\n</pre> num_epochs = 10_000 batch_size = 32  for epoch in range(1, num_epochs+1):   model.train()   idx = torch.randint(0, X_train.size(0), (batch_size,))   batch_X, batch_Y = X_train[idx], Y_train[idx]   optimizer.zero_grad()   logits = model(batch_X)     # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)   logits = logits[:, -1, :]   # (BATCH_SIZE, VOCAB_SIZE)   loss = cel(logits, batch_Y)   loss.backward()   optimizer.step()   if epoch % 1000 == 0 or epoch == 1:     print(f'Epoch {epoch}, Loss: {loss.item()}') <pre>Epoch 1, Loss: 3.6320574283599854\nEpoch 1000, Loss: 2.374105930328369\nEpoch 2000, Loss: 2.6409666538238525\nEpoch 3000, Loss: 2.6358656883239746\nEpoch 4000, Loss: 2.36672043800354\nEpoch 5000, Loss: 2.696502208709717\nEpoch 6000, Loss: 2.4992451667785645\nEpoch 7000, Loss: 2.413964033126831\nEpoch 8000, Loss: 2.83028507232666\nEpoch 9000, Loss: 2.3721745014190674\nEpoch 10000, Loss: 2.6832263469696045\n</pre> In\u00a0[25]: Copied! <pre>model.eval()\nwith torch.no_grad():\n  logits_train = model(X_train)[:, -1, :]\n  logits_val   = model(X_val)[:, -1, :]\n\n  full_loss_train = cel(logits_train, Y_train)\n  full_loss_val   = cel(logits_val, Y_val)\n\n  print(f'Train loss: {full_loss_train.item()}')\n  print(f'Validation loss: {full_loss_val.item()}')\n</pre> model.eval() with torch.no_grad():   logits_train = model(X_train)[:, -1, :]   logits_val   = model(X_val)[:, -1, :]    full_loss_train = cel(logits_train, Y_train)   full_loss_val   = cel(logits_val, Y_val)    print(f'Train loss: {full_loss_train.item()}')   print(f'Validation loss: {full_loss_val.item()}') <pre>Train loss: 2.4901065826416016\nValidation loss: 2.4812421798706055\n</pre> In\u00a0[26]: Copied! <pre># modifying code to suit our needs\ndef sample(model, n=10, block_size=3):\n  model.eval()\n  names = []\n  for _ in range(n):\n    context = ['&lt;START&gt;'] * block_size\n    name = ''\n    while True:\n      idx = [stoi[c] for c in context]\n      X = torch.tensor([idx], dtype=torch.long)\n      with torch.no_grad():\n        logits = model(X)[0, -1] # VOCAB_SIZE\n      probs = F.softmax(logits, dim=0)\n      idx_next = torch.multinomial(probs, num_samples=1).item()\n      char = itos[idx_next]\n      if char == '&lt;END&gt;':\n        break\n      name += char\n      context = context[1:] + [char]\n    names.append(name)\n  return names\n</pre> # modifying code to suit our needs def sample(model, n=10, block_size=3):   model.eval()   names = []   for _ in range(n):     context = [''] * block_size     name = ''     while True:       idx = [stoi[c] for c in context]       X = torch.tensor([idx], dtype=torch.long)       with torch.no_grad():         logits = model(X)[0, -1] # VOCAB_SIZE       probs = F.softmax(logits, dim=0)       idx_next = torch.multinomial(probs, num_samples=1).item()       char = itos[idx_next]       if char == '':         break       name += char       context = context[1:] + [char]     names.append(name)   return names In\u00a0[27]: Copied! <pre>sample(model)\n</pre> sample(model) Out[27]: <pre>['kelifo',\n 'ja',\n 'tha',\n 'elarhncasoria',\n 'ka',\n 'voratte',\n 'eniysh',\n 'th',\n 'kelld',\n 'edm']</pre> In\u00a0[27]: Copied! <pre>\n</pre>"},{"location":"notebooks/06_batchnorm_resnet/#06-batch-normalization-and-residual-blocks","title":"06. Batch Normalization and Residual Blocks\u00b6","text":"23 Mar 2025 \u00b7    <p>Important</p> <p>     The notebook is currently under revision.   </p>"},{"location":"notebooks/06_batchnorm_resnet/#batch-normalization","title":"Batch Normalization\u00b6","text":""},{"location":"notebooks/06_batchnorm_resnet/#running_stats","title":"running_stats\u00b6","text":"<p>In <code>PyTorch</code> we use <code>model.eval()</code> during inference to switch the model into evaluation mode. This is important because layers like dropout and batch normalization behave differently during training and evaluation.</p> <p>During inference, normalization should be done using statistics over the whole dataset instead of mini-batches. Without <code>bn_stats</code> in the code below, the model would normalize using the current batch's mean and standard deviation, leading to inconsistent results depending on the batch.</p> <p>The implemented <code>PyTorch</code> layers like nn.BatchNorm1d automatically calculate running statistics during training. These statistics include a running mean and a running variance for each feature channel, which are stored as non-learnable buffers inside the <code>BatchNorm</code> layer.</p> <p>$$ \\mu_{\\text{running}} = \\alpha \\, \\mu_{\\text{batch}} + (1 - \\alpha) \\, \\mu_{\\text{running}} $$</p> <p>$$ \\sigma^2_{\\text{running}} = \\alpha \\, \\sigma^2_{\\text{batch}} + (1 - \\alpha) \\, \\sigma^2_{\\text{running}} $$</p> <p>In <code>BatchNorm</code>, $\\alpha$ is defined as <code>momentum</code> which is a misnomer and has nothing to do with the momentum we had previously learned for optimization. Its values controls how quickly the <code>running_stats</code> adapt. If momentum is high, the running statistics update quickly based on new batches which can make them unstable and noisy if batches vary a lot. If it is low (by default it is set to <code>0.1</code>, but you may want to reduce it further depending on circumstances), the updates are smoother and slower, averaging batch statistics over time.</p> <p>During evaluation <code>BatchNorm</code> uses the stored running mean and variance for normalization. This ensures deterministic behavior, regardless of the input batch. These buffers are automatically updated and used unless you disable tracking by setting <code>track_running_stats=False</code>.</p> <p>A manual implementation of <code>running_stats</code> is demonstrated in Andrej Karpathy's video as well. In this notebook, we will only implemented the simpler <code>bn_stats</code>.</p>"},{"location":"notebooks/06_batchnorm_resnet/#layer-normalization","title":"Layer Normalization\u00b6","text":"<p>A rule of thumb is that batch sizes between <code>50-100</code> generally work well for batch normalization: the batch is large enough to return reliable statistics but not so large that it causes memory issues or slows down training unnecessarily. Batch size of <code>32</code> is usually the lower bound where batch normalization still provides relatively stable estimates. Batch size of <code>128</code> is also effective if the hardware allows, and can produce even smoother estimates. Beyond that the benefit often diminishes.</p> <p>If the batch size is very small due to memory limitations, batch normalization may lose its effectiveness. In such cases, it's better to consider alternatives like Layer Normalization which do not depend on the batch dimension.</p> <p>Layer normalization normalizes across features for each individual sample, not across the batch and works well for transformers where batch sizes may be small or variable. Basically, batch normalization depends on the batch, but layer normalization does not.</p> <p>Furthermore, in fully connected layers, each feature is just a single number per sample, so batch normalization computes the mean and variance across the batch for each feature. Fully connected layers don't have spatial structure, so there's nothing to average across except the batch. In convolutional layers, each feature channel height and width and is a 2D map (hence, <code>nn.BatchNorm2d</code>), so batch normalization uses not just the batch dimension, but also all the spatial positions to compute statistics. This gives more stable estimates because there are more values per channel.</p>"},{"location":"notebooks/06_batchnorm_resnet/#residual-block","title":"Residual Block\u00b6","text":"<p>Residual Network (ResNet) consists of repeated residual blocks, in the style of the VGGNet architecture. Each residual block consists of a residual (skip/shortcut) connection . We will first see what it does and then will attempt to understand the reasoning behind this simple breakthrough idea.</p>"},{"location":"notebooks/06_batchnorm_resnet/#implementation","title":"Implementation\u00b6","text":"<p>Figure 8.6.2 of Dive into Deep Learning (Chapter 8) by d2l.ai authors and contributors. Licensed under Apache 2.0</p>"},{"location":"notebooks/06_batchnorm_resnet/#reasoning","title":"Reasoning\u00b6","text":"<p>As our model is implementing a single residual block, we don't see any performance improvement. However, similar to batch normalization, the advantages will be obvious in case of 50 layers or more, with repeated residual blocks. But why adding input of the layer to the second affine transformation boosts training?</p> <p>Let's take any deep learning model. The types of functions this model can learn depend on its design (e.g. number of layers, activation functions, etc). All these possible functions we can denote as class $\\mathcal{F}$. If we cannot learn a perfect function for our data, which is usually the case, we can at least try to appoximate this function as closely as possible by minimizing a loss. We may assume that a more powerful model can learn more types of functions and show better performance. But that's not always the case. To achieve a better performance than a simpler model, our model must be capable of learning not only more functions but also all the functions the simpler model can learn. Simply, the possible function class of the more powerful model should be a superclass of the simpler model's function class $\\mathcal{F} \\subseteq \\mathcal{F}'$. If the ${F}'$ isn't an expanded version of {F}$, the new model might actually learn a function that is farther from the truth, and even show worse performance.</p> <p>Refer to the figure above, where our residual output is $f(x) = g(x) + x$. One advantage of residual blocks is their regularization effect. What if some activation nodes in our network are unnecessary and increase complexity or learn bad representations? Instead of learning weights and biases, our residual block can now learn an identity function $f(x) = x$ by simply setting that nodes parameters to zero. As a result, our inputs will propagate faster while ensuring that the learned functions are within the biggest function domain. Residual blocks not only act as a regularizer, but also, unlike, say, dropout which stops input from propagating, allow the network to learn more functions by helping inputs to \"jump over\" (skip) the nodes. And it is very important that the function classes of the model with residual blocks is a superset of the same model without such blocks. Finally, along the way, it deals with the vanishing gradient problem by simply increasing the output of each layer. To sum up, residual connection allows the model to learn more complex functions, while allowing it to easily learn simpler ones, which tackles the vanishing gradient problem and has a regularizing effect.</p>"},{"location":"notebooks/06_batchnorm_resnet/#residual-network-for-nlp-in-pytorch","title":"Residual Network for NLP in PyTorch\u00b6","text":"<p>Originally, the complete Residual Network was developed for image classification tasks, winning ImageNet competition. Each of its residual block consisted of two <code>3x3</code> convolutions (inspired  by VGGNet), both integrating batch normalization, followed by a skip connection. Even though, ResNet model relies on convolutional layer, the concept of residual connections has been adapted for NLP models as well. The infamous Transformer model, introduced in the paper titled Attention is All You Need incorporates residual connections heavily in its design, which is very similar to ResNet.</p>"},{"location":"notebooks/07_vae/","title":"07. Variational Autoencoders (VAE)","text":"<p>But the latent process of which we speak, is far from being obvious to men\u2019s minds, beset as they now are. For we mean not the measures, symptoms, or degrees of any process which can be exhibited in the bodies themselves, but simply a continued process, which, for the most part, escapes the observation of the senses. ~ Francis Bacon (Novum Organum)</p> <p>https://www.cs.toronto.edu/~rgrosse/courses/csc311_f20/readings/L07%20Probabilistic%20Models.pdf</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n</pre> import torch import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>class Decoder(nn.Module):\n  def __init__(self, latent_dim, out_dim=784):\n    super().__init__()\n    self.fc1 = nn.Linear(latent_dim, 512)\n    self.fc2 = nn.Linear(512, out_dim)\n\n  def forward(self, z):\n    z = z.view(z.size(0), -1)\n    logits = self.fc2(F.relu(self.fc1(z)))\n    return F.sigmoid(logits)\n</pre> class Decoder(nn.Module):   def __init__(self, latent_dim, out_dim=784):     super().__init__()     self.fc1 = nn.Linear(latent_dim, 512)     self.fc2 = nn.Linear(512, out_dim)    def forward(self, z):     z = z.view(z.size(0), -1)     logits = self.fc2(F.relu(self.fc1(z)))     return F.sigmoid(logits) In\u00a0[\u00a0]: Copied! <pre>LATENT_DIM = 20\nBATCH_SIZE = 32\n</pre> LATENT_DIM = 20 BATCH_SIZE = 32 <p>As mentioned above, we need to sample $z$ from a multivariate Gaussian distribution, where $\\mu \\in [-\\infty, \\infty]$ and $\\sigma^2 \\geq 0$. We will use <code>torch.randn()</code> function to simulate random generation. Exponential function <code>torch.exp()</code> will be needed for mapping infinite range to be non-negative for variance.</p> In\u00a0[\u00a0]: Copied! <pre>mean    = torch.randn(BATCH_SIZE, LATENT_DIM)\nlogvar  = torch.randn(BATCH_SIZE, LATENT_DIM)\nvar     = torch.exp(logvar) # var &gt;= 0\n</pre> mean    = torch.randn(BATCH_SIZE, LATENT_DIM) logvar  = torch.randn(BATCH_SIZE, LATENT_DIM) var     = torch.exp(logvar) # var &gt;= 0 <p>In practice, we will learn suitable values for mean and variance. We can now sample $z \\sim \\mathcal{N}(\\mu,\\; \\mathrm{diag}(\\sigma^2))$, noting that <code>torch.normal()</code> function accepts $\\sigma$ instead of $\\sigma^2$.</p> In\u00a0[\u00a0]: Copied! <pre>z = torch.normal(mean, torch.sqrt(var))\ndec = Decoder(LATENT_DIM)\np_x_given_z = dec.forward(z)\n</pre> z = torch.normal(mean, torch.sqrt(var)) dec = Decoder(LATENT_DIM) p_x_given_z = dec.forward(z) <p>Plotting the generated distribution $p_\\theta(x \\mid z)$ above will naturally produce noise, as the decoder parameters are random. We need to figure out a way to train our network for optimal parameters.</p> In\u00a0[\u00a0]: Copied! <pre>plt.imshow(p_x_given_z[0].detach().numpy().reshape(28,28), cmap='gray');\n</pre> plt.imshow(p_x_given_z[0].detach().numpy().reshape(28,28), cmap='gray'); In\u00a0[\u00a0]: Copied! <pre>class Encoder(nn.Module):\n  def __init__(self, latent_dim, out_dim=128):\n    super().__init__()\n    self.conv1   = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1)\n    self.conv2   = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1)\n    self.fc1     = nn.Linear(32 * 5 * 5, out_dim)\n    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n    self.mean    = nn.Linear(out_dim, latent_dim)\n    self.logvar  = nn.Linear(out_dim, latent_dim)\n\n  def forward(self, x):\n    out1 = self.maxpool(F.relu(self.conv1(x)))\n    out2 = self.maxpool(F.relu(self.conv2(out1)))\n    out2 = out2.view(out2.size(0), -1)\n    out3 = F.relu(self.fc1(out2))\n    mean = self.mean(out3)\n    logvar = self.logvar(out3)\n    return mean, logvar\n</pre> class Encoder(nn.Module):   def __init__(self, latent_dim, out_dim=128):     super().__init__()     self.conv1   = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1)     self.conv2   = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1)     self.fc1     = nn.Linear(32 * 5 * 5, out_dim)     self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)     self.mean    = nn.Linear(out_dim, latent_dim)     self.logvar  = nn.Linear(out_dim, latent_dim)    def forward(self, x):     out1 = self.maxpool(F.relu(self.conv1(x)))     out2 = self.maxpool(F.relu(self.conv2(out1)))     out2 = out2.view(out2.size(0), -1)     out3 = F.relu(self.fc1(out2))     mean = self.mean(out3)     logvar = self.logvar(out3)     return mean, logvar <p>Once we have our encoder, we can sample the latent variable $z$ conditioned on the input data. We will now load the MNIST dataset and demonstrate the process for a single batch of data.</p> In\u00a0[\u00a0]: Copied! <pre>DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nNUM_WORKERS = 2\nBATCH_SIZE = 64\nLATENT_DIM = 30\n</pre> DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' NUM_WORKERS = 2 BATCH_SIZE = 64 LATENT_DIM = 30 In\u00a0[\u00a0]: Copied! <pre>train_data   = torchvision.datasets.MNIST('root',train=True,transform=transforms.ToTensor(),download=True)\ntrain_loader = torch.utils.data.DataLoader(train_data,shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n</pre> train_data   = torchvision.datasets.MNIST('root',train=True,transform=transforms.ToTensor(),download=True) train_loader = torch.utils.data.DataLoader(train_data,shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.91M/9.91M [00:00&lt;00:00, 23.0MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.9k/28.9k [00:00&lt;00:00, 614kB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.65M/1.65M [00:00&lt;00:00, 5.67MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.54k/4.54k [00:00&lt;00:00, 8.95MB/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>enc = Encoder(LATENT_DIM).to(DEVICE)\ndec = Decoder(LATENT_DIM).to(DEVICE)\n</pre> enc = Encoder(LATENT_DIM).to(DEVICE) dec = Decoder(LATENT_DIM).to(DEVICE) In\u00a0[\u00a0]: Copied! <pre>imgs, _ = next(iter(train_loader))\nimgs = imgs.to(DEVICE)\n\nmean, logvar = enc.forward(imgs)\nvar = torch.exp(logvar)\n\nq_z_given_x = torch.normal(mean, torch.sqrt(var))\np_x_given_z = dec.forward(q_z_given_x)\n</pre> imgs, _ = next(iter(train_loader)) imgs = imgs.to(DEVICE)  mean, logvar = enc.forward(imgs) var = torch.exp(logvar)  q_z_given_x = torch.normal(mean, torch.sqrt(var)) p_x_given_z = dec.forward(q_z_given_x) <p>Note that the MNIST pixel intensities are continuous values in $[0,1]$, not binary values $0$ and $1$. We can still use a Bernoulli likelihood for each pixel because the Bernoulli log-likelihood is exactly the same expression used in binary cross-entropy. Thus,</p> <p>$$ p_\\theta(x \\mid z) = \\prod_{j=1}^{784} p_j^{\\,x_j} (1 - p_j)^{\\,1 - x_j}, $$</p> <p>and</p> <p>$$ \\log p_\\theta(x \\mid z) = \\sum_{j=1}^{784} x_j \\log p_j + (1 - x_j) \\log (1 - p_j) $$</p> <p>remain valid even when the observed pixel values $x_j$ are real numbers in $[0,1]$. Treating $x_j$ as a fractional target simply corresponds to using binary cross-entropy, which is well-defined for any $x_j \\in [0,1]$.</p> In\u00a0[\u00a0]: Copied! <pre>LL = -F.binary_cross_entropy(imgs.view(imgs.size(0),-1), p_x_given_z, reduction='sum')\nprint(f'Log-likelihood: {LL.item():.4f}')\n</pre> LL = -F.binary_cross_entropy(imgs.view(imgs.size(0),-1), p_x_given_z, reduction='sum') print(f'Log-likelihood: {LL.item():.4f}') <pre>Log-likelihood: -2033668.5000\n</pre> <p>The generated output will naturally again be noise, as neither encoder nor decoder parameters ($\\phi$ and $\\theta$) are optimal. The goal of VAE is to efficiently learn those parameters and maximize the likelihood.</p> In\u00a0[\u00a0]: Copied! <pre>def reparam(mean, var):\n  eps = torch.randn_like(mean)\n  std = torch.sqrt(var)\n  z = mean + std * eps\n  return z\n</pre> def reparam(mean, var):   eps = torch.randn_like(mean)   std = torch.sqrt(var)   z = mean + std * eps   return z In\u00a0[\u00a0]: Copied! <pre>LR = 0.001\nNUM_EPOCHS = 10\n</pre> LR = 0.001 NUM_EPOCHS = 10 In\u00a0[\u00a0]: Copied! <pre>optimizer = torch.optim.Adam(list(enc.parameters()) + list(dec.parameters()), lr=LR)\n</pre> optimizer = torch.optim.Adam(list(enc.parameters()) + list(dec.parameters()), lr=LR) Out[\u00a0]: <pre>Decoder(\n  (fc1): Linear(in_features=30, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=784, bias=True)\n)</pre> In\u00a0[\u00a0]: Copied! <pre>dec.train()\nfor e in range(NUM_EPOCHS):\n  loss = 0\n  for X, _ in train_loader:\n    X = X.to(DEVICE)\n    optimizer.zero_grad()\n\n    mean, logvar = enc.forward(X)\n    var = torch.exp(logvar)\n\n    q_z_given_x = reparam(mean, var)\n    p_x_given_z = dec.forward(q_z_given_x)\n\n    LL = -F.binary_cross_entropy(p_x_given_z, X.view(X.size(0), -1), reduction='sum')\n    KL = 0.5 * torch.sum(mean**2 + var - 1 - logvar)\n    ELBO = LL - KL\n    batch_loss = -ELBO\n\n    batch_loss.backward()\n    optimizer.step()\n    loss += batch_loss.item() / (BATCH_SIZE * 784)\n  print(f\"Epoch {e+1}/{NUM_EPOCHS}, Loss: {loss:.4f}\")\n</pre> dec.train() for e in range(NUM_EPOCHS):   loss = 0   for X, _ in train_loader:     X = X.to(DEVICE)     optimizer.zero_grad()      mean, logvar = enc.forward(X)     var = torch.exp(logvar)      q_z_given_x = reparam(mean, var)     p_x_given_z = dec.forward(q_z_given_x)      LL = -F.binary_cross_entropy(p_x_given_z, X.view(X.size(0), -1), reduction='sum')     KL = 0.5 * torch.sum(mean**2 + var - 1 - logvar)     ELBO = LL - KL     batch_loss = -ELBO      batch_loss.backward()     optimizer.step()     loss += batch_loss.item() / (BATCH_SIZE * 784)   print(f\"Epoch {e+1}/{NUM_EPOCHS}, Loss: {loss:.4f}\") <pre>Epoch 1/10, Loss: 121.4904\nEpoch 2/10, Loss: 121.4055\nEpoch 3/10, Loss: 121.3329\nEpoch 4/10, Loss: 121.2401\nEpoch 5/10, Loss: 121.1607\nEpoch 6/10, Loss: 121.0556\nEpoch 7/10, Loss: 121.0213\nEpoch 8/10, Loss: 120.9725\nEpoch 9/10, Loss: 120.8978\nEpoch 10/10, Loss: 120.7906\n</pre> In\u00a0[\u00a0]: Copied! <pre>dec.eval()\n\nwith torch.no_grad():\n  z = torch.randn(BATCH_SIZE, LATENT_DIM, device=DEVICE)\n  imgs = dec.forward(z)\n\nimgs = imgs.detach().cpu().numpy()\n</pre> dec.eval()  with torch.no_grad():   z = torch.randn(BATCH_SIZE, LATENT_DIM, device=DEVICE)   imgs = dec.forward(z)  imgs = imgs.detach().cpu().numpy() In\u00a0[\u00a0]: Copied! <pre>num_imgs = imgs.shape[0]\nimgs_per_row = 8\nnum_rows = (num_imgs + imgs_per_row - 1) // imgs_per_row\n\nplt.figure(figsize=(12, num_rows * 2))\n\nfor i in range(num_imgs):\n    plt.subplot(num_rows, imgs_per_row, i + 1)\n    plt.imshow(imgs[i].reshape(28, 28), cmap='gray')\n    plt.axis('off')\n\nplt.show()\n</pre> num_imgs = imgs.shape[0] imgs_per_row = 8 num_rows = (num_imgs + imgs_per_row - 1) // imgs_per_row  plt.figure(figsize=(12, num_rows * 2))  for i in range(num_imgs):     plt.subplot(num_rows, imgs_per_row, i + 1)     plt.imshow(imgs[i].reshape(28, 28), cmap='gray')     plt.axis('off')  plt.show() In\u00a0[\u00a0]: Copied! <pre>train_data = torchvision.datasets.CIFAR10('root',train=True,transform=transforms.ToTensor(),download=True)\ntrain_loader = torch.utils.data.DataLoader(train_data,shuffle=True, batch_size=32, num_workers=2)\n</pre> train_data = torchvision.datasets.CIFAR10('root',train=True,transform=transforms.ToTensor(),download=True) train_loader = torch.utils.data.DataLoader(train_data,shuffle=True, batch_size=32, num_workers=2) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 170M/170M [00:03&lt;00:00, 43.6MB/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>batch = iter(train_loader)\nimgs, labels = next(batch)\nplt.imshow(imgs[0].reshape(3, 32, 32).permute(2, 1, 0));\n</pre> batch = iter(train_loader) imgs, labels = next(batch) plt.imshow(imgs[0].reshape(3, 32, 32).permute(2, 1, 0)); In\u00a0[\u00a0]: Copied! <pre>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Encoder(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1)\n    self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1)\n    self.fc1 = nn.Linear(32 * 5 * 5, 128)\n    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n    self.mean = nn.Linear(128, 20)\n    self.log_sigma = nn.Linear(128, 20)\n\n  def forward(self, x):\n    out1 = self.maxpool(F.relu(self.conv1(x)))\n    out2 = self.maxpool(F.relu(self.conv2(out1)))\n    out2 = out2.view(out2.size(0), -1)\n    out3 = self.fc1(out2)\n    mean = self.mean(out3)\n    log_sigma = self.log_sigma(out3)\n    return mean, log_sigma\n\n  def rsample(self, mean, log_sigma):\n    eps = torch.randn_like(mean)\n    std = torch.exp(0.5 * log_sigma)\n    z = mean + std * eps\n    return z\n\nclass Decoder(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.fc1 = nn.Linear(20, 128)\n    self.fc2 = nn.Linear(128, 784)\n\n  def forward(self, x):\n    out1 = F.relu(self.fc1(x))\n    out2 = torch.sigmoid(self.fc2(out1))\n    return out2.view(out2.size(0), 1, 28, 28)\n</pre> import torch.nn as nn import torch.nn.functional as F  class Encoder(nn.Module):   def __init__(self):     super().__init__()     self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1)     self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1)     self.fc1 = nn.Linear(32 * 5 * 5, 128)     self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)     self.mean = nn.Linear(128, 20)     self.log_sigma = nn.Linear(128, 20)    def forward(self, x):     out1 = self.maxpool(F.relu(self.conv1(x)))     out2 = self.maxpool(F.relu(self.conv2(out1)))     out2 = out2.view(out2.size(0), -1)     out3 = self.fc1(out2)     mean = self.mean(out3)     log_sigma = self.log_sigma(out3)     return mean, log_sigma    def rsample(self, mean, log_sigma):     eps = torch.randn_like(mean)     std = torch.exp(0.5 * log_sigma)     z = mean + std * eps     return z  class Decoder(nn.Module):   def __init__(self):     super().__init__()     self.fc1 = nn.Linear(20, 128)     self.fc2 = nn.Linear(128, 784)    def forward(self, x):     out1 = F.relu(self.fc1(x))     out2 = torch.sigmoid(self.fc2(out1))     return out2.view(out2.size(0), 1, 28, 28) In\u00a0[\u00a0]: Copied! <pre>epochs = 50\nfor e in range(epochs):\n  loss = 0.0\n  for img, labels in train_loader:\n    img, labels = img.to(device=device), labels.to(device=device)\n    optimizer.zero_grad()\n    mean, log_sigma = enc.forward(img)\n    z = enc.rsample(mean, log_sigma)\n    out = dec.forward(z)\n    LL = -F.binary_cross_entropy(out, img, reduction='sum')\n    sigma = torch.exp(0.5 * log_sigma)\n    KL = 0.5 * torch.sum(mean*mean + torch.exp(log_sigma) - log_sigma - 1)\n    ELBO = LL - KL\n    batch_loss = -ELBO\n    batch_loss.backward()\n    optimizer.step()\n    loss += batch_loss\n  print(f'Epoch {e} loss is {loss/len(train_loader)}')\n</pre> epochs = 50 for e in range(epochs):   loss = 0.0   for img, labels in train_loader:     img, labels = img.to(device=device), labels.to(device=device)     optimizer.zero_grad()     mean, log_sigma = enc.forward(img)     z = enc.rsample(mean, log_sigma)     out = dec.forward(z)     LL = -F.binary_cross_entropy(out, img, reduction='sum')     sigma = torch.exp(0.5 * log_sigma)     KL = 0.5 * torch.sum(mean*mean + torch.exp(log_sigma) - log_sigma - 1)     ELBO = LL - KL     batch_loss = -ELBO     batch_loss.backward()     optimizer.step()     loss += batch_loss   print(f'Epoch {e} loss is {loss/len(train_loader)}')"},{"location":"notebooks/07_vae/#07-variational-autoencoders-vae","title":"07. Variational Autoencoders (VAE)\u00b6","text":"25 Nov 2025 \u00b7    <p>Important</p> <p>     The notebook is currently under revision.   </p>"},{"location":"notebooks/07_vae/#decoder-network","title":"Decoder Network\u00b6","text":"<p>Let's say our latent variable $z$ is sampled from a multivariate Gaussian distribution: $z \\sim \\mathcal{N}(\\mu,\\; \\mathrm{diag}(\\sigma^2))$. For example, drawing each variable from a normal distribution $\\mathcal{N}(0, 1)$ may look like: $ z = [-1.2,\\; 0.4,\\; 2.1,\\; -0.7,\\; \\dots].$</p> <p>We can pass an $L$-dimensional input $z$ to our neural network (decoder) with parameters $\\theta$. The decoder should produce a conditional distribution $p_\\theta(x \\mid z)$, which tells how likely an output $x$ is for a given latent $z$. By adjusting $\\theta$, we want this conditional distribution to produce samples whose overall distribution approximates the true data distribution $p^\\ast(x)$. In other words, when $z$ is drawn from some latent distribution, the resulting samples $x \\sim p_\\theta(x \\mid z)$ should resemble real observable data. The true distribution $p^\\ast(x)$ is some unknown underlying distribution of universe the data lives in.</p> <p>Let's consider MNIST dataset, where each data point is a <code>28x28</code> grayscale image. The decoder, then, should generate <code>784</code> probabilities (one for each pixel):</p> <p>$$ p_\\theta(x = 1 \\mid z) = (p_1, p_2, \\dots, p_{784}). $$</p> <p>Here, each $p_j \\in [0,1]$ is the model's predicted probability that pixel $j$ is \"on\". With these probabilities, the conditional likelihood of the image $x$ given the latent variable $z$, also denoted by $p_\\theta(x \\mid z)$ (now a scalar) is:</p> <p>$$ p_\\theta(x \\mid z) = \\prod_{j=1}^{784} \\text{Bernoulli}(x_j \\mid p_j), $$</p> <p>where $x_j$ is the observed pixel value in the data. This expression defines a Bernoulli distribution for each pixel, and the full likelihood is the product over all pixels, assuming independence. It tells us how likely a particular image $x$ is, given a latent vector $z$.</p>"},{"location":"notebooks/07_vae/#intractable-integral","title":"Intractable Integral\u00b6","text":"<p>As we have already introduced the latent variable $z$ to our probabilistic model, we can suggest that our objective is to learn the joint distribution $p(x, z)$, which has a well-known formula:</p> <p>$$ p_\\theta(x, z) = p_\\theta(x \\mid z)\\, p(z). $$</p> <p>Both terms on the right are tractable. The decoder will produce the conditional likelihood $p_\\theta(x \\mid z)$ via a forward pass. The prior $p(z)$ we can choose to be a simple Gaussian, which has a closed-form density function we can easily compute. The difficulty appears only when we try to compute the marginal likelihood (evidence) of an observed image $x$, which is our approximation of $p^\\ast(x)$, given by another well-known formula:</p> <p>$$ p_\\theta(x) = \\int p_\\theta(x, z)\\, dz. $$</p> <p>Suppose a simple example of a 1-dimensional latent variable, where $z \\in \\mathbb{R}$ is a single real number. Even in this 1D case, computing the integral exactly requires evaluating the network $p_\\theta(x \\mid z)$ at every possible real value of $z$ from $ -\\infty$ to $ +\\infty$. Even if each dimension were sampled at only $100$ points, this would require $100^{20}$ evaluations for a 20-dimensional latent, which is a number larger than the number of atoms in the universe.</p> <p>Hence, for a continuous high-dimensional latent vector, evaluating the integral above is computationally impossible due to integral not having a closed-form solution. Integration requires evaluating the decoder network at every point in an $L$-dimensional space which is intractable.</p> <p>If we could somehow calculate this integral exactly, training the model would be straightforward. For each data point, we would compute log-likelihood $\\log p_\\theta(x)$ and optimize it with respect to $\\theta$ using ordinary gradient descent. Alas, the integral is intractable.</p>"},{"location":"notebooks/07_vae/#encoder-network","title":"Encoder Network\u00b6","text":"<p>Let's introduce the posterior distribution $p_\\theta(z \\mid x)$ to our probabilistic model. This distribution tells us how likely each latent vector $z$ is after we observe a particular data point $x$. We can define the posterior through Bayes' rule:</p> <p>$$ p_\\theta(z \\mid x) = \\frac{p_\\theta(x \\mid z)\\, p(z)}{p_\\theta(x)} = \\frac{p_\\theta(x, z)}{\\int p_\\theta(x, z)\\, dz}. $$</p> <p>However, the posterior is also intractable, as the denominator is intractable. How to tackle this problem? Authors of the Variational Autoencoder (VAE) paper introduce the encoder (recognition) model which aims at optimizing variational parameters $\\phi$ such that the generated posterior $q_\\phi(z \\mid x)$ is as close as possible to the intractable true posterior $p_\\theta(z \\mid x)$.</p> <p>In simple terms, the encoder is a neural network with learnable parameters $\\phi$ that outputs an approximate mean and variance of the required Gaussian distribution for each input $x$. Instead of sampling $z$ randomly out of blue, we will sample our latent variable from the learned distribution $q_\\phi(z \\mid x)$ which is dependent on the input data.</p>"},{"location":"notebooks/07_vae/#evidence-lower-bound-elbo","title":"Evidence Lower Bound (ELBO)\u00b6","text":"<p>Recall that we couldn't maximize the marginal likelihood $\\log p_\\theta(x)$ due to intractability. It turns out, with a nice trick up our sleeves, we can rewrite the marginal likelihood as expectation. The trick is that integrating any probability density over its entire domain is equal to $1$.</p> <p>$$ p_\\theta(x) = p_\\theta(x)\\int q_\\phi(z \\mid x)\\,dz  = \\mathbb{E}_{q_\\phi(z \\mid x)}[\\,p_\\theta(x)\\,]. $$</p> <p>Our likelihood function $p_\\theta(x)$ above returns a constant for each observed data point which does not depend on $z$. A constant function means that every possible value of the variable, no matter its probability, produces exactly the same output, so the weighted average (expectation) cannot change that value.</p> <p>Once we rewrite our likelihood as expectation, we can now make use of our variational posterior and Bayes rule to reach the following equation for log-likelihood:</p> <p>$$ \\begin{aligned} \\log p_\\theta(x) &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)}[\\, \\log p_\\theta(x)\\,] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)}\\!\\left[\\, \\log \\left[ \\frac{p_\\theta(x,z)}{p_\\theta(z \\mid x)} \\right]\\right] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)}\\!\\left[\\, \\log \\left[\\frac{p_\\theta(x,z)}{q_\\phi(z \\mid x)} \\cdot \\frac{q_\\phi(z \\mid x)}{p_\\theta(z \\mid x)}\\right] \\right] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)}\\!\\left[\\log \\frac{p_\\theta(x,z)}{q_\\phi(z \\mid x)}\\right] + \\mathbb{E}_{q_\\phi(z \\mid x)}\\!\\left[\\log \\frac{q_\\phi(z \\mid x)}{p_\\theta(z \\mid x)}\\right]. \\end{aligned} $$</p> <p>Here, the equation on the right corresponds to the Kullback-Leibler divergence formula $\\mathrm{KL}\\!\\left[q_\\phi(z \\mid x)\\,\\|\\, p_\\theta(z \\mid x)\\right]$ between variational and true posteriors. As KL divergence is positive or zero (in case our encoder somehow generates the true posterior), and $p_\\theta(z \\mid x)$ is unknown, we can ignore the KL divergence and concentrate on the first equation, which we call Evidence Lower Bound (ELBO). We can safely claim that our equation on the left will return a lower-bound of the log-likelihood for observed data:</p> <p>$$\\mathcal{L}_{\\theta,\\phi}(x) \\leq \\log p_\\theta(x).$$</p> <p>We can rewrite ELBO equation further:</p> <p>$$ \\begin{aligned} \\mathcal{L}_{\\theta,\\phi}(x) &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)}\\!\\left[\\log \\frac{p_\\theta(x,z)}{q_\\phi(z \\mid x)}\\right] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)} \\big[\\log p_\\theta(x,z) - \\log q_\\phi(z \\mid x)\\big] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)} \\big[\\log p_\\theta(x \\mid z) + \\log p_\\theta(z) - \\log q_\\phi(z \\mid x)\\big] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)}\\big[\\log p_\\theta(x \\mid z)\\big] - \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\, p_\\theta(z)\\right). \\end{aligned} $$</p> <p>It turns out, instead of maximizing the log-likelihood of our evidence (a single data point) which is intractable, we can aim at maximizing ELBO, the lower bound of our evidence likelihood, which is fully tractable. We can simply plug in and calculate the solution for the equation above and optimize our network for both $\\phi$ and $\\theta$ within a single backward pass.</p>"},{"location":"notebooks/07_vae/#unbiased-gradient-estimation","title":"Unbiased Gradient Estimation\u00b6","text":"<p>We should now calculate gradients but we have a problem again. A single data point ELBO gradient is intractable as the expected value of the likelihood requires computing integral over all possible latent variables. Luckily, we can find an unbiased estimator of the gradient w.r.t. $\\theta$ with the help of Monte Carlo sampling.</p> <p>$$ \\begin{aligned} \\nabla_{\\theta} \\mathcal{L}_{\\theta,\\phi}(x) &amp;= \\nabla_{\\theta}\\, \\mathbb{E}_{q_{\\phi}(z \\mid x)}     \\left[ \\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x) \\right] \\\\ &amp;= \\mathbb{E}_{q_{\\phi}(z \\mid x)}     \\left[ \\nabla_{\\theta}\\left( \\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x) \\right) \\right] \\\\ &amp;\\simeq \\nabla_{\\theta}\\left( \\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x) \\right) \\\\ &amp;= \\nabla_{\\theta}\\log p_{\\theta}(x, z). \\end{aligned} $$</p> <p>Monte Carlo estimator allows us to draw a few samples from $q_{\\phi}(z \\mid x)$ and approximate the expectation by averaging the function values at those samples. This works because the average of randomly drawn samples converges to the true expectation as $K$ increases. Using this approximation inside the gradient gives</p> <p>$$ \\nabla_{\\theta}\\, \\mathbb{E}_{q_{\\phi}(z \\mid x)}[f(z)] \\approx \\frac{1}{K} \\sum_{k=1}^{K} \\nabla_{\\theta} f\\!\\left(z^{(k)}\\right), \\qquad z^{(k)} \\sim q_{\\phi}(z \\mid x). $$</p> <p>which is an unbiased estimate of the true gradient. During neural network training, a single sample already provides an unbiased Monte Carlo estimate, as averaging comes from the minibatch: if a batch contains $B$ data points, the stochastic gradient $\\frac{1}{B} \\sum_{i=1}^{B} \\nabla f\\!\\left(z^{(1)}_i\\right)$ acts as the Monte Carlo average. Hence, we can use $K = 1$ and rely on minibatches to reduce variance.</p>"},{"location":"notebooks/07_vae/#reparametrization-trick","title":"Reparametrization Trick\u00b6","text":"<p>Estimating gradients w.r.t variational parameters $\\phi$ is more complicated. The parameter $\\phi$ appears inside the distribution $q_{\\phi}(z \\mid x)$, which means that when we take a gradient of the ELBO, we must also differentiate through the sampling operation itself:</p> <p>$$ \\begin{aligned} \\nabla_{\\phi} \\mathcal{L}_{\\theta,\\phi}(x) &amp;= \\nabla_{\\phi}\\, \\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left[ \\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x) \\right]. \\\\ &amp;\\neq \\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left[\\nabla_{\\phi}\\, (\\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x) \\right)]. \\end{aligned} $$</p> <p>To obtain a usable gradient estimator, we rewrite the sampling process in a differentiable form. Instead of sampling $z$ directly from $q_{\\phi}(z \\mid x)$, we introduce a noise variable $\\epsilon$ that does not depend on $\\phi$:</p> <p>$$ z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x)\\,\\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0, I). $$</p> <p>This is the reparameterization trick. Now the randomness comes only from $\\epsilon$, while $z$ becomes a deterministic and differentiable function of $\\phi$. Using this transformation, the expectation can be rewritten as</p> <p>$$ \\mathbb{E}_{q_{\\phi}(z \\mid x)}[f(z)] = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[ f\\!\\left(\\mu_{\\phi}(x) + \\sigma_{\\phi}(x)\\epsilon\\right) \\right], $$</p> <p>allowing gradients to pass through $z$ via standard backpropagation. The resulting gradient becomes</p> <p>$$ \\begin{aligned} \\nabla_{\\phi} \\mathcal{L}_{\\theta,\\phi}(x) &amp;= \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[ \\nabla_{\\phi} \\left( \\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x) \\right) \\right]. \\end{aligned} $$</p> <p>where $z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x)\\, \\epsilon$ which provides a low-variance and unbiased estimator that makes VAE optimization possible. As a consequence, a single data point Monte Carlo estimator becomes</p> <p>$$ \\tilde{\\mathcal{L}}_{\\theta,\\phi}(x) = \\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x). $$</p> <p>Applying reparametrization trick allows PyTorch smoothly use autograd engine when calculating gradients.</p>"},{"location":"notebooks/07_vae/#vae-training","title":"VAE Training\u00b6","text":"<p>We now have built all the tools required for coding the training logic of VAE. Almost all the tools. As the derivation is complicated and distractive for the main text, I will note the formula for the closed-form ELBO KL-divergence term in the appendix.</p>"},{"location":"notebooks/07_vae/#vae-generation","title":"VAE Generation\u00b6","text":"<p>After we train VAE network, all we need to do is sample our $z$ from a Guassian prior and pass it to our decoder for generation. If we have done everything right, we may even see some MNIST-like numbers on the screen.</p>"},{"location":"notebooks/07_vae/#closed-form-gaussian-kl-divergence","title":"Closed-Form Gaussian KL-divergence\u00b6","text":"<p>We already know that $\\log p_\\theta(x \\mid z)$ can be treated as a BCE loss. However, we still need to figure out closed-form solutions for finding the right-hand side of ELBO, which has the following log form:</p> <p>$$ \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right) = \\mathbb{E}_{q_\\phi(z\\mid x)} \\Big[ \\log q_\\phi(z\\mid x) - \\log p(z) \\Big]. $$</p> <p>Gaussian family is one of the few distributions where the KL-divergence has a simple analytic form. For example, in 1D, approximate posterior distribution $q_\\phi(z \\mid x)$ and prior could be sampled from $\\mathcal{N}(\\mu, \\sigma^2)$ and $\\mathcal{N}(0, 1)$ respectively. Knowing that standard normal is a special case of normal distribution, we have:</p> <p>\\begin{aligned} \\log q_\\phi(z \\mid x) &amp;= \\log\\!\\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\!\\left(-\\frac{(z-\\mu)^{2}}{2\\sigma^{2}}\\right) \\right] \\\\ &amp;= -\\frac{1}{2}\\log(2\\pi\\sigma^{2}) - \\frac{(z-\\mu)^{2}}{2\\sigma^{2}}. \\end{aligned}</p> <p>Therefore, subtracting Gaussians has the following form:</p> <p>\\begin{aligned} \\log q_\\phi(z \\mid x) - \\log p(z) &amp;= \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma^{2}) - \\frac{(z-\\mu)^{2}}{2\\sigma^{2}} \\right] - \\left[ -\\frac{1}{2}\\log(2\\pi) - \\frac{z^{2}}{2} \\right] \\\\ &amp;= -\\frac{1}{2}\\log\\sigma^{2} - \\frac{(z-\\mu)^{2}}{2\\sigma^{2}} + \\frac{z^{2}}{2}. \\end{aligned}</p> <p>Substituting these moments into the KL expression with its expectation form, we obtain: $$ \\begin{aligned} \\mathbb{E}_{q_\\phi(z\\mid x)} \\Big[ \\log q_\\phi(z\\mid x) - \\log p(z) \\Big] &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)} \\Big[ -\\tfrac{1}{2}\\log\\sigma^{2} - \\tfrac{(z-\\mu)^{2}}{2\\sigma^{2}} + \\tfrac{z^{2}}{2} \\Big] \\\\[6pt] &amp;= -\\tfrac{1}{2}\\log\\sigma^{2} - \\tfrac{1}{2\\sigma^{2}} \\mathbb{E}_{q_\\phi(z \\mid x)}[(z-\\mu)^{2}] + \\tfrac{1}{2} \\mathbb{E}_{q_\\phi(z \\mid x)}[z^{2}]. \\end{aligned} $$</p> <p>which becomes, after substituting the known Gaussian moments $\\sigma^{2}$ and $\\mu^{2} + \\sigma^{2}$:</p> <p>$$ \\begin{aligned} \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right) &amp;= -\\tfrac{1}{2}\\log\\sigma^{2} - \\tfrac{1}{2} + \\tfrac{1}{2}\\mu^{2} + \\tfrac{1}{2}\\sigma^{2} \\\\[6pt] &amp;= \\tfrac{1}{2} \\left( \\mu^{2} + \\sigma^{2} - 1 - \\log\\sigma^{2} \\right). \\end{aligned} $$</p> <p>For $L$-dimensional latent variable, approximate posterior we choose to be a multivariate Gaussian with a diagonal covariance: $q_\\phi(z \\mid x)=\\mathcal{N}\\!\\big(\\mu, \\mathrm{diag}(\\sigma^2)\\big),$ and the prior a standard multivariate normal: $p(z) = \\mathcal{N}(0, I).$ Because the covariance of $q_\\phi(z \\mid x)$ is diagonal, both $q_\\phi(z \\mid x)$ and $p(z)$ factorize over dimensions: $$ q_\\phi(z \\mid x) = \\prod_{j=1}^L q_{\\phi,j}(z_j \\mid x), \\qquad p(z) = \\prod_{j=1}^L p_j(z_j). $$</p> <p>From here, we can derive the final closed-form equation for the KL-divergence between approximate posterior and prior:</p> <p>$$ \\begin{aligned} \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right) &amp;= \\sum_{j=1}^L \\mathrm{KL}\\!\\left(q_{\\phi,j}(z_j \\mid x)\\,\\|\\,p_j(z_j)\\right) \\\\ &amp;= \\tfrac{1}{2} \\sum_{j=1}^L \\left( \\mu_j^{2} + \\sigma_j^{2} - 1 - \\log\\sigma_j^{2} \\right). \\end{aligned} $$</p>"},{"location":"notebooks/07_vae/#cifar-10","title":"CIFAR-10\u00b6","text":""},{"location":"supplementary/","title":"Suppementary Material","text":"<p>Important</p> <p>     The page is currently under development.   </p> <p>This page contains optional supplementary material intended to support the main course content. It provides additional background, intuition, or alternative explanations but is not required for assessments.</p>"}]}