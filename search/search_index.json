{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to CSCI 4701!","text":"<p>This is the website of the CSCI 4701: Deep Learning course taught at ADA University. </p> <p>Deep Learning focuses on artificial neural networks and how they are trained and optimized. This course covers core concepts starting with backpropagation, including regularization and optimization techniques. Students will gain practical skills using the PyTorch framework and learn neural architectures used in both Computer Vision (CV) and Natural Language Processing (NLP), including Convolutional Neural Networks (CNNs) and Transformer-based models. The course introduces generative modeling with Variational Autoencoders (VAEs) and briefly covers current developments in deep learning, such as Latent Diffusion Models (LDMs) and Large Language Models (LLMs). </p> <p>You can navigate through the course starting from the introductory overview or directly check the practical notebooks via the navigation bar.</p>"},{"location":"advanced/","title":"Advanced Material","text":"<p>Important</p> <p>     The page is currently under development.   </p> <p>This page presents advanced topics that go beyond the scope of the course. The material is optional and intended for students who wish to explore deeper or more current developments independently.</p>"},{"location":"course/spring-2026/01_syllabus/","title":"Syllabus","text":"<p>Important</p> <p>     The content of this syllabus is subject to change. Please consistently check the course page on Blackboard and the ADA University Academic Calendar for modifications. The last day of the add/drop period, holidays, and other academic deadlines are noted in the calendar.   </p> <p>Info</p> <p>    Square brackets in the Assessment / Notes column indicate the range of classes whose material is covered by the assessment. For example, Quiz 1 [1\u20133] means that the quiz assesses material covered in classes 1 through 3.   </p> Class Topic Learning Outcomes Assessment Materials 1 Deep Learning Overview / Course Structure Describe the scope of deep learning and the course syllabus. Fulfill technological requirements. introduction/01_deep_learning 2 Mathematics of Deep Learning: Calculus / Linear Algebra Compute partial derivatives and apply the chain rule. Work with vectors, matrices, and tensors; apply norms and inner products. Understand intuition behind eigenvectors and SVD. mathematics/01_calculus mathematics/02_linear_algebra supplementary/svd 3 Gradient Descent / Backpropagation I Compute gradients on computational graphs. Perform forward and backward passes. Understand gradient descent updates and automatic differentiation (PyTorch autograd, micrograd). notebooks/01_backprop 4 Gradient Descent / Backpropagation II Implement full backpropagation. Feb 3: Quiz 1 [1\u20133]Last day to submit team member details notebooks/01_backprop 5 Activation Functions / Neuron Implement activation functions and understand non-linearity. Backpropagate over an N-dimensional neuron. notebooks/02_neural_network 6 Multilayer Perceptron (MLP) / Cross-Entropy Construct an MLP from stacked neurons. Train a simple MLP classifier on a small dataset. Understand cross-entropy loss. Feb 10: Project proposal deadline notebooks/02_neural_network 7 Images as Tensors / Training MLP on MNIST Dataset Understand image representations, tensor shapes, and batching. Use torchvision datasets and dataloaders. Train an MLP on MNIST. Feb 12: Quiz 2 [5\u20136] notebooks/03_cnn_torch 8 Convolutional Neural Networks (CNN) Define and implement 2D cross-correlation (convolution) and pooling with kernels, including padding and stride. Train a LeNet-style CNN on MNIST. Compare MLP with CNN. notebooks/03_cnn_torch 9 Mathematics of Deep Learning: Probability Theory Describe random variables; distinguish discrete and continuous distributions; work with PMF/PDF. Compute expectation, variance, and covariance. Use conditional probability, independence, and Bayes\u2019 rule. Recognize common distributions. Feb 19: Quiz 3 [7\u20138] mathematics/03_probability 10 Regularization / Optimization Recall overfitting and understand how regularization helps with it. Apply data augmentation. Apply weight decay and dropout. Handle exploding and vanishing gradients. Use Xavier and He initialization. notebooks/04_regul_optim 11 Optimization Distinguish local minima from saddle points in training dynamics. Adjust learning rate and apply schedules. Use stochastic gradient-decent (SGD) and explain its purpose. Apply momentum, RMSProp, and Adam to optimize learning. Compare optimizers based on convergence behavior and practical performance. notebooks/04_regul_optim 12 Training CNN on CIFAR-10 Dataset / Hyperparameter Tuning Train a regularized and optimized CNN on CIFAR-10. Apply hyperparameter tuning. Mar 3: Quiz 4 [10\u201311] notebooks/04_regul_optim 13 Paper Reading: AlexNet / Transfer Learning and Fine-tuning Discuss AlexNet, its key ideas and structure. Determine what is outdated. Explain how pretrained CNNs enable transfer learning, and how fine-tuning adapts them to new datasets. Demonstrate fine-tuning of an ImageNet-pretrained CNN on CIFAR-10. [ alexnet ] 14 CNN Architectures / Batch Normalization Compare AlexNet, VGG, Inception in terms of depth, parameter count, and training purpose and stability. Explain why normalization helps training deep networks. Implement batch normalization and understand training vs evaluation behavior. Understand batch-size effects and when to prefer layer normalization. notebooks/05_batchnorm_resnet 15 Residual Block / Residual Network Understand residual (skip) connections. Train a model with residual blocks. Connect residuals to vanishing gradients and regularization. Mar 12: Quiz 5 [13-14]Project milestone 1 deadline notebooks/05_batchnorm_resnet 16 Midterm Exam \u2014 Tuesday, Mar 17: Midterm Exam [1\u201315] 17 Midterm Exam Review Half-semester overview. \u2014 Holidays \u2014 Mar 20\u201330 18 Sequence Modeling: Tokenization / Bigram Model / Perplexity Understand the aims of sequence modeling. Tokenize and build a character-level bigram model and sample from it. Implement average negative log-likelihood loss and perplexity. notebooks/06_nn_ngram 19 Neural N-gram Language Model Construct a neural N-gram model and train it with mini-batch updates. notebooks/06_nn_ngram 20 Autoregressive Modeling: RNN / LSTM Explain autoregressive modeling. Describe how RNNs maintain state. Implement RNN and LSTM and identify their limitations. Apr 7: Quiz 6 [18\u201319] 21 Attention Mechanism Understand attention as weighted information selection. Derive queries, keys, and values at the tensor level. Implement attention with matrix operations and verify shapes and normalization. 22 Transformer Architecture / Self-Attention Explain self-attention and Transformer blocks. Explain how Transformers scale. Apr 14: Quiz 7 [20\u201321] 23 Transformer Blocks Assemble a Transformer block from self-attention and feed-forward sublayers. Trace signal flow. Analyze training stability and sensitivity to initialization and learning rate. 24 Paper Reading: Transformer, Vision Transformer, Swin Transformer Extract core architectural ideas and compare attention for sequences vs images. Discuss scalability and efficiency constraints. Apr 21: Quiz 8 [22\u201323] 25 Mathematics of deep learning: Information Theory and Probabilistic Modeling Compute entropy, cross-entropy, and KL divergence. Derive cross-entropy loss from maximum likelihood. Interpret common losses as probabilistic objectives. NOTE: Will be taught immediately before sequence modeling in future semesters mathematics/04_information mathematics/05_prob_modeling   [ probabalistic models ] 26 Variational Autoencoders I Introduce latent-variable generative models. Explain latent representations and probabilistic encoders/decoders. Explain approximate inference and why variational methods are needed. Apr 28: Project milestone 2 deadline notebooks/notebooks/07_vae 27 Variational Autoencoders II Understand the VAE objective (ELBO). Implement a VAE. Interpret reconstruction and regularization terms and their trade-off. notebooks/notebooks/07_vae 28 Generative Adversarial Networks (GAN) / Diffusion Models / Score Matching Explain adversarial training between generator and discriminator. Describe common failure modes (mode collapse, instability) and stabilization techniques. Formulate diffusion via forward noising and learned reverse denoising. Interpret the training objective as denoising score matching. Explain sampling as iterative probabilistic inference. May 5: Quiz 9 [25\u201327] 29 Foundation Models and Modern Trends Explain large-scale pretraining and transfer learning. Examine GPT, BERT, CLIP, and latent diffusion models (LDMs). Discuss scaling behavior and limitations. \u2014 Final Exam \u2014 Tuesday, May 12: Final Exam [1\u201329]"},{"location":"course/spring-2026/02_assesments/","title":"Assessments","text":""},{"location":"course/spring-2026/03_project/","title":"Project","text":""},{"location":"introduction/01_deep_learning/","title":"Deep Learning","text":"18 Jan 2026 <p>Artificial Intelligence (AI) is the broad field concerned with building systems that perform tasks requiring intelligence. Machine Learning is a subfield of AI that enables systems to learn patterns and make decisions from data rather than explicit rules. Deep Learning is a subfield of machine learning that uses multi-layer neural networks to learn complex representations from large datasets.</p> <p>Info</p> <p>     The following sources were used in preparing this text:   </p> <ul> <li>       Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016).       Deep Learning. MIT Press.        </li> <li>       Schmidhuber, J. (2015).       Deep Learning in Neural Networks: An Overview.       Neural Networks.     </li> <li>       Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J.      Dive into Deep Learning. d2l.ai."},{"location":"introduction/01_deep_learning/#ai-ml-dl","title":"AI / ML / DL","text":"<p>AI initially focused on what is often called the knowledge-based approach, where intelligence was treated as something that could be explicitly written down. Researchers attempted to encode reasoning as rules, symbols, and logical statements. If a human expert knew how to solve a problem, the reasoning steps would be formalized and executed by a machine.</p> <p>This approach failed when faced with the ambiguity and variability of the real world. Tasks that humans perform effortlessly, such as recognizing faces or understanding speech, are precisely the tasks that are hardest to describe step by step. Human expertise in these domains is largely implicit rather than explicit. Rule-based systems therefore became brittle, difficult to scale, and expensive to maintain. Small changes in the environment often required rewriting large portions of the system, making progress slow and fragile.</p>      Deep Learning and AI ~ Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.     <p>Machine learning offered a different perspective. Instead of programming intelligence directly, machines were allowed to learn patterns from data. Classical machine learning algorithms such as linear models, logistic regression, na\u00efve Bayes, and decision trees achieved real success in applications like medical decision support, spam filtering, and credit scoring. However, these methods relied heavily on hand-crafted features. Human designers had to decide in advance which properties of the data were relevant, and performance depended more on feature design than on the learning algorithm itself.</p> <p>This reliance on features became a serious limitation as data grew more complex. Images, audio signals, and language live in very high-dimensional spaces. In such spaces, intuition breaks down, a phenomenon often referred to as the curse of dimensionality. As dimensionality increases, data becomes sparse, distances lose their meaning, and small modeling assumptions can cause large failures. Feature engineering becomes brittle and does not scale to the richness of real-world data.</p> <p>The natural response to this problem was representation learning. Instead of manually defining features, the model learns useful representations directly from raw data. Early methods such as Principal Component Analysis (PCA), kernel methods, sparse coding, and shallow neural networks pursued this idea. They demonstrated that learning intermediate representations could significantly improve performance and reduce reliance on handcrafted features. However, these approaches were typically shallow, consisting of only one or two layers of transformation. As a result, they struggled to capture the hierarchical structure present in real-world data.</p> <p>Many perceptual tasks are inherently compositional. Images are composed of edges, edges form textures and parts, parts form objects, and objects form scenes. Speech and language exhibit similar hierarchies. Shallow models can learn simple transformations, but they cannot efficiently represent such multi-level abstractions. Attempting to do so requires an exponential number of features or parameters, making learning unstable and data-inefficient. In practice, representation learning without depth hit a ceiling: it reduced feature engineering, but it could not scale to the complexity of vision, speech, and language.</p> <p>deep learning extends representation learning by stacking many layers of nonlinear transformations. Each layer learns to represent the data at a higher level of abstraction, allowing complex structures to be built incrementally. </p> <p>At a fundamental level, both classical machine and deep learning do the same thing: they learn a function from data. The difference is not in what is learned, but in how much of the function is learned automatically. In all cases, learning amounts to selecting parameters so that a function best approximates the desired input\u2013output relationship under a given objective.</p> <p>Interestingly, deep learning did not introduce fundamentally new mathematical ideas. Many concepts, such as multi-layer neural networks, backpropagation, gradient-based optimization, and even convolutional architectures were known decades earlier. </p>"},{"location":"introduction/01_deep_learning/#biological-and-artificial-neurons","title":"Biological and Artificial Neurons","text":"<p>deep learning is not an attempt to simulate the brain. Artificial neural networks are inspired by biological neurons, but the resemblance is conceptual rather than literal. </p>        Structure of a typical neuron with Schwann cells in the peripheral nervous system ~ \"Anatomy and Physiology\" by the US National Cancer Institute's Surveillance | CC BY-SA 3.0 | Wikimedia Commons <p>A biological neuron is a living cell designed for communication in a noisy, energy-constrained environment. It receives signals through dendrites, integrates them in the soma (cell body), and, if a threshold is reached, sends an electrical pulse along the axon to other neurons through synapses. Learning occurs locally by strengthening or weakening synaptic connections through repeated interaction with the environment.</p>        Artificial Neuron ~ Funcs, Own work | CC0 | Wikimedia Commons <p>An artificial neuron is a mathematical function that combines numerical inputs and produces a numerical output. Much like how airplanes were inspired by birds but rely on entirely different aerodynamic mechanisms, the success of deep learning does not come from biological realism. Biological systems served primarily as inspiration.</p>"},{"location":"introduction/01_deep_learning/#evolution-of-deep-learning","title":"Evolution of Deep Learning","text":"<p>Learning from data predates computers. The mathematical backbone of modern deep learning is the chain rule, formalized by Gottfried Wilhelm Leibniz and later exploited by backpropagation algorithms. Carl Friedrich Gauss and Adrien-Marie Legendre used linear regression in the early nineteenth century, a method mathematically equivalent to a shallow neural network. In the mid-twentieth century, researchers such as Warren McCulloch and Walter Pitts, Frank Rosenblatt, and Bernard Widrow explored learning machines inspired by biological neurons. These early systems were limited\u2014often linear or single-layer\u2014and constrained by the theory and hardware of their time.</p> <p>Multi-layer learning systems already existed by the 1960s and 1970s. Alexey Ivakhnenko and Valentin Lapa trained models with adaptive hidden layers, while Kunihiko Fukushima introduced the Neocognitron, a hierarchical, convolution-like architecture that anticipated modern convolutional networks.</p> <p>But why did deep learning become popular only after the 2010s? The obstacle was never the lack of a correct algorithm. It was the lack of data and the cost of computationDeep learning worked because three forces aligned. Data became abundant because digital life produces it automatically. Computation became affordable because parallel hardware matured. And (less critically) software matured enough to make experimentation fast and scalable.</p>"},{"location":"introduction/01_deep_learning/#data","title":"Data","text":"<p>The modern era began when data stopped being rare. This shift was driven by broader technological changes. Digital sensors replaced analog ones, smartphones placed cameras and microphones in billions of pockets, and the internet enabled continuous sharing of images, text, audio, and video. Companies began logging user interactions by default, storage became cheap, and bandwidth increased dramatically. Data was no longer collected deliberately, it was generated automatically as a byproduct of everyday life.</p> <p>Before large-scale datasets became feasible, progress relied on small, carefully curated benchmarks. The famous MNIST dataset was collected by the National Institute of Standards and Technology (NIST), and later was modified (hence the M before NIST) for simpler usage of machine learning algorithms<sup>2</sup>. MNIST is a simple dataset of handwritten digits that allowed researchers to isolate questions about optimization, architectures, and learning dynamics without the confounding effects of scale and noise. </p>      MNIST inputs ~ Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.     <p>A symbolic moment was the creation of ImageNet (Deng et al.). ImageNet contained roughly 14 million labeled images, with about 1.2 million training images across 1,000 categories used in its main benchmark. This scale exposed the limitations of hand-crafted features. Models that performed well on small datasets failed to generalize, while systems capable of learning representations directly from data improved reliably.</p> <p>In 2012, AlexNet (Krizhevsky et al.) won the ImageNet competition by a large margin. The model was unusually large and computationally demanding, and training it required GPUs rather than CPUs. This detail is crucialDeep learning did not succeed merely because sufficient data became available, it succeeded because the models finally fit within the limits of available hardware.<sup>1</sup></p>     \"Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5).\" ~ Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems (NeurIPS)      <p>As of 2016, a rough rule of thumb is that a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category and will match or exceed human performance when trained with a dataset containing at least 10 million labeled examples.  </p> <p>Deep Learning  (Chapter I) ~ Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). MIT Press.</p>"},{"location":"introduction/01_deep_learning/#hardware","title":"Hardware","text":"<p>Training neural networks is dominated by large-scale numerical operations repeated many times. CPUs are optimized for general-purpose tasks and complex control flow, but they are inefficient for massive parallel arithmetic. GPUs, originally designed for rendering images, apply the same operation to many data points simultaneously. This made them a natural fit for neural network training.</p> <p>NVIDIA became central to deep learning because it invested early in programmable GPUs and the software needed to exploit them. Although originally developed for video games, GPUs are fundamentally optimized for massively parallel linear algebra, especially matrix and tensor operations. The introduction of CUDA exposed this capability to researchers, making large-scale matrix multiplications\u2014the core computational workload of neural networks\u2014efficient and accessible. As a result, models that once took weeks to train on CPUs could be trained in days or hours. Later accelerators such as Tensor Processing Unit (TPU) followed the same principle: deep learning scales when hardware is designed around dense linear algebra, high memory bandwidth, and parallel computation.</p>"},{"location":"introduction/01_deep_learning/#software","title":"Software","text":"<p>The relevant software emerged in parallel with hardware. Python became the dominant language for machine learning because it allowed researchers to write clear, concise code while delegating computationally intensive operations to highly optimized numerical libraries implemented in C, C++, and CUDA. This separation between high-level model logic and low-level performance-critical kernels proved decisive. Researchers could focus on ideas rather than infrastructure, iterating rapidly while still benefiting from efficient linear algebra routines running on GPUs.</p> <p>Modern deep learning frameworks such as PyTorch and TensorFlow made it possible to automate differentiation, memory management, and efficient parallel execution. As a result, experiments that once required weeks of careful implementation could be expressed in hundreds of lines of code and tested within days.</p> <p>PyTorch is primarily a tool for research and experimentation. It is designed to feel like ordinary Python code, which makes models easy to write, modify, and debug. Tools such as PyTorch Lightning build on this flexibility by handling routine tasks like training loops and logging, allowing users to keep their focus on the model itself.</p> <p>TensorFlow, on the other hand, is more strongly oriented toward engineering and deployment. It was built to support large systems that need to run reliably across different machines and environments. With the addition of Keras, TensorFlow offers a high-level interface that makes it easy to define standard models and training pipelines in a consistent way. This structure is well suited to production settings, where models must be maintained, scaled, and deployed efficiently over long periods of time.</p>"},{"location":"introduction/01_deep_learning/#transformers-and-beyond","title":"Transformers and Beyond","text":"<p>Computer Vision (CV) and Natural Language Processing (NLP) are the two main perception-oriented branches of modern DL. Both aim to convert raw, high-dimensional signals into structured representations that machines can reason over, but they operate on different data modalities and evolved under different constraints.</p> <p>CV focuses on visual data such as images and videos. Early progress was driven by convolutional neural networks (CNN). NLP deals with sequential, symbolic data such as text and speech. While early neural NLP relied on recurrent models (RNN), a major conceptual shift occurred with the introduction of the Transformer architecture (Vaswani et al., 2017), which replaced sequential recurrence with attention-based information routing. This change enabled massive parallelism, better long-range dependency modeling, and effective scaling with data and compute. The same architecture was later adapted to images via Vision Transformers (Dosovitskiy et al., 2020), revealing that vision and language could share a common computational backbone despite their different input structures.</p> <p>deep learning also extended beyond perception into decision-making. The combination of deep learning and reinforcement learning became widely visible through AlphaGo and later AlphaZero (Silver et al., 2016; 2018), which learned complex games through self-play without human examples.</p> <p>Building on the Transformer architecture, Large Language Models (LLM) such as Generative Pre-trained Transformer (GPT) marked a shift from task-specific NLP systems to general-purpose foundation models. By training a model on massive text corpora, GPT-style models learn broad linguistic, semantic, and world-level regularities that can be reused across tasks. Their success demonstrated that scale\u2014data, parameters, and compute\u2014can replace handcrafted linguistic structure, and that a single architecture can support a wide range of capabilities, including translation, summarization, reasoning, and code generation, without explicit task-specific design.</p> <ol> <li> <p>Even then, Alex Krizhevsky had to distribute training across two NVIDIA GeForce GTX 580 GPUs, each with 3 GB of memory (best at the moment), because the network did not fit on a single GPU.\u00a0\u21a9</p> </li> <li> <p>Geoffrey Hinton called this dataset \"the drosophila of ML\", a fruit fly extensively used in genetic research labs.\u00a0\u21a9</p> </li> </ol>"},{"location":"introduction/02_machine_learning/","title":"Machine Learning","text":"<p>Important</p> <p>This page is currently under development.</p>"},{"location":"introduction/03_resources/","title":"Resources","text":"<p>There are resources which I find helpful for studying deep learning. They are listed below:</p> <p>Deep Learning (Goodfellow, Bengio, Courville) is a classic book on deep learning. It is systematic and rigorous, especially for core ideas. It is well suited for building long-term foundations and for reading research papers. But it is not up to date with modern practice (e.g. transformers), and it offers little guidance on implementation.</p> <p>Dive into Deep Learning is a modern, code-based book. It is mostly up to date and tightly integrates explanations with runnable code with different frameworks. It covers a wide range of contemporary architectures and techniques. But it is less systematic (perhaps, due to many authors) and less mathematically rigirous.</p> <p>Deep Learning: Zero to Hero by Andrej Karpathy is a lecture series that builds neural networks from backpropagation to transformers from scratch. It demonstrates the implementation of major papers and core concepts in a simple manner.</p> <p>Deep Learning by 3Blue1Brown focuses on visual intuition. It is excellent for forming a general understanding about deep learning. It is technically not heavy and should be treated as a conceptual support.</p> <p>Learn PyTorch is a practical guide to PyTorch. It demonstrates how modern deep learning workflows are written and organized in code. It assumes prior understanding of important topics.</p>"},{"location":"mathematics/","title":"Mathematics of Deep Learning","text":"18 Jan 2026 \u00b7   6 min <p>Deep Learning (DL) relies on mathematics, but not on all of mathematics equally. Many topics that are common in standard mathematics curricula play little or no role in the practice of DL. The purpose of this section is to explain which parts of mathematics matter for deep learning and what role they play. </p> <p>In preparing this material, two widely used resources were consulted and found to be highly valuable: Deep Learning (Goodfellow et al., 2016) and Dive into Deep Learning (Zhang et al., online).</p> <p>Deep Learning presents the mathematics in a concise and rigorous form. Its strength lies in precision and breadth, but this compact style can make it difficult for readers to develop intuition, especially when encountering these ideas for the first time. Key concepts are often introduced quickly, with limited space for informal explanation or gradual buildup.</p> <p>Dive into Deep Learning takes a different approach, tightly integrating mathematical ideas with executable code. This makes experimentation accessible and practical, but it can also blur the boundary between mathematical concepts and their implementation. The mathematical knowledge is not always presented in a clearly systematized form.</p> <p>The goal of the present material is to combine the strengths of both approaches while addressing their limitations. Mathematical ideas are introduced carefully and explained in simple language, with implementation details separated whenever possible. Each concept is included because it plays a clear role indeep learning, not because it belongs to a traditional mathematics curriculum. The aim is to provide a conceptual foundation that supports both practical experimentation and deeper theoretical study.</p> <p>More detailed overviews can be found in the separate pages dedicated to Calculus, Linear Algebra, Probability Theory, and Information Theory. Below is a summary of the main mathematical concepts required for DL.</p>"},{"location":"mathematics/#calculus","title":"Calculus","text":"<p>Within calculus, the central idea for deep learning is the rate of change: if we change some model parameters slightly, how does the output change? Indeep learning, the output of interest is usually a single number called the loss, which measures how bad the model's prediction is. A derivative tells us how much the loss changes when we slightly change one parameter. This makes derivatives a practical tool for learning, since they indicate the direction in which parameters should be adjusted to reduce the loss.</p> <p>Partial derivatives are essential because a model typically has many parameters. A partial derivative measures how sensitive the loss is to one parameter while all other parameters are kept fixed. The gradient simply collects all these sensitivities into a single array. The chain rule explains how sensitivities propagate through a model that is built from many smaller operations, and backpropagation is the algorithm that applies the chain rule efficiently to compute gradients.</p> <p>All of this relies on an important assumption: the loss changes smoothly with respect to the parameters. This means that small changes in parameters lead to small, predictable changes in the loss, making derivatives reliable guides for optimization.</p> <p>Integration and the Fundamental Theorem of Calculus appear more quietly in the background. They underlie concepts such as expectations and averages, which are central to training objectives. Indeep learning, integrals are rarely computed by hand; understanding what integration represents is more important than learning how to calculate it.</p>"},{"location":"mathematics/#linear-algebra","title":"Linear Algebra","text":"<p>Linear algebra is the language in which deep learning models are written. Data points, parameters, and gradients are represented as vectors. Linear layers are represented as matrix\u2013vector or matrix\u2013matrix multiplications.</p> <p>What matters most is intuition. Vectors should be understood as ordered collections of numbers. Matrices should be understood as operations that transform vectors by scaling, rotating, or mixing their components. These ideas explain how information flows through a network and why many computations can be done efficiently in parallel.</p> <p>Linear algebra also explains why gradients have the same shape as parameters, why batching works, and why modern hardware such as GPUs is effective for DL. Model parameters are stored as vectors and matrices, and gradients are derivatives with respect to those parameters, so they naturally have the same structure. This one-to-one correspondence makes parameter updates straightforward: each parameter is adjusted using its matching gradient entry.</p> <p>Modern deep learning frameworks are built almost entirely on linear algebra operations. Matrix multiplication, matrix addition, and vectorized nonlinear functions form the core of both the forward and backward passes. Most performance optimizations are handled automatically by numerical libraries, allowing users to express models at a high level while relying on efficient low-level implementations. </p> <p>While matrix factorizations such as Singular Value Decomposition (SVD) are rarely invoked explicitly during training, they are used internally in optimized linear solvers, low-rank approximations, spectral normalization, and in estimating matrix norms or conditioning. Through these mechanisms, factorization-based ideas influence numerical stability, efficiency, and scaling behavior in deep learning systems without appearing directly in model code.</p> <p>Batching works because linear algebra operations naturally extend from single vectors to collections of vectors stacked into matrices or higher-dimensional tensors. Processing many data points at once is not a special trick, but a direct consequence of writing models in matrix form. GPUs are effective for deep learning for the same reason: linear algebra operations consist of many simple arithmetic operations that can be carried out in parallel. As a result, deep learning benefits directly from both the mathematical structure of linear algebra and the hardware designed to execute it efficiently.</p>"},{"location":"mathematics/#probability-theory","title":"Probability Theory","text":"<p>deep learning models are trained on data that is noisy, incomplete, and often ambiguous. Probability provides the language for describing this uncertainty and for turning learning into a well-defined mathematical problem. Indeep learning, models are often best understood not as systems that produce a single \"correct\" output, but as systems that assign probabilities to possible outcomes.</p> <p>From this perspective, a model defines a probability distribution, either explicitly or implicitly. Training the model means adjusting its parameters so that the observed data becomes more probable under this distribution. Many commonly used loss functions arise directly from this idea. Minimizing such losses is equivalent to maximizing likelihood.</p> <p>Expectations play a central role because learning is not based on a single data point, but on averages over data drawn from an underlying distribution. Training objectives are typically expectations of a loss over the data distribution, which in practice are approximated using finite datasets and minibatches.</p> <p>deep learning does not require advanced probability theory, but it does require a clear understanding of what probabilistic models represent, how likelihood and expectation relate to loss functions, and why uncertainty is an essential part of learning from real data.</p>"},{"location":"mathematics/#information-theory","title":"Information Theory","text":"<p>Information theory enters deep learning when we want to measure how different two probability distributions are. Many deep learning models define a distribution over possible outputs rather than producing a single fixed prediction. Information-theoretic quantities provide a principled way to compare these predicted distributions to the true data distribution.</p> <p>Concepts such as entropy and cross-entropy arise naturally in this setting. Entropy measures uncertainty, while cross-entropy measures how well one distribution represents another. Minimizing cross-entropy encourages the model to assign high probability to the observed data.</p> <p>A closely related quantity is the Kullback\u2013Leibler (KL) divergence, which measures how much information is lost when one distribution is used to approximate another. Many common training objectives can be interpreted as minimizing a KL divergence, even when this connection is not stated explicitly.</p>"},{"location":"mathematics/#additional-mathematics","title":"Additional Mathematics","text":"<p>In addition to the core areas discussed above, several mathematical perspectives play an important role in DL. While they may not always appear as standalone topics or require extensive formal development, they shape how models are designed, trained, and evaluated. These ideas recur across many deep learning settings and deserve explicit attention, even when they are introduced briefly.</p> <p>Statistics enters deep learning through the fact that models are trained on finite samples rather than full data-generating processes. Concepts such as generalization, overfitting, and the bias\u2013variance tradeoff describe the structural limits of what can be learned from data and how model complexity interacts with sample size and noise. These considerations shape how results should be interpreted, how sensitive conclusions are to data variation, and how confidently performance can be expected to transfer beyond the observed sample.</p> <p>Optimization theory addresses a small set of practical questions that arise once a loss function is defined. Given a highly non-convex objective with millions of parameters, how can it be minimized efficiently, and why do simple gradient-based methods work at all? How do learning rates, momentum, adaptive updates, and noise from minibatching affect training behavior? Rather than providing exact convergence proofs, optimization theory offers guidance on training stability, speed, and failure modes.</p> <p>Geometry treats representations, parameters, and activations as points in high-dimensional spaces. Distances and angles define similarity, with measures such as cosine similarity capturing directional alignment between representations. Optimization itself is a geometric process, moving parameters across a loss surface whose local curvature influences learning speed and stability. Geometric intuition about distances, neighborhoods, and curvature helps explain why certain architectures, losses, and similarity measures are effective in practice.</p> <p>Graph theory becomes relevant whenever data is structured by relations rather than simple vectors. In graph neural networks (GNNs), data is represented as nodes and edges, and learning depends directly on graph connectivity. Related ideas also appear more broadly in message passing, relational reasoning, and attention mechanisms applied to structured inputs.</p> <p>Numerical computation and stability constrain how deep learning models are implemented. Because training relies on finite-precision arithmetic, issues such as overflow, underflow, and loss of precision directly influence model behavior. Many standard techniques in DL\u2014such as normalization layers, carefully designed loss functions, and specific activation choices\u2014exist primarily to ensure stable and reliable computation.</p> <p>Together, these perspectives complement the core mathematical foundations and connect them to practical modeling, training, and evaluation. They do not replace the core framework, but they shape how it is applied and understood in real-world deep learning systems.</p>"},{"location":"mathematics/01_calculus/","title":"Calculus","text":"18 Jan 2026 <p>Calculus studies two closely related ideas: accumulation (integration) and change (differentiation). Indeep learning, learning is defined by accumulating error across data, usually as an average loss. Training then proceeds by making small changes to model parameters in order to reduce this accumulated error. Calculus provides the language and structure for both. This section builds calculus concepts from fundamentals, with the goal of understanding how they support learning, optimization, and model behavior in deep learning.</p>"},{"location":"mathematics/01_calculus/#functions","title":"Functions","text":"<p>A function maps inputs to outputs. We write this as \\(y = f(x)\\). If the input \\(x\\) changes, the output \\(y\\) usually changes as well. Some functions change slowly, some change quickly, and some change differently depending on at which point of the function you are. Calculus begins by asking how these changes are related. </p> <p>Note</p> <p>For example, changing the brightness of an image slightly may barely affect a model's output in one case, but cause a large change in another.</p>"},{"location":"mathematics/01_calculus/#integration","title":"Integration","text":"<p>An integral such as \\(\\int_a^b f(x)\\,dx\\) represents the total accumulation of the values of \\(f(x)\\) as \\(x\\) moves from \\(a\\) to \\(b\\). It is simply the continuous analogue of a summation. You can think of an integral as summing many small contributions of \\(f(x)\\) over an interval. The exact techniques for computing integrals are less important in deep learning than the idea they represent.</p> <p>Conceptually, integration means breaking an interval into many small pieces. For each piece, we take the value of \\(f(x)\\) and multiply it by the width of the piece. Adding all these pieces together gives an approximation of the total accumulation. As the pieces become smaller and more numerous, this approximation approaches the integral. Mathematically, we represent this very small width as \\(dx\\).</p>      Riemann Integration and Darboux Lower Sums. By IkamusumeFan - Own work\u00a0This plot was created with Matplotlib., CC BY-SA 3.0, Link <p>Note</p> <p>In deep learning, training is never based on a single example. A model is evaluated by how it performs across many examples, so errors must be combined into one overall value. In practice, we only have access to a limited number of training examples. A common case in machine and deep learning is mean squared error (MSE), where for each training sample we compute a prediction error, square it (so negative and positive errors do not cancel, and larger mistakes are penalized), and then average these squared errors over the dataset. Conceptually, however, this dataset-level average is not the final goal. </p> <p>The dataset is usually treated as a small collection of examples drawn from a much larger source of data. Ideally, we would like to measure the model\u2019s average error over all possible data points it might encounter, not just the ones we happened to collect. The finite average used in training should therefore be understood as a practical approximation of a more general, ideal accumulated continuous quantity. For the values \\(g(x_1), g(x_2), \\dots, g(x_N)\\), we can write</p> \\[ \\mathbb{E}_{x \\sim p}[g(x)] \\approx \\frac{1}{N}\\sum_{i=1}^N g(x_i). \\] <p>Here, the right-hand side is what we compute from data, and the left-hand side represents the ideal quantity we are trying to approximate. This ideal accumulated quantity is written precisely using the concept of an expectation. When data is described by a probability distribution \\(p(x)\\), the average value of a quantity \\(g(x)\\) is written as</p> \\[ \\mathbb{E}_{x \\sim p}[g(x)] = \\int g(x)\\,p(x)\\,dx. \\] <p>You do not need a deep understanding of probability to read this expression. Conceptually, it means: consider all possible values of \\(x\\), weight each value by how common it is, and add everything up. Many deep learning loss functions can be understood this way, as average losses over all possible data. For discrete datasets, this expectation reduces to a finite sum, while for continuous variables it is written as an integral. The integral itself is not special\u2014it is simply the mathematical way to express an average over all possible inputs when the space of inputs is continuous.</p> <p>Tip</p> <p>If the idea of expectations or probability distributions feels unfamiliar, you may want to read the page dedicated to the Probability Theory alongside this section.</p>"},{"location":"mathematics/01_calculus/#differentiation","title":"Differentiation","text":"<p>Differentiation answers the question: if we change the input slightly, how much does the output change?  Suppose we start at \\(x\\) and then move a small step \\(h\\) to \\(x+h\\). The corresponding change in the output is \\(f(x+h) - f(x)\\). By itself, this number depends on how large \\(h\\) is. To describe change in a way that does not depend on the step size, we compare the output change to the input change by forming the ratio</p> \\[ \\frac{f(x+h) - f(x)}{h}. \\] <p>This ratio is called a difference quotient. It describes the average rate of change of the function over the small interval from \\(x\\) to \\(x+h\\). The derivative is defined as the limit of this ratio as the step size approaches zero:</p> \\[ f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}. \\] <p>This definition captures the idea of an \"instantaneous\" rate of change. Intuitively, the derivative tells us the slope of the function at the point \\(x\\): if \\(f'(x)\\) is large, a tiny change in \\(x\\) causes a large change in \\(f(x)\\), if \\(f'(x)\\) is close to zero, the function is locally flat. </p> <p>Note</p> <p>Indeep learning, if increasing a model weight slightly increases the loss, then the derivative of the loss with respect to that weight is positive. That means decreasing the weight slightly should reduce the loss (at least locally).</p> <p>The derivative is not only a number; it also provides a practical approximation of how a function behaves near a given point. The key idea is that, over very small distances, a smooth function behaves almost like a straight line. If we start at a point \\(x\\) and move a small step \\(h\\), the derivative \\(f'(x)\\) tells us how steep the function is at \\(x\\). Using this slope, we can estimate how much the output will change. This leads to the approximation</p> \\[ f(x+h) \\approx f(x) + f'(x)\\,h. \\] <p>This formula should be read as a prediction: \"start from the current value \\(f(x)\\), then add the change suggested by the slope times the step size.\" The approximation becomes more accurate as the step \\(h\\) becomes smaller. Geometrically, this means that near the point \\(x\\), the function can be replaced by its tangent line. The tangent line touches the function at \\(x\\) and has the same slope there. Over a very small region, the curve and the tangent line are almost indistinguishable, which is why the linear approximation works.</p>      By Chorch - Own Work, Public Domain, Link <p>Note</p> <p>Indeep learning, training works because, at each step, we treat the loss as locally almost linear in the parameters. The gradient (see below) gives the slope of this local linear approximation. By making small parameter updates in the direction opposite to the gradient, we can reliably reduce the loss step by step, even when the overall loss function is highly complex.</p>"},{"location":"mathematics/01_calculus/#partial-derivatives","title":"Partial derivatives","text":"<p>Deep learning models depend on many parameters at once. If the loss is written as</p> \\[ L = f(\\theta_1, \\theta_2, \\dots, \\theta_n), \\] <p>then each parameter has its own partial derivative \\(\\frac{\\partial L}{\\partial \\theta_i}.\\) A partial derivative measures how the loss changes when one parameter is varied while all others are held fixed.</p> <p>Note</p> <p>For example, changing a single weight in a neural network affects the loss while all other weights remain unchanged.</p>"},{"location":"mathematics/01_calculus/#gradients","title":"Gradients","text":"<p>The gradient collects all partial derivatives into a single vector:</p> \\[ \\nabla_{\\theta} L = \\left[ \\frac{\\partial L}{\\partial \\theta_1}, \\frac{\\partial L}{\\partial \\theta_2}, \\dots, \\frac{\\partial L}{\\partial \\theta_n} \\right]. \\] <p>The gradient points in the direction where the loss increases most rapidly. Moving in the opposite direction locally reduces the loss. Each component of the gradient corresponds to one parameter and tells us how that parameter influences the loss. Training typically consists of repeated updates of the form</p> \\[ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L, \\] <p>where \\(\\eta\\) is the learning rate. Each update makes a small change. Over many updates, these small changes accumulate and reduce the overall loss.</p> <p>Tip</p> <p>The learning rate update through backward pass is discussed in the notebook dedicated to backpropagation.</p>"},{"location":"mathematics/01_calculus/#jacobian","title":"Jacobian","text":"<p>The Jacobian is the general first-order derivative for functions with vector inputs and vector outputs. If a function maps an \\(n\\)-dimensional input vector to an \\(m\\)-dimensional output vector, \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m,\\) its Jacobian is an \\(m \\times n\\) matrix defined as</p> \\[ J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\dots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\dots &amp; \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} &amp; \\frac{\\partial f_m}{\\partial x_2} &amp; \\dots &amp; \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}. \\] <p>Each entry measures how one component of the output changes when one component of the input is varied. The Jacobian therefore captures all first-order sensitivities between inputs and outputs.</p> <p>Note</p> <p>Indeep learning, layers often map vectors to vectors. Although Jacobians are rarely written explicitly, they are the objects through which changes propagate from one layer to the next. When the output is a scalar loss, the Jacobian reduces to a row vector. Conceptually, the gradient introduced earlier is simply the Jacobian of a scalar-valued function. Backpropagation avoids forming full Jacobian matrices explicitly. Instead, it efficiently computes vector\u2013Jacobian products, which is why gradients can be computed for models with millions of parameters at reasonable cost.</p> <p>Tip</p> <p>Jacobians are best understood through linear algebra. If matrices and vector transformations feel unfamiliar, you may want to read the Linear Algebra page alongside this section.</p>"},{"location":"mathematics/01_calculus/#chain-rule","title":"Chain Rule","text":"<p>deep learning models are built by composing functions. Instead of a single operation, a model applies many transformations one after another. Each transformation takes the output of the previous one as its input. To understand how changes propagate through such a model, consider a simple composition:</p> \\[ y = g(x), \\qquad L = f(y). \\] <p>Here, \\(x\\) influences \\(L\\) indirectly, through the intermediate variable \\(y\\). If we change \\(x\\) slightly, \\(y\\) will change, and that change in \\(y\\) will in turn affect \\(L\\). The chain rule formalizes this dependency.</p> <p>The chain rule states that the sensitivity of \\(L\\) with respect to \\(x\\) is the product of two sensitivities:</p> \\[ \\frac{dL}{dx} = \\frac{dL}{dy} \\cdot \\frac{dy}{dx}. \\] <p>This equation should be read step by step. First, \\(\\frac{dy}{dx}\\) tells us how a small change in \\(x\\) affects \\(y\\). Second, \\(\\frac{dL}{dy}\\) tells us how a small change in \\(y\\) affects the loss. Multiplying them gives the total effect of changing \\(x\\) on \\(L\\).</p> <p>This idea extends naturally to longer chains. If a model applies many functions in sequence, the chain rule is applied repeatedly, multiplying together the local sensitivities at each step. Each operation contributes a small piece to the overall gradient.</p>"},{"location":"mathematics/01_calculus/#taylor-expansion","title":"Taylor Expansion","text":"<p>Taylor series provides a systematic way to describe how a function behaves near a given point. It expresses a function as a sum of terms built from its derivatives at that point. Each term captures progressively finer details of how the function changes.</p> <p>For a function \\(f(x)\\) expanded around a point \\(x\\), the Taylor series in one dimension is</p> \\[ f(x+h) = f(x) + f'(x)h + \\tfrac{1}{2}f''(x)h^2 + \\tfrac{1}{6}f'''(x)h^3 + \\dots \\] <p>This expression says that the value of the function at \\(x+h\\) can be predicted by starting from the value at \\(x\\) and then adding corrections based on information about how the function changes at \\(x\\).</p> <p>In practice, we rarely use the full infinite series. Instead, we keep only the first few terms. This truncated version is called a Taylor expansion and is used as a local approximation.</p> <p>Keeping only the first-order term gives the linear approximation already used in gradient-based learning:</p> \\[ f(x+h) \\approx f(x) + f'(x)h. \\] <p>This approximation assumes that, for small updates, the function behaves almost like a straight line near the current point. It explains why gradients provide useful guidance for optimization.</p> <p>This local linear approximation relies on an important assumption: the function must be smooth enough near the point of expansion. Smoothness means that small changes in the input lead to small, predictable changes in the output, and that derivatives do not change abruptly.</p> <p>Note</p> <p>Indeep learning, loss functions are often not perfectly smooth everywhere, but they are typically piecewise smooth. This is sufficient. Taylor expansions and gradient-based updates only rely on local behavior along the training trajectory, not on global smoothness of the loss surface. A common example is the ReLU activation, which is not differentiable at zero but is differentiable almost everywhere else. Gradient-based methods rely on this local behavior and use subgradients at nondifferentiable points.</p> <p>Keeping second-order terms reveals that this linear behavior is only approximate. These higher-order terms explain why the slope itself can change as we move, motivating the need to understand second-order structure.</p> <p>Note</p> <p>Indeep learning, gradient-based learning relies on first-order Taylor approximations. Understanding why and when this approximation breaks down requires looking at second-order effects, which are captured by the Hessian.</p>"},{"location":"mathematics/01_calculus/#hessian","title":"Hessian","text":"<p>While the Jacobian describes first-order behavior\u2014how the loss changes under small parameter changes\u2014the Hessian describes second-order behavior.<sup>1</sup> It captures how these first-order sensitivities themselves change as we move in parameter space. The Hessian of \\(L\\) with respect to the parameter vector \\(\\theta\\) is a matrix of second-order partial derivatives:</p> \\[ H = \\begin{bmatrix} \\frac{\\partial^2 L}{\\partial \\theta_1^2} &amp; \\frac{\\partial^2 L}{\\partial \\theta_1 \\partial \\theta_2} &amp; \\dots &amp; \\frac{\\partial^2 L}{\\partial \\theta_1 \\partial \\theta_n} \\\\[0.5em] \\frac{\\partial^2 L}{\\partial \\theta_2 \\partial \\theta_1} &amp; \\frac{\\partial^2 L}{\\partial \\theta_2^2} &amp; \\dots &amp; \\frac{\\partial^2 L}{\\partial \\theta_2 \\partial \\theta_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 L}{\\partial \\theta_n \\partial \\theta_1} &amp; \\frac{\\partial^2 L}{\\partial \\theta_n \\partial \\theta_2} &amp; \\dots &amp; \\frac{\\partial^2 L}{\\partial \\theta_n^2} \\end{bmatrix}. \\] <p>Each entry tells us how the sensitivity with respect to one parameter changes when another parameter is varied. In this sense, the Hessian measures curvature: how the loss surface bends in different directions. Consider a simple two-parameter loss \\(L(\\theta_1, \\theta_2)\\). The diagonal entries of the Hessian describe how sharply the loss curves when we move along each parameter direction individually. The off-diagonal entries describe how changes in one parameter affect the sensitivity with respect to another parameter.</p> <p>Note</p> <p>Indeep learning, this information explains important optimization behavior. Directions with strong positive curvature correspond to narrow valleys, where large updates can easily overshoot. Directions with weak curvature correspond to flat regions, where progress can be slow. Negative curvature indicates directions where the loss bends downward, which is typical near saddle points. Although full Hessians are rarely computed explicitly in deep learning due to their size and cost, their effects are always present. Learning rate selection, optimization stability, and the behavior of training near minima and saddle points are all influenced by second-order structure.</p> <p>Tip</p> <p>Like the Jacobian, the Hessian is a linear algebra object\u2014a matrix encoding directional behavior. If matrices, eigenvalues, or curvature interpretations feel unfamiliar, you may want to read the Linear Algebra page alongside this section.</p>"},{"location":"mathematics/01_calculus/#minima-saddle-points-and-convexity","title":"Minima, saddle points, and convexity","text":"<p>A minimum is a point where small changes in any direction increase the loss. At such a point, the gradient is zero and the surrounding curvature points upward.</p> <p>A saddle point is also a point where the gradient is zero, but the behavior is mixed: the loss increases in some directions and decreases in others. This means the point is neither a true minimum nor a maximum. The distinction between minima and saddle points is determined by the local curvature described by the Hessian.</p> <p>Note</p> <p>In high-dimensional deep learning models, saddle points are far more common than poor local minima. Gradient-based methods can often escape saddle points because curvature creates unstable directions, and stochastic noise from minibatches helps push parameters away from them.</p> <p>In classical optimization, convex loss functions play a special role. For a convex function, any point where the gradient is zero is guaranteed to be a global minimum. There are no saddle points and no spurious local minima.</p> <p>Note</p> <p>Most deep learning loss functions are not convex. As a result, global guarantees do not apply. Instead, training relies on local information provided by gradients and curvature. Despite the lack of convexity, gradient-based methods work well in practice due to overparameterization, stochastic gradients, and the structure induced by modern architectures, even though no global guarantees apply.</p> <p>Gradient-based learning does not require global convexity. What matters is that, locally, the loss behaves smoothly enough for gradients and Taylor approximations to provide reliable guidance along the training trajectory.</p>"},{"location":"mathematics/01_calculus/#fundamental-theorem-of-calculus","title":"Fundamental Theorem of Calculus","text":"<p>The Fundamental Theorem of Calculus explains the precise relationship between accumulation and change. If we define an accumulated quantity</p> \\[ F(x) = \\int_a^x f(t)\\,dt, \\] <p>then \\(F(x)\\) is differentiable and</p> \\[ \\frac{d}{dx} F(x) = f(x). \\] <p>This means that differentiation recovers the rate at which accumulation occurs. Conversely, if \\(F(x)\\) is any antiderivative of \\(f(x)\\), then total accumulation over an interval can be computed as</p> \\[ \\int_a^b f(x)\\,dx = F(b) - F(a). \\] <p>Together, these statements show that local change and total accumulation are two sides of the same idea.</p> <p>Note</p> <p>Indeep learning, losses are defined as accumulated quantities, while gradients describe local change. Training works because following local gradients causes a consistent reduction in the accumulated loss over time.</p> <ol> <li> <p>For a further DL\u2013oriented treatment of gradients, Jacobians, Hessians, and numerical aspects of optimization, see Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press. Chapter 4: Numerical Computation.\u00a0\u21a9</p> </li> </ol>"},{"location":"mathematics/02_linear_algebra/","title":"Linear Algebra","text":"26 Jan 2026 <p>Linear algebra is the branch of mathematics that studies vector spaces and the linear mappings between them. In deep learning, almost all computation is formulated in the language of linear algebra: data, model parameters, activations, gradients are represented as vectors or matrices. A clear understanding of what these objects represent \u2014 and how they behave under linear operations \u2014 is necessary not only for correct implementation, but for reasoning about model structure, learning dynamics, and numerical behavior.</p> <p>Info</p> <p>The following sources were consulted in preparing this material:</p> <ul> <li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press. Chapter 2: Linear Algebra.</li> <li>Sanderson, G. Essence of Linear Algebra. 3Blue1Brown. https://www.3blue1brown.com/topics/linear-algebra</li> </ul> <p>Important</p> <p>Please note that some concepts in this material are simplified for pedagogical purposes. These simplifications slightly reduce precision but preserve the core ideas relevant to deep learning.</p>"},{"location":"mathematics/02_linear_algebra/#scalars-and-vectors","title":"Scalars and Vectors","text":"<p>A scalar is a single number (often real-valued). It is more challenging to define a vector. From a mathematician's point of view, a vector is an element of a vector space: something you can add and scale while satisfying certain axioms (closure, associativity, distributivity, etc.). The axioms exist to guarantee that linear combinations behave predictably. From these requirements, any linear map \\(f\\) between vector spaces satisfies the following combined property of additivity and homogeneity (scaling): $$ f(\\alpha x + \\beta y) = \\alpha f(x) + \\beta f(y). $$ This equation does not define vectors themselves, but rather characterizes linear transformations acting on vectors. Vectors are defined by the operations of addition and scalar multiplication and linear maps are functions that preserve this structure.</p> <p>Note</p> <p>Raw audio signals satisfy the linearity properties to a good approximation. If two sounds are played at the same time, the resulting waveform is (approximately) the sum of the individual waveforms. If the volume of a sound is increased or decreased, its waveform is scaled by a constant factor. Because audio combines by superposition and scales linearly with amplitude, it can be naturally represented as a vector and manipulated using linear algebra.</p> <p>From a physicist's point of view, a vector represents a quantity with direction and magnitude (e.g. velocity, force). You add forces, scale forces, decompose into components. The vector predicts physical behavior. Lastly, from a computer scientist's point of view, a vector is simply an array of numbers. It can represent pixel values of an image, coordinates of a point, words in a document, etc.</p> <p>Important</p> <p>In many ways, machine/deep learning borrows terminology from mathematics and uses it rather freely. Terms like vector, dimension, space, metric, manifold, and even linear are frequently misused. For example, by \"dimension\" one could assume \"vector size\". This is convenient shorthand, but it can break intuition if you don't keep in mind the underlying differences between deep learning and mathematics which can use the same tools for different purposes.</p> <p>A vector is often written explicitly as a column of numbers. For example, a vector with \\(n\\) real-valued components can be written as $$ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} \\in \\mathbb{R}^n $$</p> <p>In deep learning, such a vector is typically understood operationally: it is stored as a contiguous array of \\(n\\) real numbers and is mathematically an element of \\(\\mathbb{R}^n\\), the Cartesian product of \\(\\mathbb{R}\\) with itself \\(n\\) times. In this context, its \"dimension\" refers simply to its length \\(n\\). When \\(n = 2\\) or \\(n = 3\\), the vector can be visualized geometrically as a point or an arrow. When \\(n\\) is large, direct visualization is no longer possible, but the same algebraic operations \u2014 addition, scalar multiplication, dot products, and linear transformations \u2014 still apply. </p> <p>Note</p> <p>Depending on context, vectors can be visualized in different ways. In geometry and physics, they are often drawn as arrows representing magnitude and direction. In other settings, a vector can be viewed as a function that assigns a value to each index or coordinate. These visualizations are useful for building intuition, especially in low dimensions, but they do not alter the underlying algebraic definition of a vector. Linear algebra itself does not rely on geometric interpretation. It is fundamentally an algebraic theory of vector spaces and linear maps, and all definitions and results are independent of visualization. Geometry serves only as an intuitive aid not as a prerequisite. Beyond three dimensions, geometry in the visual sense becomes unusable. Since most representations in deep learning live in very high-dimensional spaces, geometric visualization is generally not available and plays no direct role in practice. What remains meaningful are algebraic and analytical notions \u2014 such as inner products, norms, projections, and linear maps \u2014 rather than pictures or spatial intuition.</p>"},{"location":"mathematics/02_linear_algebra/#matrices-and-tensors","title":"Matrices and Tensors","text":"<p>A matrix is a rectangular array of numbers arranged in rows and columns. Formally, a real-valued matrix with \\(m\\) rows and \\(n\\) columns is an element of \\(\\mathbb{R}^{m \\times n}\\): $$ \\mathbf{A} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{bmatrix} $$</p> <p>From a mathematical point of view, a matrix represents a linear map between vector spaces. Given a vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\), multiplication by a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) produces a new vector \\(\\mathbf{y} \\in \\mathbb{R}^m\\): $$ \\mathbf{A}\\mathbf{x} = \\mathbf{y}.$$ </p> <p>This operation encodes all linear transformations: rotations, scalings, projections, and combinations of these. The key idea is that matrices do not just store numbers; they describe how vectors are transformed. In deep learning, matrices appear everywhere. For example, in</p> <ul> <li>Model parameters (weights of fully connected layers)</li> <li>Batches of input data</li> <li>Linear layers of the form \\( \\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} \\)</li> <li>Jacobians and Hessians (implicitly, through automatic differentiation)</li> </ul> <p>Important</p> <p>In deep learning, layers of the form \\(\\mathbf{W}\\mathbf{x}+\\mathbf{b}\\) are often informally called \"linear\". This shorthand is convenient in practice, but it is useful to remember that the bias term shifts the output and allows models to represent functions that a purely linear map could not. Strictly speaking, the map \\(\\mathbf{x}\\mapsto \\mathbf{W}\\mathbf{x}\\) is linear: it preserves addition, scaling, and maps the zero vector to the zero vector. Adding a bias term \\(\\mathbf{b}\\) produces an affine map, which is a linear transformation followed by a translation. Because of this translation, affine maps do not preserve the origin.</p> <p>A matrix is stored as a 2D array in memory, but it should be understood as a single object representing a linear operation. Confusing these two viewpoints \u2014 matrix as data vs. matrix as transformation \u2014 is a common source of misunderstanding.</p> <p>Note</p> <p>When training neural networks, we rarely reason about individual entries of a matrix. Instead, we reason about the effect of the matrix as a whole: how it mixes input features, how it changes dimensionality, and how it interacts with nonlinearities. Frameworks exploit this by implementing matrix multiplication using highly optimized numerical kernels.</p> <p>A tensor is a generalization of scalars, vectors, and matrices to higher dimensions. Informally: a scalar is a 0-order tensor, a vector is a 1-order tensor, a matrix is a 2-order tensor, etc. In deep learning practice, a tensor is best understood as a multidimensional array of numbers with a fixed shape.</p> <p>Important</p> <p>In pure mathematics and physics, tensors have a precise coordinate-independent definition involving multilinear maps. In deep learning, the word tensor is used more loosely to mean n-dimensional array. This is a practical simplification, but it is important not to confuse it with the full mathematical theory of tensors.</p> <p>Tensor operations in deep learning are designed to preserve linear structure wherever possible. Linear operations (e.g. matrix multiplication, convolution) remain linear even when expressed in tensor form. Most neural network layers can be viewed as linear maps acting on tensors, followed by nonlinear functions applied elementwise. Understanding which parts of a computation are linear and which are not is essential for reasoning about optimization and numerical stability.</p> <p>Note</p> <p>High-dimensional tensors cannot be visualized geometrically. Their meaning comes from structure and indexing, not from geometrical intuition. What matters is how dimensions correspond to data and how linear operations act along specific axes (e.g. columns, rows).</p>"},{"location":"mathematics/02_linear_algebra/#transpose-identity-inversion","title":"Transpose, Identity, Inversion","text":"<p>The transpose of a matrix swaps rows and columns. For a matrix \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\), its transpose \\(\\mathbf{A}^\\top\\in\\mathbb{R}^{n\\times m}\\) is defined by \\(\\mathbf{A}^\\top_{ij} = a_{ji}.\\) A column vector becomes a row vector, and vice versa. Basically, transpose reflects (like a mirror) a matrix across its main diagonal. Elements on the diagonal remain fixed, off-diagonal elements are mirrored: $$ \\mathbf{A} = \\left[ \\begin{array}{ccc} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\end{array} \\right] \\quad\\Rightarrow\\quad \\mathbf{A}^\\top = \\left[ \\begin{array}{cc} a_{11} &amp; a_{21} \\\\ a_{12} &amp; a_{22} \\\\ a_{13} &amp; a_{23} \\end{array} \\right] $$</p> <p>The identity  matrix \\(\\mathbf{I}\\in\\mathbb{R}^{n\\times n}\\) is defined by a matrix whose diagonal are ones with all other elements being zeros: $$ \\mathbf{I} = \\left[ \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right]. $$</p> <p>The identity matrix represents the linear map that leaves every vector unchanged and acts as the same way that \\(1\\) does in the rational numbers: $$ \\mathbf{I}\\mathbf{x} = \\mathbf{x}, \\qquad \\mathbf{A}\\mathbf{I} = \\mathbf{I}\\mathbf{A} = \\mathbf{A}. $$</p> <p>Note</p> <p>Identity matrices appear implicitly in residual connections and linear solvers. Adding \\(\\mathbf{I}\\) to a matrix corresponds to biasing a transformation toward preserving information.</p> <p>The matrix inversion provides a formal way to solve linear systems of the form \\(\\mathbf{y} = \\mathbf{A}\\mathbf{x}\\). If the matrix \\(\\mathbf{A}\\) is square (\\(n \\times n\\)) and invertible, there exists a matrix \\(\\mathbf{A}^{-1}\\) such that \\(\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}.\\) Multiplying both sides of the equation by \\(\\mathbf{A}^{-1}\\) yields \\(\\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{y}.\\)</p> <p>Note</p> <p>Matrix inversion corresponds to undoing a linear transformation: applying \\(\\mathbf{A}^{-1}\\) reverses the effect of \\(\\mathbf{A}\\). In practice, however, explicit matrix inversion is rarely used in numerical computation or deep learning. It is primarily a theoretical tool. Solving linear systems is typically done using more stable and efficient methods that avoid forming \\(\\mathbf{A}^{-1}\\) directly, especially when matrices are large or ill-conditioned.</p>"},{"location":"mathematics/02_linear_algebra/#vector-and-matrix-multiplications","title":"Vector and Matrix Multiplications","text":"<p>Linear algebra uses small set of multiplication rules which make sure that the initial axioms are followed. In deep learning, nearly every forward and backward computation reduces to combinations of the operations described here.</p> <p>Dot (inner) product. For \\(\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^n\\), $ \\mathbf{x}\\cdot\\mathbf{y}=\\sum_{i=1}^n x_i y_i . $ The result is a scalar. Algebraically, the dot product is bilinear: linear in each argument when the other is held fixed<sup>1</sup>. This property is essential for gradient-based optimization. </p> <p>Note</p> <p>The dot product has several complementary interpretations. It measures how strongly \\(\\mathbf{x}\\) aligns with weights \\(\\mathbf{y}\\) by summing componentwise contributions. Geometrically (when visualization is possible), it measures alignment between vectors: large positive values indicate similar directions, values near zero indicate near-orthogonality, and negative values indicate opposing directions. In practice, it appears as neuron pre-activations, similarity scores, attention mechanisms (queries-keys), and projections.</p> <p>The dot product between vectors can also be written in matrix form as \\(\\mathbf{x}\\cdot\\mathbf{y} = \\mathbf{x}^\\top \\mathbf{y}\\), and is commutative: \\(\\mathbf{x}^\\top \\mathbf{y} = \\mathbf{y}^\\top \\mathbf{x}.\\) </p> <p>Note</p> <p>Orientation matters. \\(\\mathbf{x}\\mathbf{y}^\\top\\) and \\(\\mathbf{x}^\\top\\mathbf{y}\\) are different objects with different meanings. Many shape errors in neural network implementations come from ignoring this distinction.</p> <p>Hadamard (elementwise) product. The Hadamard product multiplies vectors componentwise: $ (\\mathbf{x}\\odot\\mathbf{y})_i = x_i y_i . $ The result is a vector in \\(\\mathbb{R}^n\\). This operation does not mix coordinates: each output component depends only on the corresponding input components. For a fixed vector, it acts as a simple coordinate-wise scaling. In deep learning, the Hadamard product is used when features are masked, gated, or rescaled individually, such as in attention masks.</p> <p>Note</p> <p>The cross product known from physics curriculum is defined only in \\(\\mathbb{R}^3\\) (and, with special constructions, \\(\\mathbb{R}^7\\)). It produces a vector orthogonal to its inputs and relies on three-dimensional geometry. Since deep learning representations typically live in high-dimensional spaces with no notion of \"orthogonal direction in space\", the cross product has no general role in deep learning and is not used in standard models.</p> <p>Matrix-vector multiplication. Let \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\) and \\(\\mathbf{x}\\in\\mathbb{R}^n\\). Then $$ \\mathbf{y}=\\mathbf{A}\\mathbf{x}\\in\\mathbb{R}^m,\\qquad y_i=\\sum_{j=1}^n a_{ij}x_j . $$ Each output component is a dot product between one row of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\). This is the fundamental linear operation in deep learning: rows act as learned feature detectors and dimensionality may change (\\(n\\to m\\)). </p> <p>Note</p> <p>A fully connected  layer has the form \\(\\mathbf{y}=\\mathbf{W}\\mathbf{x}+\\mathbf{b}\\). The nonlinearity that follows does not alter the linearity of this step.</p> <p>Matrix-matrix multiplication. For \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\) and \\(\\mathbf{B}\\in\\mathbb{R}^{n\\times p}\\), $$ \\mathbf{C}=\\mathbf{A}\\mathbf{B}\\in\\mathbb{R}^{m\\times p},\\qquad c_{ij}=\\sum_{k=1}^n a_{ik}b_{kj}. $$ This represents composition of linear maps: applying \\(\\mathbf{B}\\) then \\(\\mathbf{A}\\) equals applying \\(\\mathbf{A}\\mathbf{B}\\). Stacked linear layers, gradient propagation via transposes, and backpropagation all rely on this structure.</p> <p>Important</p> <p>Matrix multiplication satisfies distributivity and associativity, but it is not commutative \\(\\mathbf{A}\\mathbf{B}\\neq\\mathbf{B}\\mathbf{A}\\). This reflects the fact that matrix multiplication represents the composition of linear transformations. Changing the order changes which transformation is applied first, and therefore changes the result. Only in special cases\u2014when two transformations are compatible in a specific way\u2014does commutativity hold.</p> <p>Note that, for any compatible matrices: \\((\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top \\mathbf{A}^\\top .\\) The order reverses because transposition swaps rows and columns, effectively reversing the sequence of linear transformations. This property is used constantly in backpropagation, where gradients are propagated through layers via transposed weight matrices.</p> <p>Note</p> <p>Since a scalar is equal to its own transpose, this identity also explains why the dot product is commutative. Written in matrix form: \\(\\mathbf{x}^\\top \\mathbf{y} = (\\mathbf{x}^\\top \\mathbf{y})^\\top = \\mathbf{y}^\\top \\mathbf{x}.\\) What appears as a symmetry of vectors is therefore a direct consequence of more general properties of matrix transpose.</p>"},{"location":"mathematics/02_linear_algebra/#linear-dependence-and-span","title":"Linear Dependence and Span","text":"<p>A collection of vectors is linearly dependent if at least one vector in the set can be written as a linear combination of the others. Formally, vectors \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\) are linearly dependent if there exist scalars \\(\\alpha_1,\\dots,\\alpha_k\\), not all zero, such that $$ \\alpha_1 \\mathbf{v}_1 + \\alpha_2 \\mathbf{v}_2 + \\cdots + \\alpha_k \\mathbf{v}_k = \\mathbf{0}. $$ Linear dependence means redundancy: some vectors do not add new directions or information. If no such non-trivial combination exists, the vectors are linearly independent.</p> <p>Note</p> <p>In deep learning and applied linear algebra, linear dependence indicates unnecessary or duplicated features. Independent vectors represent genuinely distinct directions in a space.</p> <p>The span of a set of vectors is the collection of all vectors that can be formed by taking linear combinations of them. Given vectors \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\), their span consists of all vectors that can be written as $$ \\mathbf{s} = [\\,\\mathbf{v}_1\\ \\mathbf{v}_2\\ \\cdots\\ \\mathbf{v}_k\\,]\\boldsymbol{\\alpha}, \\qquad \\boldsymbol{\\alpha}\\in\\mathbb{R}^k. $$</p> <p>The span describes all vectors that are reachable using those directions. If the vectors are linearly dependent, their span does not grow when all vectors are included. Dependent vectors do not expand the space.</p> <p>Note</p> <p>In practice, the span corresponds to the set of outputs a linear layer can produce. Linear dependence among columns of a weight matrix limits expressiveness, while linear independence maximizes the range of representable transformations.</p> <p>Consider the linear system \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) with \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\) and \\(\\mathbf{b}\\in\\mathbb{R}^m\\). The system has a solution iff \\(\\mathbf{b}\\) lies in the span of the columns of \\(\\mathbf{A}\\). This is called the column space (or range) of \\(\\mathbf{A}\\). For the system to have a solution for all \\(\\mathbf{b}\\in\\mathbb{R}^m\\), the column space of \\(\\mathbf{A}\\) must be all of \\(\\mathbb{R}^m\\). This immediately requires \\(n\\ge m\\). Otherwise, the column space has dimension at most \\(n&lt;m\\) and cannot fill \\(\\mathbb{R}^m\\). </p> <p>Note</p> <p>For example, a \\(3\\times2\\) matrix can only produce a 2-dimensional plane inside \\(\\mathbb{R}^3\\). The equation has a solution only when \\(\\mathbf{b}\\) lies on that plane.</p> <p>The condition \\(n\\ge m\\) is necessary but not sufficient. Columns may be redundant. A matrix whose columns are linearly dependent does not expand its column space. Therefore, for the column space to equal \\(\\mathbb{R}^m\\), the matrix must have rank \\(m\\), meaning that it contains \\(m\\) linearly independent columns. This condition is necessary and sufficient for \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) to have a solution for every \\(\\mathbf{b}\\in\\mathbb{R}^m\\).</p> <p>Exercise</p> <p>Which properties must a matrix have in order to ensure uniqueness of the solution?</p> <p>A square matrix with linearly dependent columns is called a singular matrix. If \\(\\mathbf{A}\\) is invertible, the unique solution is \\(\\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}.\\)<sup>2</sup> If \\(\\mathbf{A}\\) is not square or is singular, solutions may still exist, but matrix inversion cannot be used.</p> <p>Closely related to the column space is the null space of a matrix. The null space of \\(\\mathbf{A}\\) is the set of all vectors \\(\\mathbf{x}\\) such that $ \\mathbf{A}\\mathbf{x} = \\mathbf{0}. $ Vectors in the null space are mapped to zero and therefore cannot be recovered from the output. A matrix has linearly dependent columns iff its null space contains nonzero vectors.</p>"},{"location":"mathematics/02_linear_algebra/#linear-systems","title":"Linear Systems","text":"<p>Consider the linear system \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b},\\) where \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\). A solution exists iff the vector \\(\\mathbf{b}\\) lies in the column space of \\(\\mathbf{A}\\). If \\(\\mathbf{b}\\) cannot be expressed as a linear combination of the columns of \\(\\mathbf{A}\\), then no vector \\(\\mathbf{x}\\) can satisfy the equation.</p> <p>If a solution exists, its uniqueness depends on the null space of \\(\\mathbf{A}\\). If the null space contains only the zero vector, then the solution is unique. In this case, no nonzero direction can be added to a solution without changing the output.</p> <p>If the null space contains nonzero vectors, then infinitely many solutions exist. Any solution can be modified by adding a null-space vector, producing a different input that yields the same output. In this situation, the linear map collapses information: different inputs are indistinguishable after applying \\(\\mathbf{A}\\).</p> <p>Note</p> <p>In elementary linear algebra, these cases are typically analyzed using Gaussian elimination and row-echelon form. While these procedures are essential for conceptual understanding and small problems, they are not used directly in deep learning or large-scale numerical computation. Modern frameworks instead rely on matrix factorizations and optimized solvers that achieve the same goals more efficiently and with better numerical stability.</p>"},{"location":"mathematics/02_linear_algebra/#basis-and-rank","title":"Basis and Rank","text":"<p>A basis of a vector space is a set of vectors that is both linearly independent and spanning the space. Every vector in the space can be written uniquely as a linear combination of the basis vectors. Consider the standard basis of \\(\\mathbb{R}^2\\), given by \\(\\mathbf{e}_1=[\\,1\\;\\;0\\,]^\\top\\) and \\(\\mathbf{e}_2=[\\,0\\;\\;1\\,]^\\top\\). Any vector \\(\\mathbf{x}\\in\\mathbb{R}^2\\) can be written uniquely as $$ \\mathbf{x}=x_1\\mathbf{e}_1+x_2\\mathbf{e}_2. $$ The pair \\((x_1,x_2)\\) are the coordinates of \\(\\mathbf{x}\\) in the standard basis. Now consider a different basis, \\(\\mathbf{v}_1=[\\,1\\;\\;1\\,]^\\top\\) and \\(\\mathbf{v}_2=[\\,1\\;\\;-1\\,]^\\top\\). This set is linearly independent and spans \\(\\mathbb{R}^2\\). The same vector \\(\\mathbf{x}\\) can be written as $$ \\mathbf{x}=c_1\\mathbf{v}_1+c_2\\mathbf{v}_2, $$ but the coefficients \\((c_1,c_2)\\) are different. The vector itself has not changed, only its coordinates have.</p> <p>Note</p> <p>Let \\(\\mathbf{V}=[\\,\\mathbf{v}_1\\ \\mathbf{v}_2\\,]\\). Then \\(\\mathbf{x}=\\mathbf{V}\\mathbf{c}\\) and \\(\\mathbf{c}=\\mathbf{V}^{-1}\\mathbf{x}\\). Changing basis corresponds to switching between coordinate systems using \\(\\mathbf{V}\\) and its inverse.</p> <p>If a space has a basis consisting of \\(k\\) vectors, we say the space has dimension \\(k\\). In \\(\\mathbb{R}^n\\), any basis contains exactly \\(n\\) vectors. For matrices, the analogous notion to dimension is rank. The rank of a matrix is the dimension of the space spanned by its columns (equivalently, its rows). It measures how many linearly independent directions the matrix preserves. If a matrix has rank \\(r\\), then its columns form a basis for an \\(r\\)-dimensional subspace. </p> <p>Consider the matrix \\(\\mathbf{A}= \\begin{bmatrix} 1 &amp; 1 \\\\ 2 &amp; 2 \\end{bmatrix}\\). The second row is a multiple of the first, so \\(\\mathbf{A}\\) has rank \\(1\\).</p> <p>Now take two different vectors, \\(\\mathbf{x}_1 = [\\,1\\;\\;0\\,]^\\top\\) and \\(\\mathbf{x}_2 = [\\,0\\;\\;1\\,]^\\top\\). They are clearly distinct. Multiplying by \\(\\mathbf{A}\\) gives \\(\\mathbf{A}\\mathbf{x}_1 = [\\,1\\;\\;2\\,]^\\top\\) and \\(\\mathbf{A}\\mathbf{x}_2 = [\\,1\\;\\;2\\,]^\\top\\). Although \\(\\mathbf{x}_1 \\neq \\mathbf{x}_2\\), they are mapped to the same output. The matrix cannot distinguish between directions that differ only within its null space. Information is collapsed because the transformation preserves only one independent direction.</p> <p>Note</p> <p>In deep learning, rank determines whether a linear layer preserves information or collapses it into a lower-dimensional representation. </p>"},{"location":"mathematics/02_linear_algebra/#norms","title":"Norms","text":"<p>Sometimes we need to measure the size of a vector. In machine learning, this is usually done using a norm. Formally, the \\(L_p\\) norm of a vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) is defined as $$ |\\mathbf{x}|_p=\\left(\\sum_i |x_i|^p\\right)^{1/p}, \\qquad p\\ge 1. $$</p> <p>Intuitively, a norm measures the distance from the origin to the point \\(\\mathbf{x}\\). More precisely, a norm is any function \\(f\\) satisfying:</p> <ul> <li>\\(f(\\mathbf{x})=0 \\Rightarrow \\mathbf{x}=\\mathbf{0}\\)</li> <li>\\(f(\\mathbf{x}+\\mathbf{y})\\le f(\\mathbf{x})+f(\\mathbf{y})\\) (triangle inequality)</li> <li>\\(f(\\alpha\\mathbf{x})=|\\alpha|f(\\mathbf{x})\\) for all \\(\\alpha\\in\\mathbb{R}\\)</li> </ul> <p>The \\(L_2\\) norm (\\(p=2\\)), called the Euclidean norm, is used so frequently that it is often written simply as \\(\\|\\mathbf{x}\\|\\). Its square can be written compactly as $ |\\mathbf{x}|_2^2=\\mathbf{x}^\\top\\mathbf{x}. $</p> <p>Note</p> <p>The squared \\(L_2\\) norm is often preferred in optimization because it is smooth and has simple derivatives. </p> <p>However, the squared \\(L_2\\) norm grows slowly near zero, which makes it less suitable when distinguishing exact zeros from small nonzero values matters. In such cases, the \\(L_1\\) norm, also known as the Manhattan distance, is commonly used: $ |\\mathbf{x}|_1=\\sum_i |x_i| $. Here, each component contributes linearly, so moving an element away from zero by \\(\\varepsilon\\) increases the norm by exactly \\(\\varepsilon\\). This property makes the \\(L_1\\) norm useful for encouraging sparsity.</p> <p>Note</p> <p>Sometimes one wants to count the number of nonzero entries in a vector. This quantity is often (incorrectly) called the \\(L_0\\) norm. It is not a true norm, because it is invariant under scaling (doesn't meet the third property described previously). In practice, the \\(L_1\\) norm is often used as a continuous surrogate for this count.</p> <p>Another common norm is the \\(L_\\infty\\) norm (maximum norm)<sup>3</sup>, defined as $ |\\mathbf{x}|_\\infty=\\max_i |x_i|. $ It measures the magnitude of the largest component of the vector.</p> <p>Finally, the dot product of two vectors can be expressed in terms of norms: $ \\mathbf{x}^\\top\\mathbf{y}=|\\mathbf{x}|_2\\,|\\mathbf{y}|_2\\cos\\theta, $ where \\(\\theta\\) is the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). This relation explains why the dot product measures both magnitude and alignment, and why normalized dot products are often used as similarity measures in machine/deep learning.</p> <p>Note</p> <p>Norms can also be defined for matrices. In deep learning, the most common choice is the Frobenius norm, which is directly analogous to the \\(L_2\\) norm of a vector.</p>"},{"location":"mathematics/02_linear_algebra/#diagonal-symmetric-orthogonal-matrices","title":"Diagonal, Symmetric, Orthogonal Matrices","text":"<p>A diagonal matrix consists of zeros everywhere except possibly on the main diagonal. Formally, a matrix \\(\\mathbf{D}\\) is diagonal if \\(D_{ij}=0\\) for all \\(i\\neq j\\). The identity matrix is a special case of a diagonal matrix with all diagonal entries equal to 1. We write \\(\\mathrm{diag}(\\mathbf{v})\\) to denote a square diagonal matrix whose diagonal entries are given by the vector \\(\\mathbf{v}\\).</p> <p>Diagonal matrices are important because multiplication by them is computationally efficient. The product \\(\\mathrm{diag}(\\mathbf{v})\\mathbf{x}\\) simply scales each component of \\(\\mathbf{x}\\) by the corresponding entry of \\(\\mathbf{v}\\):  $$ \\mathrm{diag}(\\mathbf{v})\\mathbf{x} = \\mathbf{v}\\odot\\mathbf{x}. $$</p> <p>Inversion is also efficient. A square diagonal matrix is invertible iff all diagonal entries are nonzero, in which case $$ \\mathrm{diag}(\\mathbf{v})^{-1} = \\mathrm{diag}\\bigl([1/v_1,\\dots,1/v_n]^\\top\\bigr). $$</p> <p>Note</p> <p>Many algorithms can be simplified and accelerated by restricting certain matrices to be diagonal.</p> <p>A symmetric matrix is a matrix equal to its transpose: $ \\mathbf{A}=\\mathbf{A}^\\top. $ Symmetric matrices often arise when entries depend on pairs of elements in an order-independent way, such as distance matrices where \\(A_{ij}=A_{ji}\\). A symmetric matrix \\(\\mathbf{A}\\) is called positive semidefinite if \\(\\mathbf{x}^\\top\\mathbf{A}\\mathbf{x}\\ge 0\\) for all \\(\\mathbf{x}\\), and positive definite if the inequality is strict for all \\(\\mathbf{x}\\neq 0\\).</p> <p>Note</p> <p>These notions describe how a matrix assigns a scalar value to every direction via the quadratic form \\(\\mathbf{x}^\\top\\mathbf{A}\\mathbf{x}\\). Matrices of the form \\(\\mathbf{A}^\\top\\mathbf{A}\\) and covariance matrices are always positive semidefinite. When a matrix is positive definite, the expression \\(\\mathbf{x}^\\top\\mathbf{A}\\mathbf{x}\\) behaves like a squared norm.</p> <p>Two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are orthogonal if \\(\\mathbf{x}^\\top\\mathbf{y}=0\\). In \\(\\mathbb{R}^n\\), at most \\(n\\) nonzero vectors can be mutually orthogonal. If the vectors are both orthogonal and have unit norm<sup>4</sup>, they are called orthonormal. An orthogonal matrix is a square matrix whose rows and columns are orthonormal: $$ \\mathbf{A}^\\top\\mathbf{A}=\\mathbf{A}\\mathbf{A}^\\top=\\mathbf{I}. $$ This implies $ \\mathbf{A}^{-1}=\\mathbf{A}^\\top, $ so orthogonal matrices are especially convenient computationally. </p> <p>Note</p> <p>Pay attention that \"orthogonal\" here means \"orthonormal\"; there is no standard term for matrices whose rows or columns are orthogonal but not normalized.</p>"},{"location":"mathematics/02_linear_algebra/#eigenvalues-and-eigenvectors","title":"Eigenvalues and Eigenvectors","text":"<p>Let \\(\\mathbf{A}\\in\\mathbb{R}^{n\\times n}\\). A nonzero vector \\(\\mathbf{v}\\) is called an eigenvector of \\(\\mathbf{A}\\) if there exists a scalar \\(\\lambda\\) such that $ \\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}. $ The scalar \\(\\lambda\\) is the corresponding eigenvalue. </p> <p>Illustrated below is the impact of the transformation matrix \\(\\mathbf{A}=\\begin{bmatrix}2&amp;1\\\\1&amp;2\\end{bmatrix}\\) on different vectors.</p> <p>      The transformation matrix preserves the directions of the magenta vectors parallel to $\\mathbf{v}_{\\lambda=1}=[\\,1\\;\\;-1\\,]^\\top$ and the blue vectors parallel to $\\mathbf{v}_{\\lambda=3}=[\\,1\\;\\;1\\,]^\\top$. Red vectors are not parallel to either eigenvector, so their directions change under the transformation. Magenta vectors keep the same length (eigenvalue $1$), while blue vectors become three times longer (eigenvalue $3$). By Lucas Vieira - Own work, Public Domain, Link <p>Eigenvectors identify directions that are preserved by the linear transformation \\(\\mathbf{A}\\). Applying \\(\\mathbf{A}\\) to an eigenvector does not change its direction; it only scales it by the factor \\(\\lambda\\). If \\(|\\lambda|&gt;1\\), vectors in that direction are stretched, if \\(|\\lambda|&lt;1\\), they are compressed, if \\(\\lambda=0\\), the direction is collapsed.</p>"},{"location":"mathematics/02_linear_algebra/#determinant","title":"Determinant","text":"<p>The determinant of a square matrix \\(\\mathbf{A}\\), denoted \\(\\det(\\mathbf{A})\\), is a scalar that summarizes how the linear transformation defined by \\(\\mathbf{A}\\) scales space. The determinant is equal to the product of the eigenvalues of \\(\\mathbf{A}\\): $$ \\det(\\mathbf{A})=\\prod_{i=1}^n \\lambda_i. $$ This interpretation becomes especially clear when \\(\\mathbf{A}\\) is diagonal or diagonalizable. For a diagonal matrix $ \\boldsymbol{\\Lambda}=\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n), $ the determinant is simply $$ \\det(\\boldsymbol{\\Lambda})=\\lambda_1\\lambda_2\\cdots\\lambda_n. $$</p> <p>Each diagonal entry scales space along one coordinate direction, and the determinant multiplies these scalings together. The following transformation scales area by a factor of 6. $$ \\boldsymbol{\\Lambda} = \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 3 \\end{bmatrix} \\;\\Rightarrow\\; \\det(\\boldsymbol{\\Lambda}) = 2\\cdot 3 = 6. $$</p> <p>Exercise</p> <p>Let \\(\\mathbf{A}=\\mathbf{Q}\\boldsymbol{\\Lambda}\\mathbf{Q}^\\top\\) be the eigendecomposition of a real symmetric matrix, where \\(\\mathbf{Q}\\) is orthogonal and \\(\\boldsymbol{\\Lambda}=\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)\\). Show that $$ \\det(\\mathbf{A})=\\det(\\boldsymbol{\\Lambda})=\\prod_i \\lambda_i, $$ and explain why the orthogonality of \\(\\mathbf{Q}\\) implies that the change of basis does not affect volume.</p> <p>Geometrically, the absolute value \\(|\\det(\\mathbf{A})|\\) measures how much the transformation expands or contracts volume. If \\(|\\det(\\mathbf{A})|&gt;1\\), volume is expanded; if \\(|\\det(\\mathbf{A})|&lt;1\\), volume is contracted. If \\(\\det(\\mathbf{A})=1\\), volume is preserved. If \\(\\det(\\mathbf{A})=0\\), space is collapsed along at least one direction, causing the transformation to lose volume entirely. In this case, the matrix is singular and not invertible.</p> <p>Note</p> <p>The determinant can also be defined directly in terms of matrix entries, without reference to eigenvalues. For example, for a \\(2\\times2\\) matrix, $$ \\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}, \\qquad \\det(\\mathbf{A}) = ad - bc. $$ For larger matrices, the determinant is defined recursively via cofactor expansion or computed using row operations. While these formulas are often used for computation, the eigenvalue interpretation provides the clearest conceptual understanding of what the determinant represents for deep learning.</p>"},{"location":"mathematics/02_linear_algebra/#eigendecomposition","title":"Eigendecomposition","text":"<p>If a matrix \\(\\mathbf{A}\\) has a full set of linearly independent eigenvectors, these can be arranged as columns of a matrix \\(\\mathbf{V}\\), with the corresponding eigenvalues placed on the diagonal of a matrix \\(\\boldsymbol{\\Lambda}\\). Such a factorization is called the eigendecomposition and the matrix can then be written as $$ \\mathbf{A}=\\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^{-1}. $$</p> <p>Eigendecomposition represents a linear transformation as:</p> <ol> <li>a change of basis into the eigenvector basis (\\(\\mathbf{V}^{-1}\\)),</li> <li>independent scalings along each eigenvector direction (\\(\\boldsymbol{\\Lambda}\\)),</li> <li>a change back to the original basis (\\(\\mathbf{V}\\)).</li> </ol> <p>Eigendecomposition is also the cleanest example of a general computational principle: decompose a matrix into parts with simple structure. Suppose \\(\\mathbf{A}=\\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^{-1}\\). Then: $$ \\begin{aligned} \\mathbf{A}^2 &amp;= (\\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^{-1})    (\\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^{-1}) \\\\ &amp;= \\mathbf{V}\\boldsymbol{\\Lambda}    (\\mathbf{V}^{-1}\\mathbf{V})    \\boldsymbol{\\Lambda}\\mathbf{V}^{-1} \\\\ &amp;= \\mathbf{V}\\boldsymbol{\\Lambda}^2\\mathbf{V}^{-1}. \\end{aligned} $$</p> <p>The middle factors \\(\\mathbf{V}^{-1}\\mathbf{V}\\) cancel to the identity. Repeating this argument gives $$ \\mathbf{A}^k=\\mathbf{V}\\boldsymbol{\\Lambda}^k\\mathbf{V}^{-1}. $$</p> <p>Thus, repeated matrix multiplication reduces to raising the diagonal entries \\(\\lambda_i\\) to the power \\(k\\): $$ \\boldsymbol{\\Lambda}^k =\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)^k =\\mathrm{diag}(\\lambda_1^k,\\dots,\\lambda_n^k). $$ Decomposition replaces repeated dense matrix multiplication with exponentiating scalars \\(\\lambda_i\\). The expensive part\u2014the interaction between coordinates\u2014disappears: in the eigenvector basis, each component is scaled independently by \\(\\lambda_i^k\\).</p> <p>Note</p> <p>Diagonal matrices are cheap to multiply, invert, and exponentiate, which is why eigendecomposition is so effective.</p> <p>Not every matrix admits eigendecomposition with real eigenvalues and eigenvectors. However, every real symmetric matrix does: $$ \\mathbf{A}=\\mathbf{Q}\\boldsymbol{\\Lambda}\\mathbf{Q}^\\top, $$ where \\(\\mathbf{Q}\\) is orthogonal and \\(\\boldsymbol{\\Lambda}\\) is real and diagonal. In this case, the inverse is especially simple: \\(\\mathbf{Q}^{-1}=\\mathbf{Q}^\\top\\).</p> <p>The eigendecomposition of a real symmetric matrix is not necessarily unique. When eigenvalues are repeated, any orthonormal basis of the corresponding eigenspace yields a valid decomposition. By convention, eigenvalues are usually ordered from largest to smallest.</p> <p>Eigenvalues immediately reveal important properties. A matrix is singular iff at least one eigenvalue is zero. For symmetric matrices, eigenvalues also characterize quadratic forms: $$ \\mathbf{x}^\\top\\mathbf{A}\\mathbf{x}, \\qquad |\\mathbf{x}|_2=1. $$ The maximum and minimum values are the largest and smallest eigenvalues, attained at the corresponding eigenvectors. </p>"},{"location":"mathematics/02_linear_algebra/#singular-value-decomposition","title":"Singular Value Decomposition","text":"<p>Eigendecomposition is defined only for square matrices. For a general matrix \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\), the analogous tool is the singular value decomposition: $$ \\mathbf{A}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^\\top, $$ where \\(\\mathbf{U}\\in\\mathbb{R}^{m\\times m}\\) and \\(\\mathbf{V}\\in\\mathbb{R}^{n\\times n}\\) are orthogonal matrices, and \\(\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{m\\times n}\\) is diagonal (not necessarily square). The diagonal entries of \\(\\boldsymbol{\\Sigma}\\) are called the singular values of \\(\\mathbf{A}\\). The columns of \\(\\mathbf{U}\\) are the left singular vectors, and the columns of \\(\\mathbf{V}\\) are the right singular vectors.</p>      Illustration of the singular value decomposition $U\\Sigma V^{*}$ of a real $2 \\times 2$ matrix $M$. Top: The action of $M$, indicated by its effect on the unit disc $D$ and the two canonical unit vectors $e_1$ and $e_2$. Left: The action of $V^{*}$, a rotation, on $D$, $e_1$, and $e_2$. Bottom: The action of $\\Sigma$, a scaling by the singular values $\\sigma_1$ horizontally and $\\sigma_2$ vertically. Right: The action of $U$, another rotation. By Georg-Johann - Own work, CC BY-SA 3.0, Link <p>Singular value decomposition is closely related to eigendecomposition. The left singular vectors of \\(\\mathbf{A}\\) are the eigenvectors of \\(\\mathbf{A}\\mathbf{A}^\\top\\), and the right singular vectors are the eigenvectors of \\(\\mathbf{A}^\\top\\mathbf{A}\\). The nonzero singular values are the square roots of the nonzero eigenvalues of \\(\\mathbf{A}^\\top\\mathbf{A}\\) (and equivalently of \\(\\mathbf{A}\\mathbf{A}^\\top\\)).</p> <p>Tip</p> <p>For a more detailed explanation on how decompositions emerge, see the supplementary material on the singular value decomposition. </p> <p>One important use of singular value decomposition is that it provides a principled way to extend matrix inversion to non-square or singular matrices via the pseudoinverse.</p>"},{"location":"mathematics/02_linear_algebra/#moorepenrose-pseudoinverse","title":"Moore\u2013Penrose Pseudoinverse","text":"<p>Matrix inversion is not defined for non-square matrices. Suppose we want a matrix \\(\\mathbf{B}\\) that acts like a left-inverse so that we can solve \\(\\mathbf{A}\\mathbf{x}=\\mathbf{y}\\) by writing \\(\\mathbf{x}=\\mathbf{B}\\mathbf{y}\\). Depending on the shape of \\(\\mathbf{A}\\), an exact solution may not exist (tall matrices) or may not be unique (wide matrices).</p> <p>The Moore\u2013Penrose pseudoinverse \\(\\mathbf{A}^+\\) provides a standard choice. It can be defined as $$ \\mathbf{A}^+ = \\lim_{\\alpha\\to 0}(\\mathbf{A}^\\top\\mathbf{A}+\\alpha\\mathbf{I})^{-1}\\mathbf{A}^\\top, $$ but in practice it is computed using the SVD. If \\(\\mathbf{A}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^\\top\\), then $$ \\mathbf{A}^+ = \\mathbf{V}\\boldsymbol{\\Sigma}^+\\mathbf{U}^\\top, $$ where \\(\\boldsymbol{\\Sigma}^+\\) is obtained by taking the reciprocal of each nonzero diagonal entry of \\(\\boldsymbol{\\Sigma}\\) and then transposing the result.</p> <p>If \\(\\mathbf{A}\\) has more columns than rows (\\(n&gt;m\\)), the system may have infinitely many solutions. The pseudoinverse returns the solution with minimal Euclidean norm \\(\\|\\mathbf{x}\\|_2\\) among all solutions. If \\(\\mathbf{A}\\) has more rows than columns (\\(m&gt;n\\)), the system may have no exact solution. In that case, \\(\\mathbf{x}=\\mathbf{A}^+\\mathbf{y}\\) minimizes the least-squares error \\(\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2\\). </p> <p>Tip</p> <p>To get a nice and clear geometric intuition on the matter, see the chapter on the method of least squares from David C. Lay's Linear Algebra and its Applications.  </p>"},{"location":"mathematics/02_linear_algebra/#trace-operator","title":"Trace Operator","text":"<p>The trace of a square matrix is the sum of its diagonal entries: \\(\\mathrm{Tr}(\\mathbf{A}) = \\sum_i A_{ii}.\\) The trace is useful because it allows scalar quantities involving matrices to be written compactly using matrix products rather than explicit summations. For example, the Frobenius norm of a matrix can be written as \\(\\|\\mathbf{A}\\|_F = \\sqrt{\\mathrm{Tr}(\\mathbf{A}\\mathbf{A}^\\top)}.\\)</p> <p>Several properties of the trace are especially important in deep learning and optimization. The trace is invariant under transposition, \\(\\mathrm{Tr}(\\mathbf{A}) = \\mathrm{Tr}(\\mathbf{A}^\\top),\\) and invariant under cyclic permutation of matrix products when dimensions are compatible: $$ \\mathrm{Tr}(\\mathbf{A}\\mathbf{B}\\mathbf{C}) = \\mathrm{Tr}(\\mathbf{B}\\mathbf{C}\\mathbf{A}) = \\mathrm{Tr}(\\mathbf{C}\\mathbf{A}\\mathbf{B}). $$ This cyclic property is essential in matrix calculus, as it allows expressions to be rearranged so that derivatives with respect to a given variable can be taken systematically.</p> <p>In deep learning, the trace operator is used mainly \"behind the scenes\". It appears in derivations of gradients, Jacobians, and Hessians, and in the formulation of losses and regularization terms involving matrix products. Modern deep learning frameworks rely on these trace identities internally when implementing automatic differentiation and optimized linear algebra algorithms, even though the trace operator itself rarely appears in user code.</p> <ol> <li> <p>A function \\(f(\\mathbf{x},\\mathbf{y})\\) is called bilinear if it is linear in each argument separately when the other argument is held fixed. For the dot product \\(f(\\mathbf{x},\\mathbf{y})=\\mathbf{x}\\cdot\\mathbf{y}\\), this means: Holding \\(\\mathbf{y}\\) fixed, the map \\(\\mathbf{x}\\mapsto \\mathbf{x}\\cdot\\mathbf{y}\\) is linear: $$ (\\alpha \\mathbf{x}_1+\\beta \\mathbf{x}_2)\\cdot\\mathbf{y} = \\alpha(\\mathbf{x}_1\\cdot\\mathbf{y})+\\beta(\\mathbf{x}_2\\cdot\\mathbf{y}). $$ Holding \\(\\mathbf{x}\\) fixed, the map \\(\\mathbf{y}\\mapsto \\mathbf{x}\\cdot\\mathbf{y}\\) is also linear: $$ \\mathbf{x}\\cdot(\\alpha \\mathbf{y}_1+\\beta \\mathbf{y}_2) = \\alpha(\\mathbf{x}\\cdot\\mathbf{y}_1)+\\beta(\\mathbf{x}\\cdot\\mathbf{y}_2). $$ Bilinearity does not mean the function is linear in both arguments at once. It means that if one vector is treated as constant, the dot product behaves exactly like a linear function of the other. This property is what allows dot products to distribute over sums and pull out scalar factors, and it is why gradients propagate cleanly through linear layers in deep learning.\u00a0\u21a9</p> </li> <li> <p>So far, inverses were defined by left multiplication: \\(\\mathbf{A}^{-1}\\mathbf{A}=\\mathbf{I}.\\) A right inverse satisfies \\(\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{I}.\\) For square matrices, left and right inverses coincide.\u00a0\u21a9</p> </li> <li> <p>The \\(L_\\infty\\) norm is also known as the uniform norm. This name comes from functional analysis: a sequence of functions \\(\\{f_n\\}\\) converges to a function \\(f\\) under the metric induced by the uniform norm iff \\(f_n\\) converges to \\(f\\) uniformly, meaning the maximum deviation \\(\\sup_x |f_n(x)-f(x)|\\) goes to zero. In finite-dimensional vector spaces, this reduces to taking the maximum absolute component.\u00a0\u21a9</p> </li> <li> <p>A unit vector is a vector with unit Euclidean norm: $ |\\mathbf{x}|_2=1. $\u00a0\u21a9</p> </li> </ol>"},{"location":"mathematics/03_probability/","title":"Probability Theory","text":"7 Feb 2026 <p>Probability theory is the mathematical framework for reasoning under uncertainty. In artificial intelligence, probability is used in two main ways: (i) as a guide for how an intelligent system should reason under uncertainty, and (ii) as a tool for analyzing and understanding the behavior of learning algorithms. Deep learning relies on probability because real-world data is noisy, incomplete, and never fully deterministic, so uncertainty is unavoidable.  </p> <p>Info</p> <p>The following source was consulted in preparing this material: Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press. Chapter 3: Probability and Information Theory.</p> <p>Important</p> <p>Some concepts in this material are simplified for pedagogical purposes. These simplifications slightly reduce precision but preserve the core ideas relevant to deep learning.</p>"},{"location":"mathematics/03_probability/#kolmogorov-axioms","title":"Kolmogorov Axioms","text":"<p>Probability is a consistent system governed by a small set of rules (axioms) that prevent contradictions. In modern mathematics, these rules are usually given by the axioms introduced by Kolmogorov<sup>1</sup>. The three axioms define probability as a function \\(P(\\cdot)\\) that assigns a number to each event in a set of valid events. For any event \\(A\\) and \\(B\\), a probability function must satisfy the following basic rules:</p> <ul> <li>It can never assign a negative value: \\(P(A) \\ge 0\\)</li> <li>It must assign probability \\(1\\) to a certain event: \\(P(\\text{certain event}) = 1\\)</li> <li>If two events cannot happen at the same time (\\(A \\cap B = \\emptyset\\)), their probabilities add up: \\(P(A \\cup B)=P(A)+P(B)\\).</li> </ul> <p>These axioms guarantee that probability remains logically consistent.<sup>2</sup> Probability assigns numbers between \\(0\\) and \\(1\\) to events in order to represent how plausible those events are, given some assumptions or information. A value of \\(0\\) means the event is impossible under the assumed model, a value of \\(1\\) means it is certain, and intermediate values represent partial uncertainty.</p>"},{"location":"mathematics/03_probability/#two-views-of-probability","title":"Two Views of Probability","text":"<p>Historically, probability was first developed to describe repeatable experiments, such as rolling dice, drawing cards, or observing outcomes in games of chance. Under this interpretation, called frequentist probability, \\(P(A)\\) represents the long-run fraction of times event \\(A\\) occurs if the experiment were repeated infinitely many times. For example, \\(P(\\text{heads})=0.5\\) means that if we toss a fair coin a very large number of times, about half of the tosses will result in heads.<sup>3</sup></p> <p>Later, probability began to be used in a broader sense: not only for repeatable experiments, but also for reasoning about unique situations where repetition is impossible. For example, a doctor may assign a probability that a patient has a disease, even though we cannot create infinitely many identical copies of the patient. In this interpretation, called Bayesian probability, probability measures a degree of belief given incomplete information.</p> <p>Although the interpretations differ, the same probability formulas apply to both. The axioms and rules of probability provide a consistent framework for reasoning under uncertainty, regardless of whether probability is interpreted as frequency or degree of belief.</p>"},{"location":"mathematics/03_probability/#random-variables","title":"Random Variables","text":"<p>In probability theory, we rarely assign probabilities directly to raw outcomes. Instead, we define a random variable, which is a variable whose value depends on the outcome of an uncertain process. A random variable does not necessarily mean the process is truly random. It simply means that, from our perspective, the value is unknown until the outcome is observed. Random variables can be:</p> <ul> <li>Discrete, meaning they can take only a finite or countably infinite set of values (e.g. \\(0,1,2,\\dots\\)).</li> <li>Continuous, meaning they can take any real value in an interval (e.g. any number in \\([0,1]\\)).</li> </ul> <p>For example, the result of a coin toss can be modeled as a discrete random variable \\(X \\in \\{0,1\\}\\), while the temperature measured by a sensor is naturally modeled as a continuous random variable. In probability notation, we usually write a random variable using a capital letter such as \\(X\\), and a specific realized value using a lowercase letter such as \\(x\\).</p> <p>Note</p> <p>In deep learning, we often model data as random variables. For example, an image can be treated as a random variable \\(X\\), and its label (such as \"cat\" or \"dog\") as another random variable \\(Y\\). The goal of learning is then to discover patterns in how these random variables relate to each other.</p>"},{"location":"mathematics/03_probability/#probability-distributions","title":"Probability Distributions","text":"<p>A random variable by itself only describes what values are possible. To reason quantitatively, we must also specify a probability distribution, which assigns probabilities to the different values the random variable can take.</p> <p>Once the outcome is observed, the random variable takes a specific value. If a random variable is discrete, its distribution is described by a probability mass function (PMF), denoted by \\(P(X=x)\\). It assigns a probability to each possible value, such that:</p> <ul> <li>\\(0 \\le P(X=x) \\le 1\\)</li> <li>\\(\\sum_x P(X=x) = 1\\)</li> </ul> <p>If a random variable is continuous, its distribution is described by a probability density function (PDF), denoted by \\(p(x)\\). The PDF must satisfy:</p> <ul> <li>\\(p(x) \\ge 0\\)</li> <li>\\(\\int_{-\\infty}^{\\infty} p(x)\\,dx = 1\\)</li> </ul> <p>Important</p> <p>For continuous variables, \\(p(x)\\) is a density, not a probability. The value \\(p(x)\\) can be greater than \\(1\\). Probabilities are obtained only by integrating over an interval: $$ P(a \\le X \\le b) = \\int_a^b p(x)\\,dx $$ The probability of observing any exact value \\(X=x\\) is always \\(0\\). For example, if \\(X\\) represents a real-valued measurement such as temperature, the probability of observing exactly \\(20.000^\\circ\\) is essentially zero, because the measurement could always end up being \\(19.999\\) or \\(20.001\\) instead. Only intervals have non-zero probability, such as \\(P(19.9 \\le X \\le 20.1)\\).</p> <p>Tip</p> <p>For integration and related topics, see the page dedicated to deep learning Calculus.</p>"},{"location":"mathematics/03_probability/#joint-and-marginal-distributions","title":"Joint and Marginal Distributions","text":"<p>So far, we have described probability distributions over a single random variable. In many real problems, we must model multiple random variables at the same time. The probability distribution over two variables \\(X\\) and \\(Y\\) together is called the joint distribution. If \\(X\\) and \\(Y\\) are discrete, the joint distribution is written as: \\(P(X=x, Y=y).\\) If \\(X\\) and \\(Y\\) are continuous, the joint distribution is written as a joint density: \\(p(x,y).\\)</p> <p>Often, we are only interested in the distribution of one variable by itself. This is called the marginal distribution, and it can be obtained by summing (discrete case) or integrating (continuous case) over the other variable. For discrete variables: $$ P(X=x) = \\sum_y P(X=x, Y=y). $$ For continuous variables: $$ p(x) = \\int p(x,y)\\,dy. $$</p> <p>Example</p> <p>Suppose \\(X\\) represents the outcome of a coin toss (tails or heads), and \\(Y\\) represents the number shown on a fair die (\\(1\\) to \\(6\\)). If we assume the coin toss does not affect the die roll (and vice versa), then each of the \\(2 \\times 6 = 12\\) outcomes is equally likely, so the joint distribution assigns probability \\(1/12\\) to every possible pair:</p> \\(X \\backslash Y\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(6\\) tails 1/12 1/12 1/12 1/12 1/12 1/12 heads 1/12 1/12 1/12 1/12 1/12 1/12 <p>To compute the marginal distribution of \\(X\\), we sum across each row:<sup>4</sup> $$ P(X=\\text{tails}) = \\sum_{y=1}^{6} P(X=\\text{tails},Y=y) = \\frac{1}{2}. $$</p> <p>Exercise</p> <p>Compute the marginal distributions \\(P(X=\\text{heads})\\) and \\(P(Y=1)\\).</p>"},{"location":"mathematics/03_probability/#conditional-probability","title":"Conditional Probability","text":"<p>In many situations, we are interested in the probability of an event given that another event has already occurred. This is called conditional probability. The conditional probability of \\(A\\) given \\(B\\) is denoted by \\(P(A \\mid B)\\).</p> <p>For discrete random variables \\(X\\) and \\(Y\\), the conditional distribution of \\(Y\\) given \\(X\\) is defined as: $$ P(Y=y \\mid X=x) = \\frac{P(X=x, Y=y)}{P(X=x)}. $$</p> <p>Important</p> <p>Many textbooks use shorthand notation such as \\(P(y \\mid x)\\) instead of \\(P(Y=y \\mid X=x)\\). We will mostly use explicit notation for clarity.</p> <p>For continuous random variables, we use probability densities instead: $$ p(y \\mid x) = \\frac{p(x,y)}{p(x)}. $$</p> <p>These formulas are only defined when \\(P(X=x)&gt;0\\) or \\(p(x)&gt;0\\), since we cannot condition on an event that never occurs.</p> <p>Example</p> <p>Suppose \\(X\\) is an image and \\(Y\\) is its label. The joint distribution \\(P(X,Y)\\) describes how often we encounter a specific image together with its correct label. The marginal distribution \\(P(X)\\) describes what kinds of images appear in the world or in our dataset, regardless of their labels. The conditional distribution \\(P(Y \\mid X)\\) describes the probability of each label given a particular image. When we train a classifier, we are essentially training a model that takes an image \\(x\\) and outputs estimates of probabilities like \\(P(Y=\\text{cat} \\mid X=x)\\) and \\(P(Y=\\text{dog} \\mid X=x)\\), where \\(x\\) could be a particular input image provided to our model.</p> <p>Marginalization can also be written in a form that uses conditional probabilities. This is known as the law of total probability. For discrete variables:</p>  $$ P(Y=y) = \\sum_x P(Y=y \\mid X=x)\\,P(X=x). $$  <p>For continuous variables: $$ p(y) = \\int p(y \\mid x)\\,p(x)\\,dx. $$</p> <p>This identity is extremely important.</p>"},{"location":"mathematics/03_probability/#independence","title":"Independence","text":"<p>In many problems, we work with multiple random variables. The relationship between these variables determines how complicated the joint distribution is. Two random variables \\(X\\) and \\(Y\\) are called independent if knowing the value of one gives no information about the other. Formally, \\(X\\) and \\(Y\\) are independent if their joint distribution factorizes into a product of marginals: $$ P(X=x, Y=y) = P(X=x)\\,P(Y=y). $$</p> <p>Equivalently, independence can be expressed using conditional probability: $$ P(Y=y \\mid X=x) = P(Y=y), $$ meaning that observing \\(X\\) does not change the probability of \\(Y\\). For continuous variables, the same definition applies using probability densities: $$ p(x,y) = p(x)\\,p(y). $$</p> <p>Note</p> <p>Independence is a very strong assumption. In real-world data, variables are often correlated. However, independence assumptions are extremely useful because they allow us to build computationally efficient models.</p> <p>Sometimes variables are not independent in general, but become independent once we condition on a third variable. We say that \\(X\\) and \\(Y\\) are conditionally independent given \\(Z\\) if: $$ P(X=x, Y=y \\mid Z=z) = \\\\  P(X=x \\mid Z=z)\\,P(Y=y \\mid Z=z). $$</p> <p>Example</p> <p>Suppose \\(Z\\) represents whether it is raining. Let \\(X\\) be whether the street is wet, and \\(Y\\) be whether people are carrying umbrellas. In general, \\(X\\) and \\(Y\\) are strongly correlated: if the street is wet, umbrellas are more likely. However, once we condition on \\(Z\\) (rain), the relationship mostly disappears: given that it is raining, the street being wet does not provide much additional information about umbrellas. This illustrates conditional independence: \\(X \\perp Y \\mid Z\\).</p>"},{"location":"mathematics/03_probability/#chain-rule","title":"Chain Rule","text":"<p>Probability has its own chain rule. Even when random variables are not independent, we can still represent any joint distribution by repeatedly applying the definition of conditional probability. For two variables, the joint distribution can be rewritten as: $$ P(X=x, Y=y) = P(Y=y \\mid X=x)\\,P(X=x). $$</p> <p>In general, for \\(n\\) random variables \\((X_1, X_2, \\dots, X_n)\\), we can expand the joint distribution as: $$ P(X_1, X_2, \\dots, X_n) = \\prod_{i=1}^{n} P(X_i \\mid X_1, \\dots, X_{i-1}). $$</p> <p>This identity is called the chain rule of probability. It is simply a consequence of how conditional probability is defined.</p> <p>The chain rule gives a valid factorization, but it is often too complex because each conditional distribution depends on many variables. Independence assumptions simplify the factorization. For example, if we assume \\(X\\) and \\(Y\\) are independent, then: $$ P(X,Y) = P(X)\\,P(Y). $$</p> <p>If we assume \\(X\\) and \\(Y\\) are conditionally independent given \\(Z\\), then: $$ P(X,Y,Z) = P(X \\mid Z)\\,P(Y \\mid Z)\\,P(Z). $$</p> <p>This type of factorization is the foundation of many probabilistic models, including Bayesian networks and graphical models.</p>"},{"location":"mathematics/03_probability/#independent-and-identically-distributed","title":"Independent and Identically Distributed","text":"<p>In deep learning, the most common simplifying assumption is that training examples are independent and identically distributed (i.i.d.). Suppose we have a dataset of samples: $$ {x^{(1)}, x^{(2)}, \\dots, x^{(m)}}. $$</p> <p>The i.i.d. assumption means that each sample is generated independently of the others, and all samples come from the same underlying distribution. Formally, if each sample is drawn from the same distribution \\(P(X)\\) and samples are independent, then the joint probability of the dataset factorizes as: $$ P(x^{(1)}, x^{(2)}, \\dots, x^{(m)}) = \\prod_{i=1}^{m} P(x^{(i)}). $$</p> <p>This assumption is extremely important because it makes learning feasible: it allows the likelihood of a dataset to be written as a product, and the log-likelihood as a sum.</p> <p>Important</p> <p>The i.i.d. assumption is often violated in practice. Examples include time series, video frames, financial data, and datasets affected by distribution shift.   However, i.i.d. is still widely used because it provides a simple baseline model of how data is generated.</p> <p>Note</p> <p>Stochastic gradient descent (SGD) implicitly relies on the i.i.d. assumption: each mini-batch is treated as a random sample from the same distribution, so its gradient is assumed to approximate the full dataset gradient.</p>"},{"location":"mathematics/03_probability/#bayes-rule","title":"Bayes' Rule","text":"<p>We have reached a crucial point in probability theory. Terminology can be dense and seem complicated, so I suggest spending some time here to clearly understand the concepts. You will frequently see the described terminology in deep learning literature. </p> <p>Tip</p> <p>For a visual intuition of Bayes' rule, see the video on Bayes' theorem.</p> <p>Bayes\u2019 rule allows us to reverse conditional probabilities. It provides a way to compute the probability of a hypothesis after observing evidence. For discrete random variables, Bayes\u2019 rule is:</p>  $$ P(X=x \\mid Y=y) = \\frac{P(Y=y \\mid X=x)\\,P(X=x)}{P(Y=y)}. $$  <p>Here, \\(P(X=x)\\) is called the prior. It describes how likely \\(x\\) was before observing \\(y\\). The term \\(P(Y=y \\mid X=x)\\) is the likelihood. It measures how compatible the observation \\(y\\) is with the hypothesis \\(x\\). The result \\(P(X=x \\mid Y=y)\\) is called the posterior.</p> <p>Note</p> <p>The denominator \\(P(Y=y)\\) acts as a normalization constant. It ensures that the posterior distribution \\(P(X \\mid Y=y)\\) sums to \\(1\\) over all possible values of \\(X\\).  </p> <p>For continuous random variables, we use probability densities instead: $$ p(x \\mid y) = \\frac{p(y \\mid x)\\,p(x)}{p(y)}. $$</p> <p>The denominator \\(p(y)\\) is the marginal probability density of observing \\(y\\). It is obtained by summing over all possible values of \\(x\\) that could have produced \\(y\\) (in the continuous case, summation becomes integration): $$ p(y) = \\int p(y \\mid x)\\,p(x)\\,dx. $$</p> <p>Note</p> <p>This continuous form of Bayes' rule will become important for us later when discussing variational autoencoders.</p> <p>Example</p> <p>Consider a spam detection example. Let \\(X\\) represent whether an email is spam (\\(X=\\text{spam}\\) or \\(X=\\text{not spam}\\)), and let \\(Y\\) represent whether the email contains the phrase \"win money\" (\\(Y=\\text{yes}\\) or \\(Y=\\text{no}\\)). </p> <p>Suppose only \\(2\\%\\) of all emails are spam, so the prior probability is \\(P(\\text{spam})=0.02\\) and \\(P(\\text{not spam})=0.98\\). Now assume spam emails contain the phrase \"win money\" \\(60\\%\\) of the time, so \\(P(\\text{yes} \\mid \\text{spam})=0.60\\), while normal emails contain it only \\(1\\%\\) of the time, so \\(P(\\text{yes} \\mid \\text{not spam})=0.01\\). If we observe an email containing \"win money\", Bayes\u2019 rule gives: $$ P(\\text{spam} \\mid \\text{yes}) = \\frac{P(\\text{yes} \\mid \\text{spam})P(\\text{spam})} {P(\\text{yes})}. $$ The numerator is \\(0.60 \\cdot 0.02 = 0.012\\). The denominator is computed by marginalization: $$ P(\\text{yes}) = P(\\text{yes} \\mid \\text{spam})P(\\text{spam}) + \\\\ P(\\text{yes} \\mid \\text{not spam})P(\\text{not spam}) = \\\\ 0.60\\cdot 0.02 + 0.01\\cdot 0.98 = 0.0218. $$ Therefore, $$ P(\\text{spam} \\mid \\text{yes}) = \\frac{0.012}{0.0218} \\approx 0.55. $$ After observing the phrase, the probability that the email is spam jumps from \\(2\\%\\) to about \\(55\\%\\). This illustrates Bayes\u2019 rule as a mechanism for updating beliefs: the likelihood tells us how strongly the evidence points toward spam, while the prior reflects how common spam is overall.</p>"},{"location":"mathematics/03_probability/#expectation-variance-covariance","title":"Expectation, Variance, Covariance","text":"<p>So far, we have described probability distributions in terms of the probability of events. However, in machine learning we often want to summarize a distribution using a few meaningful numerical quantities. The most important of these are the expectation (mean), the variance, and the covariance.</p> <p>The expectation (or expected value) of a function \\(f(X)\\) is the average value of \\(f(X)\\) when \\(X\\) is sampled from its distribution. If \\(X\\) is discrete: $$ \\mathbb{E}[f(X)] = \\sum_x P(X=x)\\,f(x). $$</p> <p>If \\(X\\) is continuous: $$ \\mathbb{E}[f(X)] = \\int p(x)\\,f(x)\\,dx. $$</p> <p>If we set \\(f(X)=X\\), we obtain the expected value of the random variable itself: $ \\mathbb{E}[X]. $</p> <p>Note</p> <p>Expectations are the probabilistic version of weighted averages. The probability distribution acts as the weights. For example, the ordinary average of \\(n\\) numbers \\(x_1, x_2, \\dots, x_n\\) is: \\(\\frac{1}{n}\\sum_{i=1}^n x_i.\\) This is exactly the expectation of a discrete random variable that takes value \\(x_i\\) with uniform probability \\(P(X=x_i)=\\frac{1}{n}\\) (e.g. fair dice): $$ \\mathbb{E}[X] = \\sum_{i=1}^n P(X=x_i)\\,x_i = \\sum_{i=1}^n \\frac{1}{n}x_i = \\frac{1}{n}\\sum_{i=1}^n x_i. $$</p> <p>Expectation has an extremely useful property: it is linear. For constants \\(\\alpha\\) and \\(\\beta\\): $$ \\mathbb{E}[\\alpha f(X) + \\beta g(X)] = \\alpha \\mathbb{E}[f(X)] + \\beta \\mathbb{E}[g(X)]. $$</p> <p>Important</p> <p>Linearity holds even when \\(f(X)\\) and \\(g(X)\\) are dependent. This is one of the most powerful tools in probability.</p> <p>The variance measures how spread out a random variable is around its mean. In other words, it tells us whether values are tightly clustered near the average or whether they fluctuate widely. It is defined as: $$ \\mathrm{Var}(X) = \\mathbb{E}\\Big[(X - \\mathbb{E}[X])^2\\Big]. $$</p> <p>Here, \\((X - \\mathbb{E}[X])\\) is the deviation from the mean. If \\(X\\) usually stays close to \\(\\mathbb{E}[X]\\), the variance is small. If \\(X\\) often takes values far from \\(\\mathbb{E}[X]\\), the variance is large.</p> <p>Note</p> <p>We square deviations because (i) it prevents positive and negative deviations from canceling out, (ii) it penalizes large deviations more strongly, and (iii) it leads to clean mathematical formulas that are easy to analyze and optimize. Using absolute deviation is possible but less convenient in theory.</p> <p>The standard deviation is defined as: $$ \\sigma(X) = \\sqrt{\\mathrm{Var}(X)}. $$  It is often more intuitive than variance because it is measured in the same scale as the original variable.</p> <p>Note</p> <p>Variance can also be rewritten in a form that is often easier to compute: $$ \\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2. $$ This identity is widely used in probability derivations and appears frequently in machine learning theory.</p> <p>The covariance measures how two random variables vary together. It captures whether they tend to increase and decrease at the same time. It is defined as: $$ \\mathrm{Cov}(X,Y) = \\mathbb{E}\\Big[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])\\Big]. $$</p> <p>If both \\(X\\) and \\(Y\\) are usually above their means at the same time (or below their means at the same time), the covariance becomes positive. If one is usually above its mean when the other is below its mean, the covariance becomes negative.</p> <ul> <li>If \\(\\mathrm{Cov}(X,Y) &gt; 0\\), \\(X\\) and \\(Y\\) tend to move in the same direction.</li> <li>If \\(\\mathrm{Cov}(X,Y) &lt; 0\\), \\(X\\) and \\(Y\\) tend to move in opposite directions.</li> <li>If \\(\\mathrm{Cov}(X,Y) = 0\\), \\(X\\) and \\(Y\\) have no linear relationship.</li> </ul> <p>Covariance measures linear dependence only. It is possible for two variables to be strongly dependent in a nonlinear way while still having covariance equal to zero. If \\(X\\) and \\(Y\\) are independent, then: \\(\\mathrm{Cov}(X,Y)=0.\\) However, the reverse is not always true: \\(\\mathrm{Cov}(X,Y)=0\\) does not guarantee independence.</p> <p>Example</p> <p>Suppose \\(X\\) is uniformly distributed on \\([-1,1]\\), and let \\(Y=X^2\\). Then \\(X\\) and \\(Y\\) are clearly dependent, because knowing \\(X\\) determines \\(Y\\). However, their covariance is still \\(0\\), because positive and negative values of \\(X\\) cancel out.</p> <p>Note</p> <p>Covariance depends on scale. A scale-independent version is the correlation coefficient (Pearson correlation): $$ \\rho(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma(X)\\sigma(Y)}, \\qquad \\sigma(X)=\\sqrt{\\mathrm{Var}(X)}. $$ It is always bounded: $$ -1 \\le \\rho(X,Y) \\le 1. $$ Correlation measures only linear dependence, so \\(\\rho(X,Y)=0\\) does not imply independence.</p> <p>In machine learning, we often deal with random vectors. If \\(X \\in \\mathbb{R}^n\\) is a random vector, then its covariance matrix is an \\(n \\times n\\) matrix defined as: $$ \\mathrm{Cov}(X)_{i,j} = \\mathrm{Cov}(X_i, X_j). $$</p> <p>The diagonal elements represent variances: $$ \\mathrm{Cov}(X_i, X_i) = \\mathrm{Var}(X_i). $$</p> <p>The off-diagonal elements represent how different dimensions vary together: $ \\mathrm{Cov}(X_i, X_j). $</p> <p>Note</p> <p>The covariance matrix is a compact way to describe how multiple variables relate to each other. For example, if your dataset has three features (height, weight, age), then the covariance matrix is a \\(3 \\times 3\\) matrix:  $$ \\Sigma = \\begin{bmatrix} \\mathrm{Var}(\\text{height}) &amp; \\mathrm{Cov}(\\text{height},\\text{weight}) &amp; \\mathrm{Cov}(\\text{height},\\text{age}) \\\\ \\mathrm{Cov}(\\text{weight},\\text{height}) &amp; \\mathrm{Var}(\\text{weight}) &amp; \\mathrm{Cov}(\\text{weight},\\text{age}) \\\\ \\mathrm{Cov}(\\text{age},\\text{height}) &amp; \\mathrm{Cov}(\\text{age},\\text{weight}) &amp; \\mathrm{Var}(\\text{age}) \\end{bmatrix}. $$  The diagonal entries measure the spread of each feature (variance), while the off-diagonal entries measure whether two features increase or decrease together (covariance).</p>"},{"location":"mathematics/03_probability/#common-probability-distributions","title":"Common Probability Distributions","text":"<p>Many probability distributions exist, but only a small number appear repeatedly in deep learning and machine learning. These distributions are used to model labels, noise, uncertainty in model outputs, etc. In practice, choosing an appropriate distribution is part of defining the assumptions of a model. A good probabilistic model is not just about fitting data \u2014 it is also about choosing a distribution that matches the structure of the problem.</p>"},{"location":"mathematics/03_probability/#uniform","title":"Uniform","text":"<p>Uniform distribution simply assigns equal probability to every possible outcome. For a discrete uniform distribution over \\(k\\) values: $$ P(X=i)=\\frac{1}{k}. $$</p> <p>For a continuous uniform distribution over an interval \\([a,b]\\): $$ p(x)=\\frac{1}{b-a}, \\qquad x \\in [a,b]. $$</p> <p>Outside the interval, the density is zero.</p>"},{"location":"mathematics/03_probability/#bernoulli","title":"Bernoulli","text":"<p>Bernoulli distribution is also simple. It models a binary random variable: \\(X \\in \\{0,1\\}.\\) It is controlled by a single parameter \\(\\phi \\in [0,1]\\), representing the probability of success: $$ P(X=1) = \\phi, \\qquad P(X=0) = 1-\\phi, $$</p> <p>which can also be written compactly as: $$ P(X=x) = \\phi^x (1-\\phi)^{1-x}. $$</p> <p>Note</p> <p>We can derive the expectation and variance of a Bernoulli random variable directly from the definition of expectation. Since \\(X \\in \\{0,1\\}\\), we have:  $$ \\mathbb{E}[X] = \\sum_{x\\in{0,1}} xP(X=x) =  0\\cdot P(X=0) + 1\\cdot P(X=1) = \\phi. $$ </p> <p>To compute the variance, we use: $$ \\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2. $$ But since \\(X\\) is binary, \\(X^2=X\\), so: $$ \\mathbb{E}[X^2] = \\mathbb{E}[X] = \\phi. $$ Therefore, $$ \\mathrm{Var}(X) = \\phi - \\phi^2 = \\phi(1-\\phi). $$</p> <p>Example</p> <p>Suppose we model whether an MNIST image is the digit \\(3\\) as a Bernoulli random variable: \\(Y \\in \\{0,1\\},\\) where \\(Y=1\\) means the image is a \\(3\\) and \\(Y=0\\) means it is not. If we assume the dataset defines an underlying distribution \\(P(Y)\\), then we can write: $$ Y \\sim \\mathrm{Bernoulli}(\\phi), $$ which implies we sample from the distribution. If \\(10\\%\\) of the MNIST dataset images are \\(3\\), then: $$ P(Y=1)=\\phi = 0.1.$$ </p> <p>In binary classification, a neural network often outputs a probability estimate: $$ \\hat{\\phi}(x) \\approx P(Y=1 \\mid X=x), $$ where \\(x\\) is an input image. The prediction can then be interpreted as sampling from a Bernoulli distribution: $$ \\hat{Y} \\sim \\mathrm{Bernoulli}(\\hat{\\phi}(x)). $$</p> <p>For instance, if the model outputs \\(\\hat{\\phi}(x)=0.92\\), it means the model assigns a \\(92\\%\\) probability that the given MNIST image is a \\(3\\).</p>"},{"location":"mathematics/03_probability/#categorical-multinoulli","title":"Categorical (Multinoulli)","text":"<p>The categorical distribution (also called the multinoulli distribution<sup>5</sup>) generalizes Bernoulli to more than two outcomes. It models a discrete variable with \\(k\\) possible states: $$ X \\in {1,2,\\dots,k}. $$</p> <p>It is parameterized by a probability vector: $$ p = (p_1, p_2, \\dots, p_k), \\qquad \\sum_{i=1}^k p_i = 1. $$</p> <p>The probability mass function is: $$ P(X=i) = p_i. $$</p> <p>Important</p> <p>Do not confuse the multinoulli (categorical) distribution with the multinomial distribution. A categorical distribution describes a single outcome from \\(k\\) categories (one draw). A multinomial distribution describes a vector of counts showing how many times each category occurs after \\(n\\) draws. In other words, multinomial is not another name for categorical \u2014 it is a different distribution with a different type of output, which can be seen as a special case of the multinomial distribution when \\(n=1\\).</p> <p>Example</p> <p>In MNIST digit classification, the label for a single image is modeled as a categorical random variable: $$ Y \\in {0,1,2,\\dots,9}. $$ We can write \\(Y \\sim \\mathrm{Categorical}(p),\\) where: $$ p = (p_0,p_1,\\dots,p_9), \\qquad \\sum_{i=0}^9 p_i = 1. $$</p> <p>A neural network outputs a probability vector using softmax: $$ \\hat{p}(x) \\approx P(Y \\mid X=x). $$ For example, if the model outputs \\(\\hat{p}_3(x)=0.85\\), this means the model assigns an \\(85\\%\\) probability that the image is the digit \\(3\\).</p> <p>Now suppose we take a mini-batch of \\(n=100\\) MNIST images and count how many of each digit appear in the batch. We might obtain a count vector such as: $$ (c_0,c_1,\\dots,c_9) = (8,11,9,7,10,12,6,14,13,10), $$ where $ \\sum_{i=0}^9 c_i = 100. $ This count vector is not categorical anymore. It is modeled by a multinomial distribution: $$ (c_0,c_1,\\dots,c_9) \\sim \\mathrm{Multinomial}(n=100, p). $$</p> <p>In other words, categorical (multinoulli) describes one label, while multinomial describes counts of labels across many samples.</p> <p>Exercise</p> <p>Let \\(Y \\sim \\mathrm{Categorical}(p)\\) with \\(k\\) categories, and represent the label as a one-hot vector \\(e_Y \\in \\mathbb{R}^k\\). Show that \\(\\mathbb{E}[e_Y] = p.\\) Then derive the covariance matrix \\(\\mathrm{Cov}(e_Y)\\).</p>"},{"location":"mathematics/03_probability/#normal-gaussian","title":"Normal (Gaussian)","text":"<p>Normal distribution, also called Gaussian distribution<sup>6</sup>, is the most important continuous distribution in machine learning. It is defined as: $$ \\mathcal{N}(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\Big(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\Big). $$</p> <p>Here, \\(\\mu\\) is the mean (center), \\(\\sigma^2\\) is the variance (spread), and \\(\\sigma\\) is the standard deviation of the distribution.</p>      For the normal distribution, the values less than one standard deviation from the mean account for 68.27% of the set; while two standard deviations from the mean account for 95.45%; and three standard deviations account for 99.73% ~ M. W. Toews - Own work, based (in concept) on figure by Jeremy Kemp, on 2005-02-09, CC BY 2.5, Link <p>This distribution has a characteristic bell curve shape: values near \\(\\mu\\) are most likely, and values far from \\(\\mu\\) become increasingly rare.</p> <p>Note</p> <p>The normal distribution is called normal because it became the standard default model for random measurement errors and noise in many scientific fields. For example, if you repeatedly measure the same quantity (temperature, weight, sensor voltage, satellite pixel reflectance), the errors often cluster around \\(0\\), while large errors are rare. The normal distribution captures exactly this pattern: small deviations are common, large deviations happen but are uncommon. </p> <p>This idea also appears in everyday life: most human characteristics such as height, reaction time, exam scores, and typing speed tend to cluster around an average, with fewer people being extremely low or extremely high. While not everything in nature is perfectly Gaussian, the normal distribution often provides a good first approximation of \"typical variation.\"</p> <p>Note</p> <p>The central limit theorem (CLT) says that when many small independent random effects add together, their sum tends to become approximately Gaussian, even if the individual effects are not Gaussian. For example, the final value of a noisy measurement is often the sum of many tiny disturbances: sensor imperfections, rounding, thermal noise, vibration, lighting changes, etc. Even if each disturbance has its own distribution, the combined noise often looks normal.</p> <p>The law of large numbers (LLN) explains why averages become stable. If we sample \\(X_1,X_2,\\dots,X_n\\) from the same distribution and compute the average:  \\(\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i,\\) then \\(\\bar{X}_n\\) tends to get closer to the true mean \\(\\mathbb{E}[X]\\) as \\(n\\) becomes large.  </p> <p>A simple example is coin tossing: with only \\(10\\) tosses, the fraction of heads may be far from \\(0.5\\), but with \\(10,000\\) tosses it will almost always be close to \\(0.5\\). In deep learning, this explains why large batches produce more stable gradient estimates than small batches.</p> <p>Tip</p> <p>See the video for a visual intuition and origins where the normal distribution formula comes from.</p> <p>In deep learning, we often model a random vector: $ X \\in \\mathbb{R}^n.$ The most important distribution over vectors is the multivariate normal distribution: $$ X \\sim \\mathcal{N}(\\mu,\\Sigma), $$ where \\(\\mu \\in \\mathbb{R}^n\\) is the mean vector and \\(\\Sigma \\in \\mathbb{R}^{n\\times n}\\) is the covariance matrix. The probability density function is:</p>  $$ \\mathcal{N}(x;\\mu,\\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\Sigma)}} \\exp\\Big(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\Big). $$  <p>The covariance matrix \\(\\Sigma\\) determines the shape of the Gaussian distribution. If different dimensions are correlated, the distribution becomes tilted, if variances are large, the distribution becomes wide.</p> <p>Note</p> <p>A very common special case is the isotropic normal, where all dimensions have the same variance and are uncorrelated:  $$ \\Sigma = \\sigma^2 I. $$ In this case, the density calculation simplifies to \\(\\Sigma^{-1} = \\frac{1}{\\sigma^2}I\\) and \\(\\det(\\Sigma) = (\\sigma^2)^n\\). The exponent also becomes proportional to the squared Euclidean distance \\(\\|x-\\mu\\|^2\\). These all are extremely useful in deep learning because the computations become fast and stable even in high dimensions.</p> <p>A slightly more flexible assumption is a diagonal normal: $$ \\Sigma = \\mathrm{diag}(\\sigma_1^2,\\sigma_2^2,\\dots,\\sigma_n^2), $$ which allows each dimension to have its own variance but still avoids expensive full matrix inversion.</p> <p>Finally, it is often convenient to use the precision matrix, defined as: $$ \\beta = \\Sigma^{-1}. $$ Precision describes the same uncertainty information as covariance, but it appears directly in the Gaussian exponent and is often easier to use in derivations.</p>"},{"location":"mathematics/03_probability/#exponential","title":"Exponential","text":"<p>Exponential distribution models the waiting time until an event happens. It is defined on \\(x \\ge 0\\) and is parameterized by a rate \\(\\lambda &gt; 0\\): $$ p(x;\\lambda) = \\lambda e^{-\\lambda x}, \\qquad x \\ge 0. $$</p> <p>Exercise</p> <p>Derive the expectation and variance of the exponential distribution.</p> <p>Note</p> <p>A key property of the exponential distribution is the memoryless property: $$ P(X &gt; s+t \\mid X &gt; s) = P(X &gt; t). $$ This means that if we have already waited \\(s\\) time units, the remaining waiting time still follows the same distribution.</p>"},{"location":"mathematics/03_probability/#laplace","title":"Laplace","text":"<p>Laplace distribution is a continuous distribution that resembles a normal distribution but has a sharper peak and heavier tails. It is defined as: $$ p(x;\\mu,b) = \\frac{1}{2b} \\exp\\Big(-\\frac{|x-\\mu|}{b}\\Big), \\qquad b&gt;0. $$</p> <p>Here, \\(\\mu\\) is the mean (center), and \\(b\\) is a scale parameter controlling spread. Its variance is: $$ \\mathrm{Var}(X) = 2b^2. $$</p> <p>Laplace noise is often used when we want a model that is more tolerant to outliers than a Gaussian. It is also closely related to \\(L_1\\) regularization: if we assume model parameters follow a Laplace prior, maximizing the posterior corresponds to an \\(L_1\\) penalty, which encourages sparsity.</p>"},{"location":"mathematics/03_probability/#dirac-delta-and-empirical-distribution","title":"Dirac Delta and Empirical Distribution","text":"<p>In deep learning, we often treat a dataset as if it defines a probability distribution. But a dataset is finite: it contains only a limited set of observed points. To represent such a distribution mathematically, we use the Dirac delta function.</p>      Schematic representation of the Dirac delta function. The height of the arrow is usually meant to specify the value of any multiplicative constant, which will give the area under the function. ~ Qef, CC BY-SA 3.0, Link <p>The Dirac delta \\(\\delta(x)\\) is not a normal function, but a mathematical object (a distribution) that behaves like a probability density concentrated at a single point. It satisfies:</p> <ul> <li>\\(\\delta(x) = 0\\) for all \\(x \\ne 0\\)</li> <li>\\(\\int_{-\\infty}^{\\infty} \\delta(x)\\,dx = 1\\)</li> </ul> <p>More generally, a delta centered at a point \\(a\\) is written as \\(\\delta(x-a)\\) and satisfies: $$ \\int_{-\\infty}^{\\infty} \\delta(x-a)\\,dx = 1. $$</p> <p>Important</p> <p>The notation \\(\\delta(x-a)\\) does not mean ordinary subtraction inside a normal function. It simply means a Dirac delta distribution centered at \\(a\\).  </p> <p>Intuitively, \\(\\delta(x-a)\\) represents an \"infinitely sharp spike\" located exactly at \\(x=a\\), with total area \\(1\\).  This is why it behaves like a tool that extracts the value of a function at \\(a\\): $$ \\int_{-\\infty}^{\\infty} f(x)\\,\\delta(x-a)\\,dx = f(a). $$</p> <p>So \\(\\delta(x-a)\\) is best understood as \"a delta located at \\(a\\)\".</p>"},{"location":"mathematics/03_probability/#empirical","title":"Empirical","text":"<p>Suppose we have a dataset of \\(m\\) samples: $$ {x^{(1)}, x^{(2)}, \\dots, x^{(m)}}. $$</p> <p>Empirical distribution \\(\\hat{p}(x)\\) is defined as: $$ \\hat{p}(x) = \\frac{1}{m} \\sum_{i=1}^{m} \\delta(x-x^{(i)}). $$</p> <p>This means that the dataset assigns equal probability mass \\(\\frac{1}{m}\\) to every observed sample.</p> <p>Note</p> <p>The empirical distribution in machine learning represents the distribution we actually train on. When we minimize training loss, we are minimizing an expectation under \\(\\hat{p}(x)\\) rather than under the true unknown distribution \\(p(x)\\).</p> <p>The empirical expectation of a function \\(f(x)\\) is:</p>  $$ \\mathbb{E}[f(X)] = \\int f(x)\\,\\hat{p}(x)\\,dx = \\frac{1}{m}\\sum_{i=1}^m f(x^{(i)}), \\qquad X \\sim \\hat{p}. $$  <p>So the standard dataset average used in deep learning is literally an expectation under the empirical distribution.</p> <p>Note</p> <p>The empirical distribution is a discrete approximation of the true data-generating distribution. Training a neural network is essentially an attempt to learn a model that generalizes beyond \\(\\hat{p}(x)\\) and performs well on the true distribution \\(p(x)\\).</p>"},{"location":"mathematics/03_probability/#mixture-distributions","title":"Mixture Distributions","text":"<p>Mixture distribution is a probability distribution formed by combining multiple simpler distributions. The idea is that the data may come from several different underlying sources, and each source has its own distribution. Suppose we have \\(K\\) component distributions: $$ p_1(x), p_2(x), \\dots, p_K(x), $$ and mixture weights: $$ \\pi_1,\\pi_2,\\dots,\\pi_K, \\qquad \\pi_k \\ge 0, \\qquad \\sum_{k=1}^K \\pi_k = 1. $$</p> <p>Then the mixture distribution is: $$ p(x) = \\sum_{k=1}^K \\pi_k p_k(x). $$</p> <p>This can be interpreted as a two-step sampling process:</p> <ol> <li>First sample a component index:    $    Z \\sim \\mathrm{Categorical}(\\pi_1,\\dots,\\pi_K).    $</li> <li>Then sample from the corresponding component distribution:    $    X \\sim p_Z(x).    $</li> </ol> <p>Note</p> <p>The variable \\(Z\\) is called a latent variable because it is not directly observed, but it explains which component generated the data.</p> <p>Note</p> <p>Mixture models are widely used in probabilistic modeling because many real-world datasets are multi-modal. For example, the distribution of human heights in a population is not perfectly Gaussian, because it is better approximated as a mixture of two Gaussians (male and female). Similarly, pixel intensities in images often come from mixtures of different object materials, lighting conditions, and textures.</p>      An example of Gaussian Mixture in image segmentation with grey histogram ~ KazukiAmakawa - Own work, CC BY-SA 4.0, Link <p>The most common mixture model is the Gaussian mixture model (GMM): $$ p(x) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x;\\mu_k,\\Sigma_k). $$</p> <p>Each component is a Gaussian with its own mean \\(\\mu_k\\) and covariance matrix \\(\\Sigma_k\\). GMMs are expressive enough to approximate many complex distributions, while still being mathematically tractable.</p> <p>Note</p> <p>Mixture models are a natural step toward deep generative models. Many modern models (including VAEs and diffusion models) can be viewed as learning complex mixtures in high-dimensional spaces.</p>"},{"location":"mathematics/03_probability/#common-functions","title":"Common Functions","text":"<p>Deep learning models often output unconstrained real numbers. However, many probabilistic parameters must satisfy constraints such as being in \\((0,1)\\) or summing to \\(1\\). For this reason, we use special nonlinear functions that map real-valued inputs into valid probability domains. The most common functions are described below.</p>"},{"location":"mathematics/03_probability/#sigmoid","title":"Sigmoid","text":"<p>The logistic (sigmoid) function maps any real number to \\((0,1)\\): $$ \\sigma(x) = \\frac{1}{1+e^{-x}} = \\frac{\\exp(x)}{\\exp(x)+\\exp(0)}. $$</p> <p>It is widely used to parameterize Bernoulli probabilities: $$ \\phi = \\sigma(z), \\qquad Y \\sim \\mathrm{Bernoulli}(\\phi). $$</p>     The logistic (sigmoid) curve ~ Qef (talk) - Created from scratch with gnuplot, Public Domain, Link <p>Sigmoid is differentiable and also satisfies the symmetry identity: $$ 1-\\sigma(x)=\\sigma(-x). $$</p> <p>Its inverse function is called the logit: $$ \\forall x \\in (0,1), \\qquad \\sigma^{-1}(x) = \\log\\Big(\\frac{x}{1-x}\\Big). $$</p> <p>Important</p> <p>Sigmoid saturates for large \\(|x|\\). When \\(x \\gg 0\\), \\(\\sigma(x)\\approx 1\\), and when \\(x \\ll 0\\), \\(\\sigma(x)\\approx 0\\). In both cases, the gradient \\(\\frac{d}{dx}\\sigma(x)\\) becomes close to \\(0\\), which slows down learning due to vanishing gradients.   For this reason, sigmoid is rarely used as a hidden-layer activation in modern deep networks, and is usually replaced by ReLU or its variants.</p> <p>Exercise</p> <p>Find the derivative of the sigmoid function.</p>"},{"location":"mathematics/03_probability/#relu","title":"ReLU","text":"<p>Important</p> <p>ReLU is not a probability concept. It does not parameterize a probability distribution and does not map values into a valid probability domain. It is primarily an activation function used in neural network architectures to improve optimization and gradient flow. We mention it here only because sigmoid saturation motivates why modern deep networks typically use ReLU-like activations in hidden layers.</p> <p>The rectified linear unit (ReLU) is the most commonly used activation function in modern deep learning. It is defined as: $$ \\mathrm{ReLU}(x)=\\max(0,x). $$</p> <p>ReLU is popular because it is simple, fast to compute, and avoids the strong saturation effect of sigmoid and tanh.</p>      Plot of the ReLU (blue) and GELU (green) functions near x = 0 ~ Ringdongdang - Own work, CC BY-SA 4.0, Link <p>ReLU is piecewise linear, so its derivative is simple: $$ \\frac{d}{dx}\\mathrm{ReLU}(x) = \\begin{cases} 0, &amp; x &lt; 0, \\\\ 1, &amp; x &gt; 0. \\end{cases} $$</p> <p>Note</p> <p>ReLU is not differentiable at \\(x=0\\). However, researchers eventually realized that, this is not a practical issue in deep learning, because \\(x=0\\) occurs with probability nearly zero for continuous-valued activations. In implementations, the gradient at \\(0\\) is usually defined as \\(0\\) (a valid subgradient choice).</p> <p>Important</p> <p>ReLU has zero gradient for all negative inputs. If a neuron consistently outputs negative values, it may stop learning completely. This is called the dying ReLU problem. Variants such as Leaky ReLU and GELU are often used to reduce this effect.</p>"},{"location":"mathematics/03_probability/#softplus","title":"Softplus","text":"<p>The softplus function maps \\(\\mathbb{R}\\) to \\((0,\\infty)\\): $$ \\zeta(x) = \\log(1+\\exp(x)). $$</p> <p>Note</p> <p>Softplus is useful when a model parameter must be strictly positive (e.g variance \\(\\sigma^2 &gt; 0\\)).</p>      Plot of the softplus function and the ramp function ~ Nbarth - This vector image includes elements that have been taken or adapted from this file:, CC0, Link <p>Softplus is closely connected to sigmoid. In particular: $$ \\log \\sigma(x) = -\\zeta(-x). $$</p> <p>And its derivative is exactly sigmoid:</p>  $$ \\frac{d}{dx}\\zeta(x) = \\frac{d}{dx}\\log(1+\\exp(x)) = \\frac{\\exp(x)}{1+\\exp(x)} = \\sigma(x). $$  <p>This is a useful fact in deep learning: softplus behaves like a smooth version of ReLU, but its gradient changes smoothly between \\(0\\) and \\(1\\). The inverse of softplus is: $$ \\forall x&gt;0, \\qquad \\zeta^{-1}(x)=\\log(\\exp(x)-1). $$</p> <p>A useful symmetry identity is: $$ \\zeta(x)-\\zeta(-x)=x. $$</p> <p>Note</p> <p>In practice, we rarely implement softplus as \\(\\log(1+\\exp(x))\\) directly, because \\(\\exp(x)\\) can overflow for large \\(x\\). Instead, deep learning libraries use numerically stable implementations.  </p> <p>Example</p> <p>PyTorch provides: <pre><code>torch.nn.functional.softplus(x)\n</code></pre> which computes softplus safely even when \\(x\\) has large magnitude.</p> <p>Softplus is commonly used when a model must output a strictly positive parameter (such as a variance), since it guarantees positivity while still being smooth and differentiable.</p>"},{"location":"mathematics/03_probability/#logarithm","title":"Logarithm","text":"<p>Recall that the logarithm function \\(\\log(x)\\) transforms products into sums. If we have independent samples, probabilities multiply: $$ P(x^{(1)},\\dots,x^{(m)}) = \\prod_{i=1}^m P(x^{(i)}). $$</p> <p>Taking the logarithm converts this product into a sum: $$ \\log P(x^{(1)},\\dots,x^{(m)}) = \\sum_{i=1}^m \\log P(x^{(i)}). $$</p> <p>This is the main reason why, say, maximum likelihood estimation is almost always performed using the log-likelihood instead of the raw likelihood.</p> <p>Note</p> <p>In machine learning, model parameters are often learned by maximum likelihood estimation (MLE).  We choose parameters \\(\\theta\\) that make the observed dataset most probable under the model. Suppose we have i.i.d. samples: $$ D={y^{(1)},\\dots,y^{(m)}}. $$</p> <p>The likelihood of the dataset is: $$ p(D \\mid \\theta) = \\prod_{i=1}^{m} p!\\big(y^{(i)} \\mid \\theta\\big). $$</p> <p>The MLE estimate \\(\\hat{\\theta}_{\\mathrm{MLE}}\\) is defined as the value of \\(\\theta\\) that maximizes this likelihood. In practice we maximize the log-likelihood instead: $$ \\log p(D \\mid \\theta) = \\sum_{i=1}^{m} \\log p!\\big(y^{(i)} \\mid \\theta\\big). $$</p> <p>Exercise</p> <p>Suppose we observe \\(m\\) i.i.d. samples \\(y^{(1)},\\dots,y^{(m)}\\) from a Bernoulli distribution with parameter \\(\\theta\\). Write the likelihood function \\(L(\\theta \\mid y^{(1)},\\dots,y^{(m)})\\) as a product. Then rewrite it as a log-likelihood (a sum).</p> <p>The logarithm is only defined for \\(x&gt;0\\). In particular: $$ \\log(0) = -\\infty $$</p> <p>Note</p> <p>This matters in deep learning because losses often contain \\(\\log(p)\\) terms. If a model assigns probability \\(p=0\\) to the true outcome, the loss becomes infinite. In practice, this is handled by using numerically stable implementations (computing loss from logits) or by clamping probabilities with a small constant \\(\\epsilon&gt;0\\): $$ \\log(p) \\;\\;\\to\\;\\; \\log(\\max(p,\\epsilon)). $$</p>      Plots of logarithm functions, with three commonly used bases. ~ Richard F. Lyon - made myself, alt version of Logarithm plots.svg with better text, CC BY-SA 3.0, Link <p>Logarithm is strictly increasing (monotonic): $$ x_1 &lt; x_2 \\quad\\Rightarrow\\quad \\log(x_1) &lt; \\log(x_2). $$</p> <p>Therefore, maximizing \\(P(x)\\) is equivalent to maximizing \\(\\log P(x)\\): $$ \\arg\\max_x P(x) = \\arg\\max_x \\log P(x). $$</p> <p>Note</p> <p>The derivative of log is \\(\\frac{1}{x}.\\) This derivative becomes very large when \\(x\\) is close to \\(0\\), which is one reason why log strongly penalizes assigning very small probability to true outcomes.</p>"},{"location":"mathematics/03_probability/#softmax","title":"Softmax","text":"<p>The softmax function maps logits \\(z\\in\\mathbb{R}^k\\) into a probability vector \\(p\\in\\mathbb{R}^k\\): $$ p = \\mathrm{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{k} \\exp(z_j)}. $$</p> <p>Softmax is also invariant to adding the same constant to all logits: $$ \\mathrm{softmax}(z)=\\mathrm{softmax}(z+c). $$</p> <p>Important</p> <p>For numerical stability, softmax is usually computed as: $$ \\mathrm{softmax}(z) = \\frac{\\exp(z-\\max(z))}{\\sum_{j=1}^k \\exp(z_j-\\max(z))}. $$</p> <p>Note</p> <p>A logit is an unconstrained score in \\((-\\infty,\\infty)\\) that becomes a probability only after applying a transformation. For binary classification: $$ \\phi=\\sigma(z), \\qquad z=\\mathrm{logit}(\\phi). $$ For multiclass classification:  $\\(p=\\mathrm{softmax}(z).\\)$</p> <p>Working with logits is often preferred in deep learning because logits are numerically stable and easier to optimize than probabilities directly.</p>"},{"location":"mathematics/03_probability/#measure-theory","title":"Measure Theory","text":"<p>When working with continuous probability distributions, some technical details require ideas from measure theory. In deep learning literature, we will encounter a few terms.</p> <p>A set \\(A\\) is said to have measure zero if its total \"size\" is zero under integration. For example, a single point has measure zero. Any countable set of points also has measure zero. For example, the set of all rational numbers has measure zero, even though it is infinite.</p> <p>A property is said to hold almost everywhere if it holds everywhere except on a measure-zero set. This means the property may fail on some special cases, but those cases occupy negligible space and do not affect integrals.</p> <p>This terminology matters because many results that are always true in discrete probability only hold almost everywhere in the continuous case. In practice, these exceptions can usually be ignored.</p> <p>Note</p> <p>This is why deep learning often ignores edge cases. For example, ReLU is not differentiable at \\(x=0\\), but this does not matter in practice because the probability of hitting exactly \\(x=0\\) is almost zero for continuous-valued activations.</p> <ol> <li> <p>Kolmogorov, A.N. (1933, 1950). Foundations of the theory of probability. New York, US: Chelsea Publishing Company.\u00a0\u21a9</p> </li> <li> <p>A Dutch book argument says that if your probability assignments are inconsistent, someone can design a set of bets that guarantees you lose money no matter what happens. For example, if you assign \\(P(A)=0.6\\) and \\(P(\\neg A)=0.6\\), then you are claiming both an event and its opposite are more likely than not. A bettor could sell you a bet on \\(A\\) and also sell you a bet on \\(\\neg A\\), and you would overpay in total, while only one of them can ever pay out. This guarantees a loss. The probability axioms prevent such contradictions.\u00a0\u21a9</p> </li> <li> <p>A real coin is not infinitely thin, so in principle it could land on its edge. In practice this outcome is extremely rare, so it is usually ignored and the sample space is approximated as having only two outcomes.\u00a0\u21a9</p> </li> <li> <p>The term marginal is said to come from the traditional way of computing these sums on paper: one writes the joint distribution in a table and records the row and column totals in the margins of the page.\u00a0\u21a9</p> </li> <li> <p>The term multinoulli was popularized by Murphy (2012) as a playful name meaning \"many Bernoullis.\"\u00a0\u21a9</p> </li> <li> <p>The distribution is called Gaussian because it was studied extensively by Carl Friedrich Gauss, especially in connection with measurement errors.\u00a0\u21a9</p> </li> </ol>"},{"location":"mathematics/04_information/","title":"Information Theory","text":"9 Feb 2026 <p>Information theory is the mathematical framework for measuring how much uncertainty or information is contained in a probability distribution. While probability theory tells us how to represent uncertainty, information theory tells us how to quantify it. In deep learning, information theory appears everywhere: in loss functions, in regularization, in probabilistic modeling (VAEs), and in the general idea of compressing data into meaningful representations.</p> <p>Info</p> <p>The following source was consulted in preparing this material: Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press. Chapter 3: Probability and Information Theory.</p> <p>Important</p> <p>Some concepts in this material are simplified for pedagogical purposes. These simplifications slightly reduce precision but preserve the core ideas relevant to deep learning.</p> <p>Note</p> <p>Information theory was established by Claude Shannon in his famous 1948 paper A Mathematical Theory of Communication, where he introduced the idea that information can be measured quantitatively, just like mass or energy. Shannon originally developed the theory to study how efficiently messages can be transmitted through noisy communication channels (e.g. radio). Today, the same mathematical tools are fundamental in deep learning, because training a model often means minimizing uncertainty and compressing information into useful representations.</p>"},{"location":"mathematics/04_information/#self-information","title":"Self-Information","text":"<p>The core idea of information theory is simple: Learning that an unlikely event happened gives more information than learning that a likely event happened. </p> <p>Example</p> <p>Learning that \"the sun rose today\" is not informative, but learning that \"a solar eclipse happened today\" is informative.</p> <p>To measure this idea mathematically, we want a quantity that behaves like surprise:</p> <ul> <li>If an event is very likely, it should have low surprise.</li> <li>If an event is rare, it should have high surprise.</li> </ul> <p>If two independent events occur, their probabilities multiply, so their surprise would multiply as well. Similar to log-likelihood, to convert multiplication into addition, we can take the logarithm.</p> <p>Important</p> <p>Multiplying probabilities quickly produces extremely small numbers. For example, if we observe \\(100\\) independent events each with probability \\(0.01\\), then the joint probability \\(p(x_{1:100})\\) is: $$ (0.01)^{100} = 10^{-200}, $$ which is essentially zero in floating-point arithmetic.   This causes numerical underflow and also leads to very small gradients when optimizing likelihoods directly. Taking the logarithm fixes this problem: $$ \\log (0.01^{100}) = 100 \\log(0.01) \\approx -460.5, $$ which is a normal-sized number. This is why deep learning almost always optimizes log-likelihood instead of likelihood: products become sums, computations stay stable, and gradients remain usable.</p> <p>Using the identity \\(\\log(1/u)=-\\log(u)\\), we obtain the self-information (also called surprisal, information content, or Shannon information) of an event \\(X=x\\): $$ I(x) = -\\log P(X=x). $$</p> <p>This definition satisfies three key properties defined by Shannon:</p> <ol> <li>Certain events contain no information: if \\(P(x)=1\\), then \\(I(x)=0\\).</li> <li>Unlikely events contain more information: smaller \\(P(x)\\) gives larger \\(I(x)\\).</li> <li>Independent information is additive: if events are independent, their probabilities multiply, so their information adds.</li> </ol> <p>Note</p> <p>In this course, \\(\\log\\) always means the natural logarithm (base \\(e\\)). When using natural logs, information is measured in nats rather than bits.</p>"},{"location":"mathematics/04_information/#entropy","title":"Entropy","text":"<p>Self-information measures the surprise of a single outcome. But often we want a single number that summarizes the uncertainty of an entire distribution. In information theory, the (Shannon) entropy of a discrete random variable \\(X\\) is the expected self-information:</p>  $$ H(X) = \\mathbb{E}[I(X)] = -\\sum_x P(X=x)\\log P(X=x). $$  <p>It implies that if \\(X\\) is almost always the same value, there is little uncertainty, so entropy is low. If \\(X\\) has many equally likely outcomes, uncertainty is high, so entropy is high. Discrete entropy is always nonnegative: \\(H(X)\\ge 0.\\)</p> <p>Note</p> <p>Entropy is maximized by the uniform distribution. If \\(X\\) takes \\(k\\) outcomes with equal probability \\(P(X=i)=1/k\\), then: $$ H(X) = -\\sum_{i=1}^{k}\\frac{1}{k}\\log\\frac{1}{k} = \\log k. $$ This matches intuition: choosing among more equally likely options is more uncertain.</p> <p>Example</p> <p>Consider a Bernoulli random variable \\(X\\sim \\mathrm{Bernoulli}(\\phi)\\): $$ P(X=1)=\\phi,\\qquad P(X=0)=1-\\phi. $$ Its entropy is: $$ H(X) = -\\phi\\log\\phi-(1-\\phi)\\log(1-\\phi). $$ This entropy is highest at \\(\\phi=0.5\\) (maximum uncertainty) and approaches \\(0\\) as \\(\\phi\\to 0\\) or \\(\\phi\\to 1\\) (almost no uncertainty).</p> <p>For a continuous random variable with density \\(p(x)\\), the analogous quantity is: $$ H(X) = -\\int p(x)\\log p(x)\\,dx. $$</p> <p>Differential (continuous) entropy measures uncertainty in a continuous distribution, but it behaves differently from discrete entropy. This happens because \\(p(x)\\) is a probability density, not a probability. A density can be greater than \\(1\\), so \\(\\log p(x)\\) can be positive. Hence, differential entropy can be negative.</p> <p>Another important difference is that differential entropy depends on the units of measurement. If we scale a continuous variable, its differential entropy changes. If \\(Y=aX\\) for some constant \\(a&gt;0\\), then: $$ H(Y)=H(X)+\\log a. $$</p> <p>Note</p> <p>This means that differential entropy is not an absolute measure of \"how much uncertainty exists.\" Changing meters to centimeters changes the value.</p> <p>Example</p> <p>Let \\(X\\sim U(0,1)\\), so \\(p(x)=1\\) on \\([0,1]\\). Then: $$ H(X) = -\\int_0^1 1\\cdot \\log(1)\\,dx = 0. $$</p> <p>Now let \\(Y\\sim U(0,0.1)\\), so \\(p(y)=10\\) on \\([0,0.1]\\). Then: $$ H(Y) = -\\int_0^{0.1} 10\\log(10)\\,dy = -\\log(10), $$ which is negative.</p>"},{"location":"mathematics/04_information/#conditional-entropy","title":"Conditional Entropy","text":"<p>Sometimes uncertainty is reduced after observing another variable. The conditional entropy measures the remaining uncertainty in \\(X\\) after we know \\(Y\\):</p>  $$ H(X \\mid Y) = -\\sum_{x,y} P(x,y)\\log P(x \\mid y). $$  <p>Equivalently, it is the expected entropy of the conditional distribution:</p>  $$ H(X \\mid Y) = \\sum_y P(y)\\,H(X \\mid Y=y). $$  <p>If \\(X\\) is completely determined by \\(Y\\), then \\(H(X\\mid Y)=0\\). If \\(X\\) and \\(Y\\) are independent, then observing \\(Y\\) gives no information about \\(X\\), so \\(H(X\\mid Y)=H(X)\\).</p>"},{"location":"mathematics/04_information/#cross-entropy","title":"Cross-Entropy","text":"<p>Entropy \\(H(P)\\) measures the uncertainty of a distribution \\(P\\). In learning, we usually have two distributions:</p> <ul> <li>The true (data) distribution \\(P\\) that generates labels.</li> <li>A model distribution \\(Q\\) that tries to predict them.</li> </ul> <p>Cross-entropy measures the expected number of nats needed to encode outcomes generated by the true distribution \\(P\\), when we use an encoding scheme (or predictive model) that assumes the distribution is \\(Q\\). In other words, it measures how well \\(Q\\) predicts samples coming from \\(P\\). If \\(Q\\) assigns low probability to events that happen frequently under \\(P\\), the cross-entropy becomes large. $$ H(P,Q) = -\\sum_x P(x)\\log Q(x). $$ Compare this with entropy: $$ H(P) = -\\sum_x P(x)\\log P(x). $$</p> <p>The only difference is what appears inside the logarithm: entropy uses the true distribution \\(P\\), while cross-entropy uses the model distribution \\(Q\\).</p> <p>Example</p> <p>Suppose \\(X\\) represents the outcome of a biased coin, and the true distribution is: $$ P(X=\\text{heads}) = 0.9, \\qquad P(X=\\text{tails}) = 0.1. $$</p> <p>The entropy of the true distribution is:  $$ H(P) = -0.9\\log(0.9) - 0.1\\log(0.1) \\approx 0.325 \\text{ nats}. $$  Now suppose we build a wrong model \\(Q\\) that assumes the coin is fair: $$ Q(X=\\text{heads}) = 0.5, \\qquad Q(X=\\text{tails}) = 0.5. $$</p> <p>The cross-entropy of \\(P\\) relative to \\(Q\\) is:  $$ H(P,Q) = -0.9\\log(0.5) - 0.1\\log(0.5) = -\\log(0.5) = \\log 2 \\approx 0.693 \\text{ nats}. $$ </p> <p>So even though the true coin has relatively low uncertainty (\\(H(P)\\approx 0.325\\)), using the wrong model distribution \\(Q\\) increases the expected code length to \\(H(P,Q)\\approx 0.693\\). </p> <p>Intuitively, this happens because the model assigns too little probability to the outcome that occurs most of the time (heads). Cross-entropy penalizes this mismatch. If we instead choose \\(Q=P\\), then: $$ H(P,P)=H(P), $$ meaning cross-entropy becomes minimal when the assumed distribution matches the true one.</p>"},{"location":"mathematics/04_information/#kullback-leibler-divergence","title":"Kullback-Leibler Divergence","text":"<p>Cross-entropy answers a practical question: If the world follows \\(P\\), but we build a model as if it were \\(Q\\), how costly is that mistake on average? But sometimes we want a cleaner question: How much worse is \\(Q\\) compared to the true distribution \\(P\\)?</p> <p>The answer is the extra penalty we pay when we use \\(Q\\) instead of \\(P\\). This difference is exactly the Kullback\u2013Leibler (KL) divergence: $$ D_{\\mathrm{KL}}(P|Q) = H(P,Q)-H(P). $$</p> <p>Equivalently, it can be written as: $$ D_{\\mathrm{KL}}(P|Q) = \\sum_x P(x)\\log\\frac{P(x)}{Q(x)}. $$</p> <p>Or for continuous distributions: $$ D_{\\mathrm{KL}}(P | Q) = \\int p(x)\\log \\frac{p(x)}{q(x)}\\,dx. $$</p> <p>So KL divergence measures the gap between cross-entropy and entropy: it is zero when \\(P=Q\\), and grows as \\(Q\\) becomes a worse approximation of \\(P\\). </p> <p>Note</p> <p>KL divergence is always nonnegative: $$ D_{\\mathrm{KL}}(P|Q)\\ge 0, $$ and equals \\(0\\) if and only if \\(P=Q\\) (almost everywhere in the continuous case).</p> <p>Important</p> <p>KL divergence is not symmetric: $$ D_{\\mathrm{KL}}(P | Q) \\ne D_{\\mathrm{KL}}(Q | P). $$ So it is not a true distance metric, but it is still one of the most important ways to compare distributions in deep learning. </p> <p>A key identity connects entropy, cross-entropy, and KL divergence: $$ H(P,Q) = H(P) + D_{\\mathrm{KL}}(P | Q). $$</p> <p>This immediately explains why minimizing cross-entropy makes sense. Since \\(H(P)\\) does not depend on the model \\(Q\\), minimizing \\(H(P,Q)\\) over \\(Q\\) is equivalent to minimizing KL divergence: $$ \\arg\\min_Q H(P,Q) = \\arg\\min_Q D_{\\mathrm{KL}}(P|Q). $$</p> <p>Note</p> <p>In classification, minimizing cross-entropy is the same as minimizing \\(D_{\\mathrm{KL}}(P\\|Q)\\) between the true label distribution and the model predictions.</p> <p>Exercise</p> <p>Let \\(P=\\mathcal{N}(\\mu_1,\\sigma_1^2)\\) and \\(Q=\\mathcal{N}(\\mu_2,\\sigma_2^2)\\). Derive a closed-form expression for \\(D_{\\mathrm{KL}}(P\\|Q)\\) in terms of \\(\\mu_1,\\mu_2,\\sigma_1,\\sigma_2\\).</p>"},{"location":"mathematics/04_information/#mutual-information","title":"Mutual Information","text":"<p>Entropy measures uncertainty in a single random variable. Mutual information measures how strongly two random variables are related: how much knowing one reduces uncertainty about the other. Between \\(X\\) and \\(Y\\), it is defined as: $$ I(X;Y) = \\sum_{x,y} P(x,y)\\log\\frac{P(x,y)}{P(x)P(y)}. $$</p> <p>For continuous variables: $$ I(X;Y) = \\int p(x,y)\\log\\frac{p(x,y)}{p(x)p(y)}\\,dx\\,dy. $$</p> <p>Note</p> <p>Mutual information compares the true joint distribution \\(P(X,Y)\\) with what the joint distribution would look like if \\(X\\) and \\(Y\\) were independent. If \\(X\\) and \\(Y\\) are independent, then \\(P(X,Y)=P(X)P(Y)\\), so \\(I(X;Y)=0\\).</p> <p>Mutual information can be written as a KL divergence: $$ I(X;Y) = D_{\\mathrm{KL}}\\big(P(X,Y)\\,|\\,P(X)P(Y)\\big). $$</p> <p>It can also be expressed using entropy:</p>  $$ I(X;Y) = H(X)-H(X\\mid Y) = H(Y)-H(Y\\mid X). $$  <p>Note</p> <p>This shows that the information is the reduction in uncertainty of \\(X\\) after observing \\(Y\\) (and vice versa). It is always nonnegative: \\(I(X;Y)\\ge 0\\).</p>"},{"location":"mathematics/05_prob_modeling/","title":"Probabilistic Modeling","text":"9 Feb 2026 <p>Statistical modeling is the practice of describing real-world data using mathematical models with unknown parameters. In machine learning, statistical models are often expressed in probabilistic form, meaning we assume data is generated from a probability distribution. Probabilistic modeling is therefore the practice of describing how data is generated using probability distributions. Instead of treating observations as deterministic, we assume they are generated by an underlying random process, often with noise and uncertainty.</p> <p>Info</p> <p>The following sources were consulted in preparing this material: </p> <ul> <li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press. Chapter 3: Probability and Information Theory.</li> <li>Grosse, R. (2020). Lecture 7: Probabilistic Models. CSC 311: Introduction to Machine Learning, University of Toronto.</li> </ul> <p>Important</p> <p>Some concepts in this material are simplified for pedagogical purposes. These simplifications slightly reduce precision but preserve the core ideas relevant to deep learning.</p>"},{"location":"mathematics/05_prob_modeling/#likelihood","title":"Likelihood","text":"<p>In probability, we often write expressions like \\(p(y \\mid \\theta)\\), where \\(\\theta\\) is a parameter of a model, and \\(y\\) is a possible outcome. The same expression can be interpreted in two different ways: as a probability or as a likelihood. The likelihood is not a different formula \u2014 it is the same function, interpreted differently.</p> <ul> <li>Probability treats \\(\\theta\\) as fixed and \\(y\\) as uncertain. It answers: If the model parameter is \\(\\theta\\), how likely is outcome \\(y\\)?</li> <li>Likelihood treats \\(y\\) as fixed (because we already observed it) and views the same expression as a function of \\(\\theta\\). It answers: Given the observed outcome \\(y\\), which values of \\(\\theta\\) make it most plausible?</li> </ul> <p>For continuous variables, \\(p(y\\mid\\theta)\\) is a probability density rather than a probability. The likelihood function is defined as: $$ L(\\theta \\mid y) = p(y \\mid \\theta). $$</p> <p>Important</p> <p>Likelihood is not a probability distribution over \\(\\theta\\). In general: $$ \\int L(\\theta \\mid y)\\,d\\theta \\ne 1. $$ Likelihood values only measure relative support for different parameter values of \\(\\theta\\).</p> <p>Example</p> <p>A coin toss can be modeled as \\(Y \\sim \\mathrm{Bernoulli}(\\theta),\\) where \\(Y \\in \\{0,1\\}\\), and \\(\\theta\\) is the probability of observing \\(Y=1\\) (e.g., heads). If the coin is weighted, then \\(\\theta \\ne 0.5\\). The probability of observing outcome \\(y\\) is: $$ p(y \\mid \\theta) = \\theta^y(1-\\theta)^{1-y}. $$</p> <p>If \\(\\theta\\) is fixed, this is a probability statement about the random outcome \\(Y\\). But after observing \\(y\\), the same expression becomes a likelihood function of \\(\\theta\\): $$ L(\\theta \\mid y) = \\theta^y(1-\\theta)^{1-y}. $$</p> <p>For example, if we observe \\(y=1\\) (heads), then: $$ L(\\theta \\mid y=1) = \\theta, $$ which is maximized near \\(\\theta=1\\). If we observe \\(y=0\\) (tails), then: $$ L(\\theta \\mid y=0) = 1-\\theta, $$ which is maximized near \\(\\theta=0\\).</p> <p>In practice, likelihood values can become extremely small, because they often involve multiplying many probabilities. For this reason, we usually work with the log-likelihood: $$ \\log L(\\theta \\mid y) = \\log p(y \\mid \\theta). $$</p> <p>Note</p> <p>Log-likelihood is used because it turns products into sums. If we assume i.i.d. samples \\(y^{(1)},\\dots,y^{(m)}\\), then: $$ p(y^{(1)},\\dots,y^{(m)} \\mid \\theta) = \\prod_{i=1}^{m} p(y^{(i)} \\mid \\theta), $$ so the log-likelihood becomes: $$ \\log p(y^{(1)},\\dots,y^{(m)} \\mid \\theta) = \\sum_{i=1}^{m} \\log p(y^{(i)} \\mid \\theta). $$</p>"},{"location":"mathematics/05_prob_modeling/#maximum-likelihood-estimation","title":"Maximum Likelihood Estimation","text":"<p>Once we choose a probabilistic model \\(p(x \\mid \\theta)\\), the main question becomes: which parameter values \\(\\theta\\) best explain the observed dataset? Given a dataset of \\(m\\) samples: $$ D = {x^{(1)},x^{(2)},\\dots,x^{(m)}}, $$ the likelihood of the dataset is: $$ p(D \\mid \\theta) = \\prod_{i=1}^{m} p(x^{(i)} \\mid \\theta), $$ assuming the samples are i.i.d. Maximum likelihood estimation (MLE) chooses the parameter values that maximize this likelihood:</p>  $$ \\hat{\\theta}_{\\mathrm{MLE}} = \\arg\\max_{\\theta} p(D \\mid \\theta) $$  <p>With log-likelihood, the formula becomes:</p>  $$ \\hat{\\theta}_{\\mathrm{MLE}} = \\arg\\max_{\\theta} \\log p(D \\mid \\theta) = \\arg\\max_{\\theta} \\sum_{i=1}^{m} \\log p(x^{(i)} \\mid \\theta). $$  <p>Because the logarithm is monotonic, maximizing likelihood and maximizing log-likelihood produce the same solution.</p> <p>Note</p> <p>MLE is the standard statistical justification behind many deep learning loss functions. In practice, training often means choosing parameters \\(\\theta\\) so that the observed dataset becomes as likely as possible under the model.</p>"},{"location":"mathematics/05_prob_modeling/#negative-log-likelihood","title":"Negative Log-Likelihood","text":"<p>In deep learning, we usually convert MLE into a minimization problem. This leads to the negative log-likelihood loss: $$ \\mathcal{L}(\\theta) = -\\log p(D\\mid \\theta). $$</p> <p>Minimizing negative log-likelihood is equivalent to maximizing likelihood, so negative log-likelihood is the most common probabilistic form of a training objective.</p>"},{"location":"mathematics/05_prob_modeling/#binary-cross-entropy","title":"Binary Cross-Entropy","text":"<p>In binary classification, the label is \\(y \\in \\{0,1\\}\\) and the model predicts the probability of the positive class: $$ \\hat{p} = p_\\theta(Y=1 \\mid x), $$ where \\(\\theta\\) represents the model parameters (weights).</p> <p>Under a Bernoulli model, the likelihood of observing \\(y\\) is: $$ p_\\theta(y\\mid x) = \\hat{p}^{\\,y}(1-\\hat{p})^{1-y}. $$</p> <p>Taking the negative logarithm gives the binary cross-entropy loss:</p>  $$ \\mathcal{L}_{\\mathrm{BCE}}(y,\\hat{p}) = -y\\log(\\hat{p})-(1-y)\\log(1-\\hat{p}). $$  <p>So binary cross-entropy is exactly the negative log-likelihood of a Bernoulli model.</p> <p>Example</p> <p>Suppose the true label is \\(y=1\\) (positive class). If the model predicts \\(\\hat{p}=0.9\\), then: $$ \\mathcal{L}_{\\mathrm{BCE}} = -\\log(0.9) \\approx 0.105. $$</p> <p>If the model predicts \\(\\hat{p}=0.1\\), then: $$ \\mathcal{L}_{\\mathrm{BCE}} = -\\log(0.1) \\approx 2.303. $$</p> <p>The loss is small when the model assigns high probability to the correct label, and large when it assigns low probability.</p>"},{"location":"mathematics/05_prob_modeling/#categorical-cross-entropy","title":"Categorical Cross-Entropy","text":"<p>In multiclass classification with \\(k\\) classes, the label is: $$ y \\in {1,2,\\dots,k}. $$</p> <p>The model outputs a probability vector \\(\\hat{p}\\in\\mathbb{R}^k\\). The likelihood of observing class \\(y\\) is the probability assigned to that class: $$ p(y\\mid x) = \\hat{p}_y. $$</p> <p>Therefore, the negative log-likelihood becomes: $$ \\mathcal{L}(y,\\hat{p}) = -\\log(\\hat{p}_y). $$</p> <p>If we represent the label as a one-hot vector \\(e_y\\), the same loss can be written as: $$ \\mathcal{L}(y,\\hat{p}) = -\\sum_{i=1}^k e_{y,i}\\log(\\hat{p}_i). $$</p> <p>So categorical cross-entropy is exactly the negative log-likelihood of a categorical distribution.</p> <p>Note</p> <p>Cross-entropy loss is widely used because minimizing negative log-likelihood is equivalent to maximizing the likelihood of the observed labels under the model. In practice, neural networks usually output logits (unnormalized scores) rather than probabilities. Cross-entropy is computed using numerically stable implementations that combine softmax and log into a single operation: $$ \\mathcal{L}(y,z) = -\\log\\left(\\mathrm{softmax}(z)_y\\right), $$ where \\(z \\in \\mathbb{R}^k\\) is the logits vector.</p> <p>Example</p> <p>Suppose we have \\(k=3\\) classes and the true class is \\(y=2\\). If the model predicts \\(\\hat{p}=(0.1,0.8,0.1)\\), then: $$ \\mathcal{L} = -\\log(0.8) \\approx 0.223. $$</p> <p>If the model predicts \\(\\hat{p}=(0.4,0.2,0.4)\\), then: $$ \\mathcal{L} = -\\log(0.2) \\approx 1.609. $$</p> <p>The loss increases sharply when the model assigns low probability to the correct class.</p>"},{"location":"mathematics/05_prob_modeling/#bayesian-inference","title":"Bayesian Inference","text":"<p>MLE treats the model parameters \\(\\theta\\) as fixed but unknown. In Bayesian inference, we instead treat \\(\\theta\\) as a random variable and represent uncertainty about its value using probability distributions. Before observing data, we have a certain belief \\(p(\\theta).\\) about the data distribution. After observing a dataset \\(D\\), we update this belief using Bayes' rule: $$ p(\\theta \\mid D) = \\frac{p(D \\mid \\theta)\\,p(\\theta)}{p(D)}. $$</p> <p>Here:</p> <ul> <li>\\(p(\\theta)\\) is the prior, representing our belief about \\(\\theta\\) before seeing data.</li> <li>\\(p(D\\mid\\theta)\\) is the likelihood, measuring how well \\(\\theta\\) explains the observed data.</li> <li>\\(p(\\theta\\mid D)\\) is the posterior, representing our updated belief after seeing data.</li> <li>\\(p(D)\\) is the marginal likelihood (or evidence), which normalizes the posterior: $$ p(D) = \\int p(D \\mid \\theta)\\,p(\\theta)\\,d\\theta. $$</li> </ul> <p>Note</p> <p>Bayesian inference provides a way to combine prior assumptions with observed data. Instead of producing a single best estimate of parameters, it produces a full probability distribution over plausible parameter values.  When the dataset is small, the posterior remains broad. As more data is observed, the posterior typically becomes more concentrated around parameter values that explain the data well.</p>"},{"location":"mathematics/05_prob_modeling/#maximum-a-posteriori-estimation","title":"Maximum A Posteriori Estimation","text":"<p>MLE chooses parameters \\(\\theta\\) that maximize the likelihood of the observed dataset:</p>  $$ \\hat{\\theta}_{\\mathrm{MLE}} = \\arg\\max_{\\theta} p(D \\mid \\theta). $$  <p>In Bayesian inference, we instead compute the posterior distribution: $$ p(\\theta \\mid D) = \\frac{p(D \\mid \\theta)\\,p(\\theta)}{p(D)}. $$</p> <p>Maximum a posteriori estimation (MAP) chooses the parameter value that maximizes the posterior:</p>  $$ \\hat{\\theta}_{\\mathrm{MAP}} = \\arg\\max_{\\theta} p(\\theta \\mid D). $$  <p>Since \\(p(D)\\) does not depend on \\(\\theta\\), maximizing the posterior is equivalent to:</p>  $$ \\hat{\\theta}_{\\mathrm{MAP}} = \\arg\\max_{\\theta} p(D \\mid \\theta)\\,p(\\theta). $$  <p>Taking the logarithm gives the common optimization form:</p>  $$ \\hat{\\theta}_{\\mathrm{MAP}} = \\arg\\max_{\\theta} \\Big( \\log p(D \\mid \\theta) + \\log p(\\theta) \\Big). $$  <p>MAP can be seen as MLE with an additional term \\(\\log p(\\theta)\\) that encourages parameter values that are consistent with the prior.</p> <p>Example</p> <p>Consider linear regression with Gaussian noise: $$ y = w^\\top x + \\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0,\\sigma^2). $$</p> <p>This implies the likelihood: $$ p(D \\mid w) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^m (y^{(i)} - w^\\top x^{(i)})^2 \\right). $$</p> <p>Maximizing this likelihood (MLE) is equivalent to minimizing the mean squared error. Now assume a Gaussian prior over weights: $$ w \\sim \\mathcal{N}(0,\\tau^2 I). $$</p> <p>The MAP objective becomes: $$ \\hat{w}_{\\mathrm{MAP}} = \\arg\\max_w \\Big( \\log p(D \\mid w) + \\log p(w) \\Big). $$</p> <p>Since the Gaussian prior contributes a penalty term proportional to \\(\\|w\\|^2\\), MAP becomes equivalent to minimizing: $$ \\sum_{i=1}^m (y^{(i)} - w^\\top x^{(i)})^2 + \\lambda |w|^2. $$</p> <p>This is exactly \\(L_2\\) regularization (ridge regression). Therefore, MAP estimation provides a probabilistic justification for weight decay in deep learning.</p>"},{"location":"mathematics/05_prob_modeling/#latent-variable-models","title":"Latent Variable Models","text":"<p>Quote</p> <p>But the latent process of which we speak, is far from being obvious to men\u2019s minds, beset as they now are. For we mean not the measures, symptoms, or degrees of any process which can be exhibited in the bodies themselves, but simply a continued process, which, for the most part, escapes the observation of the senses. ~\u200aFrancis Bacon (Novum Organum) </p> <p>In many real-world problems, the observed data \\(x\\) is influenced by hidden (latent) factors that we do not directly measure. A latent variable model introduces an unobserved random variable \\(z\\) to represent this hidden structure. The model assumes that data is generated in two steps:</p> <ol> <li>Latent variable was sampled \\(z \\sim p(z).\\)</li> <li>Observation conditioned on \\(z\\) was generated \\(x \\sim p(x \\mid z).\\)</li> </ol> <p>Together, this defines the joint distribution: $$ p(x,z) = p(x \\mid z)\\,p(z). $$</p> <p>Since \\(z\\) is not observed, the probability of an observation \\(x\\) is obtained by marginalizing over all possible latent values: $$ p(x) = \\int p(x,z)\\,dz = \\int p(x \\mid z)\\,p(z)\\,dz. $$</p> <p>Note</p> <p>In practice, latent variable models are powerful because they can represent complex data-generating processes, such as clustering, hidden states, or abstract representations. However, they are also more difficult to train, because computing the marginal likelihood often requires intractable integration (or summation) over latent variables. Many important machine learning models can be viewed as latent variable models, including mixture models, hidden Markov models (HMMs), and variational autoencoders (VAEs).</p>"},{"location":"mathematics/05_prob_modeling/#mixture-models","title":"Mixture Models","text":"<p>In the probability section, we introduced mixture distributions as weighted combinations of simpler distributions:</p>  $$ p(x) = \\sum_{k=1}^{K} \\pi_k\\,p_k(x), \\qquad \\pi_k \\ge 0, \\qquad \\sum_{k=1}^{K}\\pi_k = 1. $$  <p>In probabilistic modeling, the same idea is interpreted as a latent variable model. We assume that each data point was generated by one of \\(K\\) hidden components. This is modeled by introducing a latent variable: $$ Z \\in {1,2,\\dots,K}, $$ which represents the unknown component assignment. The generative process is:</p> <ol> <li>Sample a component index: \\(Z \\sim \\mathrm{Categorical}(\\pi_1,\\dots,\\pi_K).\\)</li> <li>Sample the observation from the chosen component: \\(X \\sim p(x \\mid Z=k).\\)</li> </ol> <p>The marginal distribution of \\(X\\) is therefore: $$ p(x) = \\sum_{k=1}^{K} \\pi_k\\,p(x \\mid Z=k). $$</p> <p>Note</p> <p>The key modeling idea is that \\(Z\\) is not observed.   Learning a mixture model means learning both the component distributions and the hidden assignments of data points.</p>"},{"location":"mathematics/05_prob_modeling/#expectation-maximization","title":"Expectation-Maximization","text":"<p>Many probabilistic models contain latent variables, such as mixture models. In these models, the likelihood involves marginalizing over hidden variables. If the latent variable \\(z\\) is discrete, the likelihood contains a sum: $$ p(D \\mid \\theta) = \\prod_{i=1}^{m} \\sum_{z^{(i)}} p(x^{(i)}, z^{(i)} \\mid \\theta). $$</p> <p>If the latent variable \\(z\\) is continuous, the likelihood contains an integral: $$ p(D \\mid \\theta) = \\prod_{i=1}^{m} \\int p(x^{(i)}, z^{(i)} \\mid \\theta)\\,dz^{(i)}. $$</p> <p>Directly maximizing this likelihood is often difficult, because of the intractable sum (or integral) over latent variables. The Expectation-Maximization (EM) algorithm is an iterative method for maximum likelihood estimation in latent variable models. It alternates between estimating the latent variables (softly) and updating the parameters.</p> <p>In the E-step (Expectation), we compute the posterior distribution of the latent variable given the current parameters: $$ p(z^{(i)} \\mid x^{(i)}, \\theta^{(t)}). $$</p> <p>This gives a soft assignment of each data point to latent states or mixture components.</p> <p>In the M-step (Maximization), we update the parameters by maximizing the expected log-likelihood under these soft assignments:</p>  $$ \\theta^{(t+1)} = \\arg\\max_{\\theta} \\mathbb{E}_{z \\sim p(z \\mid x,\\theta^{(t)})} \\left[ \\log p(x,z \\mid \\theta) \\right]. $$  <p>This step improves the likelihood by fitting the model parameters to the expected latent structure. EM repeats these two steps until convergence.  Each iteration is guaranteed not to decrease the data likelihood.</p> <p>Example</p> <p>EM is most commonly associated with Gaussian Mixture Models (GMMs). Consider a GMM with \\(K\\) Gaussian components: $$ p(x) = \\sum_{k=1}^{K} \\pi_k\\,\\mathcal{N}(x;\\mu_k,\\Sigma_k). $$</p> <p>Each data point \\(x^{(i)}\\) is assumed to be generated by an unknown component \\(z^{(i)} \\in \\{1,\\dots,K\\}\\).</p> <p>E-step. Compute the posterior probability that point \\(x^{(i)}\\) belongs to component \\(k\\): $$ \\gamma_{ik} = p(z^{(i)}=k \\mid x^{(i)},\\theta) = \\frac{ \\pi_k\\,\\mathcal{N}(x^{(i)};\\mu_k,\\Sigma_k) }{ \\sum_{j=1}^{K} \\pi_j\\,\\mathcal{N}(x^{(i)};\\mu_j,\\Sigma_j) }. $$</p> <p>The values \\(\\gamma_{ik}\\) are called responsibilities, and they act as soft cluster assignments.</p> <p>M-step. Update the parameters using these responsibilities: $$ N_k = \\sum_{i=1}^{m} \\gamma_{ik}, \\qquad \\pi_k = \\frac{N_k}{m}, $$ $$ \\mu_k = \\frac{1}{N_k}\\sum_{i=1}^{m} \\gamma_{ik} x^{(i)}, $$ $$ \\Sigma_k = \\frac{1}{N_k}\\sum_{i=1}^{m} \\gamma_{ik} (x^{(i)}-\\mu_k)(x^{(i)}-\\mu_k)^\\top. $$</p> <p>EM repeats these steps until the parameters converge. Intuitively, the E-step estimates soft cluster memberships, and the M-step recomputes the cluster parameters based on those memberships.</p>"},{"location":"mathematics/05_prob_modeling/#structured-probabilistic-models","title":"Structured Probabilistic Models","text":"<p>In many deep learning problems, we work with multiple random variables. Modeling the full joint distribution \\(p(x_1,\\dots,x_n)\\) directly is often impractical, because the number of possible interactions grows rapidly with \\(n\\). Instead, we exploit the fact that most variables interact only with a small subset of others.</p> <p>A structured probabilistic model (also called a graphical model) represents a joint probability distribution using a graph.   Each node represents a random variable, and edges represent direct probabilistic dependencies.</p> <p>In a directed graphical model (also called a Bayesian network), edges are arrows that indicate conditional dependence. A Bayesian network must be a directed acyclic graph (DAG) (it cannot contain directed cycles). The joint distribution factorizes into conditional probabilities: $$ p(x_1,\\dots,x_n) = \\prod_{i=1}^{n} p(x_i \\mid \\mathrm{Pa}(x_i)), $$ where \\(\\mathrm{Pa}(x_i)\\) denotes the parents of \\(x_i\\) in the graph.</p>      An example of a directed acyclic graphical model (Bayesian network). In this example, $A$ is a parent of $B$, $C$, and $D$, and $C$ is also a parent of $D$, so the factorization is $p(A,B,C,D)=p(A)\\,p(B\\mid A)\\,p(C\\mid A)\\,p(D\\mid A,C)$. ~ RJE42 - Own work, CC BY-SA 4.0, Link <p>In an undirected graphical model (Markov Random Field), edges do not have direction. Instead of conditional probabilities, the distribution is represented using non-negative functions called potential functions. The joint distribution is written as: $$ p(x) = \\frac{1}{Z} \\prod_{i=1}^{m} \\phi^{(i)}(C^{(i)}), $$ where each \\(\\phi^{(i)}\\) is a potential function over a clique \\(C^{(i)}\\), and \\(Z\\) is a normalizing constant called the partition function. Unlike conditional probabilities, potential functions are not required to sum to \\(1\\). They only need to be non-negative.</p>      Example of an undirected graphical model. For example, the edge $A-B$ indicates that $A$ and $B$ are not independent. Since there are no edges between $B$, $C$, and $D$, the model assumes they are conditionally independent given $A$. ~ RJE42 - Own work, CC BY-SA 4.0, Link <p>Note</p> <p>In an undirected graph, edges do not represent causal direction. Instead, an edge between two nodes indicates that the variables directly interact.</p>"},{"location":"mathematics/05_prob_modeling/#generative-vs-discriminative-modeling","title":"Generative vs Discriminative Modeling","text":"<p>Probabilistic models are often divided into two categories. A generative model describes how the data is generated by modeling the joint distribution \\(p(x,y).\\) Using the joint distribution, we can answer different types of questions, such as generating new samples \\(x\\) or perform classification using Bayes' rule. Examples of generative models include Naive Bayes, GMMs, HMMs, VAEs, etc.</p> <p>A discriminative model focuses directly on predicting the target variable from the input by modeling: \\(p(y \\mid x),\\) or by learning a direct decision function. Discriminative models do not attempt to model the full data distribution \\(p(x)\\), and are often preferred when the main goal is prediction. Examples include logistic regression, support vector machines (SVMs), and neural networks trained with cross-entropy.</p> <p>Note</p> <p>Generative models are typically more flexible and can be used for sampling and missing-data problems, but they can be harder to train. Discriminative models often achieve better performance in supervised prediction tasks when large labeled datasets are available.</p> <p>In this page, we introduced probabilistic modeling as a way to describe how data can be generated from probability distributions. We saw how the likelihood function turns observed data into a tool for choosing good model parameters. This leads to maximum likelihood estimation, and to negative log-likelihood, which is the basis of many loss functions used in deep learning. We also introduced Bayesian inference, where model parameters are treated as uncertain and described using probability distributions. MAP estimation was presented as a practical way to combine the likelihood with a prior. Finally, we discussed latent variable models, mixture models, and the Expectation-Maximization algorithm, which are useful when data is generated by hidden factors. We also introduced structured probabilistic models, which use graphs to represent dependencies between random variables, and explained the difference between generative and discriminative modeling.</p>"},{"location":"notebooks/","title":"Lecture Notebooks","text":"<p>See the navigation bar for notebooks. There will be updates throughout the semester as we discuss the topics.</p>"},{"location":"notebooks/01_backprop/","title":"01. From Derivatives to Backpropagation","text":"<p>We will go from illustrating differentiation and finding derivatives in Python, all the way down till the implementation of the backpropagation algorithm. Even if mathematically the idea of derivatives is very familiar, it still makes sense to see its various visualization plots via code.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[2]: Copied! <pre>def f(x):\n  return x**2\n</pre> def f(x):   return x**2 In\u00a0[3]: Copied! <pre>x = 3.0\nfor h in [10, 1, 0.1, 0]:\n  print(f\"If we shift input by {h}, output becomes {f(x+h)}\")\n</pre> x = 3.0 for h in [10, 1, 0.1, 0]:   print(f\"If we shift input by {h}, output becomes {f(x+h)}\") <pre>If we shift input by 10, output becomes 169.0\nIf we shift input by 1, output becomes 16.0\nIf we shift input by 0.1, output becomes 9.610000000000001\nIf we shift input by 0, output becomes 9.0\n</pre> <p>The well-known equation for the change $ \\Delta y = f(x + \\Delta x) - f(x)$ we can code and  visualize for various cases as follows. Again, feel free to modify the values and functions to observe the behavior of change.</p> In\u00a0[4]: Copied! <pre>h = 1.0\n\ndx = h\ndy = f(x+h) - f(x)\n\nprint(f\"\u0394x: {dx}\")\nprint(f\"\u0394y: {dy}\")\nprint(f\"When you change x by {dx} unit, y changes by {dy} units.\")\n</pre> h = 1.0  dx = h dy = f(x+h) - f(x)  print(f\"\u0394x: {dx}\") print(f\"\u0394y: {dy}\") print(f\"When you change x by {dx} unit, y changes by {dy} units.\") <pre>\u0394x: 1.0\n\u0394y: 7.0\nWhen you change x by 1.0 unit, y changes by 7.0 units.\n</pre> In\u00a0[5]: Copied! <pre>def plot_delta(x, h, start=-4, stop=4, num=30):\n  # `np.linspace` returns an array of num inputs within a range.\n  x_all = np.linspace(start, stop, num)\n  y_all = f(x_all)\n\n  plt.figure(figsize=(4, 4))\n  plt.plot(x_all, y_all)\n\n  # dx &amp; dy\n  plt.plot([x, x + h], [f(x), f(x)], color='r')\n  plt.plot([x + h, x + h], [f(x), f(x + h)], color='r')\n</pre> def plot_delta(x, h, start=-4, stop=4, num=30):   # `np.linspace` returns an array of num inputs within a range.   x_all = np.linspace(start, stop, num)   y_all = f(x_all)    plt.figure(figsize=(4, 4))   plt.plot(x_all, y_all)    # dx &amp; dy   plt.plot([x, x + h], [f(x), f(x)], color='r')   plt.plot([x + h, x + h], [f(x), f(x + h)], color='r') In\u00a0[6]: Copied! <pre>plot_delta(x=2, h=1)\n</pre> plot_delta(x=2, h=1) <p>How to find if the ouput changes significantly when we change the input by some amount $h$? We should be familiar with the rate of change ratio when we choose a small step size $h$:</p> <p>$$ \\dfrac{\\Delta y}{\\Delta x} = \\dfrac{f(x + h) - f(x)}{h}.$$</p> In\u00a0[7]: Copied! <pre>def plot_roc(x, h):\n  dx = h\n  dy = f(x + h) - f(x)\n\n  plot_delta(x, h)\n  print(f\"Rate of change is {dy / dx}\")\n</pre> def plot_roc(x, h):   dx = h   dy = f(x + h) - f(x)    plot_delta(x, h)   print(f\"Rate of change is {dy / dx}\") In\u00a0[8]: Copied! <pre>plot_roc(3, 1)\n</pre> plot_roc(3, 1) <pre>Rate of change is 7.0\n</pre> In\u00a0[9]: Copied! <pre>plot_roc(3, 0.5)\n</pre> plot_roc(3, 0.5) <pre>Rate of change is 6.5\n</pre> In\u00a0[10]: Copied! <pre>plot_roc(1, 1)\n</pre> plot_roc(1, 1) <pre>Rate of change is 3.0\n</pre> In\u00a0[11]: Copied! <pre>plot_roc(-2, 0.5)\n</pre> plot_roc(-2, 0.5) <pre>Rate of change is -3.5\n</pre> <p>The rate of change for different values of $h$ are different at the same point $x$. We would like to come up with a single value that would tell how significantly $y$ changes at a given point $x$ within the function:</p> <p>$$ L = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} $$</p> <p>Simply, this limit tells us how much the value of the output will change when we change the input by just a very small amount.</p> <p>Note</p> <p>         Essentially, derivative is a function: it assigns to each input $ x $ the rate at which the original function changes at that point. Meaning, for each input, the limit above produces a value, and as $ x $ changes, this value changes as well.     </p> In\u00a0[12]: Copied! <pre>x = 3\nh = 0.000001 # The limit of h approaching 0\nd = (f(x + h) - f(x)) / h\nf\"The value of derivative function is {d}\"\n</pre> x = 3 h = 0.000001 # The limit of h approaching 0 d = (f(x + h) - f(x)) / h f\"The value of derivative function is {d}\" Out[12]: <pre>'The value of derivative function is 6.000001000927568'</pre> <p>When we extend this idea to multivariable calculus, we use partial derivatives. A partial derivative with respect to (wrt) a given variable measures how much the output changes when we nudge that variable alone by a very small amount, while keeping all other variables fixed. Below we will see how it behaves during addition $f(x, y)=x+y$.</p> In\u00a0[13]: Copied! <pre>f = lambda x, y: x + y\n\nx = 2\ny = 3\n\nf(x, y)\n</pre> f = lambda x, y: x + y  x = 2 y = 3  f(x, y) Out[13]: <pre>5</pre> <p>Partial derivatives w.r.t $x$ an $y$ will be as follows:</p> In\u00a0[14]: Copied! <pre>h = 0.000001\n(f(x + h, y) - f(x, y)) / h\n</pre> h = 0.000001 (f(x + h, y) - f(x, y)) / h Out[14]: <pre>1.000000000139778</pre> In\u00a0[15]: Copied! <pre>h = 0.000001\n(f(x, y+h) - f(x, y)) / h\n</pre> h = 0.000001 (f(x, y+h) - f(x, y)) / h Out[15]: <pre>1.000000000139778</pre> <p>No matter what the input values are, the expression will always approach $1.0$ for addition, because adding a constant increases the output by the same amount as the input change.</p> In\u00a0[16]: Copied! <pre>for x, y in zip([-20, 2, 3], [300, 75, 10]):\n  print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}')\n</pre> for x, y in zip([-20, 2, 3], [300, 75, 10]):   print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}') <pre>x=-20, y=300: 0.9999999974752427\nx=2, y=75: 0.9999999974752427\nx=3, y=10: 1.0000000010279564\n</pre> <p>Indeed, if we have a simple addition $x + y$, then increasing $x$ or $y$ by some amount will increase the result by the exact same amount. Assertion will work for any number $h$ gets.</p> In\u00a0[17]: Copied! <pre>h = 10\nassert f(x+h, y) - f(x, y) == h\nassert f(x, y+h) - f(x, y) == h\n</pre> h = 10 assert f(x+h, y) - f(x, y) == h assert f(x, y+h) - f(x, y) == h <p>Let's now see the case for multiplication $f(x, y)=x * y$.</p> In\u00a0[18]: Copied! <pre>f = lambda x, y: x * y\n</pre> f = lambda x, y: x * y In\u00a0[19]: Copied! <pre>x = 2\ny = 3\nh = 1e-5 # scientific notation for 0.00001\n(f(x + h, y) - f(x, y)) / h # wrt x\n</pre> x = 2 y = 3 h = 1e-5 # scientific notation for 0.00001 (f(x + h, y) - f(x, y)) / h # wrt x Out[19]: <pre>3.000000000064062</pre> In\u00a0[20]: Copied! <pre>for x in [-20, 2, 3]:\n  print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}')\n</pre> for x in [-20, 2, 3]:   print(f'x={x}, y={y}: {(f(x + h, y) - f(x, y)) / h}') <pre>x=-20, y=3: 2.999999999531155\nx=2, y=3: 3.000000000064062\nx=3, y=3: 3.000000000064062\n</pre> In\u00a0[21]: Copied! <pre>x = 10\nh = 5\npdx = (f(x+h, y) - f(x, y)) / h\nprint(pdx, y)\nassert round(pdx, 2) == round(y, 2)\n</pre> x = 10 h = 5 pdx = (f(x+h, y) - f(x, y)) / h print(pdx, y) assert round(pdx, 2) == round(y, 2) <pre>3.0 3\n</pre> <p>Finally, we will consider a complex function with three variables $f(x, y, z)=x^2+y^3-z$.</p> In\u00a0[22]: Copied! <pre>def f(x, y, z):\n  return x**2 + y**3 - z\n</pre> def f(x, y, z):   return x**2 + y**3 - z In\u00a0[23]: Copied! <pre>x = 2\ny = 3\nz = 4\n\nf(x, y, z)\n</pre> x = 2 y = 3 z = 4  f(x, y, z) Out[23]: <pre>27</pre> In\u00a0[24]: Copied! <pre>h = 1\n\nf(x + h, y, z)\n</pre> h = 1  f(x + h, y, z) Out[24]: <pre>32</pre> In\u00a0[25]: Copied! <pre>f(x + h, y, z) - f(x, y, z)\n</pre> f(x + h, y, z) - f(x, y, z) Out[25]: <pre>5</pre> In\u00a0[26]: Copied! <pre>(f(x + h, y, z) - f(x, y, z)) / h\n</pre> (f(x + h, y, z) - f(x, y, z)) / h Out[26]: <pre>5.0</pre> In\u00a0[27]: Copied! <pre>h = 0.00001 # change in limit\npdx = (f(x + h, y, z) - f(x, y, z)) / h\npdx\n</pre> h = 0.00001 # change in limit pdx = (f(x + h, y, z) - f(x, y, z)) / h pdx Out[27]: <pre>4.000010000027032</pre> In\u00a0[28]: Copied! <pre>assert 2*x == round(pdx)\n</pre> assert 2*x == round(pdx) <p>Partial derivative w.r.t $x$ is $2x$ (by the power rule), and for $x=2$ indeed we get $4$.</p> <p>Execise</p> <p>     Code the partial derivative w.r.t $y$ and $z$ and verify if the result correct.   </p> <p>Tip</p> <p>     It is recommended to run this notebook on Google Colab, where Graphviz is available by default. For local development, graph visualization requires both the Python wrapper (<code>pip install graphviz</code>) and the Graphviz system executable to be installed and available on the system PATH.   </p> In\u00a0[29]: Copied! <pre># This is a graph visualization code from micrograd, no need to understand the details\n# https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb\nfrom graphviz import Digraph\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n\n    for n in nodes:\n        dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n\n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> # This is a graph visualization code from micrograd, no need to understand the details # https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb from graphviz import Digraph  def trace(root):     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root, format='svg', rankdir='LR'):     \"\"\"     format: png | svg | ...     rankdir: TB (top to bottom graph) | LR (left to right)     \"\"\"     assert rankdir in ['LR', 'TB']     nodes, edges = trace(root)     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})      for n in nodes:         dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')         if n._op:             dot.node(name=str(id(n)) + n._op, label=n._op)             dot.edge(str(id(n)) + n._op, str(id(n)))      for n1, n2 in edges:         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot <p>The purpose of the <code>Value</code> class below is to store both numerical values and the history of operations that produced them. Each instance represents a node in a [computation graph](computation graph). When a <code>Value</code> object is created, it stores:</p> <ul> <li>the numerical result of a computation (<code>data</code>),</li> <li>references to the input values that produced it (<code>_prev</code>),</li> <li>the operation that combined those inputs (<code>_op</code>),</li> <li>label provided by us in order to see it on the computation graph (<code>label</code>),</li> <li>gradient of the final output w.r.t this value (<code>grad</code>).</li> </ul> <p>Arithmetic operations such as addition and multiplication are overloaded so that, instead of returning plain numbers, they return new <code>Value</code> objects. These new objects remember which values were combined and how they were combined. As computations are chained together, this process builds a directed graph, later used to propagate gradients.</p> In\u00a0[30]: Copied! <pre># Value class stores a number and \"remembers\" information about its origins\nclass Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self.grad = 0\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+')\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), \"*\")\n    return out\n\n  def __sub__(self, other):\n    return self + (Value(-1) * other) # self + (-other)\n\n  def __repr__(self):\n    return f\"Value(data={self.data}, grad={self.grad})\"\n</pre> # Value class stores a number and \"remembers\" information about its origins class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self.grad = 0    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+')     return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), \"*\")     return out    def __sub__(self, other):     return self + (Value(-1) * other) # self + (-other)    def __repr__(self):     return f\"Value(data={self.data}, grad={self.grad})\" <p>Below is the example of a simple expression built with the help of <code>Value</code> class: $L=c*d$ where $c=a+b$.</p> In\u00a0[31]: Copied! <pre>a = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b; c.label = 'c'\nd = Value(10, label='d')\nL = c * d; L.label = 'L'\n</pre> a = Value(5, label='a') b = Value(3, label='b') c = a + b; c.label = 'c' d = Value(10, label='d') L = c * d; L.label = 'L' In\u00a0[32]: Copied! <pre>print(a, a._prev)\nprint(L, L._prev)\n</pre> print(a, a._prev) print(L, L._prev) <pre>Value(data=5, grad=0) ()\nValue(data=80, grad=0) (Value(data=8, grad=0), Value(data=10, grad=0))\n</pre> <p>We can use the helper function built with <code>graphviz</code> above to plot our computation graph as a nice visualization.</p> In\u00a0[33]: Copied! <pre>draw_dot(c)\n</pre> draw_dot(c) Out[33]: In\u00a0[34]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[34]: <p>Gradient is vector of partial derivatives. We want to know how much changing each variable will affect the output of <code>L</code> (loss). We will store those partial derivatives inside each <code>grad</code> variable of each <code>Value</code> object. We start by noting that the derivative of a variable with respect to itself is $1$ (you get the same $dx/dy$).</p> In\u00a0[35]: Copied! <pre>L.grad = 1.0\n\nf = lambda x: x\nh = 1e-5\npdx = (f(x + h) - f(x)) / h\nassert round(pdx) == 1\n</pre> L.grad = 1.0  f = lambda x: x h = 1e-5 pdx = (f(x + h) - f(x)) / h assert round(pdx) == 1 <p>Now let's see how changing other variables will affect the eventual result.</p> In\u00a0[36]: Copied! <pre># extended version of the function we saw previously\ndef f(ha=0, hb=0, hc=0, hd=0):\n  a = Value(5 + ha, label='a')\n  b = Value(3 + hb, label='b')\n  c = a + b + Value(hc); c.label = 'c'\n  d = Value(10 + hd, label='d')\n  L = c * d; L.label = 'L'\n  return L.data\n</pre> # extended version of the function we saw previously def f(ha=0, hb=0, hc=0, hd=0):   a = Value(5 + ha, label='a')   b = Value(3 + hb, label='b')   c = a + b + Value(hc); c.label = 'c'   d = Value(10 + hd, label='d')   L = c * d; L.label = 'L'   return L.data In\u00a0[37]: Copied! <pre>h = 1e-5\n(f(hd=h) - f()) / h\n</pre> h = 1e-5 (f(hd=h) - f()) / h Out[37]: <pre>7.999999999697137</pre> <p>From the computational graph we can also know that $L=c*d$. When we change the value of $d$ just a little bit (derivative of $L$ wrt $d$) the value of $L$ will change by the amount of $c$, which is $8.0$. We saw it above in the partial derivative of a multiplication.</p> In\u00a0[38]: Copied! <pre>d.grad = c.data\nc.grad = d.data\n</pre> d.grad = c.data c.grad = d.data <p>With the same logic, the derivative of $L$ wrt $c$ will be the value of $d$, which is $10.0$ in our specific case. We can verify it.</p> In\u00a0[39]: Copied! <pre>(f(hc=h) - f()) / h\n</pre> (f(hc=h) - f()) / h Out[39]: <pre>10.000000000331966</pre> <p>To determine how much changing earlier variables in the computation graph will affect the loss, we can apply the chain rule. Simply, the derivative of $L$ wrt $a$ is the derivative of $c$ wrt $a$ multiplied by the derivative of $L$ wrt $c$:</p> <p>$$ \\frac{dL}{da} = \\frac{dL}{dc} \\cdot \\frac{dc}{da}. $$</p> <p>The derivate of $c$ both wrt $a$ and $b$ is $1.0$ due to the property of addition we had seen previously. From here:</p> In\u00a0[40]: Copied! <pre>a.grad = 1.0 * c.grad\nb.grad = 1.0 * c.grad\n\na.grad, b.grad\n</pre> a.grad = 1.0 * c.grad b.grad = 1.0 * c.grad  a.grad, b.grad Out[40]: <pre>(10.0, 10.0)</pre> <p>We can verify it as well. Let's see how much $L$ gets affected, when we shift $a$ or $b$ by a very small amount.</p> In\u00a0[41]: Copied! <pre>(f(ha=h) - f()) / h\n</pre> (f(ha=h) - f()) / h Out[41]: <pre>10.000000000331966</pre> In\u00a0[42]: Copied! <pre>(f(hb=h) - f()) / h\n</pre> (f(hb=h) - f()) / h Out[42]: <pre>10.000000000331966</pre> <p>We will finally redraw the manually updated computation graph.</p> In\u00a0[43]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[43]: <p>It basically implies that, for example, changing the value of $a$ by $1.0$ unit (from $5$ to $6$) will increase the value of $L$ by $10$ units (from $80$ to $90$).</p> In\u00a0[44]: Copied! <pre>f(ha=1), f(hb=1), f(hc=1), f(hd=1)\n</pre> f(ha=1), f(hb=1), f(hc=1), f(hd=1) Out[44]: <pre>(90, 90, 90, 88)</pre> <p>What we saw above was one backward pass done manually. We are mainly interested in the signs of partial derivatives to know if they are positively or negatively influencing the eventual loss $L$ of our model. In our case, all the derivatives are positive and influence loss positively.  We have to simply nudge the values in the opposite direction of the gradient to bring the loss down. This is known as gradient descent.</p> <p>$$ \\theta \\leftarrow \\theta - \\eta \\, \\nabla_\\theta L $$</p> <p>Here, $ \\theta $ denotes a model parameter, $ \\nabla_\\theta L $ is the gradient of the loss wrt that parameter, and $ \\eta $ is the learning rate. The learning rate controls how large each update step is during gradient descent. If the learning rate is too small, the parameters change very slowly and training takes a long time. If the learning rate is too large, the updates can overshoot the minimum and cause the loss to increase or oscillate. We will discuss learning rate in a greater detail in the future when discussing optimization.</p> In\u00a0[45]: Copied! <pre>lr = 0.01\n\na.data -= lr * a.grad\nb.data -= lr * b.grad\nd.data -= lr * d.grad\n\n# we skip c which is controlled by the values of a and b\n# pay attention that the rest are leaf nodes in the computation graph\n</pre> lr = 0.01  a.data -= lr * a.grad b.data -= lr * b.grad d.data -= lr * d.grad  # we skip c which is controlled by the values of a and b # pay attention that the rest are leaf nodes in the computation graph <p>Note</p> <p>     In case the loss is a negative value (not common), we will need to gradient ascend the loss upwards towards zero and change the sign to <code>+=</code> from <code>-=</code>. Note that the values of parameters (a, b, d) can decrease or increase depending on the sign of <code>grad</code>.   </p> <p>We will now do a single forward pass to see if loss has been decreased. Recall that the previous loss was <code>80</code>.</p> In\u00a0[46]: Copied! <pre>c = a + b\nL = c * d\n\nL.data\n</pre> c = a + b L = c * d  L.data Out[46]: <pre>77.376</pre> <p>It seems like we optimized our values and brought down the loss.</p> <p>Manually calculating gradient is good only for educational purposes. We should implement automatic backward pass which will calculate gradients. To support this, we will rewrite our <code>Value</code> class to store a <code>_backward</code> function. This function will enforce the local gradient associated with the operation that produced the value. Initially, <code>_backward</code> does nothing.</p> <p>For addition, the local derivatives are: $ \\frac{\\partial c}{\\partial a} = 1 $ and $ \\frac{\\partial c}{\\partial b} = 1 $. Hence, the gradient flowing into $c$ (stored in <code>out.grad</code>) is passed unchanged to both inputs:</p> <ul> <li><code>a.grad += out.grad</code></li> <li><code>b.grad += out.grad</code></li> </ul> <p>This is why, in the <code>_backward</code> function for addition, both operands receive the same gradient contribution.</p> <p>For multiplication, the local derivatives are: $ \\frac{\\partial c}{\\partial a} = b $ and $ \\frac{\\partial c}{\\partial b} = a $. During backpropagation, the incoming gradient is scaled by the opposite operand:</p> <ul> <li><code>a.grad += b.data * out.grad</code></li> <li><code>b.grad += a.data * out.grad</code></li> </ul> <p>This reflects the chain rule: each variable\u2019s gradient depends on how the output changes with respect to that variable. By storing a <code>_backward</code> function at every node and calling these functions in reverse topological order, the full gradient of the loss with respect to all intermediate values can be computed automatically.</p> In\u00a0[47]: Copied! <pre>class Value:\n  def __init__(self, data, _prev=(), _op='', label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self.grad = 0.0\n    self._backward = lambda: None # initially it is a function which does nothing\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+')\n\n    def _backward():\n      self.grad = 1.0 * out.grad\n      other.grad = 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), \"*\")\n\n    def _backward():\n      self.grad = other.data * out.grad\n      other.grad = self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __sub__(self, other):\n    return self + (Value(-1) * other)\n\n  def __repr__(self):\n    return f\"Value(data={self.data}, grad={self.grad})\"\n</pre> class Value:   def __init__(self, data, _prev=(), _op='', label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self.grad = 0.0     self._backward = lambda: None # initially it is a function which does nothing    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+')      def _backward():       self.grad = 1.0 * out.grad       other.grad = 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), \"*\")      def _backward():       self.grad = other.data * out.grad       other.grad = self.data * out.grad     out._backward = _backward      return out    def __sub__(self, other):     return self + (Value(-1) * other)    def __repr__(self):     return f\"Value(data={self.data}, grad={self.grad})\" In\u00a0[48]: Copied! <pre># Recreating the same function\na = Value(5, label='a')\nb = Value(3, label='b')\nc = a + b; c.label = 'c'\nd = Value(10, label='d')\nL = c * d; L.label = 'L'\n</pre> # Recreating the same function a = Value(5, label='a') b = Value(3, label='b') c = a + b; c.label = 'c' d = Value(10, label='d') L = c * d; L.label = 'L' In\u00a0[49]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[49]: <p>We should not forget to initialize the gradient of the loss to be $1.0$ and then call <code>backward</code> function. With correct implementation, we will get the same results which we manually calculated previously.</p> In\u00a0[50]: Copied! <pre>L.grad = 1.0\nL._backward()\nc._backward()\n</pre> L.grad = 1.0 L._backward() c._backward() In\u00a0[51]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[51]: <p>Exercise</p> <p>     Make sure that all operations and their partial derivatives can be calculated (e.g. division, power).   </p> In\u00a0[52]: Copied! <pre># optimization\nlr = 0.05\na.data -= lr * a.grad\nb.data -= lr * b.grad\nd.data -= lr * d.grad\n\n# forward pass\nc = a + b\nL = c * d\n\n# backward pass\nL.grad = 1.0\nL._backward()\nc._backward()\n\nL.data # loss\n</pre> # optimization lr = 0.05 a.data -= lr * a.grad b.data -= lr * b.grad d.data -= lr * d.grad  # forward pass c = a + b L = c * d  # backward pass L.grad = 1.0 L._backward() c._backward()  L.data # loss Out[52]: <pre>67.2</pre> <p>We have now trained the model for a single epoch. Even though what we do is oversimplistic and not precise, the main intuition and concepts behind training a neural network will be the same. We will now train the model for multiple epochs until we reduce the loss down to zero.</p> In\u00a0[53]: Copied! <pre>while True:\n  # optimization\n  a.data -= lr * a.grad\n  b.data -= lr * b.grad\n  d.data -= lr * d.grad\n\n  # forward pass\n  c = a + b\n  L = c * d\n\n  # backward pass\n  L.grad = 1.0\n  L._backward()\n  c._backward()\n\n  if L.data &lt; 0:\n    break\n\n  print(f'Loss: {round(L.data,2)}')\n</pre> while True:   # optimization   a.data -= lr * a.grad   b.data -= lr * b.grad   d.data -= lr * d.grad    # forward pass   c = a + b   L = c * d    # backward pass   L.grad = 1.0   L._backward()   c._backward()    if L.data &lt; 0:     break    print(f'Loss: {round(L.data,2)}') <pre>Loss: 55.87\nLoss: 45.77\nLoss: 36.68\nLoss: 28.42\nLoss: 20.81\nLoss: 13.69\nLoss: 6.91\nLoss: 0.34\n</pre> In\u00a0[54]: Copied! <pre>import torch\n\na = torch.tensor(5.0);    a.requires_grad = True\nb = torch.tensor(3.0);    b.requires_grad = True\nc = a + b\nd = torch.tensor(10.0);   d.requires_grad = True\nL = c * d\n</pre> import torch  a = torch.tensor(5.0);    a.requires_grad = True b = torch.tensor(3.0);    b.requires_grad = True c = a + b d = torch.tensor(10.0);   d.requires_grad = True L = c * d In\u00a0[55]: Copied! <pre>L.backward()\n</pre> L.backward() In\u00a0[56]: Copied! <pre>a.grad, b.grad, d.grad\n</pre> a.grad, b.grad, d.grad Out[56]: <pre>(tensor(10.), tensor(10.), tensor(8.))</pre> <p>Due to practical needs, we will go further in our PyTorch implementation and train a model. See the code below and pay attention to <code>requires_grad</code>, <code>no_grad()</code>, and <code>zero_()</code> functions (we will implement them as well in our next notebook on neural networks)</p> <p>The learnable parameters are created with <code>requires_grad=True</code>, which tells PyTorch to track all operations on them and build a computation graph automatically. This replaces the manual <code>_backward</code> logic used earlier.</p> <p>At the beginning of each epoch, gradients must be cleared using <code>zero_()</code>. PyTorch accumulates gradients by default, so failing to reset them would mix gradients from different iterations.</p> <p>The forward pass computes the loss using standard operations. Calling <code>backward()</code> on the loss automatically computes all gradients via automatic differentiation. There is no need to call backward functions on intermediate values.</p> <p>Parameter updates are performed inside a <code>no_grad()</code> block. This prevents PyTorch from tracking the update operations themselves, ensuring that gradient history does not grow across epochs. For large networks, not using <code>torch.no_grad()</code> may strain resources.</p> In\u00a0[57]: Copied! <pre>a = torch.tensor(5.0, requires_grad = True);\nb = torch.tensor(3.0, requires_grad = True);\nc = a + b\nd = torch.tensor(10.0,requires_grad = True);\nL = c * d\n\n# hyperparameters\nlr = 0.03\nepochs = 10\n\nfor _ in range(epochs):\n  # backward pass\n  L.backward()\n\n  # optimization (gradient descent)\n  with torch.no_grad():\n    a -= lr * a.grad\n    b -= lr * b.grad\n    d -= lr * d.grad\n\n  # avoids accumulating gradients\n  # comment the code below out to see how it affects the learning\n  a.grad.zero_()\n  b.grad.zero_()\n  d.grad.zero_()\n\n  # forward pass\n  c = a + b\n  L = c * d\n\n  print(f'Loss: {L.data:.2f}')\n</pre> a = torch.tensor(5.0, requires_grad = True); b = torch.tensor(3.0, requires_grad = True); c = a + b d = torch.tensor(10.0,requires_grad = True); L = c * d  # hyperparameters lr = 0.03 epochs = 10  for _ in range(epochs):   # backward pass   L.backward()    # optimization (gradient descent)   with torch.no_grad():     a -= lr * a.grad     b -= lr * b.grad     d -= lr * d.grad    # avoids accumulating gradients   # comment the code below out to see how it affects the learning   a.grad.zero_()   b.grad.zero_()   d.grad.zero_()    # forward pass   c = a + b   L = c * d    print(f'Loss: {L.data:.2f}') <pre>Loss: 72.22\nLoss: 65.00\nLoss: 58.26\nLoss: 51.97\nLoss: 46.08\nLoss: 40.53\nLoss: 35.30\nLoss: 30.35\nLoss: 25.63\nLoss: 21.11\n</pre> <p>This notebook introduced the core ideas behind gradient-based learning. It began with differentiation and extended the concept to partial derivatives for multivariable functions. Computation graphs were then introduced to represent how values depend on one another and to make gradient flow explicit. Using this structure, the gradient and the chain rule were used to explain how gradients were propagated backward from the loss to earlier variables. Gradient descent was presented as the mechanism for updating parameters in the direction that reduces the loss. The roles of the forward pass and backward pass were clarified and combined to describe model training with backpropagation. Finally, these ideas were demonstrated through a PyTorch implementation using automatic differentiation. In the next notebook, a neural network will be trained using a more advanced training engine.</p>"},{"location":"notebooks/01_backprop/#01-from-derivatives-to-backpropagation","title":"01. From Derivatives to Backpropagation\u00b6","text":"25 Jan 2025 /   2 Feb 2026 <p>Tip</p> <p>      It will be helpful to revise the necessary Calculus needed for deep learning for understanding the material that follows.   </p>"},{"location":"notebooks/01_backprop/#differentiation","title":"Differentiation\u00b6","text":"<p>The code below shows the function $f(x) = x^2$ in action. Our goal is to see how much the output changes as we modify the input by some value h. You can modify and see behavior for more values on other functions as well.</p>"},{"location":"notebooks/01_backprop/#partial-derivatives","title":"Partial Derivatives\u00b6","text":""},{"location":"notebooks/01_backprop/#computation-graph","title":"Computation Graph\u00b6","text":"<p>Info</p> <p>The following source was consulted in preparing this material: Andrej Karpathy's lecture on Micrograd.</p> <p>Micrograd is an educational library that demonstrates the core ideas behind automatic differentiation. The idea behind it is the same one used in major deep learning frameworks. <code>Micrograd</code> closely mirrors the logic of the PyTorch autograd engine. In this and the following notebooks, we will modify <code>micrograd</code> further to improve correspondence with PyTorch and make the explanations clearer.</p>"},{"location":"notebooks/01_backprop/#gradient","title":"Gradient\u00b6","text":""},{"location":"notebooks/01_backprop/#chain-rule","title":"Chain Rule\u00b6","text":""},{"location":"notebooks/01_backprop/#gradient-descent","title":"Gradient Descent\u00b6","text":""},{"location":"notebooks/01_backprop/#forward-pass","title":"Forward Pass\u00b6","text":""},{"location":"notebooks/01_backprop/#backward-pass","title":"Backward Pass\u00b6","text":""},{"location":"notebooks/01_backprop/#training-model","title":"Training Model\u00b6","text":"<p>We can now train the model by repeatedly performing a forward pass, a backward pass, and an optimization (gradient descent) step to reduce the loss. The term backpropagation is often used informally to describe the entire training step of a neural network, but this is a convention rather than a precise definition. Strictly speaking, backpropagation refers only to the backward pass, where gradients of the loss with respect to all intermediate values and parameters are computed using the chain rule.</p>"},{"location":"notebooks/01_backprop/#backpropagation-with-pytorch","title":"Backpropagation with PyTorch\u00b6","text":"<p>Tip</p> <p> PyTorch is preinstalled and configured for Google Colab. For local development, PyTorch can be installed via <code>pip install torch</code>. If you plan to use a GPU, make sure the installed PyTorch version matches your CUDA setup.   </p> <p>As mentioned in the beginning, our manual implementation is built into PyTorch. We will do a forward and backward pass with the help of autograd engine and check if the gradients are what we had previosuly calculated. As gradients need not to be calculated for, say, leaf nodes, to optimizate calculation further, <code>requires_grad</code> is set to <code>False</code> by default, which we need to update.</p>"},{"location":"notebooks/02_neural_network/","title":"02. From Neuron to Neural Network","text":"<p>In the previous notebook on backpropagation we saw the internals of the autograd engine and a basic implementation of gradient descent. Our simplistic backpropagation algorithm was optimizing the output of a simple function. Towards the end, we also saw the PyTorch implementation of the learned concepts.</p> In\u00a0[1]: Copied! <pre># This is a graph visualization code from micrograd, no need to understand the details\n# https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb\nfrom graphviz import Digraph\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n\n    for n in nodes:\n        dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n\n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n</pre> # This is a graph visualization code from micrograd, no need to understand the details # https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb from graphviz import Digraph  def trace(root):     nodes, edges = set(), set()     def build(v):         if v not in nodes:             nodes.add(v)             for child in v._prev:                 edges.add((child, v))                 build(child)     build(root)     return nodes, edges  def draw_dot(root, format='svg', rankdir='LR'):     \"\"\"     format: png | svg | ...     rankdir: TB (top to bottom graph) | LR (left to right)     \"\"\"     assert rankdir in ['LR', 'TB']     nodes, edges = trace(root)     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})      for n in nodes:         dot.node(name=str(id(n)), label = \"{ %s | data %.3f | grad %.3f }\" % (n.label, n.data, n.grad), shape='record')         if n._op:             dot.node(name=str(id(n)) + n._op, label=n._op)             dot.edge(str(id(n)) + n._op, str(id(n)))      for n1, n2 in edges:         dot.edge(str(id(n1)), str(id(n2)) + n2._op)      return dot <p>The function $f(x) = x * w$ is a linear function always passing from origin. The real world data, however, will be much more complex, and in order to describe a pattern in the data our Machine Learning model should return a more flexible function. For that, we will do two things: add bias $b$ (recall affine transformations from linear algebra) and bring non-linearity with an activation function. We can choose different non-linear activation functions with the condition that it should be differentiable (otherwise we won't be able to calculate gradients for backpropagation). We will implement sigmoid (logistic) activation function which has the following formula:</p> <p>$$ \\sigma(x) = \\frac{1}{1 + e^{-x}}. $$</p> <p>Sigmoid function not only makes a linear function non-linear and continuous, but also maps any value of $x$ to be between $0$ and $1$. It may be useful when we want to, say, predict probabilities for different output classes.</p> In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[3]: Copied! <pre>def sigmoid(x):\n  return 1.0 / (1 + np.exp(-x))\n</pre> def sigmoid(x):   return 1.0 / (1 + np.exp(-x)) In\u00a0[4]: Copied! <pre># below is a simple linear function where any activation will be applied\n# recall that sigmoid is one of many possible choices\ndef f(x, w=0.5, b=10, activation=None):\n  out = x * w + b\n  return activation(out) if activation else out\n</pre> # below is a simple linear function where any activation will be applied # recall that sigmoid is one of many possible choices def f(x, w=0.5, b=10, activation=None):   out = x * w + b   return activation(out) if activation else out In\u00a0[5]: Copied! <pre>def plot(f, x, activation=None):\n  plt.figure(figsize=(4, 4))\n  x_all = np.linspace(-50, 50, 100)\n  y_all = f(x_all, activation=activation)\n  plt.plot(x_all, y_all)\n  plt.scatter(x, f(x, activation=activation), color='r')\n  plt.show()\n</pre> def plot(f, x, activation=None):   plt.figure(figsize=(4, 4))   x_all = np.linspace(-50, 50, 100)   y_all = f(x_all, activation=activation)   plt.plot(x_all, y_all)   plt.scatter(x, f(x, activation=activation), color='r')   plt.show() <p>What we wrote above is a bunch of helper functions to keep us flexible when testing different functions. We can start with a simple linear function with default weight and bias.</p> In\u00a0[6]: Copied! <pre>x = -20\nplot(f, x) # f(x)=0.5x+10\n</pre> x = -20 plot(f, x) # f(x)=0.5x+10 <p>Now we will plot the exact same point mapped into the non-linear sigmoid function, which has the range between $0$ and $1$.</p> <p>Tip</p> <p>     Try out different $x$ values and see the plots and gain intuition on how the mapping gets shifted.   </p> In\u00a0[7]: Copied! <pre>plot(f, x, sigmoid)\n</pre> plot(f, x, sigmoid) <p>Exercise</p> <p>     Implement other activation functions (e.g. ReLU) and see their plot. What could be the distadvantage of using sigmoid activation function?   </p> <p>We will now make some updates to the <code>Value</code> class which we have been developing over the course of the previous notebook. Not only the sigmoid function should be added to the class \u2013 we should be able to calculate the derivative of it as well.</p> <p>Exercise</p> <p>     Find the derivative of the sigmoid function. </p> In\u00a0[8]: Copied! <pre>class Value:\n  def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):\n    self.data = data\n    self._prev = _prev\n    self._op = _op\n    self.label = label\n    self._backward = lambda: None\n    self.grad = 0.0\n    self.requires_grad = requires_grad\n\n  def __add__(self, other):\n    data = self.data + other.data\n    out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad += 1.0 * out.grad\n      if other.requires_grad:\n        other.grad += 1.0 * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __mul__(self, other):\n    data = self.data * other.data\n    out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad += other.data * out.grad\n      if other.requires_grad:\n        other.grad += self.data * out.grad\n    out._backward = _backward\n\n    return out\n\n  def __sub__(self, other):\n    return self + (Value(-1) * other)\n\n  def sigmoid(self):\n    s = 1.0 / (1 + np.exp(-self.data))\n    out = Value(s, (self, ), 'sigmoid', self.requires_grad)\n\n    def _backward():\n      if self.requires_grad:\n        self.grad += s * (1.0 - s) * out.grad\n    out._backward = _backward\n\n    return out\n\n  def build_topo(self):\n    # Builds a topological ordering of the computation graph\n    topo = []\n    visited = set()\n\n    def _build_topo(node):\n      if node not in visited:\n        visited.add(node)\n        for child in node._prev:\n          _build_topo(child)\n        topo.append(node)\n    _build_topo(self)\n\n    return topo\n\n  def parameters(self):\n    # Returns all trainable nodes reachable from this node\n    topo = self.build_topo()\n    return [node for node in topo if node.requires_grad and len(node._prev) == 0]\n\n\n  def backward(self):\n    # Computes gradients for all nodes in the graph\n    if self.requires_grad:\n      topo = self.build_topo()\n      self.grad = 1.0\n      for node in reversed(topo):\n        node._backward()\n\n  def optimize(self, lr=0.01):\n    # Applies gradient descent to all trainable parameters\n    for node in self.parameters():\n      node.data -= lr * node.grad\n\n  def zero_(self):\n    self.grad = 0.0\n\n  def zero_grad(self):\n    # Resets gradients of all trainable parameters in the graph\n    for node in self.parameters():\n      node.grad = 0.0\n\n  def __repr__(self):\n    return f'Value(data={self.data}, grad={self.grad}, label={self.label})'\n</pre> class Value:   def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):     self.data = data     self._prev = _prev     self._op = _op     self.label = label     self._backward = lambda: None     self.grad = 0.0     self.requires_grad = requires_grad    def __add__(self, other):     data = self.data + other.data     out = Value(data, (self, other), '+', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad += 1.0 * out.grad       if other.requires_grad:         other.grad += 1.0 * out.grad     out._backward = _backward      return out    def __mul__(self, other):     data = self.data * other.data     out = Value(data, (self, other), '*', self.requires_grad or other.requires_grad)      def _backward():       if self.requires_grad:         self.grad += other.data * out.grad       if other.requires_grad:         other.grad += self.data * out.grad     out._backward = _backward      return out    def __sub__(self, other):     return self + (Value(-1) * other)    def sigmoid(self):     s = 1.0 / (1 + np.exp(-self.data))     out = Value(s, (self, ), 'sigmoid', self.requires_grad)      def _backward():       if self.requires_grad:         self.grad += s * (1.0 - s) * out.grad     out._backward = _backward      return out    def build_topo(self):     # Builds a topological ordering of the computation graph     topo = []     visited = set()      def _build_topo(node):       if node not in visited:         visited.add(node)         for child in node._prev:           _build_topo(child)         topo.append(node)     _build_topo(self)      return topo    def parameters(self):     # Returns all trainable nodes reachable from this node     topo = self.build_topo()     return [node for node in topo if node.requires_grad and len(node._prev) == 0]     def backward(self):     # Computes gradients for all nodes in the graph     if self.requires_grad:       topo = self.build_topo()       self.grad = 1.0       for node in reversed(topo):         node._backward()    def optimize(self, lr=0.01):     # Applies gradient descent to all trainable parameters     for node in self.parameters():       node.data -= lr * node.grad    def zero_(self):     self.grad = 0.0    def zero_grad(self):     # Resets gradients of all trainable parameters in the graph     for node in self.parameters():       node.grad = 0.0    def __repr__(self):     return f'Value(data={self.data}, grad={self.grad}, label={self.label})' <p>Once the engine is set, we are now ready to start our discussions and build our neural network step by step.</p>  Artificial Neuron ~ By Funcs - Own work, CC0, Link <p>An artificial neuron is simply a linear function passing through an activation function  (e.g. $\\sigma(x * w + b)$). The illustration above describes an $N$-dimensional neuron, accepting inputs $x_1, x_2, \\dots, x_n$. The function $f$ we had above is a very simple neuron with $1$-dimensional input.</p> <p>Exercise</p> <p>     What could be input values of a neuron for predicting the probability of a customer cancelling their subscription?   </p> <p>We will initially implement a simple <code>Neuron</code> class in 3D ($2$-dimensional input values and an output value). The function will receive two inputs <code>x1</code> and <code>x2</code>, which will become <code>Value</code> objects. Their weights <code>w1</code> and <code>w2</code> will be initialized randomly from the uniform distribution $w_1, w_2 \\sim \\mathcal{U}(-1, 1) $ and will determine how much each of the inputs influences the outcome.</p> In\u00a0[9]: Copied! <pre>class Neuron:\n  def __init__(self):\n    self.w1 = Value(np.random.uniform(-1, 1), label='w1', requires_grad=True)\n    self.w2 = Value(np.random.uniform(-1, 1), label='w2', requires_grad=True)\n    self.b = Value(0, label='b', requires_grad=True)\n\n  def __call__(self, x1, x2):\n    out = x1 * self.w1 + x2 * self.w2 + self.b\n    return out.sigmoid()\n</pre> class Neuron:   def __init__(self):     self.w1 = Value(np.random.uniform(-1, 1), label='w1', requires_grad=True)     self.w2 = Value(np.random.uniform(-1, 1), label='w2', requires_grad=True)     self.b = Value(0, label='b', requires_grad=True)    def __call__(self, x1, x2):     out = x1 * self.w1 + x2 * self.w2 + self.b     return out.sigmoid() <p>Now we can initialize our inputs and neuron to see our computation graph. Our loss will be simple: the squared error between the ground truth label $y$ and the predicted probability. It has a minimum exactly when prediction is equal to the ground truth. Thanks to our backpropagation engine, we now have the <code>backward()</code> function which can calculate the partial derivates for all the nodes. Once we plot the graph, pay attention that the input and leaf node gradients which we have no control over are not calculated, thanks to <code>requires_grad</code>.</p> <p>!!! note Our input values $x_1$ and $x_2$ may correspond to a customer who made the purchase ($y = 1$).</p> In\u00a0[10]: Copied! <pre>x1 = Value(2, label='x1')\nx2 = Value(3, label='x2')\ny  = Value(1, label= 'y')\n\nn = Neuron()\n\npred = n(x1, x2);              pred.label = 'pred'\nL = (y - pred) * (y - pred);   L.label = 'loss'\n\nL.backward()\n</pre> x1 = Value(2, label='x1') x2 = Value(3, label='x2') y  = Value(1, label= 'y')  n = Neuron()  pred = n(x1, x2);              pred.label = 'pred' L = (y - pred) * (y - pred);   L.label = 'loss'  L.backward() In\u00a0[11]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[11]: In\u00a0[12]: Copied! <pre># The helper function here is for plotting, no need to understand it\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_neuron_3d(neuron, x_range=(-5, 5), num=100, activation='sigmoid'):\n  x1_vals = np.linspace(x_range[0], x_range[1], num)\n  x2_vals = np.linspace(x_range[0], x_range[1], num)\n  X1, X2 = np.meshgrid(x1_vals, x2_vals)\n  Z = np.zeros_like(X1)\n\n  for i in range(X1.shape[0]):\n    for j in range(X1.shape[1]):\n      x1 = Value(X1[i, j])\n      x2 = Value(X2[i, j])\n      Z[i, j] = neuron(x1, x2).data\n\n  fig = plt.figure(figsize=(10, 8))\n  ax = fig.add_subplot(111, projection='3d')\n  ax.plot_surface(X1, X2, Z, cmap='viridis')\n  ax.set_title(f'Neuron output with {activation} activation')\n  plt.show()\n</pre> # The helper function here is for plotting, no need to understand it from mpl_toolkits.mplot3d import Axes3D  def plot_neuron_3d(neuron, x_range=(-5, 5), num=100, activation='sigmoid'):   x1_vals = np.linspace(x_range[0], x_range[1], num)   x2_vals = np.linspace(x_range[0], x_range[1], num)   X1, X2 = np.meshgrid(x1_vals, x2_vals)   Z = np.zeros_like(X1)    for i in range(X1.shape[0]):     for j in range(X1.shape[1]):       x1 = Value(X1[i, j])       x2 = Value(X2[i, j])       Z[i, j] = neuron(x1, x2).data    fig = plt.figure(figsize=(10, 8))   ax = fig.add_subplot(111, projection='3d')   ax.plot_surface(X1, X2, Z, cmap='viridis')   ax.set_title(f'Neuron output with {activation} activation')   plt.show() In\u00a0[13]: Copied! <pre>plot_neuron_3d(n)\n</pre> plot_neuron_3d(n) <p>The ground truth label tells us that we should push our neuron output towards <code>1.0</code>. In other words, as our loss $L$ here is a simple squared error value corresponding to $(y - y_hat)^2$, we should try to minimize the loss down to zero with backpropagation. Let's repeat the backpropagation in multiple epochs to reduce loss. We will also print the parameters to see when our neuron function returns a higher probability for the given input values. And we will make sure to not forget to reset the gradients. What you see below is a typical training workflow you will encounter in many implementations, most closely associated with PyTorch.</p> In\u00a0[14]: Copied! <pre>x1 = Value(2, label='x1')\nx2 = Value(3, label='x2')\ny  = Value(1, label= 'y')\n\nn = Neuron()\n</pre> x1 = Value(2, label='x1') x2 = Value(3, label='x2') y  = Value(1, label= 'y')  n = Neuron() In\u00a0[15]: Copied! <pre># hyperparameters\nlr = 0.01\nepochs = 1000\n\nfor e in range(epochs):\n  # forward pass\n  pred = n(x1, x2)\n  L = (y - pred) * (y - pred)\n\n  # gradient reset\n  L.zero_grad()\n  # backward pass\n  L.backward()\n  # gradient descent\n  L.optimize(lr)\n\n  if (e + 1) % 100 == 0:\n    print(f'Epoch [{e + 1}/{epochs}], Loss: {L.data:.4f}')\n\nprint(f'\\nInputs: {x1, x2}')\nprint(f'Parameters: {n.w1} {n.w2} {n.b}')\nprint(f'Prediction probability: {pred.data}')\n</pre> # hyperparameters lr = 0.01 epochs = 1000  for e in range(epochs):   # forward pass   pred = n(x1, x2)   L = (y - pred) * (y - pred)    # gradient reset   L.zero_grad()   # backward pass   L.backward()   # gradient descent   L.optimize(lr)    if (e + 1) % 100 == 0:     print(f'Epoch [{e + 1}/{epochs}], Loss: {L.data:.4f}')  print(f'\\nInputs: {x1, x2}') print(f'Parameters: {n.w1} {n.w2} {n.b}') print(f'Prediction probability: {pred.data}')  <pre>Epoch [100/1000], Loss: 0.9761\nEpoch [200/1000], Loss: 0.9648\nEpoch [300/1000], Loss: 0.9352\nEpoch [400/1000], Loss: 0.7494\nEpoch [500/1000], Loss: 0.0621\nEpoch [600/1000], Loss: 0.0185\nEpoch [700/1000], Loss: 0.0102\nEpoch [800/1000], Loss: 0.0069\nEpoch [900/1000], Loss: 0.0052\nEpoch [1000/1000], Loss: 0.0042\n\nInputs: (Value(data=2, grad=0.0, label=x1), Value(data=3, grad=0.0, label=x2))\nParameters: Value(data=0.07754978581518703, grad=-0.015539894979151484, label=w1) Value(data=0.6650625956942215, grad=-0.023309842468727228, label=w2) Value(data=0.526204300126792, grad=-0.007769947489575742, label=b)\nPrediction probability: 0.9355595866688061\n</pre> <p>We have just now trained our $2$-dimensional input neuron to find suitable parameter values for achieving a high probability for the given input values. Now we would like to create an $N$-dimensional neuron which will accept much more inputs, similar to what we saw in the illustration of artificial neuron. As a consequence, our neuron will have to learn the parameter values for $N$-dimensional weights:  $w_1, w_2, \\dots, w_n$.</p> In\u00a0[16]: Copied! <pre>class Neuron:\n  def __init__(self, N):\n    self.W = [Value(np.random.uniform(-1, 1), label=f'w{i}', requires_grad=True) for i in range(N)]\n    self.b = Value(0, label='b', requires_grad=True)\n\n  def __call__(self, X):\n    out = sum((x * w for x, w in zip(X, self.W)), self.b)\n    return out.sigmoid()\n</pre> class Neuron:   def __init__(self, N):     self.W = [Value(np.random.uniform(-1, 1), label=f'w{i}', requires_grad=True) for i in range(N)]     self.b = Value(0, label='b', requires_grad=True)    def __call__(self, X):     out = sum((x * w for x, w in zip(X, self.W)), self.b)     return out.sigmoid() <p>We will now see the training output of our $N$-dimensional neuron which will accept $N$ <code>Value</code> inputs as a list. Recall from your introductory machine learning course that linear model followed by a sigmoid (logistic) activation forms the model used in logistic regression.</p> In\u00a0[17]: Copied! <pre>X = [Value(x, label=f'x{i}') for i, x in enumerate([5, 0.4, -1, -2])]\n\nn = Neuron(len(X))\n\npred = n(X);                   pred.label = 'pred'\nL = (y - pred) * (y - pred);   L.label = 'loss'\n</pre> X = [Value(x, label=f'x{i}') for i, x in enumerate([5, 0.4, -1, -2])]  n = Neuron(len(X))  pred = n(X);                   pred.label = 'pred' L = (y - pred) * (y - pred);   L.label = 'loss' In\u00a0[18]: Copied! <pre>draw_dot(L)\n</pre> draw_dot(L) Out[18]: In\u00a0[19]: Copied! <pre># hyperparameters\nlr = 0.01\nepochs = 1000\n\nfor e in range(epochs):\n  pred = n(X)\n  L = (y - pred) * (y - pred)\n\n  L.zero_grad()\n  L.backward()\n  L.optimize(lr)\n\n  if (e + 1) % 100 == 0:\n    print(f'Epoch [{e + 1}/{epochs}], Loss: {L.data:.4f}')\n\nprint(f'\\nInputs: {X}')\nprint(f'Parameters: {n.W} {n.b}')\nprint(f'Prediction probability: {pred.data}')\n</pre> # hyperparameters lr = 0.01 epochs = 1000  for e in range(epochs):   pred = n(X)   L = (y - pred) * (y - pred)    L.zero_grad()   L.backward()   L.optimize(lr)    if (e + 1) % 100 == 0:     print(f'Epoch [{e + 1}/{epochs}], Loss: {L.data:.4f}')  print(f'\\nInputs: {X}') print(f'Parameters: {n.W} {n.b}') print(f'Prediction probability: {pred.data}') <pre>Epoch [100/1000], Loss: 0.0087\nEpoch [200/1000], Loss: 0.0045\nEpoch [300/1000], Loss: 0.0030\nEpoch [400/1000], Loss: 0.0023\nEpoch [500/1000], Loss: 0.0018\nEpoch [600/1000], Loss: 0.0015\nEpoch [700/1000], Loss: 0.0013\nEpoch [800/1000], Loss: 0.0011\nEpoch [900/1000], Loss: 0.0010\nEpoch [1000/1000], Loss: 0.0009\n\nInputs: [Value(data=5, grad=0.0, label=x0), Value(data=0.4, grad=0.0, label=x1), Value(data=-1, grad=0.0, label=x2), Value(data=-2, grad=0.0, label=x3)]\nParameters: [Value(data=0.5976493875597305, grad=-0.008520328402593043, label=w0), Value(data=-0.8659527976184913, grad=-0.0006816262722074435, label=w1), Value(data=-0.9094188137928199, grad=0.0017040656805186085, label=w2), Value(data=0.06623841805723128, grad=0.003408131361037217, label=w3)] Value(data=0.07054619736664514, grad=-0.0017040656805186085, label=b)\nPrediction probability: 0.9703680714362221\n</pre>  Artificial Neuron Network ~ By en:User:Cburnett - Own work\u00a0This W3C-unspecified vector image was created with Inkscape ., CC BY-SA 3.0, Link <p>We managed to train our single neuron to learn a function for our input values. In reality, however, data is much more complex and we need to learn more complication functions. How to achieve that? By chaining many neurons together.</p> <p>You can think that each neuron will basically learn some portion of the overall function. What we see above is an illustration of an artificial neural network. In the input layer we have three neurons, each separately accepting $N$-dimensional input values. The output values of each neuron are then fully connected, as inputs to the hidden layer with four neurons (note that there can be more than one hidden layer). And finally, the output of hidden layer neurons are passed as inputs to the output layer, which may, for example, predict probability scores for two classes.</p> <p>We will now try to implement a fully connected feedforward neural network, which is often referred to as multilayer perceptron (MLP).</p> In\u00a0[20]: Copied! <pre>class Layer:\n  def __init__(self, N, count):\n    self.neurons = [Neuron(N) for _ in range(count)]\n\n  def __call__(self, X):\n    outs = [n(X) for n in self.neurons]\n    return outs[0] if len(outs) == 1 else outs # flattening dimension if a single element\n</pre> class Layer:   def __init__(self, N, count):     self.neurons = [Neuron(N) for _ in range(count)]    def __call__(self, X):     outs = [n(X) for n in self.neurons]     return outs[0] if len(outs) == 1 else outs # flattening dimension if a single element <p>The code above creates a list of <code>count</code> number of neurons, each accepting <code>N</code> dimensional input. Let's build our layers shown in the illustration above and connect them. Note that the input dimension of the next layer is the amount of neurons in the previous layer.</p> In\u00a0[21]: Copied! <pre># input data and its dimension\nX = [Value(x, label=f'x{i}') for i, x in enumerate([1, 4, -3, -2, 3])]\nN = len(X)\n</pre> # input data and its dimension X = [Value(x, label=f'x{i}') for i, x in enumerate([1, 4, -3, -2, 3])] N = len(X) In\u00a0[22]: Copied! <pre># creating layers\nin_layer = Layer(N, 3)\nhid_layer = Layer(3, 4)\nout_layer = Layer(4, 2)\n</pre> # creating layers in_layer = Layer(N, 3) hid_layer = Layer(3, 4) out_layer = Layer(4, 2) In\u00a0[23]: Copied! <pre># output of each layer is input to the next\nX_hidden = in_layer(X)\nX_output = hid_layer(X_hidden)\nout = out_layer(X_output)\n</pre> # output of each layer is input to the next X_hidden = in_layer(X) X_output = hid_layer(X_hidden) out = out_layer(X_output) In\u00a0[24]: Copied! <pre># let's plot either one of the outputs\ndraw_dot(out[0])\n</pre> # let's plot either one of the outputs draw_dot(out[0]) Out[24]: <p>We will further abstract away the neuron and layer creation inside the <code>MLP</code> class. Here, <code>counts</code> will provide the length of each layer as a list. We will then reimplement the exact same network.</p> In\u00a0[25]: Copied! <pre>class MLP:\n  def __init__(self, N, counts):\n    dims = [N] + counts # concatenates dimensions\n    self.layers = [Layer(dims[i], dims[i+1]) for i in range(len(dims)-1)]\n\n  def __call__(self, X):\n    out = X\n    for layer in self.layers:\n      out = layer(out)\n    return out\n</pre> class MLP:   def __init__(self, N, counts):     dims = [N] + counts # concatenates dimensions     self.layers = [Layer(dims[i], dims[i+1]) for i in range(len(dims)-1)]    def __call__(self, X):     out = X     for layer in self.layers:       out = layer(out)     return out In\u00a0[26]: Copied! <pre>nn = MLP(N, [3, 4, 2])\nout = nn(X)\ndraw_dot(out[0]) # out[1] will return the second output\n</pre> nn = MLP(N, [3, 4, 2]) out = nn(X) draw_dot(out[0]) # out[1] will return the second output Out[26]: <p>It is time to evaluate our network. Even though applying multilayer perceptron to the <code>make_moons</code> dataset is overkill (as the dataset is simple), it will be a nice demonstration of our  classifier's capacity. This time, we will be imitating the well-known scikit-learn library.</p> <p>The <code>make_moons</code> dataset consists of $2$-dimensional input samples with two possible output classes. Each sample represents a point in the plane, and the task is to classify it into one of the two interleaving half-moon shapes. Because the classes are not linearly separable, simple linear models such as logistic regression are limited when solving this task.</p> In\u00a0[27]: Copied! <pre>from sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=500, noise=0.1, random_state=42)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nprint(f'Train data shape: {X_train.shape}, {y_train.shape}')\nprint(f'Test data shape: {X_test.shape}, {y_test.shape}')\nprint(f'Input Samples:\\n {X_train[:5]}')\nprint(f'Labels:\\n {y_train[:5]}')\n</pre> from sklearn.datasets import make_moons from sklearn.model_selection import train_test_split  X, y = make_moons(n_samples=500, noise=0.1, random_state=42)  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  print(f'Train data shape: {X_train.shape}, {y_train.shape}') print(f'Test data shape: {X_test.shape}, {y_test.shape}') print(f'Input Samples:\\n {X_train[:5]}') print(f'Labels:\\n {y_train[:5]}')  <pre>Train data shape: (400, 2), (400,)\nTest data shape: (100, 2), (100,)\nInput Samples:\n [[ 0.74890763 -0.43771955]\n [ 1.52835581 -0.39408701]\n [ 0.25912638  0.45185818]\n [-0.94183179  0.48862655]\n [ 1.91516125  0.13943777]]\nLabels:\n [1 1 1 0 1]\n</pre> <p>Let's try out two provided classifiers, <code>LogisticRegression</code> and <code>MLPClassifier</code>, the latter of which can be seen as a generalization of the former, which we will soon implement in its simplistic form. As we have already discussed, <code>LogisticRegression</code> is basically our <code>Neuron</code> class which uses sigmoid (logistic) function as its activation. And in fact, logistic regression will simply be our MLP with the layer size for just a single neuron. Thanks to <code>numpy</code> vectorization and other optimizations, the <code>sklearn</code> implementations will be extremely quick.</p> In\u00a0[28]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n</pre> from sklearn.linear_model import LogisticRegression from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score In\u00a0[29]: Copied! <pre>model = LogisticRegression()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\naccuracy = accuracy_score(y_test, preds)\nf\"Logistic Regression Accuracy: {accuracy:.2f}\"\n</pre> model = LogisticRegression() model.fit(X_train, y_train) preds = model.predict(X_test) accuracy = accuracy_score(y_test, preds) f\"Logistic Regression Accuracy: {accuracy:.2f}\" Out[29]: <pre>'Logistic Regression Accuracy: 0.84'</pre> In\u00a0[30]: Copied! <pre>model = MLPClassifier(max_iter=1000)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\naccuracy = accuracy_score(y_test, preds)\nf\"MLP Classifier Accuracy: {accuracy:.2f}\"\n</pre> model = MLPClassifier(max_iter=1000) model.fit(X_train, y_train) preds = model.predict(X_test) accuracy = accuracy_score(y_test, preds) f\"MLP Classifier Accuracy: {accuracy:.2f}\" Out[30]: <pre>'MLP Classifier Accuracy: 1.00'</pre> In\u00a0[31]: Copied! <pre>X_train = [[Value(x) for x in sample] for sample in X_train]\nX_test  = [[Value(x) for x in sample] for sample in X_test]\ny_train = [Value(y) for y in y_train]\ny_test  = [Value(y) for y in y_test]\n</pre> X_train = [[Value(x) for x in sample] for sample in X_train] X_test  = [[Value(x) for x in sample] for sample in X_test] y_train = [Value(y) for y in y_train] y_test  = [Value(y) for y in y_test] <p>We will attept to train our classifier in the style of <code>scikit-learn</code> models, more specifically MLPClassifier. Since our minimal <code>Value</code> class does not include logarithms yet, we will train using mean squared error as a simple alternative.</p> <p>Exercise</p> <p>     In standard binary classification, the usual choice is binary cross-entropy (log loss). Try to modify our backpropagation engine to include the operation of logarithm and update our loss function below to achieve a higher accuracy. We will introduce the concept of entropy when discussing information theory.   </p> In\u00a0[32]: Copied! <pre>class Classifier:\n  def __init__(self, layer_sizes=[3, 1]):\n    self.layer_sizes = layer_sizes\n    self.nn = None\n    self.L = None\n    self.iterations = 0\n\n  def forward(self, Xs):\n    return [self.nn(X) for X in Xs]\n\n  def predict(self, X_test):\n    return self.forward(X_test)\n\n  def train(self, X_train, y_train, lr, epoch, num_epochs, verbose):\n    preds = self.forward(X_train)\n    self.L = self.mean_squared_error(y_train, preds)\n    self.L.zero_grad()\n    self.L.backward()\n    self.L.optimize(lr=lr)\n\n    if verbose:\n      print(f'[{epoch}/{num_epochs}] loss: {self.L.data:.4f}')\n\n  def fit(self, X_train, y_train, lr=0.01, num_epochs=1000):\n    if self.nn is None:\n      self.nn = MLP(len(X_train[0]), self.layer_sizes)\n    for i in range(num_epochs):\n      do_print = (i % 100 == 99) or (i == 0)\n      self.train(X_train, y_train, lr, i + 1, num_epochs, do_print)\n    self.iterations += num_epochs\n\n  def mean_squared_error(self, y_train, preds):\n    inv_n = Value(1.0 / len(y_train))\n    return sum([(y - p) * (y - p) for y, p in zip(y_train, preds)],Value(0)) * inv_n\n\n  def score(self, y_test, preds, threshold=0.5):\n    correct = 0\n    for y, p in zip(y_test, preds):\n      pred_label = 1 if p.data &gt;= threshold else 0\n      true_label = 1 if y.data &gt;= 0.5 else 0\n      correct += (pred_label == true_label)\n    return correct / len(y_test)\n</pre> class Classifier:   def __init__(self, layer_sizes=[3, 1]):     self.layer_sizes = layer_sizes     self.nn = None     self.L = None     self.iterations = 0    def forward(self, Xs):     return [self.nn(X) for X in Xs]    def predict(self, X_test):     return self.forward(X_test)    def train(self, X_train, y_train, lr, epoch, num_epochs, verbose):     preds = self.forward(X_train)     self.L = self.mean_squared_error(y_train, preds)     self.L.zero_grad()     self.L.backward()     self.L.optimize(lr=lr)      if verbose:       print(f'[{epoch}/{num_epochs}] loss: {self.L.data:.4f}')    def fit(self, X_train, y_train, lr=0.01, num_epochs=1000):     if self.nn is None:       self.nn = MLP(len(X_train[0]), self.layer_sizes)     for i in range(num_epochs):       do_print = (i % 100 == 99) or (i == 0)       self.train(X_train, y_train, lr, i + 1, num_epochs, do_print)     self.iterations += num_epochs    def mean_squared_error(self, y_train, preds):     inv_n = Value(1.0 / len(y_train))     return sum([(y - p) * (y - p) for y, p in zip(y_train, preds)],Value(0)) * inv_n    def score(self, y_test, preds, threshold=0.5):     correct = 0     for y, p in zip(y_test, preds):       pred_label = 1 if p.data &gt;= threshold else 0       true_label = 1 if y.data &gt;= 0.5 else 0       correct += (pred_label == true_label)     return correct / len(y_test) <p>Our classifier is ready and we can now train our model and note the accuracy. However, unlike the optimized classifiers of the <code>sklearn</code> library, it will be much slower and inefficient. Try out experiments with different hyperparameters (e.g. layer sizes, learning rates), and notice how it affects the training process and loss. As mentioned earlier, logistic regression is recovered by using a network with no hidden layer and a single sigmoid output with <code>layer_sizes=[1]</code>.</p> In\u00a0[33]: Copied! <pre># model = Classifier([1])\nmodel = Classifier([4, 1])\n</pre> # model = Classifier([1]) model = Classifier([4, 1]) In\u00a0[34]: Copied! <pre>model.fit(X_train, y_train, lr=0.1, num_epochs=1000)\n</pre> model.fit(X_train, y_train, lr=0.1, num_epochs=1000) <pre>[1/1000] loss: 0.2977\n[100/1000] loss: 0.2506\n[200/1000] loss: 0.2343\n[300/1000] loss: 0.2134\n[400/1000] loss: 0.1866\n[500/1000] loss: 0.1591\n[600/1000] loss: 0.1374\n[700/1000] loss: 0.1228\n[800/1000] loss: 0.1132\n[900/1000] loss: 0.1066\n[1000/1000] loss: 0.1019\n</pre> In\u00a0[35]: Copied! <pre>preds = model.predict(X_train)\nprint(f'Custom MLP classifier accuracy on train data: {model.score(y_train, preds):.2f}')\n</pre> preds = model.predict(X_train) print(f'Custom MLP classifier accuracy on train data: {model.score(y_train, preds):.2f}') <pre>Custom MLP classifier accuracy on train data: 0.85\n</pre> In\u00a0[36]: Copied! <pre>preds = model.predict(X_test)\nprint(f'Custom MLP classifier accuracy on test data: {model.score(y_test, preds):.2f}')\n</pre> preds = model.predict(X_test) print(f'Custom MLP classifier accuracy on test data: {model.score(y_test, preds):.2f}') <pre>Custom MLP classifier accuracy on test data: 0.82\n</pre> <p>We had just built and trained a neural network step by step, starting with activation functions and updated backpropagation engine. We defined an artificial neuron as a weighted sum followed by a nonlinear activation, extended it to an $N$-dimensional neuron, and combined multiple neurons into layers to form a multilayer perceptron. We then trained this network on a real dataset and wrapped it in a custom MLP classifier with a familiar <code>sklearn</code> interface. In the next notebook, we will introduce convolutional neural networks (CNN) and apply them to the well-known handwritten digit dataset.</p>"},{"location":"notebooks/02_neural_network/#02-from-neuron-to-neural-network","title":"02. From Neuron to Neural Network\u00b6","text":"8 Feb 2025 /   6 Feb 2026 <p>Info</p> <p>The following source was consulted in preparing this material: Andrej Karpathy's lecture on Micrograd.</p>"},{"location":"notebooks/02_neural_network/#activation-function","title":"Activation Function\u00b6","text":""},{"location":"notebooks/02_neural_network/#backpropagation-engine","title":"Backpropagation Engine\u00b6","text":"<p>At this point, we need to take a breath and improve our <code>Value</code> class. This step will make our future demonstrations smooth for discussing artificial neurons and neural network.</p> <p>As have been noted, adding activation (sigmoid) logic should  be the first step. However, we will make further adjustments to align our code with the PyTorch implementation we had discussed earlier and simplify our future workflow. Understanding the details in the code is preferable but not necessary. Gaining intuition for the purpose of each function is enough to proceed to the next sections.</p> <p>We add the <code>requires_grad</code> flag to tell us which parameters are trainable and requires gradient calculation and update. For example, it doesn't make sense to modify the training inputs for our model which is provided to us through collected data. Indeed, we shouldn't spend resources for calculating unnecessary gradients. Our goal is to nudge only the parameters, as well as the nodes dependent on them, in order to minimize the eventual loss.</p> <p>Our computation graph will soon be much bigger than what it was before. The <code>_backward</code> pass function we call manually on each node is not scalable. Ideally, we should have a single function <code>backward()</code> to calculate all the gradients, which we previously saw in the PyTorch implementation. For that, we will need to sort the nodes of the computation graph (from input nodes until the output node). We can achieve that with the topological sort algorithm implemented for micrograd.</p> <p>Lastly, we will add the function <code>optimize()</code> which will calculate gradient descent while using this sorted topology. Instead of overriding gradients (<code>=</code>), we will accumulate them (<code>+=</code>) to avoid gradient update bugs when using the same node more than once in an operation. And as a consequence, we will have to reset gradients with <code>zero_()</code> (again, similar to PyTorch) so that the gradients of different backward passes will not affect each other (it serves the exact same purpose as <code>self.grad = 0.0</code> was serving before the gradient accumulation).</p> <p>Important</p> <p>     Although we try to imitate PyTorch\u2019s API as closely as possible, there is a certain difference in how gradient resetting is handled.     In our implementation, <code>Value.zero_()</code> resets only the gradient stored in this single <code>Value</code> node (<code>self.grad</code>), while <code>Value.zero_grad()</code> clears gradients for all trainable nodes collected in <code>self.params</code>.     In PyTorch, <code>tensor.zero_()</code> is a general in-place operation that zeros the tensor\u2019s data (not its gradients), whereas gradient clearing is typically performed via <code>optimizer.zero_grad()</code>, which resets gradients for all parameters tracked by the optimizer (an individual gradient tensor may also be reset via <code>param.grad.zero_()</code> if it exists).   </p>"},{"location":"notebooks/02_neural_network/#artificial-neuron","title":"Artificial Neuron\u00b6","text":""},{"location":"notebooks/02_neural_network/#n-dimensional-neuron","title":"N-dimensional Neuron\u00b6","text":""},{"location":"notebooks/02_neural_network/#artificial-neural-network","title":"Artificial Neural Network\u00b6","text":""},{"location":"notebooks/02_neural_network/#training-on-a-dataset","title":"Training on a Dataset\u00b6","text":""},{"location":"notebooks/02_neural_network/#custom-mlp-classifier","title":"Custom MLP Classifier\u00b6","text":"<p>We will now prepare our dataset by converting data items to <code>Value</code> objects.</p>"},{"location":"notebooks/03_cnn_torch/","title":"03. From Kernel to Convolutional Neural Network","text":"<p>In previous notebooks, we learned how to build a neural network. We will  discuss many optimization and regularization techniques (e.g. parameter initialization, momentum, weight decay, dropout, batch normalization, etc.) to improve the efficiency of our training. But before that, we need to understand convolutional neural networks (CNN) which are used in image processing and gain a basic understanding of the PyTorch framework. And even before that, we need to know what an image is.</p> In\u00a0[1]: Copied! <pre>import torch\n</pre> import torch <p>Tip</p> <p>     PyTorch libraries come pre-installed in Google Colab. If you are using local development, you should use official PyTorch installer.   </p> <p>The code below is for plotting purposes. Even though we call it a grayscale image, matplotlib is actually displaying the matrix values using a color map (<code>cmap</code>), meaning the intensities could be mapped not only to black and white but to any range of two colors depending on the chosen color map.</p> In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\n%matplotlib inline\n\n# this is for plotting, no need to understand\ndef plot(img_tensors, titles=None, cmap='gray'):\n  if not isinstance(img_tensors, list):\n    img_tensors = [img_tensors]\n  if not isinstance(titles, list):\n    titles = [titles] * len(img_tensors)\n  fig, axes = plt.subplots(1, len(img_tensors), figsize=(len(img_tensors) * 4, 4))\n  if len(img_tensors) == 1:\n    axes = [axes]\n\n  for ax, img_tensor, title in zip(axes, img_tensors, titles):\n    img_tensor = img_tensor.cpu()\n    img = img_tensor.permute(1, 2, 0).numpy()\n    ax.imshow(img, cmap=cmap)\n    ax.set_title(title)\n    ax.axis(\"off\")\n  plt.show()\n</pre> import matplotlib.pyplot as plt %matplotlib inline  # this is for plotting, no need to understand def plot(img_tensors, titles=None, cmap='gray'):   if not isinstance(img_tensors, list):     img_tensors = [img_tensors]   if not isinstance(titles, list):     titles = [titles] * len(img_tensors)   fig, axes = plt.subplots(1, len(img_tensors), figsize=(len(img_tensors) * 4, 4))   if len(img_tensors) == 1:     axes = [axes]    for ax, img_tensor, title in zip(axes, img_tensors, titles):     img_tensor = img_tensor.cpu()     img = img_tensor.permute(1, 2, 0).numpy()     ax.imshow(img, cmap=cmap)     ax.set_title(title)     ax.axis(\"off\")   plt.show() <p>We will now generate matrices of dimension $height \\times width$. In case of grayscale image, we need only a single matrix. For true color images, we need three of such matrices, one for each color channel (RGB). To generate random intensities of the required shape, we will call <code>torch.rand</code>.</p> In\u00a0[3]: Copied! <pre>color = ['gray','rgb'][0] # change 0 to 1 for rgb\n\nH = W = 6\nC = 3 if color == 'rgb' else 1\n\nimg = torch.rand((C, H, W))\nplot(img)\n</pre> color = ['gray','rgb'][0] # change 0 to 1 for rgb  H = W = 6 C = 3 if color == 'rgb' else 1  img = torch.rand((C, H, W)) plot(img) In\u00a0[4]: Copied! <pre>img\n</pre> img Out[4]: <pre>tensor([[[0.3080, 0.0262, 0.5574, 0.4494, 0.2400, 0.8864],\n         [0.2101, 0.3828, 0.2922, 0.1948, 0.1911, 0.4098],\n         [0.5587, 0.7565, 0.3989, 0.3920, 0.1915, 0.2599],\n         [0.1623, 0.2396, 0.0493, 0.6253, 0.5448, 0.6362],\n         [0.2075, 0.0440, 0.0468, 0.0656, 0.8846, 0.8893],\n         [0.9567, 0.7053, 0.4556, 0.0996, 0.2754, 0.6344]]])</pre> <p>Technically, if we have image labels, and enough images in the dataset, we can already train a neural network to make predictions. For that, we can turn our 2D (grayscale) or 3D (RGB) tensors into $1$-dimensional vectors and feed in to our model. For example, and image with input dimensions $3 \\times 32 \\times 32$, which corresponds to a $32 \\times 32$ pixel RGB image, can be reshaped into a single dimensional vector of size <code>3*32*32</code> which will be the input dimension of our network.</p> <p>Exercise</p> <p>      What will be the shape of an $6 \\times 6$ grayscale image after reshaping?   </p> <p>At this point, we will not delve into details. It is worth, however, to understand that <code>img.reshape(-1)</code>, <code>img.reshape(1,-1)</code>, <code>img.reshape(1,1,-1)</code> will all return a flattened image matrix that will look similar while having different dimensions (shapes).</p> <p>Tip</p> <p>      See this numpy tutorial to gain experinece with matrix manipulations and other useful operations (e.g. broadcasting).   </p> In\u00a0[5]: Copied! <pre>img.reshape(1, 1, -1)\n</pre> img.reshape(1, 1, -1) Out[5]: <pre>tensor([[[0.3080, 0.0262, 0.5574, 0.4494, 0.2400, 0.8864, 0.2101, 0.3828,\n          0.2922, 0.1948, 0.1911, 0.4098, 0.5587, 0.7565, 0.3989, 0.3920,\n          0.1915, 0.2599, 0.1623, 0.2396, 0.0493, 0.6253, 0.5448, 0.6362,\n          0.2075, 0.0440, 0.0468, 0.0656, 0.8846, 0.8893, 0.9567, 0.7053,\n          0.4556, 0.0996, 0.2754, 0.6344]]])</pre> <p>We will download and train our model on the MNIST dataset mentioned in our introduction, which has 70,000 grayscale $28 \\times 28$ images of hand-written digits with their labels. Our main goal, for now, is to familiarize ourselves with PyTorch.</p> <p>Tip</p> <p>      See the official Datasets and DataLoaders tutorial for further guidance.   </p> <p>The code below will download and load the MNIST dataset by enforcing tensor-valued representations of images.</p> In\u00a0[6]: Copied! <pre>from torchvision import datasets, transforms\n\n# modify the root path accordingly (e.g. './data')\ntrain_data = datasets.MNIST(root='../../_drafts/data', train=True,  transform=transforms.ToTensor(), download=True)\ntest_data  = datasets.MNIST(root='../../_drafts/data', train=False, transform=transforms.ToTensor(), download=True)\n</pre> from torchvision import datasets, transforms  # modify the root path accordingly (e.g. './data') train_data = datasets.MNIST(root='../../_drafts/data', train=True,  transform=transforms.ToTensor(), download=True) test_data  = datasets.MNIST(root='../../_drafts/data', train=False, transform=transforms.ToTensor(), download=True) <p>Exercise</p> <p>       What other transforms can we apply to our dataset and why?What is the use of <code>num_workers</code> and <code>batch_size</code> demonstrated below? Why do we shuffle the <code>train</code> data but not the <code>test</code> data?   </p> In\u00a0[7]: Copied! <pre>train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, num_workers=2, shuffle=True)\ntest_loader  = torch.utils.data.DataLoader(test_data,  batch_size=64, num_workers=2, shuffle=False)\n</pre> train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, num_workers=2, shuffle=True) test_loader  = torch.utils.data.DataLoader(test_data,  batch_size=64, num_workers=2, shuffle=False) In\u00a0[9]: Copied! <pre>X_train, y_train = next(iter(train_loader)) # gets the images of the batch\nplot(X_train[0], f'Label: {y_train[0].item()} | {X_train[0].shape}')\n</pre> X_train, y_train = next(iter(train_loader)) # gets the images of the batch plot(X_train[0], f'Label: {y_train[0].item()} | {X_train[0].shape}') <p>In PyTorch, there are more than one way to manipulate a multi-dimensional tensor into a different dimension. It is worth understanding the internals of PyTorch and the workings of the equivalent methods below, especially the difference of memory management between <code>reshape</code> and <code>view</code> functions. We can see the number of elements (pixels) with <code>numel</code> function as described below.</p> In\u00a0[10]: Copied! <pre># X_train[0].view(-1).shape[0]\n# X_train[0].reshape(-1).shape[0]\n# X_train[0].flatten().shape[0]\nX_train[0].numel() # 1x28x28\n</pre> # X_train[0].view(-1).shape[0] # X_train[0].reshape(-1).shape[0] # X_train[0].flatten().shape[0] X_train[0].numel() # 1x28x28 Out[10]: <pre>784</pre> <p>We will now build a simple neural network and train our model on MNIST. This time, not from the scratch :). When we build our custom network, we need to inherit from <code>torch.nn.Module</code> which will handle many useful operations (e.g. automatic differentiation). It will also enforce us to define our forward pass function. Our activation function will be non-linear rectified linear unit.</p> <p>Fully connected (FC) layer (also known as Dense layer) is simply a network layer where all the neurons are connected to the previous layer neurons (recall multilayer perceptrons (MLP)). When applying forward pass, we should reshape our input dimensions from <code>(B, C, H, W)</code> to <code>(B, C*H*W)</code>, so that we can pass the input from <code>train_loader</code> to our FC layer.</p> <p>Exercise</p> <p>        Implement different ways of reshaping tensor <code>(B, C, H, W)</code> to <code>(B, C * H * W)</code>.   </p> In\u00a0[11]: Copied! <pre>import torch.nn as nn\nimport torch.optim as optim\n\nclass MLP(nn.Module):\n  def __init__(self, input_size, hidden_size, output_size):\n    super(MLP, self).__init__()\n    self.fc1 = nn.Linear(input_size, hidden_size)\n    self.fc2 = nn.Linear(hidden_size, output_size)\n    self.relu = nn.ReLU()\n\n  def forward(self, X):\n    X = X.view(X.shape[0], -1)\n    return self.fc2(self.relu(self.fc1(X)))\n</pre> import torch.nn as nn import torch.optim as optim  class MLP(nn.Module):   def __init__(self, input_size, hidden_size, output_size):     super(MLP, self).__init__()     self.fc1 = nn.Linear(input_size, hidden_size)     self.fc2 = nn.Linear(hidden_size, output_size)     self.relu = nn.ReLU()    def forward(self, X):     X = X.view(X.shape[0], -1)     return self.fc2(self.relu(self.fc1(X))) <p>We will now initialize our model, define our loss and optimizer with the help of our framework. For pedagogical purposes, we will delay our discussion of some major concepts to future notebooks.</p> <p>Exercise</p> <p>        What should be the input and output layer sizes of our model?   </p> In\u00a0[12]: Copied! <pre>model = MLP(784, 128, 10)\ncel = nn.CrossEntropyLoss() # for multiclass classification\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n</pre> model = MLP(784, 128, 10) cel = nn.CrossEntropyLoss() # for multiclass classification optimizer = optim.SGD(model.parameters(), lr=0.001) <p>Tip</p> <p> Stochastic gradient descent (SGD) and other optimizers we will discuss in our next notebook. Our meterial on information theory will clarify the reasoning behind the cross-entropy loss and related concepts.   </p> In\u00a0[13]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model.to(device) Out[13]: <pre>MLP(\n  (fc1): Linear(in_features=784, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n  (relu): ReLU()\n)</pre> <p>Tip</p> <p>        It is possible to run faster training calculations on GPU with <code>cuda</code>. You can change the runtime in Google Colab through the menu <code>Runtime -&gt; Change runtime type</code>. Note that we migrate our data to the available device with <code>.to()</code>.   </p> <p>As PyTorch is highly optimized and the dataset images are small in size, training will be quick, especially with GPU. We should move our data in batches to the GPU memory as well in order to make it compatible with the model. The separate training steps we have repeatedly discussed in the previous lectures, hence should be familiar. We will train our model for only three epochs to quickly see interesting mistakes of our model, but feel free to train the model for a longer period to achieve a higher accuracy.</p> In\u00a0[14]: Copied! <pre>epochs = 3\n\nfor e in range(epochs):\n  loss = 0.0\n  for X_train, y_train in train_loader:\n    X_train, y_train = X_train.to(device), y_train.to(device)\n    # resetting gradients\n    optimizer.zero_grad()\n    # forward pass\n    preds = model(X_train)\n    # calculating loss\n    batch_loss = cel(preds, y_train)\n    # backward pass\n    batch_loss.backward()\n    # gradient descent\n    optimizer.step()\n    loss += batch_loss.item()\n  print(f\"Epoch: {e+1}/{epochs}, Loss: {loss/len(train_loader):.4f}\")\n</pre> epochs = 3  for e in range(epochs):   loss = 0.0   for X_train, y_train in train_loader:     X_train, y_train = X_train.to(device), y_train.to(device)     # resetting gradients     optimizer.zero_grad()     # forward pass     preds = model(X_train)     # calculating loss     batch_loss = cel(preds, y_train)     # backward pass     batch_loss.backward()     # gradient descent     optimizer.step()     loss += batch_loss.item()   print(f\"Epoch: {e+1}/{epochs}, Loss: {loss/len(train_loader):.4f}\") <pre>Epoch: 1/3, Loss: 2.2153\nEpoch: 2/3, Loss: 1.9955\nEpoch: 3/3, Loss: 1.7073\n</pre> <p>Forward passing unseen data through a trained network to measure how well the model is doing is called inference. At this stage, we do not need to calculate gradients (<code>torch.no_grad()</code>). We will also call <code>model.eval()</code> which is not necessary for our simple network, yet is a good practice to follow. As <code>y_test</code> is a tensor of batch size and the output dimension <code>(B, 10)</code>, we will find top prediction labels on each batch (<code>dim=1</code>) and store them for future visualization. We will then sum the correct predictions and divide by the image count in the test dataset to get the final accuracy score.</p> In\u00a0[15]: Copied! <pre>model.eval()\n\ncorrect = 0\nall_preds = []\n\nwith torch.no_grad():\n  for X_test, y_test in test_loader:\n    X_test, y_test = X_test.to(device), y_test.to(device)\n    preds = model(X_test)\n    _, top_preds = torch.max(preds, dim=1)\n    correct += (top_preds == y_test).sum().item()\n    all_preds.append(top_preds)\n\naccuracy = 100 * correct / len(test_loader.dataset)\nprint(f\"Accuracy on test data: {accuracy:.2f}%\")\n</pre> model.eval()  correct = 0 all_preds = []  with torch.no_grad():   for X_test, y_test in test_loader:     X_test, y_test = X_test.to(device), y_test.to(device)     preds = model(X_test)     _, top_preds = torch.max(preds, dim=1)     correct += (top_preds == y_test).sum().item()     all_preds.append(top_preds)  accuracy = 100 * correct / len(test_loader.dataset) print(f\"Accuracy on test data: {accuracy:.2f}%\") <pre>Accuracy on test data: 74.48%\n</pre> <p>Let's visualize some of our predictions in the last test batch. Pay attention to the false predictions of our model. Can you guess why the model have made those mistakes?</p> In\u00a0[29]: Copied! <pre>plot(\n    [X_test[i] for i in [15, 6, 7, 8]],\n    [f\"Pred: {all_preds[-1][i]}, True: {y_test[i]}\" for i in [15, 6, 7, 8]]\n)\n</pre> plot(     [X_test[i] for i in [15, 6, 7, 8]],     [f\"Pred: {all_preds[-1][i]}, True: {y_test[i]}\" for i in [15, 6, 7, 8]] ) <p>Our simple MLP with two FC layers acheived a pretty high accuracy. But that was mainly due to the simplicity of the dataset. Processing bigger datasets, also with a more challenging goal in mind (e.g. object detection, segmentation, etc), in addition to huge computational resources, requires from a model to understand spatial relationship in the images.</p> <p>When we reshape our image pixels into a single dimension two major things happen: (i) input size of our neurons drastically increases (a $224 \\times 224$ pixel RGB image is more than 150,000 input dimension for a FC neuron), and (ii) we lose important information about the pixels: their spatial location. It would make sense, if we could somehow also train our model to learn, for example, which pixels are close to each other. Most probably a combination of pixels (superpixels) make up a wheel of a car, or an ear of an animal. It would be great to look at pixels not as distinct and unrelated items (which FC layer does), but as connected and spatially related items.</p> <p>Info</p> <p>     The following source was consulted in preparing this material: Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J. Dive into Deep Learning. Cambridge University Press.     Chapter 7: Convolutional Neural Networks.   </p> <p>The word \"convolution\" is actually a misnomer, and \"cross-correlation\" would be a more precise name for the operation which we will discuss now. As can be seen from the figure below, we have a 2D input tensor (image) and a kernel of size $2 \\times 2$. We can put such a kernel onto four different locations in the image (top left, which we see in the image, top right, bottom left and bottom right). The output is calculated by a simple dot product: all the overlapping values are multiplied to each other and summed up.</p> <p>Exercise</p> <p>      Given the image dimensions and the kernel size, how many times can we move kernel over the image?   </p>      Cross-correlation operation ~ Zhang et al., Dive into Deep Learning, Fig. 7.2.1.     CC BY-SA 4.0.     <p>The code below sets up the input image and kernel described in the figure.</p> In\u00a0[30]: Copied! <pre>img = torch.arange(9).reshape((1,3,3))\nkernel = torch.arange(4).reshape((1,2,2))\n\nimg, kernel\n</pre> img = torch.arange(9).reshape((1,3,3)) kernel = torch.arange(4).reshape((1,2,2))  img, kernel Out[30]: <pre>(tensor([[[0, 1, 2],\n          [3, 4, 5],\n          [6, 7, 8]]]),\n tensor([[[0, 1],\n          [2, 3]]]))</pre> <p>If the kernel width is the same as image width, we can put kernel only once on each row of the image. If kernel width is $1$ pixel less than the image width dimension, we can put kernel twice on the image row. The same rule applies to height and vertical movement of the kernel over the image. From that, we can determine the number of steps kernel will move over the image.</p> In\u00a0[31]: Copied! <pre>horizontal_steps = img.shape[1] - kernel.shape[1] + 1 # height\nvertical_steps   = img.shape[2] - kernel.shape[2] + 1 # width\n</pre> horizontal_steps = img.shape[1] - kernel.shape[1] + 1 # height vertical_steps   = img.shape[2] - kernel.shape[2] + 1 # width <p>We will now use for loops for calculating cross-correlation operation, but note that we do it for simplicity, and using loops isn't an efficient choice. Whenever we can, we should use vectorized operations provided by Pytroch. Finally, pay attention how <code>squeeze/unsqueeze</code> functions take care of the channel dimension below.</p> <p>Important</p> <p>     Vectorization in PyTorch is the practice of replacing explicit loops with optimized tensor operations. It leverages highly efficient C++ and CUDA kernels to speed up computation on both CPUs and GPUs. This is a fundamental performance optimization in deep learning. Therefore, loops in code should be avoided as much as possible.   </p> In\u00a0[32]: Copied! <pre>out = torch.zeros((horizontal_steps, vertical_steps))\nfor i in range(horizontal_steps):\n  for j in range(vertical_steps):\n    patch = img.squeeze()[i:kernel.shape[1]+i, j:kernel.shape[2]+j]\n    out[i, j] = torch.sum(kernel.squeeze() * patch)\nout = out.unsqueeze(0)\nout\n</pre> out = torch.zeros((horizontal_steps, vertical_steps)) for i in range(horizontal_steps):   for j in range(vertical_steps):     patch = img.squeeze()[i:kernel.shape[1]+i, j:kernel.shape[2]+j]     out[i, j] = torch.sum(kernel.squeeze() * patch) out = out.unsqueeze(0) out Out[32]: <pre>tensor([[[19., 25.],\n         [37., 43.]]])</pre> <p>The good news is that we do not have to implement complicated (and inefficient) cross-correlation operations in higher dimensions. <code>torch.nn.functional</code> provides functions for that, which, however accept arguments in 4-dimensions, the first dimension being batch size. Again, pay attention to <code>squeeze</code> and <code>unsqueeze</code> shapes, which will add and remove the fourth (batch) dimension.</p> In\u00a0[33]: Copied! <pre>import torch.nn.functional as F\n\nout = F.conv2d(img.unsqueeze(0), kernel.unsqueeze(0)).squeeze(1)\nout\n</pre> import torch.nn.functional as F  out = F.conv2d(img.unsqueeze(0), kernel.unsqueeze(0)).squeeze(1) out Out[33]: <pre>tensor([[[19, 25],\n         [37, 43]]])</pre> <p>Kernels are capable of extracting relevant features from images. We can choose different kernels, depending on what we try to detect in the image. To see how kernel values influence the output of the cross-correlation operation, let's see the following edge detection example. Edges in the toy image below are the points where pixel values change extremely (from $0$ to $1$ or vice versa).</p> In\u00a0[34]: Copied! <pre>img = torch.zeros((1, 6, 8))\nimg[:, :, 2:6] = 1.0\nplot(img)\n</pre> img = torch.zeros((1, 6, 8)) img[:, :, 2:6] = 1.0 plot(img) In\u00a0[35]: Copied! <pre>img, img.shape\n</pre> img, img.shape Out[35]: <pre>(tensor([[[0., 0., 1., 1., 1., 1., 0., 0.],\n          [0., 0., 1., 1., 1., 1., 0., 0.],\n          [0., 0., 1., 1., 1., 1., 0., 0.],\n          [0., 0., 1., 1., 1., 1., 0., 0.],\n          [0., 0., 1., 1., 1., 1., 0., 0.],\n          [0., 0., 1., 1., 1., 1., 0., 0.]]]),\n torch.Size([1, 6, 8]))</pre> <p>Exercise</p> <p>      What kind of kernel would be appropriate for detecting edges?   </p> <p>Exercise</p> <p>      Try to understand why the kernel below detects the edges. Hint: manually calculate the cross-correlation step.   </p> In\u00a0[36]: Copied! <pre>kernel = torch.tensor([[[1.0, -1.0]]])\nout = F.conv2d(img.unsqueeze(0), kernel.unsqueeze(0)).squeeze(1)\nplot(out)\n</pre> kernel = torch.tensor([[[1.0, -1.0]]]) out = F.conv2d(img.unsqueeze(0), kernel.unsqueeze(0)).squeeze(1) plot(out) In\u00a0[37]: Copied! <pre>out, out.shape\n</pre> out, out.shape Out[37]: <pre>(tensor([[[ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n          [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n          [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n          [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n          [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n          [ 0., -1.,  0.,  0.,  0.,  1.,  0.]]]),\n torch.Size([1, 6, 7]))</pre> <p>There exists many ready kernels to manipulate images for sharpening, blurring, edge detection, etc. But what if we wanted to learn more complicated kernels, how to achieve that? Can we learn correct kernel values, say, for detecting an ear of a dog? It turns out, given image and the edge detected output, we can train a model and learn the kernel.</p> <p>Instead of MLP with linear (FC) layers, we will use a single convolutional layer provided by PyTorch to learn the kernel. <code>nn.Conv2d</code> requires from us to specify the input dimensions, when <code>nn.LazyConv2d</code> will determine the dimension dynamically, when we pass the input image to the model. Note that we are overfitting the model (layer) to a single example.</p> <p>Our goal is to learn a single kernel of size $1 \\times 2$ for a grayscale image. What the convolutional layer does under the hood we will learn soon. How to train a model we had already seen when training MLP on MNIST. Notice how our model manages to quickly learn a perfect kernel.</p> In\u00a0[38]: Copied! <pre>model = nn.LazyConv2d(out_channels=1, kernel_size=(1, 2)).to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nX_img = img.to(device).unsqueeze(0)\ny_out = out.to(device).unsqueeze(0)\n\nepochs = 100\n\nfor e in range(epochs):\n  optimizer.zero_grad()\n  preds = model(X_img)\n  loss = torch.sum((preds - y_out) ** 2) # mse\n  loss.backward()\n  optimizer.step()\n\nprint(f'Epoch: {e+1}/{epochs}, Loss: {loss:.4f}')\nprint(f'Learned Kernel: {model.weight.data}')\n</pre> model = nn.LazyConv2d(out_channels=1, kernel_size=(1, 2)).to(device) optimizer = optim.SGD(model.parameters(), lr=0.01)  X_img = img.to(device).unsqueeze(0) y_out = out.to(device).unsqueeze(0)  epochs = 100  for e in range(epochs):   optimizer.zero_grad()   preds = model(X_img)   loss = torch.sum((preds - y_out) ** 2) # mse   loss.backward()   optimizer.step()  print(f'Epoch: {e+1}/{epochs}, Loss: {loss:.4f}') print(f'Learned Kernel: {model.weight.data}') <pre>Epoch: 100/100, Loss: 0.0000\nLearned Kernel: tensor([[[[ 1.0000, -1.0000]]]], device='cuda:0')\n</pre> <p>As our final examples, let's take (i) a simple blur kernel which averages pixels within its frame, and (ii) a Gaussian blur kernel which is basically giving more weight to the pixels that are closer to the middle, by considering Gaussian (normal) distribution . We will apply them on a photo of Eileen Collins and see the output. Note that Gaussian blur is commonly used for detecting edges</p> <p>Exercise</p> <p>     Modify the size of the kernel to see how it influences the output.   </p> In\u00a0[41]: Copied! <pre>from skimage import data\n\ntest_img = data.astronaut()\ntest_img = torch.tensor(test_img, dtype=torch.float32).permute(2,0,1) # C, H, W\ntest_img = test_img.mean(0).unsqueeze(0) # RGB to grayscale\n\nH = W = 5 # try out bigger kernel sizes\nmean, std = 0.0, 1.0 # mean and standard deviation of Gaussian distribution\n\nblur_kernel = torch.ones((1,H,W)) * 1/H*W # averaging pixels\ngaussian_blur_kernel = torch.normal(mean, std, size=(1,H,W)) # pixels closer to middle get more weight\n\nblurred  = F.conv2d(test_img.unsqueeze(0), blur_kernel.unsqueeze(0)).squeeze(1)\ngaussian = F.conv2d(test_img.unsqueeze(0), gaussian_blur_kernel.unsqueeze(0)).squeeze(1)\n\nplot([test_img, blurred, gaussian], ['original', 'blurred', 'gaussian'])\n</pre> from skimage import data  test_img = data.astronaut() test_img = torch.tensor(test_img, dtype=torch.float32).permute(2,0,1) # C, H, W test_img = test_img.mean(0).unsqueeze(0) # RGB to grayscale  H = W = 5 # try out bigger kernel sizes mean, std = 0.0, 1.0 # mean and standard deviation of Gaussian distribution  blur_kernel = torch.ones((1,H,W)) * 1/H*W # averaging pixels gaussian_blur_kernel = torch.normal(mean, std, size=(1,H,W)) # pixels closer to middle get more weight  blurred  = F.conv2d(test_img.unsqueeze(0), blur_kernel.unsqueeze(0)).squeeze(1) gaussian = F.conv2d(test_img.unsqueeze(0), gaussian_blur_kernel.unsqueeze(0)).squeeze(1)  plot([test_img, blurred, gaussian], ['original', 'blurred', 'gaussian']) <p>Tip</p> <p>     See this visualization and this demonstration (in the beginning of the video) for gaining further intuition on how different kernels influence an image. The video titled But what is a convolution? explains cross-correlation and kernel operations in a nice visual way.   </p> <p>When we find <code>valid</code> cross-correlation, we move kernel within the constraints of the image. As a consequence, we pass through boundaries only once, losing some spatial information (pixels on the corners are barely used). Padding resolves this issue.</p> In\u00a0[42]: Copied! <pre>img = torch.arange(1., 10).reshape((1,3,3))\npadded_img = F.pad(img, (1,1,1,1)) # left right top bottom\nplot([img, padded_img], [f'{img}', f'{padded_img}'])\n</pre> img = torch.arange(1., 10).reshape((1,3,3)) padded_img = F.pad(img, (1,1,1,1)) # left right top bottom plot([img, padded_img], [f'{img}', f'{padded_img}']) <p>By default, we step with kernel one row/column at a time (as in the image above). Stride determines the step size of our kernel, which is used when we want to reduce the output dimensionality after cross-correlation even further. It may be useful when the kernel size is big, we need faster computation, etc.</p> <p>Note</p> <p>     From this point on, we will replace the more accurate word \"cross-correlation\" with the accepted terminology \"convolution\". Simply put, convolution operation is equivalent to the 90 degrees rotated cross-correlation operation.   </p>      Kernel convolution with unit padding and stride ~ By Michael Plotke - Own work, CC BY-SA 3.0, Link <p>Exercise</p> <p>   Manually calculate the strided convolution below.   </p> In\u00a0[43]: Copied! <pre>kernel = torch.ones((1,1,2,2))\n\nmanual_padding = F.conv2d(padded_img.unsqueeze(0), kernel).squeeze(1)\nconv2d_padding = F.conv2d(img.unsqueeze(0), kernel, padding=1).squeeze(1)\nconv2d_stride  = F.conv2d(img.unsqueeze(0), kernel, padding=1, stride=2).squeeze(1)\n\nplot([manual_padding, conv2d_padding, conv2d_stride],\n     [f'{manual_padding}', f'{conv2d_padding}', f'{conv2d_stride}'])\n</pre> kernel = torch.ones((1,1,2,2))  manual_padding = F.conv2d(padded_img.unsqueeze(0), kernel).squeeze(1) conv2d_padding = F.conv2d(img.unsqueeze(0), kernel, padding=1).squeeze(1) conv2d_stride  = F.conv2d(img.unsqueeze(0), kernel, padding=1, stride=2).squeeze(1)  plot([manual_padding, conv2d_padding, conv2d_stride],      [f'{manual_padding}', f'{conv2d_padding}', f'{conv2d_stride}']) <p>Exercise</p> <p>     Create 100 random images (colored) and calculate the output dimension after the convolution operation. Calculate and print out how many features will be passed to the next layer after flattening the feature maps for all the cases below:      Image Size Kernel Size Stride Padding 30\u00d730 3\u00d73 1 0 224\u00d7224 5\u00d75 2 1 528\u00d7528 7\u00d77 3 2 </p> <p>Note</p> <p>     In practice, we need to compute the output size of convolution layers when designing CNN architectures, debugging tensor shape mismatches, and determining how many features will be produced before flattening. For that, we need to realize that the output spatial size of a convolution depends on four parameters: input size, kernel size, padding, and stride and come up with the following formula which is also used by deep learning frameworks internally:     $$     \\text{out} = \\left\\lfloor \\frac{\\text{input} - \\text{kernel} + 2\\cdot\\text{padding}}{\\text{stride}} \\right\\rfloor + 1     $$     Intuitively, subtracting kernel size from an input dimension tells how much the kernel can slide inside the input, padding increases the usable space by adding extra pixels around the borders, and stride controls how many pixels the kernel jumps at each step. The floor operation appears because the kernel must fully fit inside the padded image. This formula is applied separately for height and width.   </p> In\u00a0[44]: Copied! <pre>img = torch.cat((torch.arange(9.), torch.arange(1.,10))).reshape((1,2,3,3))\nimg\n</pre> img = torch.cat((torch.arange(9.), torch.arange(1.,10))).reshape((1,2,3,3)) img Out[44]: <pre>tensor([[[[0., 1., 2.],\n          [3., 4., 5.],\n          [6., 7., 8.]],\n\n         [[1., 2., 3.],\n          [4., 5., 6.],\n          [7., 8., 9.]]]])</pre> In\u00a0[45]: Copied! <pre>kernels = torch.cat((torch.arange(4.), torch.arange(1.,5))).reshape((1,2,2,2))\nkernels\n</pre> kernels = torch.cat((torch.arange(4.), torch.arange(1.,5))).reshape((1,2,2,2)) kernels Out[45]: <pre>tensor([[[[0., 1.],\n          [2., 3.]],\n\n         [[1., 2.],\n          [3., 4.]]]])</pre> In\u00a0[46]: Copied! <pre>conv1 = F.conv2d(img[:,0:1,:,:], kernels[:,0:1,:,:])\nconv2 = F.conv2d(img[:,1:2,:,:], kernels[:,1:2,:,:])\n\nconv1 + conv2\n</pre> conv1 = F.conv2d(img[:,0:1,:,:], kernels[:,0:1,:,:]) conv2 = F.conv2d(img[:,1:2,:,:], kernels[:,1:2,:,:])  conv1 + conv2 Out[46]: <pre>tensor([[[[ 56.,  72.],\n          [104., 120.]]]])</pre> In\u00a0[47]: Copied! <pre># or equivalently\nF.conv2d(img, kernels)\n</pre> # or equivalently F.conv2d(img, kernels) Out[47]: <pre>tensor([[[[ 56.,  72.],\n          [104., 120.]]]])</pre> <p>We may want to not only deal with multiple channels, but also output multiple channels. The simplistic reasoning for it could be that each output channel may hold different feature information. For that, we will apply convolution of each kernel set to the whole image (all dimensions) and eventually concatenate the results along the channel axis.</p> In\u00a0[48]: Copied! <pre>torch.cat([F.conv2d(img, kernels+i) for i in range(3)], dim=0)\n</pre> torch.cat([F.conv2d(img, kernels+i) for i in range(3)], dim=0) Out[48]: <pre>tensor([[[[ 56.,  72.],\n          [104., 120.]]],\n\n\n        [[[ 76., 100.],\n          [148., 172.]]],\n\n\n        [[[ 96., 128.],\n          [192., 224.]]]])</pre> <p>The advantage of convolutional layers is that, unlike FC layers, they take into account spatial information about the pixels (their locations, neighbors, etc). But how can we make sure that our predictions will not be very sensitive to small changes in pixel locations? Because a cat image where the cat is streching is still a cat image, and our model should correctly classify it, even if it hasn't seen a stretching cat during training. As we will see, applying a pooling operation will help with spatial invariance, as well as downsample the representations of the previous layer.</p> <p>The figure below describes maximum pooling operation, which resembles a kernel, but instead of convolution, simply takes the maximum pixel value within the given range. Similarly, an average pooling operation, averages all the corresponding pixels and is equivalent to the simple blur kernel which we saw previously.</p>      Pooling operation ~ Zhang et al.,     Dive into Deep Learning,     Fig. 7.5.1.     CC BY-SA 4.0.    <p>Below is the code implementation of the described operations, first implemented manually, then calculated by using ready functions provided by PyTorch.</p> In\u00a0[49]: Copied! <pre>img_channel = img.unbind(1)[0] # unbinds from the channel dimension and takes the first channel\nimg_channel\n</pre> img_channel = img.unbind(1)[0] # unbinds from the channel dimension and takes the first channel img_channel Out[49]: <pre>tensor([[[0., 1., 2.],\n         [3., 4., 5.],\n         [6., 7., 8.]]])</pre> In\u00a0[50]: Copied! <pre>operation = ['avg','max'][1] # change to 0 for average poolig\n\n# Compare the code below with the convolution code\nkernel_height = kernel_width = 2\n\nhorizontal_steps = img_channel.shape[1] - kernel_height + 1\nvertical_steps   = img_channel.shape[2] - kernel_width  + 1\n\nout = torch.zeros((horizontal_steps, vertical_steps))\nfor i in range(horizontal_steps):\n  for j in range(vertical_steps):\n    patch = img_channel.squeeze(0)[i:i+kernel_height, j:j+kernel_width]\n    out[i, j] = patch.mean() if operation == 'avg' else patch.max()\n\nout.unsqueeze(0)\n</pre> operation = ['avg','max'][1] # change to 0 for average poolig  # Compare the code below with the convolution code kernel_height = kernel_width = 2  horizontal_steps = img_channel.shape[1] - kernel_height + 1 vertical_steps   = img_channel.shape[2] - kernel_width  + 1  out = torch.zeros((horizontal_steps, vertical_steps)) for i in range(horizontal_steps):   for j in range(vertical_steps):     patch = img_channel.squeeze(0)[i:i+kernel_height, j:j+kernel_width]     out[i, j] = patch.mean() if operation == 'avg' else patch.max()  out.unsqueeze(0) Out[50]: <pre>tensor([[[4., 5.],\n         [7., 8.]]])</pre> In\u00a0[51]: Copied! <pre># or equivalently\nnn.MaxPool2d(kernel_size=(2,2), stride=1, padding=0)(img_channel)\n</pre> # or equivalently nn.MaxPool2d(kernel_size=(2,2), stride=1, padding=0)(img_channel) Out[51]: <pre>tensor([[[4., 5.],\n         [7., 8.]]])</pre> In\u00a0[52]: Copied! <pre>nn.AvgPool2d(kernel_size=(2,2), stride=1, padding=0)(img_channel)\n</pre> nn.AvgPool2d(kernel_size=(2,2), stride=1, padding=0)(img_channel) Out[52]: <pre>tensor([[[2., 3.],\n         [5., 6.]]])</pre> <p>In multiple dimensions, instead of summing channel outputs of each kernel, we will concatenate them after the pooling operation, which makes sense, as we have to retain channel information. Notice that the output channel is 2D, as we pass both image channels to <code>nn.MaxPool2d()</code>.</p> In\u00a0[53]: Copied! <pre>nn.MaxPool2d(kernel_size=2, stride=1, padding=0)(img)\n</pre> nn.MaxPool2d(kernel_size=2, stride=1, padding=0)(img) Out[53]: <pre>tensor([[[[4., 5.],\n          [7., 8.]],\n\n         [[5., 6.],\n          [8., 9.]]]])</pre> <p>We will now implement the well-known LeNet-5 (1998) architecture. Its convolutional block consists of a convolution layer, sigmoid activation function (convolution is a linear function), and average pooling layer. Kernel size of convolutional layers is $5 \\times 5$ (with initial padding of $2$). Two convolutional layers output $6$ and $16$ feature maps respectively. Average pooling kernel size is $2 \\times 2$ (with stride $2$).</p> <p>Important</p> <p>     Sigmoid often causes vanishing gradients and average pooling tends to smooth out strong local features. The superiority of max-pooling and rectifier activation were discovered only about a decade later, which soon became a default choice. </p> <p>Once we extract the relevant features with convolutional blocks, we will have to make predictions with FC layers. For that, we must flatten the feature maps similar to what we did when using our simple <code>MLP</code> class, which was directly accepting the image as its inputs, instead of convolutional feature maps. FC layer sizes proposed in the paper are $120$, $84$, $10$, classifying 10 handwritten digits.</p> <p>Note</p> <p>     The origins of CNN go back to the 1980 paper called              Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position          by Kunihiko Fukushima. As computational power increased and the field developed, an influential CNN implementation     by Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner was introduced in the 1998 paper called              Gradient-Based Learning Applied to Document Recognition     .     The convolutional network architecture proposed for detecting handwritten digits (and more) was named after its first author as     LeNet.   </p>      Data flow in LeNet: the input is a handwritten digit, the output is a probability over 10 possible outcomes ~ Zhang et al.,     Dive into Deep Learning,     Fig. 7.6.1. CC BY-SA 4.0.    <p>Exercise</p> <p>     Code the model architecture by using the knowledge of the <code>MLP</code> class and the information provided above.     </p> <p>We will now code and train a modern implementation with <code>LeNet</code>, similar to <code>MLP</code>. We will replace average pooling with maximum pooling layer, sigmoid with ReLU activation, and the original Gaussian decoder with softmax function. We will use <code>Lazy</code> versions of the layers in order to not hard-code input dimensions.</p> <p>Note</p> <p>    During training we will use cross-entropy loss, which applies <code>log_softmax</code> internally in a numerically stable way, so we do not include a softmax layer inside the model. </p> In\u00a0[\u00a0]: Copied! <pre>class LeNet(nn.Module):\n  def __init__(self):\n    super(LeNet, self).__init__()\n    self.conv1 = nn.LazyConv2d(out_channels=6, kernel_size=(5,5), padding=2)\n    self.conv2 = nn.LazyConv2d(out_channels=16, kernel_size=(5,5))\n    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n    self.fc1 = nn.LazyLinear(out_features=84)\n    self.fc2 = nn.LazyLinear(out_features=10)\n    self.act = nn.ReLU()\n\n  def forward(self, X):\n    block1 = self.pool(self.act(self.conv1(X)))\n    block2 = self.pool(self.act(self.conv2(block1)))\n    flatten = block2.view(block2.shape[0], -1)\n    logits = self.fc2(self.act(self.fc1(flatten)))\n    return logits\n</pre> class LeNet(nn.Module):   def __init__(self):     super(LeNet, self).__init__()     self.conv1 = nn.LazyConv2d(out_channels=6, kernel_size=(5,5), padding=2)     self.conv2 = nn.LazyConv2d(out_channels=16, kernel_size=(5,5))     self.pool = nn.MaxPool2d(kernel_size=2, stride=2)     self.fc1 = nn.LazyLinear(out_features=84)     self.fc2 = nn.LazyLinear(out_features=10)     self.act = nn.ReLU()    def forward(self, X):     block1 = self.pool(self.act(self.conv1(X)))     block2 = self.pool(self.act(self.conv2(block1)))     flatten = block2.view(block2.shape[0], -1)     logits = self.fc2(self.act(self.fc1(flatten)))     return logits In\u00a0[\u00a0]: Copied! <pre>mlp = nn.Sequential(\n    nn.Flatten(),\n    nn.LazyLinear(out_features=128),\n    nn.ReLU(),\n    nn.LazyLinear(out_features=10),\n)\n\nlenet = nn.Sequential(\n    nn.LazyConv2d(out_channels=6, kernel_size=5, padding=2),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.LazyConv2d(out_channels=16, kernel_size=5),\n    nn.ReLU(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Flatten(),\n    nn.LazyLinear(out_features=84),\n    nn.ReLU(),\n    nn.LazyLinear(out_features=10),\n)\n</pre> mlp = nn.Sequential(     nn.Flatten(),     nn.LazyLinear(out_features=128),     nn.ReLU(),     nn.LazyLinear(out_features=10), )  lenet = nn.Sequential(     nn.LazyConv2d(out_channels=6, kernel_size=5, padding=2),     nn.ReLU(),     nn.MaxPool2d(kernel_size=2, stride=2),     nn.LazyConv2d(out_channels=16, kernel_size=5),     nn.ReLU(),     nn.AvgPool2d(kernel_size=2, stride=2),     nn.Flatten(),     nn.LazyLinear(out_features=84),     nn.ReLU(),     nn.LazyLinear(out_features=10), ) In\u00a0[\u00a0]: Copied! <pre>class Classifier:\n  def __init__(self, model, lr=0.01):\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.model = model.to(self.device)\n    self.optimizer = optim.SGD(self.model.parameters(), lr=lr)\n    self.loss_fn = nn.CrossEntropyLoss()\n\n  def fit(self, train_loader, num_epochs=10):\n    self.model.train()\n    for epoch in range(num_epochs):\n      loss = 0.0\n      for X_train, y_train in train_loader:\n        X_train, y_train = X_train.to(self.device), y_train.to(self.device)\n        self.optimizer.zero_grad()\n        logits = self.model(X_train)\n        batch_loss = self.loss_fn(logits, y_train)\n        batch_loss.backward()\n        loss += batch_loss.item()\n        self.optimizer.step()\n      print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}\")\n\n  def inference(self, test_loader):\n    self.model.eval()\n    correct = 0\n    all_preds = []\n    with torch.no_grad():\n      for X_test, y_test in test_loader:\n        X_test, y_test = X_test.to(self.device), y_test.to(self.device)\n        logits = self.model(X_test)\n        _, top_preds = torch.max(logits, dim=1)\n        correct += (top_preds == y_test).sum().item()\n        all_preds.append(top_preds)\n    accuracy = 100 * correct / len(test_loader.dataset)\n    print(f\"Accuracy on test data: {accuracy:.2f}%\")\n</pre> class Classifier:   def __init__(self, model, lr=0.01):     self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     self.model = model.to(self.device)     self.optimizer = optim.SGD(self.model.parameters(), lr=lr)     self.loss_fn = nn.CrossEntropyLoss()    def fit(self, train_loader, num_epochs=10):     self.model.train()     for epoch in range(num_epochs):       loss = 0.0       for X_train, y_train in train_loader:         X_train, y_train = X_train.to(self.device), y_train.to(self.device)         self.optimizer.zero_grad()         logits = self.model(X_train)         batch_loss = self.loss_fn(logits, y_train)         batch_loss.backward()         loss += batch_loss.item()         self.optimizer.step()       print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}\")    def inference(self, test_loader):     self.model.eval()     correct = 0     all_preds = []     with torch.no_grad():       for X_test, y_test in test_loader:         X_test, y_test = X_test.to(self.device), y_test.to(self.device)         logits = self.model(X_test)         _, top_preds = torch.max(logits, dim=1)         correct += (top_preds == y_test).sum().item()         all_preds.append(top_preds)     accuracy = 100 * correct / len(test_loader.dataset)     print(f\"Accuracy on test data: {accuracy:.2f}%\") <p>It is important to note that convolutions are expensive matrix operations, when GPUs will come in aid. Also, do not rush to compare <code>MLP</code> with <code>LeNet</code>, as our <code>MNIST</code> dataset is very simple. Indeed, almost three decades have passed since the first implementation of LeNet-5!</p> In\u00a0[57]: Copied! <pre>clf_mlp = Classifier(mlp)\nclf_mlp.fit(train_loader)\nclf_mlp.inference(test_loader)\n</pre> clf_mlp = Classifier(mlp) clf_mlp.fit(train_loader) clf_mlp.inference(test_loader) <pre>Epoch: 1/10, Loss: 2.2945\nEpoch: 2/10, Loss: 2.2347\nEpoch: 3/10, Loss: 2.0805\nEpoch: 4/10, Loss: 1.9504\nEpoch: 5/10, Loss: 1.8545\nEpoch: 6/10, Loss: 1.7976\nEpoch: 7/10, Loss: 1.7676\nEpoch: 8/10, Loss: 1.7202\nEpoch: 9/10, Loss: 1.6920\nEpoch: 10/10, Loss: 1.6772\nAccuracy on test data: 83.01%\n</pre> In\u00a0[58]: Copied! <pre>clf_lenet = Classifier(lenet)\nclf_lenet.fit(train_loader)\nclf_lenet.inference(test_loader)\n</pre> clf_lenet = Classifier(lenet) clf_lenet.fit(train_loader) clf_lenet.inference(test_loader) <pre>Epoch: 1/10, Loss: 2.3023\nEpoch: 2/10, Loss: 2.3013\nEpoch: 3/10, Loss: 2.2997\nEpoch: 4/10, Loss: 2.2959\nEpoch: 5/10, Loss: 2.2707\nEpoch: 6/10, Loss: 1.9000\nEpoch: 7/10, Loss: 1.6895\nEpoch: 8/10, Loss: 1.6566\nEpoch: 9/10, Loss: 1.6208\nEpoch: 10/10, Loss: 1.5827\nAccuracy on test data: 89.61%\n</pre> <p>In this notebook, we introduced the core building blocks of convolutional neural networks and applied them to handwritten digit classification on the MNIST dataset. We studied the cross-correlation operation, the role of kernels, and how padding and stride control the spatial output size of convolution layers. We then extended convolution to multiple input and output channels, introduced max pooling and average pooling for downsampling, and assembled a modern version of the LeNet architecture. We also introduced PyTorch and demonstrated its capacity when implementing training and inference of deep learning models. In the next notebook, we will discuss many optimization and regularization techniques of artificial neural networks.</p>"},{"location":"notebooks/03_cnn_torch/#03-from-kernel-to-convolutional-neural-network","title":"03. From Kernel to Convolutional Neural Network\u00b6","text":"15 Feb 2025 /   6 Feb 2026"},{"location":"notebooks/03_cnn_torch/#images-with-pytorch","title":"Images with PyTorch\u00b6","text":"<p>A grayscale image is simply a 2D array holding pixel values, often ranging between $0$ and $1$ (normalized) or $0$ and $255$ (8 bits), representing the range of intensities between two colors: black and white. A colored image often is represented by the RGB color model, where we have 3 sets (channels) of 2D arrays for red, green, and blue. Combinations of three values represent additional colors to black and white (e.g. $[255, 0, 0]$ is red).</p> <p>We will create a toy image to get a feeling of how images are stored in the memory. We will also delibarately use <code>torch</code> library to familiarize ourselves with PyTorch functions. In PyTorch, images are stored in <code>(C, H, W)</code> format, corresponding to channel, height, and width. Note that batch size can also be included <code>(B, C, H, W)</code>. Batch is simply a subset of a dataset. We  should process our dataset in batches in order to not load all the images at once to the RAM memory, which is usually limited to several Gigabytes.</p> <p>Tip</p> <p>     If you are using Google Colab, see the top right corner of the notebook for RAM information.   </p> <p>Tensor is the main data abstraction in PyTorch, similar to <code>numpy</code> arrays. Tensors are optimized for GPU and can calculate gradients with its autograd engine. Inspired by micrograd the <code>Value</code> class which have been developing over the course previous notebooks was merely a simplified (scalar) version of <code>torch.Tensor</code>.</p>"},{"location":"notebooks/03_cnn_torch/#mnist-dataset","title":"MNIST Dataset\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#training-model-on-mnist","title":"Training Model on MNIST\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#cross-correlation-operation","title":"Cross-Correlation Operation\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#kernels","title":"Kernels\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#padding-stride","title":"Padding &amp; Stride\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#multiple-channels","title":"Multiple Channels\u00b6","text":"<p>So far we have been working with grayscale images. When we increase the channels $C$, we will simply have $C$ number of kernels. We will apply convolution of each kernel to their corresponding channels and eventually sum them up.</p>      Cross-correlation computation with two input channels ~ Zhang et al.,     Dive into Deep Learning,     Fig. 7.4.1.     CC BY-SA 4.0.    <p>We will implement the operation descibed in the figure by using ready PyTorch functions.</p>"},{"location":"notebooks/03_cnn_torch/#maximum-average-pooling","title":"Maximum &amp; Average Pooling\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#convolutional-neural-network","title":"Convolutional Neural Network\u00b6","text":""},{"location":"notebooks/03_cnn_torch/#sequential-module-in-pytorch","title":"Sequential Module in PyTorch\u00b6","text":"<p>Before testing our convolutional network, we will rewrite our original classes in order to demonstrate the power and clarity of <code>nn.Sequential</code> module in <code>PyTorch</code>. The code below is self-explanatory and is equivalent to <code>MLP</code> and <code>LeNet</code> classes we had initilaized previously. Notice how simple it is.</p>"},{"location":"notebooks/03_cnn_torch/#training-inference","title":"Training &amp; Inference\u00b6","text":"<p>We will abstract away our model initalization, train and inference codes before testing our models, similar to when we built a classifier from scratch in the style of scikit-learn library to train on the <code>make_moons</code> dataset. Both the training and inference logic we had seen in this notebook earlier. The only difference is that the code is now modularized to ease experimentations.</p>"},{"location":"notebooks/04_regul_optim/","title":"04. Regularization and Optimization","text":"In\u00a0[37]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef generate_data(samples=100, N=100, test_size=0.5, b=0.05, scale=0.01):\n  X = torch.randn(samples, N)\n  w = torch.randn(N, 1) * scale # scaling will ease training\n  n = torch.randn(samples, 1) * scale # noise is not bias!\n  y = torch.matmul(X, w) + n + b\n\n  size = int(samples * test_size)\n  X_train, X_test = X[:-size], X[-size:]\n  y_train, y_test = y[:-size], y[-size:]\n\n  return X_train, X_test, y_train, y_test\n</pre> import torch import matplotlib.pyplot as plt %matplotlib inline  def generate_data(samples=100, N=100, test_size=0.5, b=0.05, scale=0.01):   X = torch.randn(samples, N)   w = torch.randn(N, 1) * scale # scaling will ease training   n = torch.randn(samples, 1) * scale # noise is not bias!   y = torch.matmul(X, w) + n + b    size = int(samples * test_size)   X_train, X_test = X[:-size], X[-size:]   y_train, y_test = y[:-size], y[-size:]    return X_train, X_test, y_train, y_test In\u00a0[38]: Copied! <pre>X_train, _, y_train, _ = generate_data(100, 1) # 1D input for plotting\nplt.scatter(X_train, y_train);\n</pre> X_train, _, y_train, _ = generate_data(100, 1) # 1D input for plotting plt.scatter(X_train, y_train); <p>We will now generate a bigger dataset (both in the number of samples and dimensions), a good deal of which will be used for testing (so that we can illustrate overfitting). We will then create a simple linear regression model and train it. When training we will implement the $l_2$ regularization discussed above to see how it affects the test loss.</p> In\u00a0[39]: Copied! <pre>INPUT_DIM = 200\n\nX_train, X_test, y_train, y_test = generate_data(samples=100, N=INPUT_DIM, test_size=0.5, scale=0.01)\n</pre> INPUT_DIM = 200  X_train, X_test, y_train, y_test = generate_data(samples=100, N=INPUT_DIM, test_size=0.5, scale=0.01) In\u00a0[40]: Copied! <pre>def train(model, optimizer, loss_fn, num_epochs=1000, lambda_=0.1, verbose=True):\n  # needed for plotting\n  train_losses = []\n  test_losses = []\n\n  for epoch in range(num_epochs):\n    optimizer.zero_grad()\n    pred = model(X_train)\n    # regularization\n    l2 = sum(p.pow(2).sum() for p in model.parameters())/2\n    loss = loss_fn(pred, y_train) + lambda_ * l2 # see formula above\n    loss.backward()\n    optimizer.step()\n\n    # inference during training\n    with torch.no_grad():\n      test_pred = model(X_test)\n      test_loss = loss_fn(test_pred, y_test)\n\n    # needed for plotting\n    train_losses.append(loss.item())\n    test_losses.append(test_loss.item())\n\n    if epoch % 200 == 0 and verbose:\n      print(f'Epoch {epoch}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n\n  if verbose:\n    plt.figure(figsize=(6, 4))\n    plt.plot(range(num_epochs), train_losses, label='Train Loss')\n    plt.plot(range(num_epochs), test_losses, label='Test Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n</pre> def train(model, optimizer, loss_fn, num_epochs=1000, lambda_=0.1, verbose=True):   # needed for plotting   train_losses = []   test_losses = []    for epoch in range(num_epochs):     optimizer.zero_grad()     pred = model(X_train)     # regularization     l2 = sum(p.pow(2).sum() for p in model.parameters())/2     loss = loss_fn(pred, y_train) + lambda_ * l2 # see formula above     loss.backward()     optimizer.step()      # inference during training     with torch.no_grad():       test_pred = model(X_test)       test_loss = loss_fn(test_pred, y_test)      # needed for plotting     train_losses.append(loss.item())     test_losses.append(test_loss.item())      if epoch % 200 == 0 and verbose:       print(f'Epoch {epoch}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')    if verbose:     plt.figure(figsize=(6, 4))     plt.plot(range(num_epochs), train_losses, label='Train Loss')     plt.plot(range(num_epochs), test_losses, label='Test Loss')     plt.xlabel('Epochs')     plt.ylabel('Loss') <p>Try to understand why the line which calculates regularization function's output has two <code>sum</code> functions: <code>l2 = sum(p.pow(2).sum() for p in model.parameters())/2</code>. We have plotting logic in the function above for demonstration purposes, and we will give different $\u03bb$ values to customize our regularizer $R$ ($\u03bb$=0 implies no regularization).</p> In\u00a0[41]: Copied! <pre>import torch.nn as nn\nimport torch.optim as optim\n\nmodel = nn.Linear(INPUT_DIM, 1)\noptimizer = optim.SGD(model.parameters(), lr=0.001)\nmse = nn.MSELoss()\n</pre> import torch.nn as nn import torch.optim as optim  model = nn.Linear(INPUT_DIM, 1) optimizer = optim.SGD(model.parameters(), lr=0.001) mse = nn.MSELoss() In\u00a0[42]: Copied! <pre>train(model, optimizer, mse, lambda_=0.0) # no regularization\n</pre> train(model, optimizer, mse, lambda_=0.0) # no regularization <pre>Epoch 0, Train Loss: 0.3334, Test Loss: 0.3407\nEpoch 200, Train Loss: 0.0219, Test Loss: 0.2706\nEpoch 400, Train Loss: 0.0038, Test Loss: 0.2625\nEpoch 600, Train Loss: 0.0009, Test Loss: 0.2619\nEpoch 800, Train Loss: 0.0002, Test Loss: 0.2623\n</pre> In\u00a0[43]: Copied! <pre>train(model, optimizer, mse, lambda_=5.0) # with regularization\n</pre> train(model, optimizer, mse, lambda_=5.0) # with regularization <pre>Epoch 0, Train Loss: 0.6721, Test Loss: 0.2603\nEpoch 200, Train Loss: 0.0960, Test Loss: 0.0480\nEpoch 400, Train Loss: 0.0190, Test Loss: 0.0179\nEpoch 600, Train Loss: 0.0087, Test Loss: 0.0133\nEpoch 800, Train Loss: 0.0073, Test Loss: 0.0125\n</pre> <p>As can be observed, without regularization, model overfits heavily on the train data. Even though the loss of the train data reaches a very small value very quick, the loss of the test split struggles to decline after epoch 200 or so. On the contrary, regularization has a great effect during training, bringing the test loss down very quickly (in our oversimplified example, even faster than the train split).</p> <p>Tip</p> <p>     Try out different $\\lambda$ to see how different regularization intensities influence the output.   </p> In\u00a0[44]: Copied! <pre>X = torch.arange(1,7)\nf\"Initial layer activations: {X}\"\n</pre> X = torch.arange(1,7) f\"Initial layer activations: {X}\" Out[44]: <pre>'Initial layer activations: tensor([1, 2, 3, 4, 5, 6])'</pre> In\u00a0[45]: Copied! <pre>prob = 0.5 # try out different values between 0 and 1\nmask = (torch.rand(X.shape) &gt; prob ).float()\nf\"Random mask with dropout probability {prob}: {mask}\"\n</pre> prob = 0.5 # try out different values between 0 and 1 mask = (torch.rand(X.shape) &gt; prob ).float() f\"Random mask with dropout probability {prob}: {mask}\" Out[45]: <pre>'Random mask with dropout probability 0.5: tensor([0., 0., 0., 1., 0., 1.])'</pre> In\u00a0[46]: Copied! <pre>X_dropout = X * mask\nf\"Layer activations after dropout {X_dropout}\"\n</pre> X_dropout = X * mask f\"Layer activations after dropout {X_dropout}\" Out[46]: <pre>'Layer activations after dropout tensor([0., 0., 0., 4., 0., 6.])'</pre> In\u00a0[47]: Copied! <pre>X_dropout_scaled = X_dropout / (1.0 - prob + 1e-9)\nf\"Scaled activations after dropout {X_dropout_scaled}\"\n</pre> X_dropout_scaled = X_dropout / (1.0 - prob + 1e-9) f\"Scaled activations after dropout {X_dropout_scaled}\" Out[47]: <pre>'Scaled activations after dropout tensor([ 0.,  0.,  0.,  8.,  0., 12.])'</pre> <p>Exercise</p> <p>     Explain why scaling activations is required for dropout.   </p> <p>The idea behind dropout is that it injects noise to the network during training. If a model can achieve good accuracy with noise, it implies that the model has learned a more generalizable function. Dropping out certain proportion of neurons in each iteration basically guides the model to train a smaller network. Scaling is done so that the expected sum of activations will roughly remain the same to compensate for missing activations.</p> <p>Important</p> <p>   Although a useful technique, dropout should be used carefully in order to not hurt the overall performance of the model.   </p> <p>Note</p> <p>   Note that in some framework implementations <code>keep_prob</code> flag can be used during dropout, which keeps all the nodes in case of being set to <code>1.0</code>. In the PyTorch implementation, the same is achieved by doing the opposite and setting the probability to $0$.</p> <p>We will now implement an equivalent of nn.Dropout module of PyTorch. Dropout probability is usually set higher for bigger layers and layers closer to the input. Once the training is over, dropout is usually switched off (recall <code>model.eval()</code>). However, it can also be used in the test time for estimating the uncertainty of the neural network.</p> In\u00a0[48]: Copied! <pre>class Dropout(nn.Module):\n  def __init__(self, prob):\n    super().__init__()\n    assert 0 &lt;= prob &lt;= 1\n    self.prob = prob\n    self.epsilon = 1e-8\n\n  def forward(self, X):\n    if self.training: # model.eval() will set training to false\n      mask = (torch.rand_like(X) &gt; self.prob).float()\n      X = X * mask / (1.0 - self.prob + self.epsilon)\n    return X\n</pre> class Dropout(nn.Module):   def __init__(self, prob):     super().__init__()     assert 0 &lt;= prob &lt;= 1     self.prob = prob     self.epsilon = 1e-8    def forward(self, X):     if self.training: # model.eval() will set training to false       mask = (torch.rand_like(X) &gt; self.prob).float()       X = X * mask / (1.0 - self.prob + self.epsilon)     return X In\u00a0[49]: Copied! <pre>model = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(INPUT_DIM, 256),\n    nn.ReLU(),\n    Dropout(0.5), # nn.Dropout(0.5)\n    nn.Linear(256, 128),\n    nn.ReLU(),\n    Dropout(0.3), # nn.Dropout(0.3)\n    nn.Linear(128, 10)\n)\n</pre> model = nn.Sequential(     nn.Flatten(),     nn.Linear(INPUT_DIM, 256),     nn.ReLU(),     Dropout(0.5), # nn.Dropout(0.5)     nn.Linear(256, 128),     nn.ReLU(),     Dropout(0.3), # nn.Dropout(0.3)     nn.Linear(128, 10) ) In\u00a0[50]: Copied! <pre>X = torch.tensor([\n  [1, 2, 3, 4],\n  [5, 6, 7, 8],\n  [9,10,11,12],\n  [13,14,15,16]\n])\n\nX_flip = torch.flip(X, dims=[1])\n\nX, X_flip\n</pre> X = torch.tensor([   [1, 2, 3, 4],   [5, 6, 7, 8],   [9,10,11,12],   [13,14,15,16] ])  X_flip = torch.flip(X, dims=[1])  X, X_flip Out[50]: <pre>(tensor([[ 1,  2,  3,  4],\n         [ 5,  6,  7,  8],\n         [ 9, 10, 11, 12],\n         [13, 14, 15, 16]]),\n tensor([[ 4,  3,  2,  1],\n         [ 8,  7,  6,  5],\n         [12, 11, 10,  9],\n         [16, 15, 14, 13]]))</pre> <p>Regularization methods aim to reduce overfitting and, consequently, improve the generalization ability of models. So far, we have looked at three main regularization techniques: $l_2$ regularization, dropout, and data augmentation. To conclude this section on regularization, it is also worth mentioning a technique called early stopping, which simply stops training when the validation loss stops improving. We will now go a step further and discuss how to optimize our models to efficiently minimize the loss and converge to a good solution.</p> <p>By chain rule, we know that the gradients are multiplied within the neural network. Hence, multiplying many gradients that have large values along our network will \"explode\" the final gradient, resulting in unnessarily big jumps and missing the minimum of the function (and thus not converging). The opposite is also true: if the gradients are too small then multiplying lots of small values will result in a number that is almost zero, causing the vanishing gradient problem. With gradients almost zero, values during backpropagation will not get updated and the learning will stop.</p> In\u00a0[51]: Copied! <pre>W = torch.normal(mean=0, std=1, size=(3, 3)) # Gaussian distribution matrix\n\nnum_epochs = 20\nfor _ in range(num_epochs):\n  W = W @ torch.normal(mean=0, std=1, size=(3, 3)) # matrix multiplication of N such matrices\nW\n</pre> W = torch.normal(mean=0, std=1, size=(3, 3)) # Gaussian distribution matrix  num_epochs = 20 for _ in range(num_epochs):   W = W @ torch.normal(mean=0, std=1, size=(3, 3)) # matrix multiplication of N such matrices W Out[51]: <pre>tensor([[-213.2879, -109.1465, -217.0422],\n        [ -71.5859,  -35.5326,  -73.4408],\n        [  33.6598,   15.1526,   35.3727]])</pre> <p>We will now take three activation functions (<code>sigmoid</code>, <code>relu</code>, <code>leaky_relu</code>) and plot their functions together with their graident values.</p> In\u00a0[52]: Copied! <pre>plt.figure(figsize=(14,3))\n\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n\nacts = [\n  (\"Sigmoid\", torch.sigmoid),\n  (\"ReLU\", torch.relu),\n  (\"Leaky ReLU\", lambda t: torch.nn.functional.leaky_relu(t, 0.2)),\n]\n\nfor i, (name, f) in enumerate(acts, 1):\n  x.grad = None\n  y = f(x)\n  y.backward(torch.ones_like(x))\n\n  plt.subplot(1, 3, i)\n  plt.plot(x.detach().numpy(), y.detach().numpy(), label=name)\n  plt.plot(x.detach().numpy(), x.grad.detach().numpy(), label=\"Gradient\")\n  plt.legend()\n\nplt.show()\n</pre> plt.figure(figsize=(14,3))  x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)  acts = [   (\"Sigmoid\", torch.sigmoid),   (\"ReLU\", torch.relu),   (\"Leaky ReLU\", lambda t: torch.nn.functional.leaky_relu(t, 0.2)), ]  for i, (name, f) in enumerate(acts, 1):   x.grad = None   y = f(x)   y.backward(torch.ones_like(x))    plt.subplot(1, 3, i)   plt.plot(x.detach().numpy(), y.detach().numpy(), label=name)   plt.plot(x.detach().numpy(), x.grad.detach().numpy(), label=\"Gradient\")   plt.legend()  plt.show() <p>Exercise</p> <p>      Try to interpret what the plots mean.   </p> <p>As can be seen, for the sigmoid function, the gradients are almost zero for low and high ends of the function (saddle points) due to its gradient being $\u03c3(x)(1\u2212\u03c3(x))$. Since the publication of the AlexNet paper, in order to avoid the vanishing gradient problem, ReLU activation is usually preferred over sigmoid.</p> <p>Important</p> <p>     When the output is mapped to  negative values, the ReLU derivates become zero,  causing so-called dying ReLU problem, which can be solved to some degree with the help of Leaky ReLU (or other ReLU variants) where the gradients do not become exactly zero when the output value. Still, in practice, ReLU is usually robust enough.   </p> In\u00a0[53]: Copied! <pre>n_out = 10\nfor n_in in [10**i for i in range(5)]:\n  X = torch.randn(n_in)\n  W = torch.normal(mean=0, std=1, size=(n_in, n_out))\n  O = X @ W\n  print(f'n_in: {n_in}, var(o): {O.var().item()}')\n</pre> n_out = 10 for n_in in [10**i for i in range(5)]:   X = torch.randn(n_in)   W = torch.normal(mean=0, std=1, size=(n_in, n_out))   O = X @ W   print(f'n_in: {n_in}, var(o): {O.var().item()}') <pre>n_in: 1, var(o): 0.16098813712596893\nn_in: 10, var(o): 3.3858141899108887\nn_in: 100, var(o): 181.25555419921875\nn_in: 1000, var(o): 259.7041931152344\nn_in: 10000, var(o): 5145.6123046875\n</pre> <p>Exercise</p> <p>     How can we avoid the exploding gradient problem of random initialization?   </p> <p>To mitigate the exploding gradient problem, an obvious trick is to ensure stability of the variance by enforcing $n_\\textrm{in} \\sigma^2 = 1$. (Xavier) Glorot and Bengio (2010) proposed the ingenious solution of setting the standard deviation to $\\sigma = \\sqrt{\\frac{1}{n_\\textrm{in}}}$.</p> <p>Exercise</p> <p>     How would you extend this reasoning to the case of backpropagation?   </p> <p>With the same logic, to keep the gradient variance consistent, we should ensure $n_\\textrm{out} \\sigma^2 = 1$, where $n_\\textrm{out}$ is the number of output neurons. To satisfy both cases, we average the variance scaling, obtaining $(n_\\textrm{in} + n_\\textrm{out}) \\sigma^2 = 2$, or equivalently $\\sigma = \\sqrt{\\frac{2}{n_\\textrm{in} + n_\\textrm{out}}}$.</p> <p>Xavier initialization is mainly used for sigmoid and tanh activations. The paper He et al. (2015) addresses rectified non-linearities. Since the ReLU function does not suffer from the vanishing gradient problem in the same way as sigmoid or tanh, backpropagation does not shrink the variance as severely. Hence, (Kaiming) He initialization takes the form $\\sigma = \\sqrt{\\frac{2}{n_\\textrm{in}}}$.</p> In\u00a0[95]: Copied! <pre>n_out = 10\nfor n_in in [10**i for i in range(5)]:\n  X = torch.randn(n_in)\n  W = torch.normal(mean=0, std=torch.sqrt(torch.tensor(2.0 / n_in)), size=(n_in, n_out))\n  O = X @ W\n  print(f'n_in: {n_in}, var(o): {O.var().item():.4f}')\n</pre> n_out = 10 for n_in in [10**i for i in range(5)]:   X = torch.randn(n_in)   W = torch.normal(mean=0, std=torch.sqrt(torch.tensor(2.0 / n_in)), size=(n_in, n_out))   O = X @ W   print(f'n_in: {n_in}, var(o): {O.var().item():.4f}') <pre>n_in: 1, var(o): 0.8965\nn_in: 10, var(o): 1.9641\nn_in: 100, var(o): 1.3890\nn_in: 1000, var(o): 1.4248\nn_in: 10000, var(o): 1.3977\n</pre> <p>As $n_\\textrm{in}$ grows, the variance of the output stays roughly constant, showing that our initialization ($\\sigma=\\sqrt{2/n_\\textrm{in}}$) prevents activations from exploding as width increases.</p> In\u00a0[3]: Copied! <pre>import numpy as np\n\nf1 = lambda x: x * np.cos(np.pi * x)\nf2 = lambda x: x**3\n\nx1 = np.linspace(-1, 2, 100)\nx2 = np.linspace(-2, 2, 100)\n\nfs = lambda X, Y: X**2 - Y**2\n\nxs = np.linspace(-2, 2, 160)\nys = np.linspace(-2, 2, 160)\nX, Y = np.meshgrid(xs, ys)\nZ = fs(X, Y)\n\nfig = plt.figure(figsize=(15, 4))\n\nax1 = fig.add_subplot(1, 3, 1)\nax1.plot(x1, f1(x1), label=r'$f(x)=x\\cos(\\pi x)$')\nax1.scatter(-0.3, f1(-0.3), color=[1.0, 0.5, 0.5], label=\"Local Min\")\nax1.scatter(1.1, f1(1.1), color='g', label=\"Global Min\")\nax1.set_title(\"Local &amp; Global Minima\")\nax1.legend()\n\nax2 = fig.add_subplot(1, 3, 2)\nax2.plot(x2, f2(x2), label=r'$f(x)=x^3$')\nax2.scatter(0, 0, color=[1.0, 0.5, 0.5], label=\"Inflection Point\")\nax2.set_title(\"Inflection Point\")\nax2.legend()\n\nax3 = fig.add_subplot(1, 3, 3, projection=\"3d\")\nax3.plot_surface(X, Y, Z, linewidth=0, antialiased=True, alpha=0.9)\nax3.scatter(0, 0, 0, color=[1.0, 0.5, 0.5], s=60, label=\"Saddle point\")\nax3.set_title(r\"$f(x,y)=x^2-y^2$\")\nax3.set_xlabel(\"x\")\nax3.set_ylabel(\"y\")\nax3.set_zlabel(\"f(x,y)\")\nax3.legend()\n\nplt.tight_layout()\nplt.show()\n</pre> import numpy as np  f1 = lambda x: x * np.cos(np.pi * x) f2 = lambda x: x**3  x1 = np.linspace(-1, 2, 100) x2 = np.linspace(-2, 2, 100)  fs = lambda X, Y: X**2 - Y**2  xs = np.linspace(-2, 2, 160) ys = np.linspace(-2, 2, 160) X, Y = np.meshgrid(xs, ys) Z = fs(X, Y)  fig = plt.figure(figsize=(15, 4))  ax1 = fig.add_subplot(1, 3, 1) ax1.plot(x1, f1(x1), label=r'$f(x)=x\\cos(\\pi x)$') ax1.scatter(-0.3, f1(-0.3), color=[1.0, 0.5, 0.5], label=\"Local Min\") ax1.scatter(1.1, f1(1.1), color='g', label=\"Global Min\") ax1.set_title(\"Local &amp; Global Minima\") ax1.legend()  ax2 = fig.add_subplot(1, 3, 2) ax2.plot(x2, f2(x2), label=r'$f(x)=x^3$') ax2.scatter(0, 0, color=[1.0, 0.5, 0.5], label=\"Inflection Point\") ax2.set_title(\"Inflection Point\") ax2.legend()  ax3 = fig.add_subplot(1, 3, 3, projection=\"3d\") ax3.plot_surface(X, Y, Z, linewidth=0, antialiased=True, alpha=0.9) ax3.scatter(0, 0, 0, color=[1.0, 0.5, 0.5], s=60, label=\"Saddle point\") ax3.set_title(r\"$f(x,y)=x^2-y^2$\") ax3.set_xlabel(\"x\") ax3.set_ylabel(\"y\") ax3.set_zlabel(\"f(x,y)\") ax3.legend()  plt.tight_layout() plt.show() In\u00a0[11]: Copied! <pre>def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps):\n  path = [initial_point]\n  x = initial_point\n  for _ in range(num_steps):\n    grad = grad_f(x)\n    x -= learning_rate * grad\n    path.append(x)\n  return np.array(path)\n\ndef plot_descent(f, grad_f, point, steps, start, end, learning_rates=[0.1, 0.3, 0.9, 1.0], figsize=(14,4)):\n  x_vals = np.linspace(start, end, 100)\n  y_vals = f(x_vals)\n\n  fig, axs = plt.subplots(1, len(learning_rates), figsize=figsize)\n  for i, lr in enumerate(learning_rates):\n    path = gradient_descent(f, grad_f, lr, point, steps)\n    axs[i].plot(x_vals, y_vals, color='b')\n    axs[i].plot(path, f(path), marker='o', label=f'LR = {lr}', color=[1.0, 0.5, 0.5])\n    axs[i].set_title(f'LR = {lr}')\n</pre> def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps):   path = [initial_point]   x = initial_point   for _ in range(num_steps):     grad = grad_f(x)     x -= learning_rate * grad     path.append(x)   return np.array(path)  def plot_descent(f, grad_f, point, steps, start, end, learning_rates=[0.1, 0.3, 0.9, 1.0], figsize=(14,4)):   x_vals = np.linspace(start, end, 100)   y_vals = f(x_vals)    fig, axs = plt.subplots(1, len(learning_rates), figsize=figsize)   for i, lr in enumerate(learning_rates):     path = gradient_descent(f, grad_f, lr, point, steps)     axs[i].plot(x_vals, y_vals, color='b')     axs[i].plot(path, f(path), marker='o', label=f'LR = {lr}', color=[1.0, 0.5, 0.5])     axs[i].set_title(f'LR = {lr}') In\u00a0[12]: Copied! <pre>f = lambda x: x**2\ngrad_f = lambda x: 2*x\n\nplot_descent(f, grad_f, 1.5, 10, -2, 2)\n</pre> f = lambda x: x**2 grad_f = lambda x: 2*x  plot_descent(f, grad_f, 1.5, 10, -2, 2) <p>The role of the learning rate changes during training. At the beginning of optimization the parameters are usually far from good solutions and gradients tend to be larger. Larger learning rates help the model move quickly across the loss function and reach useful regions faster. Later in training, when the parameters approach a minimum, gradients become smaller and noisier. If the learning rate remains large, the optimization can oscillate around the minimum. Because of this issue, it is common to use a learning rate scheduler. A scheduler adjusts the learning rate during training so that optimization can move quickly at first and become more stable later.</p> <p>A simple strategy is step decay, where the learning rate is multiplied by a constant factor (for example $0.1$) every $N$ epochs. Another technique is learning rate warmup, where training starts with a small learning rate that gradually increases during the first iterations. This avoids unstable updates when the network parameters are still randomly initialized. After the warmup phase the learning rate reaches its main value and then gradually decreases according to the chosen schedule.</p> <p>Note</p> <p>     In practice, learning rate decay can also slightly reduce overfitting. As the learning rate becomes smaller later in training, parameter updates stabilize and the model is less likely to keep adapting aggressively to the training data, which can improve generalization.   </p> <p>In the example below the learning rate first increases slightly (warmup) and then gradually decreases (cosine decay). This allows the optimization to move quickly at the beginning and take smaller, more precise steps later.</p> In\u00a0[17]: Copied! <pre>import numpy as np\n\ndef lr_schedule(step, total_steps):\n  warmup_steps = int(0.2 * total_steps)\n\n  if step &lt; warmup_steps:\n    return step / warmup_steps\n\n  progress = (step - warmup_steps) / (total_steps - warmup_steps)\n  return 0.5 * (1 + np.cos(np.pi * progress))\n\ntotal_steps = 10\nbase_lr = 0.1\n\nfor step in range(total_steps):\n  lr = base_lr * lr_schedule(step, total_steps)\n  print(f\"step {step:02d}: lr = {lr:.4f}\")\n</pre> import numpy as np  def lr_schedule(step, total_steps):   warmup_steps = int(0.2 * total_steps)    if step &lt; warmup_steps:     return step / warmup_steps    progress = (step - warmup_steps) / (total_steps - warmup_steps)   return 0.5 * (1 + np.cos(np.pi * progress))  total_steps = 10 base_lr = 0.1  for step in range(total_steps):   lr = base_lr * lr_schedule(step, total_steps)   print(f\"step {step:02d}: lr = {lr:.4f}\") <pre>step 00: lr = 0.0000\nstep 01: lr = 0.0500\nstep 02: lr = 0.1000\nstep 03: lr = 0.0962\nstep 04: lr = 0.0854\nstep 05: lr = 0.0691\nstep 06: lr = 0.0500\nstep 07: lr = 0.0309\nstep 08: lr = 0.0146\nstep 09: lr = 0.0038\n</pre> <p>In the examples below, we can see how different values of learning rates may postiviely or negatively influence the outcome depending on whether we are close to the local or global minima.</p> In\u00a0[\u00a0]: Copied! <pre>c = np.array(0.15 * np.pi)\nf = lambda x: x * np.cos(c * x)\ngrad_f = lambda x: np.cos(c * x) - c * x * np.sin(c * x)\n</pre> c = np.array(0.15 * np.pi) f = lambda x: x * np.cos(c * x) grad_f = lambda x: np.cos(c * x) - c * x * np.sin(c * x) In\u00a0[\u00a0]: Copied! <pre>plot_descent(f, grad_f, point=10, steps=10, start=-13, end=13, learning_rates=[0.3, 1.3, 2.0])\n</pre> plot_descent(f, grad_f, point=10, steps=10, start=-13, end=13, learning_rates=[0.3, 1.3, 2.0]) In\u00a0[\u00a0]: Copied! <pre>plot_descent(f, grad_f, point=-7, steps=5, start=-13, end=13, learning_rates=[0.5, 2.7, 3.5])\n</pre> plot_descent(f, grad_f, point=-7, steps=5, start=-13, end=13, learning_rates=[0.5, 2.7, 3.5]) In\u00a0[21]: Copied! <pre>def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, momentum=0.9):\n  assert momentum &lt;= 1 and momentum &gt;= 0\n  path = [initial_point]\n  x = initial_point\n\n  beta = momentum\n  v = 0.0\n  for _ in range(num_steps):\n    grad = grad_f(x)\n    v = beta * v + (1 - beta) * grad\n    x -= learning_rate * v\n    path.append(x)\n  return np.array(path)\n\ndef plot_descent(f, grad_f, point, steps, start, end, learning_rate, momentums=[0.1, 0.5, 0.9], figsize=(14,4)):\n  x_vals = np.linspace(start, end, 100)\n  y_vals = f(x_vals)\n\n  fig, axs = plt.subplots(1, len(momentums), figsize=figsize)\n  for i, momentum in enumerate(momentums):\n    path = gradient_descent(f, grad_f, learning_rate, point, steps, momentum)\n    axs[i].plot(x_vals, y_vals, color='b')\n    axs[i].plot(path, f(path), marker='o', label=f'M = {momentum}', color=[1.0, 0.5, 0.5])\n    axs[i].set_title(f'M = {momentum}')\n</pre> def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, momentum=0.9):   assert momentum &lt;= 1 and momentum &gt;= 0   path = [initial_point]   x = initial_point    beta = momentum   v = 0.0   for _ in range(num_steps):     grad = grad_f(x)     v = beta * v + (1 - beta) * grad     x -= learning_rate * v     path.append(x)   return np.array(path)  def plot_descent(f, grad_f, point, steps, start, end, learning_rate, momentums=[0.1, 0.5, 0.9], figsize=(14,4)):   x_vals = np.linspace(start, end, 100)   y_vals = f(x_vals)    fig, axs = plt.subplots(1, len(momentums), figsize=figsize)   for i, momentum in enumerate(momentums):     path = gradient_descent(f, grad_f, learning_rate, point, steps, momentum)     axs[i].plot(x_vals, y_vals, color='b')     axs[i].plot(path, f(path), marker='o', label=f'M = {momentum}', color=[1.0, 0.5, 0.5])     axs[i].set_title(f'M = {momentum}') In\u00a0[22]: Copied! <pre>plot_descent(f, grad_f, point=-7, steps=35, start=-13, end=13, learning_rate=1.1, momentums=[0.0, 0.6, 0.9])\n</pre> plot_descent(f, grad_f, point=-7, steps=35, start=-13, end=13, learning_rate=1.1, momentums=[0.0, 0.6, 0.9]) In\u00a0[25]: Copied! <pre>def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, gamma=0.9):\n  assert gamma &lt;= 1 and gamma &gt;= 0\n  path = [initial_point]\n  x = initial_point\n\n  s = 0.0\n  epsilon=1e-8\n  for _ in range(num_steps):\n    grad = grad_f(x)\n    s = gamma * s + (1 - gamma) * grad**2\n    x -= learning_rate * grad / (np.sqrt(s) + epsilon)\n    path.append(x)\n  return np.array(path)\n\ndef plot_descent(f, grad_f, point, steps, start, end, learning_rate, gammas=[0.9, 0.99, 0.999], figsize=(14,4)):\n  x_vals = np.linspace(start, end, 100)\n  y_vals = f(x_vals)\n\n  fig, axs = plt.subplots(1, len(gammas), figsize=figsize)\n  for i, gamma in enumerate(gammas):\n    path = gradient_descent(f, grad_f, learning_rate, point, steps, gamma)\n    axs[i].plot(x_vals, y_vals, color='b')\n    axs[i].plot(path, f(path), marker='o', label=f'G = {gamma}', color=[1.0, 0.5, 0.5])\n    axs[i].set_title(f'G = {gamma}')\n</pre> def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, gamma=0.9):   assert gamma &lt;= 1 and gamma &gt;= 0   path = [initial_point]   x = initial_point    s = 0.0   epsilon=1e-8   for _ in range(num_steps):     grad = grad_f(x)     s = gamma * s + (1 - gamma) * grad**2     x -= learning_rate * grad / (np.sqrt(s) + epsilon)     path.append(x)   return np.array(path)  def plot_descent(f, grad_f, point, steps, start, end, learning_rate, gammas=[0.9, 0.99, 0.999], figsize=(14,4)):   x_vals = np.linspace(start, end, 100)   y_vals = f(x_vals)    fig, axs = plt.subplots(1, len(gammas), figsize=figsize)   for i, gamma in enumerate(gammas):     path = gradient_descent(f, grad_f, learning_rate, point, steps, gamma)     axs[i].plot(x_vals, y_vals, color='b')     axs[i].plot(path, f(path), marker='o', label=f'G = {gamma}', color=[1.0, 0.5, 0.5])     axs[i].set_title(f'G = {gamma}') In\u00a0[26]: Copied! <pre>plot_descent(f, grad_f, point=12, steps=10, start=-13, end=13, learning_rate=0.5, gammas=[0, 0.7, 0.9])\n</pre> plot_descent(f, grad_f, point=12, steps=10, start=-13, end=13, learning_rate=0.5, gammas=[0, 0.7, 0.9]) In\u00a0[65]: Copied! <pre>def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, b1=0.9, b2=0.999, normalize=False):\n  assert b1 &lt;= 1 and b1 &gt;= 0\n  assert b2 &lt;= 1 and b2 &gt;= 0\n  path = [initial_point]\n  x = initial_point\n\n  v = 0.0\n  s = 0.0\n  epsilon=1e-8\n  for t in range(1, num_steps + 1):\n    grad = grad_f(x)\n    v = b1 * v + (1 - b1) * grad\n    s = b2 * s + (1 - b2) * grad**2\n    # as our function and plot are simple, we will keep\n    # unnormalized option as well for demonstration purposes\n    v_hat = v / (1 - b1**t) if normalize else v\n    s_hat = s / (1 - b2**t) if normalize else s\n    x -= learning_rate * v_hat / (np.sqrt(s_hat) + epsilon)\n    path.append(x)\n  return np.array(path)\n\ndef plot_descent(f, grad_f, point, steps, start, end, learning_rate, b1_vals=[0.9, 0.99, 0.999], b2_vals=[0.1, 0.99, 0.999], figsize=(14,4)):\n  x_vals = np.linspace(start, end, 100)\n  y_vals = f(x_vals)\n\n  fig, axs = plt.subplots(len(b1_vals), len(b2_vals), figsize=figsize)\n  for i, b1 in enumerate(b1_vals):\n    for j, b2 in enumerate(b2_vals):\n      path = gradient_descent(f, grad_f, learning_rate, point, steps, b1, b2)\n      axs[i, j].plot(x_vals, y_vals, color='b')\n      axs[i, j].plot(path, f(path), marker='o', label=f'B1 = {b1}, B2 = {b2}', color=[1.0, 0.5, 0.5])\n      axs[i, j].set_title(f'B1 = {b1}, B2 = {b2}')\n</pre> def gradient_descent(f, grad_f, learning_rate, initial_point, num_steps, b1=0.9, b2=0.999, normalize=False):   assert b1 &lt;= 1 and b1 &gt;= 0   assert b2 &lt;= 1 and b2 &gt;= 0   path = [initial_point]   x = initial_point    v = 0.0   s = 0.0   epsilon=1e-8   for t in range(1, num_steps + 1):     grad = grad_f(x)     v = b1 * v + (1 - b1) * grad     s = b2 * s + (1 - b2) * grad**2     # as our function and plot are simple, we will keep     # unnormalized option as well for demonstration purposes     v_hat = v / (1 - b1**t) if normalize else v     s_hat = s / (1 - b2**t) if normalize else s     x -= learning_rate * v_hat / (np.sqrt(s_hat) + epsilon)     path.append(x)   return np.array(path)  def plot_descent(f, grad_f, point, steps, start, end, learning_rate, b1_vals=[0.9, 0.99, 0.999], b2_vals=[0.1, 0.99, 0.999], figsize=(14,4)):   x_vals = np.linspace(start, end, 100)   y_vals = f(x_vals)    fig, axs = plt.subplots(len(b1_vals), len(b2_vals), figsize=figsize)   for i, b1 in enumerate(b1_vals):     for j, b2 in enumerate(b2_vals):       path = gradient_descent(f, grad_f, learning_rate, point, steps, b1, b2)       axs[i, j].plot(x_vals, y_vals, color='b')       axs[i, j].plot(path, f(path), marker='o', label=f'B1 = {b1}, B2 = {b2}', color=[1.0, 0.5, 0.5])       axs[i, j].set_title(f'B1 = {b1}, B2 = {b2}') In\u00a0[66]: Copied! <pre>plot_descent(f, grad_f, point=15, steps=20, start=-15, end=25, learning_rate=0.7, b1_vals=[0, 0.6, 0.9], b2_vals=[0, 0.6, 0.9], figsize=(12, 12))\n</pre> plot_descent(f, grad_f, point=15, steps=20, start=-15, end=25, learning_rate=0.7, b1_vals=[0, 0.6, 0.9], b2_vals=[0, 0.6, 0.9], figsize=(12, 12)) <p>Instead of simple grayscale MNIST dataset, we will choose CIFAR-10 this time to train our model on. It consists of 50,000 training and 10,000 test images of size $32 \\times 32$ divided into 10 classes. An alternative version of the dataset called CIFAR-100 has 100 classes. Unlike MNIST, images have three channels (RGB) which makes computation more expensive and the task of the model more difficult.</p> <p>We have provided overview of augmentation above. In practice, augmentation is implemented directly in the dataset pipeline so that every time a sample is loaded, a slightly different version of the image is produced. Several simple transformations are commonly used. For example, random crop with padding introduces small translations and prevents the model from relying on exact pixel locations, whereas, horizontal flip makes the model invariant to left\u2013right orientation. On the other hand, normalization stabilizes optimization by ensuring that pixel values have consistent scale across channels.</p> <p>During training these transformations are stochastic, meaning that each epoch sees slightly different inputs. In contrast, the test pipeline is deterministic and only converts the image to a tensor and normalizes it.</p> <p>Note</p> <p>     In more advanced pipelines, stronger augmentation strategies can be applied. Two widely used examples are particularly useful for small datasets like CIFAR because they significantly reduce overfitting: MixUp (Zhang et al, 2017) linearly combines two images and their labels helping the model learn smoother decision boundaries.  CutMix (Sangdoo et al, 2019) inserts a rectangular patch from one image into another, and the labels are mixed proportionally to the patch area which encourages the model to rely on distributed visual evidence rather than a single region.    </p> <p>Below we define the PyTorch dataset transformations and create the training and test loaders.</p> In\u00a0[34]: Copied! <pre>from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nDATA_PATH = '../../_drafts/data' # modify accordingly (e.g. to './data')\n\n# Per-channel (R,G,B) mean and standard deviation of the CIFAR-10 training set.These values were computed once from the dataset and are commonly reused.\ncifar_mean = (0.4914, 0.4822, 0.4465)\ncifar_std  = (0.2470, 0.2435, 0.2616)\n\ntrain_tfms = transforms.Compose([\n  transforms.RandomCrop(32, padding=4),\n  transforms.RandomHorizontalFlip(),\n  transforms.TrivialAugmentWide(),\n  transforms.ToTensor(),\n  transforms.Normalize(cifar_mean, cifar_std),\n])\n\ntest_tfms = transforms.Compose([\n  transforms.ToTensor(),\n  transforms.Normalize(cifar_mean, cifar_std),\n])\n\ntrain_data = datasets.CIFAR10(root=DATA_PATH, train=True,  download=True, transform=train_tfms)\ntest_data  = datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=test_tfms)\n\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True,  pin_memory=True, num_workers=2)\ntest_loader  = DataLoader(test_data,  batch_size=64, shuffle=False, pin_memory=True, num_workers=2)\n</pre> from torchvision import datasets, transforms from torch.utils.data import DataLoader  DATA_PATH = '../../_drafts/data' # modify accordingly (e.g. to './data')  # Per-channel (R,G,B) mean and standard deviation of the CIFAR-10 training set.These values were computed once from the dataset and are commonly reused. cifar_mean = (0.4914, 0.4822, 0.4465) cifar_std  = (0.2470, 0.2435, 0.2616)  train_tfms = transforms.Compose([   transforms.RandomCrop(32, padding=4),   transforms.RandomHorizontalFlip(),   transforms.TrivialAugmentWide(),   transforms.ToTensor(),   transforms.Normalize(cifar_mean, cifar_std), ])  test_tfms = transforms.Compose([   transforms.ToTensor(),   transforms.Normalize(cifar_mean, cifar_std), ])  train_data = datasets.CIFAR10(root=DATA_PATH, train=True,  download=True, transform=train_tfms) test_data  = datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=test_tfms)  train_loader = DataLoader(train_data, batch_size=64, shuffle=True,  pin_memory=True, num_workers=2) test_loader  = DataLoader(test_data,  batch_size=64, shuffle=False, pin_memory=True, num_workers=2) <pre>Files already downloaded and verified\nFiles already downloaded and verified\n</pre> <p>Some portion of the code should be familiar and self-explanatory. We need to briefly discuss others. <code>TrivialAugmentWide()</code> applies one randomly selected augmentation with a random magnitude. The operation is sampled from a pool of common image transformations such as rotation, color adjustment, translation, or contrast changes. Because the operation and its strength are chosen automatically, this method provides strong augmentation while keeping the code simple.</p> <p>Note</p> <p>     Note that this augmentation is applied in addition to the earlier random crop and horizontal flip. Although some transformations in the pool may partially overlap with the earlier ones, keeping the basic augmentations explicitly in the pipeline ensures that these important variations appear consistently during training. The additional stochastic augmentation simply increases diversity beyond these core transformations.   </p> <p>Pixel values originally lie in the range $[0, 255]$. <code>ToTensor()</code> converts them into floating point numbers scaled to $[0, 1]$. The images are then normalized using the dataset statistics: mean and standard deviation. Normalization centers the input distribution around zero and scales it so that each channel has comparable variance. This stabilizes optimization and typically improves convergence.</p> <p>The <code>DataLoader</code> objects create minibatches of images and labels that are fed to the model during training. Shuffling is enabled for the training data because modern neural networks are optimized with minibatch SGD. If the samples were always processed in the same order, consecutive batches could contain very similar examples (for example images from the same class appearing together in the dataset). This can bias the gradient estimates and slow down learning. Shuffling ensures that each minibatch is a more random mixture of samples, which makes the gradient a better approximation of the true dataset gradient and usually improves convergence.</p> <p>Exercise</p> <p>     For the test loader, why shuffling is unnecessary?   </p> <p>The loader also includes a few parameters that affect performance rather than the learning algorithm itself: <code>num_workers</code> controls how many CPU processes prepare data in parallel. Loading images, applying transformations, and forming batches takes time. If this work is done by a single process, the GPU may sit idle waiting for the next batch. Increasing the number of workers allows multiple batches to be prepared simultaneously, which helps keep the GPU busy.</p> <p>On the other hand, <code>pin_memory=True</code> is useful when training on a GPU. Normally, data is first stored in regular CPU memory and then copied to GPU memory. When memory is \"pinned\" (page-locked), these transfers can happen faster and more efficiently. In practice this often reduces the time spent moving data to the GPU. Together, parallel workers and pinned memory help the training loop spend more time computing gradients and less time waiting for data.</p> In\u00a0[35]: Copied! <pre>idx_to_class = {v: k for k, v in train_data.class_to_idx.items()}\nidx_to_class\n</pre> idx_to_class = {v: k for k, v in train_data.class_to_idx.items()} idx_to_class Out[35]: <pre>{0: 'airplane',\n 1: 'automobile',\n 2: 'bird',\n 3: 'cat',\n 4: 'deer',\n 5: 'dog',\n 6: 'frog',\n 7: 'horse',\n 8: 'ship',\n 9: 'truck'}</pre> In\u00a0[54]: Copied! <pre>X_batch, y_batch = next(iter(train_loader))\nx = X_batch[0]  # (C,H,W), normalized\ny = y_batch[0].item()\n\nmean = torch.tensor(cifar_mean).view(3, 1, 1)\nstd  = torch.tensor(cifar_std).view(3, 1, 1)\n\nx_vis = (x * std + mean).clamp(0, 1)  # back to [0,1]\n\nplt.imshow(x_vis.permute(1, 2, 0))\nplt.title(idx_to_class[y])\nplt.axis(\"off\");\n</pre> X_batch, y_batch = next(iter(train_loader)) x = X_batch[0]  # (C,H,W), normalized y = y_batch[0].item()  mean = torch.tensor(cifar_mean).view(3, 1, 1) std  = torch.tensor(cifar_std).view(3, 1, 1)  x_vis = (x * std + mean).clamp(0, 1)  # back to [0,1]  plt.imshow(x_vis.permute(1, 2, 0)) plt.title(idx_to_class[y]) plt.axis(\"off\"); In\u00a0[55]: Copied! <pre># Note that augmentation is applied during training time on the fly\nf\"Train/test split size: {len(train_data)}/{len(test_data)}\"\n</pre> # Note that augmentation is applied during training time on the fly f\"Train/test split size: {len(train_data)}/{len(test_data)}\" Out[55]: <pre>'Train/test split size: 50000/10000'</pre> In\u00a0[56]: Copied! <pre>f\"Train/test batch count: {len(train_loader)}/{len(test_loader)}\"\n</pre> f\"Train/test batch count: {len(train_loader)}/{len(test_loader)}\" Out[56]: <pre>'Train/test batch count: 782/157'</pre> In\u00a0[60]: Copied! <pre>import torch\nfrom PIL import Image\n\nimg = Image.fromarray(train_data.data[0])\nlabel = train_data.targets[0]\n\nfig, axes = plt.subplots(2,4, figsize=(10,5))\n\nfor ax in axes.flat:\n    aug = train_data.transform(img)\n    aug = aug * std + mean     # undo normalization\n    aug = aug.clamp(0,1)\n    ax.imshow(aug.permute(1,2,0))\n    ax.axis(\"off\")\n\nplt.suptitle(f\"Data augmentation of an image labeled {idx_to_class[label]}\")\nplt.show()\n</pre> import torch from PIL import Image  img = Image.fromarray(train_data.data[0]) label = train_data.targets[0]  fig, axes = plt.subplots(2,4, figsize=(10,5))  for ax in axes.flat:     aug = train_data.transform(img)     aug = aug * std + mean     # undo normalization     aug = aug.clamp(0,1)     ax.imshow(aug.permute(1,2,0))     ax.axis(\"off\")  plt.suptitle(f\"Data augmentation of an image labeled {idx_to_class[label]}\") plt.show() <p>We will now train our custom CNN model by applying regularization and optimization methods we have discussed throughout this notebook. Pay attention to the architecture, try to understand the input dimensions. In order to initialize our weights non-randomly, we will not use <code>nn.Lazy</code> modules.</p> In\u00a0[63]: Copied! <pre>model = nn.Sequential(\n    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Flatten(),\n    nn.Linear(128 * 4 * 4, 256),\n    nn.ReLU(),\n    nn.Dropout(0.5), # Dropout\n    nn.Linear(256, 10)\n)\n</pre> model = nn.Sequential(     nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),     nn.ReLU(),     nn.MaxPool2d(kernel_size=2, stride=2),     nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),     nn.ReLU(),     nn.MaxPool2d(kernel_size=2, stride=2),     nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),     nn.ReLU(),     nn.MaxPool2d(kernel_size=2, stride=2),     nn.Flatten(),     nn.Linear(128 * 4 * 4, 256),     nn.ReLU(),     nn.Dropout(0.5), # Dropout     nn.Linear(256, 10) ) In\u00a0[\u00a0]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model.to(device) Out[\u00a0]: <pre>Sequential(\n  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (4): ReLU()\n  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (7): ReLU()\n  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (9): Flatten(start_dim=1, end_dim=-1)\n  (10): Linear(in_features=2048, out_features=256, bias=True)\n  (11): ReLU()\n  (12): Dropout(p=0.5, inplace=False)\n  (13): Linear(in_features=256, out_features=10, bias=True)\n)</pre> In\u00a0[64]: Copied! <pre># Moment weigths are by default 0.9 and 0.999 for Adam optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.005, betas=(0.9, 0.999), weight_decay=1e-4)\nloss_fn = nn.CrossEntropyLoss()\n</pre> # Moment weigths are by default 0.9 and 0.999 for Adam optimizer optimizer = optim.Adam(model.parameters(), lr=0.005, betas=(0.9, 0.999), weight_decay=1e-4) loss_fn = nn.CrossEntropyLoss() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nC:\\Users\\SHAHAL~1\\AppData\\Local\\Temp/ipykernel_9084/2725183420.py in &lt;module&gt;\n      1 # Moment weigths are by default 0.9 and 0.999 for Adam optimizer\n----&gt; 2 optimizer = optim.Adam(model.parameters(), lr=0.005, betas=(0.9, 0.999), weight_decay=1e-4)\n      3 loss_fn = nn.CrossEntropyLoss()\n\nNameError: name 'optim' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre># Kaiming He Initialization\nfor layer in model:\n  if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n    nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n    if layer.bias is not None:\n      nn.init.zeros_(layer.bias)\n</pre> # Kaiming He Initialization for layer in model:   if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):     nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')     if layer.bias is not None:       nn.init.zeros_(layer.bias) In\u00a0[\u00a0]: Copied! <pre>model.train()\n\nnum_epochs = 100\nfor epoch in range(num_epochs):\n  loss = 0.0\n  for X_train, y_train in train_loader:\n    X_train, y_train = X_train.to(device), y_train.to(device)\n    optimizer.zero_grad()\n    preds = model(X_train)\n    batch_loss = loss_fn(preds, y_train)\n    batch_loss.backward()\n    loss += batch_loss.item()\n    optimizer.step()\n\n  if (epoch + 1) % 10 == 0 or epoch == 0:\n    print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}')\n</pre> model.train()  num_epochs = 100 for epoch in range(num_epochs):   loss = 0.0   for X_train, y_train in train_loader:     X_train, y_train = X_train.to(device), y_train.to(device)     optimizer.zero_grad()     preds = model(X_train)     batch_loss = loss_fn(preds, y_train)     batch_loss.backward()     loss += batch_loss.item()     optimizer.step()    if (epoch + 1) % 10 == 0 or epoch == 0:     print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss/len(train_loader):.4f}') <pre>Epoch: 1/50, Loss: 1.6063\nEpoch: 2/50, Loss: 1.3142\nEpoch: 3/50, Loss: 1.2205\nEpoch: 4/50, Loss: 1.1668\nEpoch: 5/50, Loss: 1.1335\nEpoch: 6/50, Loss: 1.0996\nEpoch: 7/50, Loss: 1.0810\nEpoch: 8/50, Loss: 1.0594\nEpoch: 9/50, Loss: 1.0465\nEpoch: 10/50, Loss: 1.0295\nEpoch: 11/50, Loss: 1.0175\nEpoch: 12/50, Loss: 1.0196\nEpoch: 13/50, Loss: 0.9891\nEpoch: 14/50, Loss: 0.9799\nEpoch: 15/50, Loss: 0.9781\nEpoch: 16/50, Loss: 0.9729\nEpoch: 17/50, Loss: 0.9789\nEpoch: 18/50, Loss: 0.9651\nEpoch: 19/50, Loss: 0.9611\nEpoch: 20/50, Loss: 0.9605\nEpoch: 21/50, Loss: 0.9545\nEpoch: 22/50, Loss: 0.9485\nEpoch: 23/50, Loss: 0.9362\nEpoch: 24/50, Loss: 0.9436\nEpoch: 25/50, Loss: 0.9309\nEpoch: 26/50, Loss: 0.9296\nEpoch: 27/50, Loss: 0.9240\nEpoch: 28/50, Loss: 0.9205\nEpoch: 29/50, Loss: 0.9164\nEpoch: 30/50, Loss: 0.9104\nEpoch: 31/50, Loss: 0.9124\nEpoch: 32/50, Loss: 0.9160\nEpoch: 33/50, Loss: 0.9211\nEpoch: 34/50, Loss: 0.9067\nEpoch: 35/50, Loss: 0.9168\nEpoch: 36/50, Loss: 0.9151\nEpoch: 37/50, Loss: 0.9065\nEpoch: 38/50, Loss: 0.9048\nEpoch: 39/50, Loss: 0.9024\nEpoch: 40/50, Loss: 0.8971\nEpoch: 41/50, Loss: 0.9015\nEpoch: 42/50, Loss: 0.9055\nEpoch: 43/50, Loss: 0.8923\nEpoch: 44/50, Loss: 0.8804\nEpoch: 45/50, Loss: 0.8855\nEpoch: 46/50, Loss: 0.8931\nEpoch: 47/50, Loss: 0.8802\nEpoch: 48/50, Loss: 0.8820\nEpoch: 49/50, Loss: 0.8748\nEpoch: 50/50, Loss: 0.8847\n</pre> In\u00a0[\u00a0]: Copied! <pre>model.eval()\n\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n  for X_test, y_test in test_loader:\n    X_test, y_test = X_test.to(device), y_test.to(device)\n    preds = model(X_test)\n    _, predicted = torch.max(preds, 1)\n    total += y_test.size(0)\n    correct += (predicted == y_test).sum().item()\n\naccuracy = 100 * correct / total\nprint(f'Accuracy on the test set: {accuracy:.2f}%')\n</pre> model.eval()  correct = 0 total = 0 with torch.no_grad():   for X_test, y_test in test_loader:     X_test, y_test = X_test.to(device), y_test.to(device)     preds = model(X_test)     _, predicted = torch.max(preds, 1)     total += y_test.size(0)     correct += (predicted == y_test).sum().item()  accuracy = 100 * correct / total print(f'Accuracy on the test set: {accuracy:.2f}%') <pre>Accuracy on the test set: 68.98%\n</pre> In\u00a0[\u00a0]: Copied! <pre>learning_rates = [1e-4, 5e-4, 1e-3]\n\nresults = {}\n\nfor lr in learning_rates:\n    model = nn.Sequential(\n        nn.Conv2d(3,32,3,padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(2),\n        nn.Conv2d(32,64,3,padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(2),\n        nn.Flatten(),\n        nn.Linear(64*8*8,128),\n        nn.ReLU(),\n        nn.Linear(128,10)\n    ).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    loss_fn = nn.CrossEntropyLoss()\n\n    model.train()\n\n    for epoch in range(5):  # short training for comparison\n        for X, y in train_loader:\n            X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            loss = loss_fn(model(X), y)\n            loss.backward()\n            optimizer.step()\n\n    # evaluate\n    model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for X, y in test_loader:\n            X, y = X.to(device), y.to(device)\n            preds = model(X)\n            _, predicted = torch.max(preds,1)\n            total += y.size(0)\n            correct += (predicted == y).sum().item()\n\n    acc = 100 * correct / total\n    results[lr] = acc\n    print(f\"lr={lr} -&gt; accuracy={acc:.2f}%\")\n\nprint(results)\n</pre> learning_rates = [1e-4, 5e-4, 1e-3]  results = {}  for lr in learning_rates:     model = nn.Sequential(         nn.Conv2d(3,32,3,padding=1),         nn.ReLU(),         nn.MaxPool2d(2),         nn.Conv2d(32,64,3,padding=1),         nn.ReLU(),         nn.MaxPool2d(2),         nn.Flatten(),         nn.Linear(64*8*8,128),         nn.ReLU(),         nn.Linear(128,10)     ).to(device)      optimizer = torch.optim.Adam(model.parameters(), lr=lr)     loss_fn = nn.CrossEntropyLoss()      model.train()      for epoch in range(5):  # short training for comparison         for X, y in train_loader:             X, y = X.to(device), y.to(device)             optimizer.zero_grad()             loss = loss_fn(model(X), y)             loss.backward()             optimizer.step()      # evaluate     model.eval()     correct = 0     total = 0      with torch.no_grad():         for X, y in test_loader:             X, y = X.to(device), y.to(device)             preds = model(X)             _, predicted = torch.max(preds,1)             total += y.size(0)             correct += (predicted == y).sum().item()      acc = 100 * correct / total     results[lr] = acc     print(f\"lr={lr} -&gt; accuracy={acc:.2f}%\")  print(results) <p>Exercise</p> <p>     Perform a small hyperparameter sweep for the CIFAR-10 model. Try at least three different learning rates, record the final test accuracy for each run, and identify which value gives the best result. As an additional experiment, keep the best learning rate and vary one more parameter such as dropout or weight decay. Observe how the training and test accuracy change.   </p> <p>We built and trained a model for the more challenging CIFAR-10 dataset using PyTorch. Along the way we constructed a complete deep learning pipeline: loading data with <code>DataLoader</code>, applying data augmentation, normalizing inputs, defining a CNN architecture, and training it efficiently on batches of data. We also discussed how optimization and regularization influence model performance. Optimization methods such as SGD or Adam determine how the model parameters are updated during training, while techniques such as data augmentation, dropout, and weight decay help control model complexity and improve generalization. We also explored how hyperparameter tuning can further improve results by systematically experimenting with different settings.</p>"},{"location":"notebooks/04_regul_optim/#04-regularization-and-optimization","title":"04. Regularization and Optimization\u00b6","text":"28 Feb 2025 /    21 Feb 2026 <p>Info</p> <p>     The following source was consulted in preparing this material: Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J. Dive into Deep Learning. Cambridge University Press.     <ul> <li> Chapter 3.7: Weight Decay</li> <li> Chapter 5.4: Numerical Stability and Initialization</li> <li> Chapter 5.6: Dropout</li> <li>Chapter 12: Optimization Algorithms</li> <li>Chapter 14.1: Image Augmentation</li> </ul> </p> <p>Our artificial neural networks which we built from scratch in previous notebooks were in their simplistic form and very inefficient, causing slow and inefficient training. In this notebook we will introduce many regularization and optimization techniques which will improve our model performance.</p> <p>Regularization is about controlling how flexible or complex a neural network is so that it does not memorize the training data. Regularization techniques such as weight decay (penalizing large weights), dropout (randomly turning off neurons during training), data augmentation, and early stopping make the model more stable and less sensitive to noise. The goal is a better balance between bias and variance so that the model generalizes well.</p> <p>Optimization is about how we train the model. Even a well-designed network can train slowly or get stuck if the optimization method is poor. Optimization algorithms such as SGD with momentum, Adam, or learning rate scheduling help the model find good parameter values more efficiently. They reduce the training loss faster and more reliably.</p> <p>Tip</p> <p>     From machine learning, you should already be familiar with the concepts of     underfitting and overfitting. Recall  bias and variance with     StatQuest video     and the     MLU visualization.   </p>"},{"location":"notebooks/04_regul_optim/#weight-decay","title":"Weight Decay\u00b6","text":"<p>The loss function $L$ (e.g. MSE, cross-entropy) can be regularized by adding to it a regularizer $R$, where \u03bb is the regularization hyperparameter that controls the strength of the regularization function (recall the learning rate hyperparameter):</p> <p>$$ L(w, b) + \\lambda R(w) $$</p> <p>Even if there are many different regularizers for loss, the most common and practically efficient one is $l_2$ (ridge) regularization, which is called weight decay in the context of deep learning. It has the following formula:</p> <p>$$ L(w, b) + \\frac{\\lambda}{2} \\sum_{i=1}^{d} w_i^2 $$</p> <p>As can be seen, the eventual loss increases for higher weight values. Hence, backpropagation will not only reduce the chosen loss function but will also strive for smaller weights. You can imagine that, in the limit, weights are going to approach zero, reducing the impact of the corresponding neuron on the outcome. Less neuron impact means getting simpler function to avoid overfitting.</p> <p>Note</p> <p>     When $\u03bb$ is zero we restore the original loss function, when $\u03bb$ is large it shifts the attention from the original loss function to $R$, constraining $w$ further. The number $2$ exists in the equation so that when the derivative of the square will be found, it will get cancelled out with it, simplifying the derivative to $ \u03bbw_i $. Recall from linear algebra that we are finding the square of the euclidean norm of a $d$-dimensional vector, again to ease computation: we remove the burden of finding the square root in $l_2$.   </p> <p>For demonstration, we will first generate random data corresponding to an $N$-dimensional linear function, where each data point will be shifted by some amount of noise.</p>"},{"location":"notebooks/04_regul_optim/#dropout","title":"Dropout\u00b6","text":"<p>The more complicated the model is, the more chance it has to overfit the train data. In order to improve the generalization ability of our model to unseen data, we may aim for a simpler model.Regularization with weight decay achieved that to some degree but it's not the only solution. Srivastava et al. (2014) building on top of the earlier idea developed by Bishop (1995) came up with the concept called dropout. Dropout literally drops out some neurons during training by setting their activations to zero.</p>      Dropout is simply \"turning off\" neurons ~ Dive into Deep Learning, Fig. 5.6.1.     CC BY-SA 4.0.    <p>We will implement the operation in its simplistic form below.</p>"},{"location":"notebooks/04_regul_optim/#data-augmentation","title":"Data Augmentation\u00b6","text":"<p>Data augmentation is a regularization technique that creates new training examples by applying random, label-preserving transformations to existing data. Instead of training on a fixed dataset, we train on slightly different versions of the same samples at each epoch. This increases the effective size and diversity of the training set. Augmentation forces the model to learn features that remain stable under small perturbations. As a result of this noise, memorization becomes harder and generalization improves.</p> <p>Important</p> <p>     In practice, augmentation is applied only to the training data. The test split should remain unchanged so that evaluation reflects the true data distribution. Augmentations must also preserve the label. For example, flipping a handwritten $9$ may turn it into a $6$, which breaks the supervision signal.   </p> <p>Note</p> <p>     Data augmentation is especially powerful in computer vision, where transformations such as small rotations, crops, flips, and color changes preserve the semantic meaning of the image.   </p> <p>Consider a small $4 \\times 4$ image represented as a tensor. A horizontal flip produces a different pixel arrangement, but it represents the same object. Training on both versions encourages the model to rely less on exact pixel positions.</p>"},{"location":"notebooks/04_regul_optim/#exploding-vanishing-gradients","title":"Exploding &amp; Vanishing Gradients\u00b6","text":"<p>When building neural network from scratch in earlier notebooks, we have been initilializing our parameters rather randomly. It turns out, by taking certain measures when initializing weights, we can influence the speed and convergence of our training. But before that, we need to discuss how poor parameter initialization can cause problems formally known as vanishing (exploding) gradient problem.</p> <p>Tip</p> <p>     See the material on probability theory in case of having difficulties with the concepts described in this and following sections.   </p>"},{"location":"notebooks/04_regul_optim/#weight-initialization","title":"Weight Initialization\u00b6","text":"<p>Proper weight initialization is crucial for stable training of deep neural networks. If the initial weights are chosen poorly, activations may explode or vanish as they propagate through layers.</p> <p>To understand this phenomenon, let us analyze a very simple linear layer without activation and bias. We know that the next layer's values will be affected by the following sum: $\\text{out}_{i} = \\sum_{j=1}^{n_\\textrm{in}} w_{ij} x_j$. We assume that the weights are initialized from a zero-mean normal distribution $w_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$. Recall that the expectation $E$ of a random variable represents the average value you would expect if you repeated the experiment many times.</p>  $$ \\begin{aligned} \\mathbb{E}[\\text{out}_i]  = \\sum_{j=1}^{n_{\\mathrm{in}}} \\mathbb{E}[w_{ij} x_j]  &amp;= \\sum_{j=1}^{n_{\\mathrm{in}}} \\mathbb{E}[w_{ij}] \\, \\mathbb{E}[x_j] \\\\ &amp;= \\sum_{j=1}^{n_{\\mathrm{in}}} 0 \\cdot \\mathbb{E}[x_j]  = 0. \\end{aligned} $$  <p>Also recall that variance measures the spread of data around its mean, where squaring is applied so that negative and positive values will not cancel each others out. Considering that weights and inputs are independent:</p>  $$ \\begin{aligned} \\mathrm{Var}[\\text{out}_i]  &amp;= \\mathbb{E}[\\text{out}_i^2] - (\\mathbb{E}[\\text{out}_i])^2 \\\\ &amp;= \\sum_{j=1}^{n_{\\mathrm{in}}} \\mathbb{E}[w_{ij}^2 x_j^2] - 0 = \\sum_{j=1}^{n_{\\mathrm{in}}} \\mathbb{E}[w_{ij}^2] \\, \\mathbb{E}[x_j^2] \\\\ &amp;= n_{\\mathrm{in}} \\sigma^2 \\gamma^2 \\end{aligned} $$  <p>The term $n_{\\mathrm{in}} \\sigma^2 \\gamma^2$ appears because the output is a sum of $n_{\\mathrm{in}}$ independent random terms. For a single term $w_{ij} x_j$, we assume the weights have variance $\\sigma^2$ and the inputs have variance $\\gamma^2$. Under independence and zero-mean assumptions, the variance of the product becomes</p> <p>$$\\mathrm{Var}[w_{ij} x_j] = \\mathrm{Var}[w_{ij}] \\, \\mathrm{Var}[x_j] = \\sigma^2 \\gamma^2.$$</p> <p>Since the output is a sum of $n_{\\mathrm{in}}$ such independent terms, and the variance of independent variables adds, the total variance becomes $n_{\\mathrm{in}} \\sigma^2 \\gamma^2.$ In other words, each input contributes some spread to the output, and adding more inputs increases the total spread linearly. This is why the variance of the output grows proportionally to the number of input neurons.</p>"},{"location":"notebooks/04_regul_optim/#local-minima-and-saddle-points","title":"Local Minima and Saddle Points\u00b6","text":"<p>When minimizing a loss function with gradient descent, we often deal with many local and global minima. A local minimum is a point where the loss is smaller than in its immediate neighborhood, but it is not necessarily the smallest value over the entire function. When optimization reaches such a region, progress may slow because gradients become small near the bottom of the valley. In practice, noise or a slightly larger learning rate can sometimes help the parameters escape shallow local minima and continue searching for better solutions.</p> <p>It is also possible to reach a point where the gradient becomes zero (or very close to zero), yet the point is not a local or global minimum. Such points are called saddle points. A classic example is the two-dimensional function $f(x,y)=x^2-y^2$. The surface resembles a horse saddle with a saddle point at $(0,0)$: the function curves upward in one direction and downward in another. Because neural network loss functions live in very high-dimensional spaces, saddle-type critical points and flat directions tend to appear more often than true local minima.</p>"},{"location":"notebooks/04_regul_optim/#learning-rate","title":"Learning Rate\u00b6","text":"<p>We have seen from previous experiments how learning rate can affect the training process. When the learning rate is low, parameters take smaller steps towards the minimum, and training becomes slow. When the learning rate is high, it can overshoot the local minima, oscillate around them, or may even never converge.</p>"},{"location":"notebooks/04_regul_optim/#stochastic-gradient-descent","title":"Stochastic Gradient Descent\u00b6","text":"<p>A slightly modified version of the gradient descent, the minibatch Stochastic Gradient Descent (SGD) has the ability to avoid local minima, as the variation of gradients in the minibatches bring noise and affect the parameter update. In SGD, instead of averaging the gradients of all data (batch) samples, we randomly choose only a single sample to approximate the average gradient. In case of minibatch SGD, we choose a small data subset (minibatch) instead of a single data point (SGD) or all data points (BGD) and average the minibatch gradients to approximate the average of full batch gradients.</p> <p>We saw how different values of the learning rate can positively or negatively influence optimization. At the same time, SGD itself introduces randomness because each minibatch produces a slightly different gradient estimate. This variability slightly changes the update direction at every step and can help the optimization continue moving instead of getting stuck in local minima, even when the learning rate remains fixed.</p> <p>Note</p> <p>   In modern neural networks the main difficulty is often not true local minima but saddle points. Because the gradient magnitude in saddle points becomes very small, deterministic batch gradient descent can slow down training significantly. The small variations introduced by minibatch SGD change the gradient estimate from step to step, which helps the parameters continue moving.   </p>"},{"location":"notebooks/04_regul_optim/#momentum","title":"Momentum\u00b6","text":"<p>We have observed that gradient descent can be sensitive to the choice of learning rate and may oscillate when gradients change direction. One way to stabilize the updates is to encourage the optimization process to keep moving in consistent directions. It can be achieved with momentum, which adds a velocity term with acceleration to parameter updates.</p> <p>Tip</p> <p>   Momentum behaves similarly to how a ball would roll down a hill. It builds speed as we move in the direction of the gradient, helping the training converge faster and more smoothly. This explanation, however, is oversimplistic. If you have a good mathematical background, you can find a nice explanation in this distill article on momentum.   </p> <p>Recall that gradient descent for parameter $\\theta$ had the following formula at step $t$ where $\\nabla_{\\theta} L(\\theta_t)$ is gradient of loss and $\\eta $ is learning rate:</p> <p>$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} L(\\theta_t)$$</p> <p>Momentum introduces a velocity term $v$ that accumulates gradients from previous steps. It is scaled by a momentum hyperparameter $\\beta$, where $\\beta \\in (0,1)$. Unlike the learning rate hyperparameter, $\\beta$ is rarely tuned and is commonly set to $0.9$. Instead of following only the gradient of the current step, the update becomes a weighted average of recent gradients. As a result, parameters continue moving in consistent directions and oscillations are reduced.</p> <p>$$ v_t = \\beta v_{t-1} +\\nabla_{\\theta} L(\\theta_t) \\\\ \\theta_{t+1} = \\theta_t - \\eta v_t $$</p> <p>You will usually see a slightly modified version of the velocity, which scales the current gradient by $(1-\\beta)$:</p> <p>$$ v_t = \\beta v_{t-1} + (1 - \\beta)\\nabla_{\\theta} L(\\theta_t)$$</p> <p>You can imagine $\\beta$ value to be an averaging factor. As the gradients accumulate, we may want to give less weight to the current gradient, which $1-\\beta$ achieves. When $\\beta=0$, momentum disappears and the update reduces to standard gradient descent. When $\\beta$ becomes very close to $1$, the optimizer keeps a very long memory of past gradients, which makes updates smoother but slower to respond to new information. For this reason, typical values are $0.9$, $0.99$, or sometimes $0.999$, but never exactly $1$.</p> <p>Note</p> <p>   The value $\\beta = 0.9$ is widely used in practice because it keeps a useful memory of recent gradients without making the optimizer too slow to react. With this value, the moving average roughly reflects information from the last $\\frac{1}{1-\\beta}$ updates. Larger values (e.g. $0.99$) make the updates very smooth but slower to adapt, while smaller values reduce the benefit of momentum.   </p>"},{"location":"notebooks/04_regul_optim/#rmsprop","title":"RMSProp\u00b6","text":"<p>We can further optimize learning with the RMSProp algorithm. RMSProp adjusts the parameter step size (learning rate) based on the historical magnitude of the parameter gradients. For that, the learning rate is divided by a running average of squared gradients. This helps to avoid large updates in regions where gradients are large, while allowing for bigger steps in flatter regions of the function. Squaring and then desquaring (finding the square root) is a common technique for dealing with negative values (similar to finding MSE or standard deviation).</p> <p>Note</p> <p>     RMSProp has a common abbreviation with MSE: the algorithm's full title is Root Mean Squared Propagation.    </p> <p>The state $s$ in the formula below (which should seem familiar) holds the running average of squared gradients, which is then used to scale the learning rate for each parameter.</p> <p>$$s_t = \\gamma s_{t-1} + (1 - \\gamma) \\nabla_{\\theta} L(\\theta_t)^2$$</p> <p>The updated parameter $\\theta$ is then divided by the square root of this state vector. To avoid division by zero and achieve numerical stability $\u03f5$ (a very small number close to zero) is added. The term $\\sqrt{s_t}$ represents the root of the running average of squared gradients, normalizing the gradient for each parameter.</p> <p>$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{s_t} + \\epsilon} \\nabla_{\\theta} L(\\theta_t)$$</p> <p>RMSprop often performs well when dealing with noisy or sparse gradients, and the parameter $\\gamma$ typically ranges between $0.9$ and $0.99$.</p> <p>Note</p> <p>     Note that RMSProp updates each parameter individually. Its formula may appear complicated, but the idea is simple: each parameter update is scaled using the recent magnitudes of its gradients. If a parameter has consistently large gradients, its update becomes smaller, if the gradients are small, the update remains larger.   </p> <p>In the example below, we optimize a single parameter so that the behavior of RMSProp can be visualized easily. Many deep learning ideas are first illustrated in low dimensions for clarity, but the same principle extends to models with many parameters. In practice, RMSProp adjusts the update of each parameter using its recent gradients, so different parameters move with different step sizes.</p>"},{"location":"notebooks/04_regul_optim/#adam","title":"Adam\u00b6","text":"<p>You might have noted certain similarities in momentum and RSMProp formulas. But can't we simply make the best out of both worlds and integrate them into a single gradient descent function? It turns out we can. Adam (Kingma and Ba, 2014) stands for Adaptive Moment Estimation and does exactly that:</p> <p>$$ \\begin{aligned} v_t &amp;= \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla_{\\theta} L(\\theta_t) \\\\ s_t &amp;= \\beta_2 s_{t-1} + (1 - \\beta_2) \\nabla_{\\theta} L(\\theta_t)^2. \\end{aligned} $$</p> <p>We should note a couple of more things. The common values for weights are $\\beta_1=0.9$ and $\\beta_2=0.999$ (why not $\\beta_2=1.0$ we discussed above), as variance estimate $s_t$ is much slower than the momentum. As we initilize it together with velocity to zero ($v=s=0$) we initially get bias towards smaller values. It can be fixed with normalization:</p> <p>$$\\hat{v_t} = \\frac{v_t}{1 - \\beta_1^t}, \\quad \\hat{s_t} = \\frac{s_t}{1 - \\beta_2^t}$$</p> <p>Here $v_t$ and $s_t$ are called moments. Let's see how normalized moments get affected after the first two iterations:</p>  $$ \\beta_1 = 0.9^1 = 0.9 \\quad \\Rightarrow \\quad \\frac{1}{1 - \\beta_1^1} = \\frac{1}{0.1} = 10 \\\\ \\beta_1^2 = 0.9^2 = 0.81 \\quad \\Rightarrow \\quad \\frac{1}{1 - \\beta_1^2} = \\frac{1}{0.19} \\approx 5.26 \\\\ \\beta_2^1 = 0.999^1 = 0.999 \\quad \\Rightarrow \\quad \\frac{1}{1 - \\beta_2^1} = \\frac{1}{0.001} = 1000 \\\\ \\beta_2^2 = 0.999^2 = 0.998001 \\quad \\Rightarrow \\quad \\frac{1}{1 - \\beta_2^2} = \\frac{1}{0.001999} \\approx 500 $$  <p>Finally, we can integrate the accumulated values into our gradient descent formula to get the final step size:</p> <p>$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{s_t}} + \\epsilon} \\hat{v_t}$$</p>"},{"location":"notebooks/04_regul_optim/#training-efficient-model","title":"Training Efficient Model\u00b6","text":""},{"location":"notebooks/04_regul_optim/#hyperparameter-tuning","title":"Hyperparameter Tuning\u00b6","text":"<p>So far we have defined a model architecture and trained it using a specific set of choices: learning rate, optimizer, batch size, dropout rate, and so on. We should already know that these choices are called hyperparameters. Unlike the model weights, hyperparameters are not learned during training \u2014 they are selected by the practitioner. Typical hyperparameters in deep learning include:</p> <ul> <li>learning rate</li> <li>optimizer (SGD, Adam, RMSProp)</li> <li>batch size</li> <li>number of epochs</li> <li>dropout probability</li> <li>weight decay</li> <li>network width or depth</li> <li>etc.</li> </ul> <p>Different values can lead to very different training behavior. Some settings make optimization unstable, while others allow the model to converge quickly and generalize well. Because we usually do not know the best configuration in advance, we experiment with multiple candidates. A practical strategy is to perform a parameter sweep. Instead of trying arbitrary values, we define a small set of reasonable candidates and train the model with each of them to observe how sensitive the optimization is to parameters.</p> <p>Note</p> <p>     This approach is sometimes called grid search when we try all combinations of several hyperparameters. In practice we often start with a simple sweep over one parameter (usually the learning rate), then refine the search around the most promising region.   </p> <p>A simple workflow used in many experiments is:</p> <ol> <li>Start with common default values.</li> <li>Sweep the learning rate.</li> <li>Adjust regularization (dropout or weight decay).</li> <li>Increase model capacity if the model underfits.</li> <li>Compare validation or test accuracy.</li> </ol> <p>This iterative process gradually reveals which configuration works best for the dataset. Below we illustrate a small learning-rate sweep for our CIFAR-10 model.</p>"},{"location":"notebooks/05_batchnorm_resnet/","title":"06. Batch Normalization and Residual Blocks","text":"<p>Increasing the number of layers in neural networks for learning more advanced functions is challenging due to issues like vanishing gradients. VGGNet partially addressed this problem by using repetitive blocks that stack multiple convolutional layers before downsampling with max-pooling. For instance, two consecutive <code>3x3</code> convolutional layers achieve the same receptive field as a single <code>5x5</code> convolution, while preserving a higher spatial resolution for the next layer. In simpler terms, repeating a smaller kernel allows the network to access the same input pixels while retaining more detail for subsequent processing. Larger kernels blur (downsample) the image more aggressively, which can lead to the loss of important details and force the network to reduce resolution earlier in the architecture and stop.</p> <p>Despite this breakthrough, VGGNet was still limited and showed diminishing returns beyond <code>19</code> layers (hence, <code>vgg19</code> architecture). Another architecture was introduced the same year with the paper titled Going Deeper with Convolutions. It was named Inception because of the internet meme from the infamous Inception movie. I am not joking. If you don't believe me, scroll down the paper for references section and check out the very first reference.</p> <p>Inception architecture, and its implementation, <code>GoogLeNet</code> model (a play on words: 1) was developed by Google researchers, and 2) pays homage to the LeNet architecture), significantly reduced parameter count and leveraged the advantages of the <code>1x1</code> convolution kernel (see the Network in Network paper which also introduced <code>Global Average Pooling (GAP)</code> layer). Despite enabling deeper networks with far fewer parameters, Inception did not fully resolve the core training and convergence problems faced by very deep models.</p> <p>Batch Normalization and Residual Networks emerged as two major solutions for efficiently training neural networks as deep as <code>100</code> layers and more. We will now set up the data environment and go on discussing the core ideas and implementations of both papers.</p> In\u00a0[1]: Copied! <pre>import requests\nimport random\nimport string\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n</pre> import requests import random import string import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import TensorDataset, DataLoader In\u00a0[2]: Copied! <pre>########## DATA SETUP ##########\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\nresponse = requests.get(url)\nwords = response.text.splitlines()\nrandom.shuffle(words)\n\nchars = list(string.ascii_lowercase)\nstoi = {ch: i for i, ch in enumerate(chars)}\nstoi['&lt;START&gt;'] = len(stoi)\nstoi['&lt;END&gt;'] = len(stoi)\nitos = {i: ch for ch, i in stoi.items()}\n\nBLOCK_SIZE = 3\nVOCAB_SIZE = len(stoi)\nEMBED_SIZE = 10\nLAYER_SIZE = 100\n\nlen(words), BLOCK_SIZE, VOCAB_SIZE, words[0]\n</pre> ########## DATA SETUP ##########  url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\" response = requests.get(url) words = response.text.splitlines() random.shuffle(words)  chars = list(string.ascii_lowercase) stoi = {ch: i for i, ch in enumerate(chars)} stoi[''] = len(stoi) stoi[''] = len(stoi) itos = {i: ch for ch, i in stoi.items()}  BLOCK_SIZE = 3 VOCAB_SIZE = len(stoi) EMBED_SIZE = 10 LAYER_SIZE = 100  len(words), BLOCK_SIZE, VOCAB_SIZE, words[0] Out[2]: <pre>(32033, 3, 28, 'wafi')</pre> <p>A quick sidenote: it is encouraged to split the data into training, validation (also called dev), and test sets. When the dataset is not large, an <code>80/10/10</code> split is a reasonable ratio for allocation. For larger datasets (e.g. with one million images), it is fine to allocate <code>90%</code> or more of your data for training. The training set is used to update the model's parameters. The validation set is used for tuning hyperparameters (e.g. testing different learning rates, regularization strengths, etc.). The test split should ideally be used only once to report the final performance of the selected model (e.g. for inclusion in a research paper).</p> In\u00a0[3]: Copied! <pre>########## DATA PREP ##########\n\ndef get_ngrams(start=0, end=None):\n  X, Y = [], []\n  for word in words[start:end]:\n    context = ['&lt;START&gt;'] * BLOCK_SIZE\n    for ch in list(word) + ['&lt;END&gt;']:\n      X.append([stoi[c] for c in context])\n      Y.append(stoi[ch])\n      context = context[1:] + [ch]\n  return torch.tensor(X), torch.tensor(Y)\n\ndef split_data(p=80):\n  train_end = int(p/100 * len(words))\n  remaining = len(words) - train_end\n  val_end = train_end + remaining // 2\n\n  X_train, Y_train = get_ngrams(end=train_end)\n  X_val, Y_val = get_ngrams(start=train_end, end=val_end)\n  X_test, Y_test = get_ngrams(start=val_end, end=len(words))\n\n  return {\n    'train': (X_train, Y_train),\n    'val':   (X_val, Y_val),\n    'test':  (X_test, Y_test),\n  }\n\ndata = split_data()\n\nX_train, Y_train = data['train']\nX_val, Y_val = data['val']\nX_test, Y_test = data['test']\n\nlen(X_train), len(X_val), len(X_test)\n</pre> ########## DATA PREP ##########  def get_ngrams(start=0, end=None):   X, Y = [], []   for word in words[start:end]:     context = [''] * BLOCK_SIZE     for ch in list(word) + ['']:       X.append([stoi[c] for c in context])       Y.append(stoi[ch])       context = context[1:] + [ch]   return torch.tensor(X), torch.tensor(Y)  def split_data(p=80):   train_end = int(p/100 * len(words))   remaining = len(words) - train_end   val_end = train_end + remaining // 2    X_train, Y_train = get_ngrams(end=train_end)   X_val, Y_val = get_ngrams(start=train_end, end=val_end)   X_test, Y_test = get_ngrams(start=val_end, end=len(words))    return {     'train': (X_train, Y_train),     'val':   (X_val, Y_val),     'test':  (X_test, Y_test),   }  data = split_data()  X_train, Y_train = data['train'] X_val, Y_val = data['val'] X_test, Y_test = data['test']  len(X_train), len(X_val), len(X_test) Out[3]: <pre>(182535, 22720, 22891)</pre> <p>Batch normalization normalizes the inputs within a mini-batch before passing them to the next layer. That is, for each input feature $x_i$, we subtract the batch mean and divide by the batch standard deviation. A small constant  $\\epsilon$ is commonly added for maintaining numerical stability (to avoid zero division):</p> <p>$$ \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} $$</p> <p>This standardization gives $\\hat{x}_i$ a mean close to 0 and a standard deviation close to 1 over the batch. This may limit the model's capacity if left unchanged. Therefore, we introduce learnable parameters $\\gamma$ (scale) and $\\beta$ (shift) for flexibility:</p> <p>$$ BN = \\gamma \\hat{x}_i + \\beta $$</p> <p>Batch normalization is typically applied after the affine transformation ($Wx + b$) and before the non-linearity (e.g., ReLU):</p> <p>$$ act = \\phi(\\textrm{BN}(Wx)) $$</p> <p>Pay attention that we omitted $b$ when using batch normalization. In practice, the bias $b$ becomes redundant, because the shifting role is already handled by $\\beta$. Recall that <code>PyTorch</code> has <code>bias=False</code> option  as well (e.g. in <code>nn.Conv2d()</code>).</p> <p>Batch normalization improves convergence in optimization and has regularization effect. The original paper by Ioffe and Szegedyattributes this to reducing internal covariate shift \u2014 i.e. the shift in the distribution of layer inputs during training as parameters in earlier layers change. But this intuition is challenged. You can read more about that in d2l book chapter dedicated to batch normalization.</p> In\u00a0[4]: Copied! <pre>########## PARAMETER SETUP ##########\n\ndef get_params(batch_norm=True):\n  C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)\n\n  W1 = torch.randn((BLOCK_SIZE * EMBED_SIZE, LAYER_SIZE), requires_grad = True)\n  b1 = torch.zeros(LAYER_SIZE, requires_grad=True)\n\n  W2 = torch.randn((LAYER_SIZE, VOCAB_SIZE), requires_grad = True)\n  b2 = torch.zeros(VOCAB_SIZE, requires_grad=True)\n\n  params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n\n  if batch_norm:\n    gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)\n    beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)\n    params['gamma'] = gamma\n    params['beta'] = beta\n    # we can add additional code for omitting b1 in case of using beta (BN bias)\n\n  return params\n</pre> ########## PARAMETER SETUP ##########  def get_params(batch_norm=True):   C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)    W1 = torch.randn((BLOCK_SIZE * EMBED_SIZE, LAYER_SIZE), requires_grad = True)   b1 = torch.zeros(LAYER_SIZE, requires_grad=True)    W2 = torch.randn((LAYER_SIZE, VOCAB_SIZE), requires_grad = True)   b2 = torch.zeros(VOCAB_SIZE, requires_grad=True)    params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}    if batch_norm:     gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)     beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)     params['gamma'] = gamma     params['beta'] = beta     # we can add additional code for omitting b1 in case of using beta (BN bias)    return params In\u00a0[5]: Copied! <pre>########## FORWARD PASS ##########\n\n@torch.no_grad() # applies \"with torch.no_grad()\" to the whole function\ndef get_bn_stats(X_train, params):\n  emb = params['C'][X_train]\n  out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']\n  mean, std = out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5\n  return mean, std\n\ndef forward(X, params, batch_norm=False, bn_stats=None):\n  emb = params['C'][X]\n  out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']\n\n  if batch_norm:\n    mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)\n    out = (out - mean) / std\n    out = params['gamma'] * out + params['beta']\n\n  act = torch.tanh(out)\n  logits = act @ params['W2'] + params['b2']\n  return logits\n</pre> ########## FORWARD PASS ##########  @torch.no_grad() # applies \"with torch.no_grad()\" to the whole function def get_bn_stats(X_train, params):   emb = params['C'][X_train]   out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']   mean, std = out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5   return mean, std  def forward(X, params, batch_norm=False, bn_stats=None):   emb = params['C'][X]   out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']    if batch_norm:     mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)     out = (out - mean) / std     out = params['gamma'] * out + params['beta']    act = torch.tanh(out)   logits = act @ params['W2'] + params['b2']   return logits In\u00a0[6]: Copied! <pre>########## TRAINING &amp; INFERENCE ##########\n\ndef train(X, Y, params, num_epochs=100, lr=0.1, batch_size=None, batch_norm=False):\n  for epoch in range(1, num_epochs+1):\n    if batch_size:\n      idx = torch.randint(0, X.size(0), (batch_size,))\n      batch_X, batch_Y = X[idx], Y[idx]\n    else:\n      batch_X, batch_Y = X, Y\n\n    logits = forward(batch_X, params, batch_norm)\n    loss = F.cross_entropy(logits, batch_Y)\n\n    for p in params.values():\n      if p.grad is not None:\n        p.grad.zero_()\n    loss.backward()\n\n    with torch.no_grad():\n      for p in params.values():\n        p.data -= lr * p.grad\n\n    if epoch % (1000 if batch_size else 10) == 0:\n      print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n@torch.no_grad()\ndef evaluate(X, Y, params, batch_norm=False, bn_stats=None):\n  logits = forward(X, params, batch_norm, bn_stats)\n  loss = F.cross_entropy(logits, Y)\n  print(f\"Loss: {loss.item():.4f}\")\n</pre> ########## TRAINING &amp; INFERENCE ##########  def train(X, Y, params, num_epochs=100, lr=0.1, batch_size=None, batch_norm=False):   for epoch in range(1, num_epochs+1):     if batch_size:       idx = torch.randint(0, X.size(0), (batch_size,))       batch_X, batch_Y = X[idx], Y[idx]     else:       batch_X, batch_Y = X, Y      logits = forward(batch_X, params, batch_norm)     loss = F.cross_entropy(logits, batch_Y)      for p in params.values():       if p.grad is not None:         p.grad.zero_()     loss.backward()      with torch.no_grad():       for p in params.values():         p.data -= lr * p.grad      if epoch % (1000 if batch_size else 10) == 0:       print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")  @torch.no_grad() def evaluate(X, Y, params, batch_norm=False, bn_stats=None):   logits = forward(X, params, batch_norm, bn_stats)   loss = F.cross_entropy(logits, Y)   print(f\"Loss: {loss.item():.4f}\") In\u00a0[7]: Copied! <pre>########## TEST ##########\n\nepochs = 10_000\nlr = 0.01\nbatch_size = 32\nbatch_norm = True\ninit = True\n</pre> ########## TEST ##########  epochs = 10_000 lr = 0.01 batch_size = 32 batch_norm = True init = True In\u00a0[8]: Copied! <pre>params = get_params(batch_norm=batch_norm)\n\n# xavier for tanh, kaiming for relu\nif init:\n  nn.init.xavier_uniform_(params['W1'])\n  nn.init.xavier_uniform_(params['W2'])\n</pre> params = get_params(batch_norm=batch_norm)  # xavier for tanh, kaiming for relu if init:   nn.init.xavier_uniform_(params['W1'])   nn.init.xavier_uniform_(params['W2']) In\u00a0[9]: Copied! <pre>train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm)\n</pre> train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm) <pre>Epoch 1000, Loss: 2.7862\nEpoch 2000, Loss: 2.5897\nEpoch 3000, Loss: 2.3340\nEpoch 4000, Loss: 2.4985\nEpoch 5000, Loss: 2.7501\nEpoch 6000, Loss: 2.2715\nEpoch 7000, Loss: 2.6008\nEpoch 8000, Loss: 2.0696\nEpoch 9000, Loss: 2.3910\nEpoch 10000, Loss: 2.4777\n</pre> In\u00a0[10]: Copied! <pre>bn_stats = get_bn_stats(X_train, params) if batch_norm else None\n\nprint('Train and Validation losses:')\nevaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats)\nevaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats)\n</pre> bn_stats = get_bn_stats(X_train, params) if batch_norm else None  print('Train and Validation losses:') evaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats) evaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats) <pre>Train and Validation losses:\nLoss: 2.3337\nLoss: 2.3345\n</pre> In\u00a0[11]: Copied! <pre>########## SAMPLING ##########\n\n# minor changes to what we had previously for adapting to new code\ndef sample(params, n=10, batch_norm=False, bn_stats=None):\n  names = []\n  for _ in range(n):\n    context = ['&lt;START&gt;'] * BLOCK_SIZE\n    name = ''\n    while True:\n      X = torch.tensor([[stoi[c] for c in context]])\n      logits = forward(X, params, batch_norm, bn_stats)\n      probs = F.softmax(logits, dim=1)\n      id = torch.multinomial(probs, num_samples=1).item()\n      char = itos[id]\n      if char == '&lt;END&gt;':\n        break\n      name += char\n      context = context[1:] + [char]\n    names.append(name)\n  return names\n</pre> ########## SAMPLING ##########  # minor changes to what we had previously for adapting to new code def sample(params, n=10, batch_norm=False, bn_stats=None):   names = []   for _ in range(n):     context = [''] * BLOCK_SIZE     name = ''     while True:       X = torch.tensor([[stoi[c] for c in context]])       logits = forward(X, params, batch_norm, bn_stats)       probs = F.softmax(logits, dim=1)       id = torch.multinomial(probs, num_samples=1).item()       char = itos[id]       if char == '':         break       name += char       context = context[1:] + [char]     names.append(name)   return names In\u00a0[12]: Copied! <pre>sample(params, batch_norm=batch_norm, bn_stats=bn_stats)\n</pre> sample(params, batch_norm=batch_norm, bn_stats=bn_stats) Out[12]: <pre>['khuc',\n 'boka',\n 'lyq',\n 'rasyanrith',\n 'onna',\n 'helia',\n 'brhaylanio',\n 'boleiklak',\n 'ekbnqron',\n 'aren']</pre> <p>Hence, the idea of the residual connection is very simple. Before the second activation function, we add the previous input to the affine transformation. You can imagine the simplified code as below:</p> In\u00a0[13]: Copied! <pre>def residual_block(X):\n  act = torch.relu(X @ params['W1'] + params['b1'])\n  out = act @ params['W2'] + params['b2']\n  return torch.relu(out + X)\n</pre> def residual_block(X):   act = torch.relu(X @ params['W1'] + params['b1'])   out = act @ params['W2'] + params['b2']   return torch.relu(out + X) <p>However, If we attempt to directly run the code above, we will see a shape mismatch, as our final layer returns a matrix of dimension <code>VOCAB_SIZE</code> which is not equal to the input dimension <code>BLOCK_SIZE * EMBED_SIZE</code>.</p> <p>Exercise: Modifying the <code>forward</code> function by adding a residual connection.</p> In\u00a0[14]: Copied! <pre>def forward(X, params, batch_norm=False, bn_stats=None, residual=True):\n  emb = params['C'][X]\n  out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']\n\n  if batch_norm:\n    mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)\n    out = (out - mean) / std\n    out = params['gamma'] * out + params['beta']\n\n  act = torch.tanh(out + emb) if residual else torch.tanh(out)\n  logits = act @ params['W2'] + params['b2']\n  return logits\n</pre> def forward(X, params, batch_norm=False, bn_stats=None, residual=True):   emb = params['C'][X]   out = emb.view(emb.shape[0], -1) @ params['W1'] + params['b1']    if batch_norm:     mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)     out = (out - mean) / std     out = params['gamma'] * out + params['beta']    act = torch.tanh(out + emb) if residual else torch.tanh(out)   logits = act @ params['W2'] + params['b2']   return logits In\u00a0[15]: Copied! <pre>X = params['C'][X_train].view(X_train.shape[0], -1)\nX.shape\n</pre> X = params['C'][X_train].view(X_train.shape[0], -1) X.shape Out[15]: <pre>torch.Size([182535, 30])</pre> <p>What to do? For demonstration purposes we will have to add another layer.</p> <p>Exercise (Advanced): Train a three layer model with batch normalization and residual connections.</p> In\u00a0[16]: Copied! <pre>def get_params(batch_norm=True):\n  C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)\n\n  in_features = BLOCK_SIZE * EMBED_SIZE\n\n  W1 = torch.randn((in_features, LAYER_SIZE), requires_grad = True)\n  b1 = torch.zeros(LAYER_SIZE, requires_grad=True)\n\n  W2 = torch.randn((LAYER_SIZE, in_features), requires_grad = True)\n  b2 = torch.zeros(in_features, requires_grad=True)\n\n  W3 = torch.randn((in_features, VOCAB_SIZE), requires_grad = True)\n  b3 = torch.zeros(VOCAB_SIZE, requires_grad=True)\n\n  params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}\n\n  if batch_norm:\n    gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)\n    beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)\n    params['gamma'] = gamma\n    params['beta'] = beta\n\n  return params\n</pre> def get_params(batch_norm=True):   C = torch.randn((VOCAB_SIZE, EMBED_SIZE), requires_grad=True)    in_features = BLOCK_SIZE * EMBED_SIZE    W1 = torch.randn((in_features, LAYER_SIZE), requires_grad = True)   b1 = torch.zeros(LAYER_SIZE, requires_grad=True)    W2 = torch.randn((LAYER_SIZE, in_features), requires_grad = True)   b2 = torch.zeros(in_features, requires_grad=True)    W3 = torch.randn((in_features, VOCAB_SIZE), requires_grad = True)   b3 = torch.zeros(VOCAB_SIZE, requires_grad=True)    params = {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}    if batch_norm:     gamma = torch.ones((1, LAYER_SIZE), requires_grad=True)     beta = torch.zeros((1, LAYER_SIZE), requires_grad=True)     params['gamma'] = gamma     params['beta'] = beta    return params In\u00a0[17]: Copied! <pre>def forward(X, params, batch_norm=False, bn_stats=None, residual=True):\n  emb = params['C'][X].view(X.shape[0], -1)\n  out = emb @ params['W1'] + params['b1']\n\n  if batch_norm:\n    mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)\n    out = (out - mean) / std\n    out = params['gamma'] * out + params['beta']\n\n  act = torch.relu(out)\n  out2 = act @ params['W2'] + params['b2']\n\n  if residual:\n    out2 = out2 + emb\n\n  logits = torch.tanh(out2) @ params['W3'] + params['b3']\n  return logits\n</pre> def forward(X, params, batch_norm=False, bn_stats=None, residual=True):   emb = params['C'][X].view(X.shape[0], -1)   out = emb @ params['W1'] + params['b1']    if batch_norm:     mean, std = bn_stats if bn_stats else (out.mean(0, keepdim=True), out.std(0, keepdim=True) + 1e-5)     out = (out - mean) / std     out = params['gamma'] * out + params['beta']    act = torch.relu(out)   out2 = act @ params['W2'] + params['b2']    if residual:     out2 = out2 + emb    logits = torch.tanh(out2) @ params['W3'] + params['b3']   return logits In\u00a0[18]: Copied! <pre>params = get_params()\nparams.keys()\n</pre> params = get_params() params.keys() Out[18]: <pre>dict_keys(['C', 'W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'gamma', 'beta'])</pre> In\u00a0[19]: Copied! <pre># we are using relu in intermediate layer\nif init:\n  nn.init.kaiming_uniform_(params['W1'])\n  nn.init.kaiming_uniform_(params['W2']);\n</pre> # we are using relu in intermediate layer if init:   nn.init.kaiming_uniform_(params['W1'])   nn.init.kaiming_uniform_(params['W2']); In\u00a0[20]: Copied! <pre>train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm)\n</pre> train(X_train, Y_train, params, num_epochs=epochs, lr=lr, batch_size=batch_size, batch_norm=batch_norm) <pre>Epoch 1000, Loss: 2.5227\nEpoch 2000, Loss: 2.9970\nEpoch 3000, Loss: 2.5845\nEpoch 4000, Loss: 2.3321\nEpoch 5000, Loss: 2.2630\nEpoch 6000, Loss: 2.5062\nEpoch 7000, Loss: 2.8853\nEpoch 8000, Loss: 2.3080\nEpoch 9000, Loss: 2.7023\nEpoch 10000, Loss: 2.8854\n</pre> In\u00a0[21]: Copied! <pre>bn_stats = get_bn_stats(X_train, params) if batch_norm else None\n\nprint('Train and Validation losses:')\nevaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats)\nevaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats)\n</pre> bn_stats = get_bn_stats(X_train, params) if batch_norm else None  print('Train and Validation losses:') evaluate(X_train, Y_train, params, batch_norm=batch_norm, bn_stats=bn_stats) evaluate(X_val, Y_val, params, batch_norm=batch_norm, bn_stats=bn_stats) <pre>Train and Validation losses:\nLoss: 2.3690\nLoss: 2.3698\n</pre> In\u00a0[22]: Copied! <pre>class ResidualBlock(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.emb = nn.Embedding(VOCAB_SIZE, EMBED_SIZE)\n    self.fc1 = nn.Linear(in_features=EMBED_SIZE, out_features=LAYER_SIZE, bias=False)\n    self.fc2 = nn.Linear(in_features=LAYER_SIZE, out_features=EMBED_SIZE, bias=False)\n    self.fc3 = nn.Linear(in_features=EMBED_SIZE, out_features=VOCAB_SIZE, bias=True)\n    self.bn1 = nn.LazyBatchNorm1d()\n    self.bn2 = nn.LazyBatchNorm1d()\n    nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n\n  # nn.LazyBatchNorm1d in 3D input expects shape (batch, channels, length) = (B, C, T)\n  # it normalizes across the batch and time (token, block) dimensions for each channel, independently\n  # we need to move that dimension to the middle (axis 1) with transpose(1, 2)\n  def forward(self, X):\n    emb = self.emb(X)                     # (BATCH_SIZE, BLOCK_SIZE, EMBED_SIZE)\n    out = self.fc1(emb).transpose(1, 2)   # (BATCH_SIZE, LAYER_SIZE, BLOCK_SIZE) for BatchNorm1d\n    out = self.bn1(out).transpose(1, 2)   # back to our dimensions\n    act = F.relu(out)\n    out = self.fc2(act).transpose(1, 2)\n    out = self.bn2(out).transpose(1, 2)\n    out += emb                            # shortcut connection\n    logits = self.fc3(out)                # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)\n    return logits\n</pre> class ResidualBlock(nn.Module):   def __init__(self):     super().__init__()     self.emb = nn.Embedding(VOCAB_SIZE, EMBED_SIZE)     self.fc1 = nn.Linear(in_features=EMBED_SIZE, out_features=LAYER_SIZE, bias=False)     self.fc2 = nn.Linear(in_features=LAYER_SIZE, out_features=EMBED_SIZE, bias=False)     self.fc3 = nn.Linear(in_features=EMBED_SIZE, out_features=VOCAB_SIZE, bias=True)     self.bn1 = nn.LazyBatchNorm1d()     self.bn2 = nn.LazyBatchNorm1d()     nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')    # nn.LazyBatchNorm1d in 3D input expects shape (batch, channels, length) = (B, C, T)   # it normalizes across the batch and time (token, block) dimensions for each channel, independently   # we need to move that dimension to the middle (axis 1) with transpose(1, 2)   def forward(self, X):     emb = self.emb(X)                     # (BATCH_SIZE, BLOCK_SIZE, EMBED_SIZE)     out = self.fc1(emb).transpose(1, 2)   # (BATCH_SIZE, LAYER_SIZE, BLOCK_SIZE) for BatchNorm1d     out = self.bn1(out).transpose(1, 2)   # back to our dimensions     act = F.relu(out)     out = self.fc2(act).transpose(1, 2)     out = self.bn2(out).transpose(1, 2)     out += emb                            # shortcut connection     logits = self.fc3(out)                # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)     return logits In\u00a0[23]: Copied! <pre>model = ResidualBlock()\ncel = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel.train()\n</pre> model = ResidualBlock() cel = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.01) model.train() Out[23]: <pre>ResidualBlock(\n  (emb): Embedding(28, 10)\n  (fc1): Linear(in_features=10, out_features=100, bias=False)\n  (fc2): Linear(in_features=100, out_features=10, bias=False)\n  (fc3): Linear(in_features=10, out_features=28, bias=True)\n  (bn1): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bn2): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)</pre> In\u00a0[24]: Copied! <pre>num_epochs = 10_000\nbatch_size = 32\n\nfor epoch in range(1, num_epochs+1):\n  model.train()\n  idx = torch.randint(0, X_train.size(0), (batch_size,))\n  batch_X, batch_Y = X_train[idx], Y_train[idx]\n  optimizer.zero_grad()\n  logits = model(batch_X)     # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)\n  logits = logits[:, -1, :]   # (BATCH_SIZE, VOCAB_SIZE)\n  loss = cel(logits, batch_Y)\n  loss.backward()\n  optimizer.step()\n  if epoch % 1000 == 0 or epoch == 1:\n    print(f'Epoch {epoch}, Loss: {loss.item()}')\n</pre> num_epochs = 10_000 batch_size = 32  for epoch in range(1, num_epochs+1):   model.train()   idx = torch.randint(0, X_train.size(0), (batch_size,))   batch_X, batch_Y = X_train[idx], Y_train[idx]   optimizer.zero_grad()   logits = model(batch_X)     # (BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)   logits = logits[:, -1, :]   # (BATCH_SIZE, VOCAB_SIZE)   loss = cel(logits, batch_Y)   loss.backward()   optimizer.step()   if epoch % 1000 == 0 or epoch == 1:     print(f'Epoch {epoch}, Loss: {loss.item()}') <pre>Epoch 1, Loss: 3.6320574283599854\nEpoch 1000, Loss: 2.374105930328369\nEpoch 2000, Loss: 2.6409666538238525\nEpoch 3000, Loss: 2.6358656883239746\nEpoch 4000, Loss: 2.36672043800354\nEpoch 5000, Loss: 2.696502208709717\nEpoch 6000, Loss: 2.4992451667785645\nEpoch 7000, Loss: 2.413964033126831\nEpoch 8000, Loss: 2.83028507232666\nEpoch 9000, Loss: 2.3721745014190674\nEpoch 10000, Loss: 2.6832263469696045\n</pre> In\u00a0[25]: Copied! <pre>model.eval()\nwith torch.no_grad():\n  logits_train = model(X_train)[:, -1, :]\n  logits_val   = model(X_val)[:, -1, :]\n\n  full_loss_train = cel(logits_train, Y_train)\n  full_loss_val   = cel(logits_val, Y_val)\n\n  print(f'Train loss: {full_loss_train.item()}')\n  print(f'Validation loss: {full_loss_val.item()}')\n</pre> model.eval() with torch.no_grad():   logits_train = model(X_train)[:, -1, :]   logits_val   = model(X_val)[:, -1, :]    full_loss_train = cel(logits_train, Y_train)   full_loss_val   = cel(logits_val, Y_val)    print(f'Train loss: {full_loss_train.item()}')   print(f'Validation loss: {full_loss_val.item()}') <pre>Train loss: 2.4901065826416016\nValidation loss: 2.4812421798706055\n</pre> In\u00a0[26]: Copied! <pre># modifying code to suit our needs\ndef sample(model, n=10, block_size=3):\n  model.eval()\n  names = []\n  for _ in range(n):\n    context = ['&lt;START&gt;'] * block_size\n    name = ''\n    while True:\n      idx = [stoi[c] for c in context]\n      X = torch.tensor([idx], dtype=torch.long)\n      with torch.no_grad():\n        logits = model(X)[0, -1] # VOCAB_SIZE\n      probs = F.softmax(logits, dim=0)\n      idx_next = torch.multinomial(probs, num_samples=1).item()\n      char = itos[idx_next]\n      if char == '&lt;END&gt;':\n        break\n      name += char\n      context = context[1:] + [char]\n    names.append(name)\n  return names\n</pre> # modifying code to suit our needs def sample(model, n=10, block_size=3):   model.eval()   names = []   for _ in range(n):     context = [''] * block_size     name = ''     while True:       idx = [stoi[c] for c in context]       X = torch.tensor([idx], dtype=torch.long)       with torch.no_grad():         logits = model(X)[0, -1] # VOCAB_SIZE       probs = F.softmax(logits, dim=0)       idx_next = torch.multinomial(probs, num_samples=1).item()       char = itos[idx_next]       if char == '':         break       name += char       context = context[1:] + [char]     names.append(name)   return names In\u00a0[27]: Copied! <pre>sample(model)\n</pre> sample(model) Out[27]: <pre>['kelifo',\n 'ja',\n 'tha',\n 'elarhncasoria',\n 'ka',\n 'voratte',\n 'eniysh',\n 'th',\n 'kelld',\n 'edm']</pre> In\u00a0[27]: Copied! <pre>\n</pre>"},{"location":"notebooks/05_batchnorm_resnet/#06-batch-normalization-and-residual-blocks","title":"06. Batch Normalization and Residual Blocks\u00b6","text":"23 Mar 2025 \u00b7    <p>Important</p> <p>     The notebook is currently under revision.   </p>"},{"location":"notebooks/05_batchnorm_resnet/#batch-normalization","title":"Batch Normalization\u00b6","text":""},{"location":"notebooks/05_batchnorm_resnet/#running_stats","title":"running_stats\u00b6","text":"<p>In <code>PyTorch</code> we use <code>model.eval()</code> during inference to switch the model into evaluation mode. This is important because layers like dropout and batch normalization behave differently during training and evaluation.</p> <p>During inference, normalization should be done using statistics over the whole dataset instead of mini-batches. Without <code>bn_stats</code> in the code below, the model would normalize using the current batch's mean and standard deviation, leading to inconsistent results depending on the batch.</p> <p>The implemented <code>PyTorch</code> layers like nn.BatchNorm1d automatically calculate running statistics during training. These statistics include a running mean and a running variance for each feature channel, which are stored as non-learnable buffers inside the <code>BatchNorm</code> layer.</p> <p>$$ \\mu_{\\text{running}} = \\alpha \\, \\mu_{\\text{batch}} + (1 - \\alpha) \\, \\mu_{\\text{running}} $$</p> <p>$$ \\sigma^2_{\\text{running}} = \\alpha \\, \\sigma^2_{\\text{batch}} + (1 - \\alpha) \\, \\sigma^2_{\\text{running}} $$</p> <p>In <code>BatchNorm</code>, $\\alpha$ is defined as <code>momentum</code> which is a misnomer and has nothing to do with the momentum we had previously learned for optimization. Its values controls how quickly the <code>running_stats</code> adapt. If momentum is high, the running statistics update quickly based on new batches which can make them unstable and noisy if batches vary a lot. If it is low (by default it is set to <code>0.1</code>, but you may want to reduce it further depending on circumstances), the updates are smoother and slower, averaging batch statistics over time.</p> <p>During evaluation <code>BatchNorm</code> uses the stored running mean and variance for normalization. This ensures deterministic behavior, regardless of the input batch. These buffers are automatically updated and used unless you disable tracking by setting <code>track_running_stats=False</code>.</p> <p>A manual implementation of <code>running_stats</code> is demonstrated in Andrej Karpathy's video as well. In this notebook, we will only implemented the simpler <code>bn_stats</code>.</p>"},{"location":"notebooks/05_batchnorm_resnet/#layer-normalization","title":"Layer Normalization\u00b6","text":"<p>A rule of thumb is that batch sizes between <code>50-100</code> generally work well for batch normalization: the batch is large enough to return reliable statistics but not so large that it causes memory issues or slows down training unnecessarily. Batch size of <code>32</code> is usually the lower bound where batch normalization still provides relatively stable estimates. Batch size of <code>128</code> is also effective if the hardware allows, and can produce even smoother estimates. Beyond that the benefit often diminishes.</p> <p>If the batch size is very small due to memory limitations, batch normalization may lose its effectiveness. In such cases, it's better to consider alternatives like Layer Normalization which do not depend on the batch dimension.</p> <p>Layer normalization normalizes across features for each individual sample, not across the batch and works well for transformers where batch sizes may be small or variable. Basically, batch normalization depends on the batch, but layer normalization does not.</p> <p>Furthermore, in fully connected layers, each feature is just a single number per sample, so batch normalization computes the mean and variance across the batch for each feature. Fully connected layers don't have spatial structure, so there's nothing to average across except the batch. In convolutional layers, each feature channel height and width and is a 2D map (hence, <code>nn.BatchNorm2d</code>), so batch normalization uses not just the batch dimension, but also all the spatial positions to compute statistics. This gives more stable estimates because there are more values per channel.</p>"},{"location":"notebooks/05_batchnorm_resnet/#residual-block","title":"Residual Block\u00b6","text":"<p>Residual Network (ResNet) consists of repeated residual blocks, in the style of the VGGNet architecture. Each residual block consists of a residual (skip/shortcut) connection . We will first see what it does and then will attempt to understand the reasoning behind this simple breakthrough idea.</p>"},{"location":"notebooks/05_batchnorm_resnet/#implementation","title":"Implementation\u00b6","text":"<p>Figure 8.6.2 of Dive into Deep Learning (Chapter 8) by d2l.ai authors and contributors. Licensed under Apache 2.0</p>"},{"location":"notebooks/05_batchnorm_resnet/#reasoning","title":"Reasoning\u00b6","text":"<p>As our model is implementing a single residual block, we don't see any performance improvement. However, similar to batch normalization, the advantages will be obvious in case of 50 layers or more, with repeated residual blocks. But why adding input of the layer to the second affine transformation boosts training?</p> <p>Let's take any deep learning model. The types of functions this model can learn depend on its design (e.g. number of layers, activation functions, etc). All these possible functions we can denote as class $\\mathcal{F}$. If we cannot learn a perfect function for our data, which is usually the case, we can at least try to appoximate this function as closely as possible by minimizing a loss. We may assume that a more powerful model can learn more types of functions and show better performance. But that's not always the case. To achieve a better performance than a simpler model, our model must be capable of learning not only more functions but also all the functions the simpler model can learn. Simply, the possible function class of the more powerful model should be a superclass of the simpler model's function class $\\mathcal{F} \\subseteq \\mathcal{F}'$. If the ${F}'$ isn't an expanded version of {F}$, the new model might actually learn a function that is farther from the truth, and even show worse performance.</p> <p>Refer to the figure above, where our residual output is $f(x) = g(x) + x$. One advantage of residual blocks is their regularization effect. What if some activation nodes in our network are unnecessary and increase complexity or learn bad representations? Instead of learning weights and biases, our residual block can now learn an identity function $f(x) = x$ by simply setting that nodes parameters to zero. As a result, our inputs will propagate faster while ensuring that the learned functions are within the biggest function domain. Residual blocks not only act as a regularizer, but also, unlike, say, dropout which stops input from propagating, allow the network to learn more functions by helping inputs to \"jump over\" (skip) the nodes. And it is very important that the function classes of the model with residual blocks is a superset of the same model without such blocks. Finally, along the way, it deals with the vanishing gradient problem by simply increasing the output of each layer. To sum up, residual connection allows the model to learn more complex functions, while allowing it to easily learn simpler ones, which tackles the vanishing gradient problem and has a regularizing effect.</p>"},{"location":"notebooks/05_batchnorm_resnet/#residual-network-for-nlp-in-pytorch","title":"Residual Network for NLP in PyTorch\u00b6","text":"<p>Originally, the complete Residual Network was developed for image classification tasks, winning ImageNet competition. Each of its residual block consisted of two <code>3x3</code> convolutions (inspired  by VGGNet), both integrating batch normalization, followed by a skip connection. Even though, ResNet model relies on convolutional layer, the concept of residual connections has been adapted for NLP models as well. The infamous Transformer model, introduced in the paper titled Attention is All You Need incorporates residual connections heavily in its design, which is very similar to ResNet.</p>"},{"location":"notebooks/06_nn_ngram/","title":"05. Neural Network N-Gram Model","text":"In\u00a0[1]: Copied! <pre>import requests\n\nurl = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\nresponse = requests.get(url)\ndata = response.text\nwords = data.splitlines()\n</pre> import requests  url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\" response = requests.get(url) data = response.text words = data.splitlines() In\u00a0[2]: Copied! <pre>print('Length:', len(words))\nprint(words[:5] )\n</pre> print('Length:', len(words)) print(words[:5] ) <pre>Length: 32033\n['emma', 'olivia', 'ava', 'isabella', 'sophia']\n</pre> <p>This data provides information about names. For example, by having five examples <code>['emma', 'olivia', 'ava', 'isabella', 'sophia']</code> we may conclude that the probability of <code>'a'</code> being the last letter is <code>1.0</code> after which the word will certainly end, or that the letter <code>'o'</code> is more likely to be at the beginning of the name. Our goal will be to to predict the most probable next character. A common technique is to take track of bigrams of characters (there can be N-grams of words, etc., which have both advantages and disadvantages).</p> In\u00a0[3]: Copied! <pre>def get_bigrams(n):\n  bigrams = {}\n  for w in words[:n]:\n    w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n    for ch1, ch2 in zip(w, w[1:]):\n      b = (ch1, ch2)\n      bigrams[b] = bigrams.get(b, 0) + 1\n  return bigrams\n</pre> def get_bigrams(n):   bigrams = {}   for w in words[:n]:     w = [''] + list(w) + ['']     for ch1, ch2 in zip(w, w[1:]):       b = (ch1, ch2)       bigrams[b] = bigrams.get(b, 0) + 1   return bigrams <p>Change <code>n</code> up to 5 and take note of the bigram counts.</p> In\u00a0[4]: Copied! <pre>get_bigrams(n=2)\n</pre> get_bigrams(n=2) Out[4]: <pre>{('&lt;START&gt;', 'e'): 1,\n ('e', 'm'): 1,\n ('m', 'm'): 1,\n ('m', 'a'): 1,\n ('a', '&lt;END&gt;'): 2,\n ('&lt;START&gt;', 'o'): 1,\n ('o', 'l'): 1,\n ('l', 'i'): 1,\n ('i', 'v'): 1,\n ('v', 'i'): 1,\n ('i', 'a'): 1}</pre> In\u00a0[5]: Copied! <pre>from collections import Counter\n\nbigrams = get_bigrams(len(words))\nCounter(bigrams).most_common()\n</pre> from collections import Counter  bigrams = get_bigrams(len(words)) Counter(bigrams).most_common() Out[5]: <pre>[(('n', '&lt;END&gt;'), 6763),\n (('a', '&lt;END&gt;'), 6640),\n (('a', 'n'), 5438),\n (('&lt;START&gt;', 'a'), 4410),\n (('e', '&lt;END&gt;'), 3983),\n (('a', 'r'), 3264),\n (('e', 'l'), 3248),\n (('r', 'i'), 3033),\n (('n', 'a'), 2977),\n (('&lt;START&gt;', 'k'), 2963),\n (('l', 'e'), 2921),\n (('e', 'n'), 2675),\n (('l', 'a'), 2623),\n (('m', 'a'), 2590),\n (('&lt;START&gt;', 'm'), 2538),\n (('a', 'l'), 2528),\n (('i', '&lt;END&gt;'), 2489),\n (('l', 'i'), 2480),\n (('i', 'a'), 2445),\n (('&lt;START&gt;', 'j'), 2422),\n (('o', 'n'), 2411),\n (('h', '&lt;END&gt;'), 2409),\n (('r', 'a'), 2356),\n (('a', 'h'), 2332),\n (('h', 'a'), 2244),\n (('y', 'a'), 2143),\n (('i', 'n'), 2126),\n (('&lt;START&gt;', 's'), 2055),\n (('a', 'y'), 2050),\n (('y', '&lt;END&gt;'), 2007),\n (('e', 'r'), 1958),\n (('n', 'n'), 1906),\n (('y', 'n'), 1826),\n (('k', 'a'), 1731),\n (('n', 'i'), 1725),\n (('r', 'e'), 1697),\n (('&lt;START&gt;', 'd'), 1690),\n (('i', 'e'), 1653),\n (('a', 'i'), 1650),\n (('&lt;START&gt;', 'r'), 1639),\n (('a', 'm'), 1634),\n (('l', 'y'), 1588),\n (('&lt;START&gt;', 'l'), 1572),\n (('&lt;START&gt;', 'c'), 1542),\n (('&lt;START&gt;', 'e'), 1531),\n (('j', 'a'), 1473),\n (('r', '&lt;END&gt;'), 1377),\n (('n', 'e'), 1359),\n (('l', 'l'), 1345),\n (('i', 'l'), 1345),\n (('i', 's'), 1316),\n (('l', '&lt;END&gt;'), 1314),\n (('&lt;START&gt;', 't'), 1308),\n (('&lt;START&gt;', 'b'), 1306),\n (('d', 'a'), 1303),\n (('s', 'h'), 1285),\n (('d', 'e'), 1283),\n (('e', 'e'), 1271),\n (('m', 'i'), 1256),\n (('s', 'a'), 1201),\n (('s', '&lt;END&gt;'), 1169),\n (('&lt;START&gt;', 'n'), 1146),\n (('a', 's'), 1118),\n (('y', 'l'), 1104),\n (('e', 'y'), 1070),\n (('o', 'r'), 1059),\n (('a', 'd'), 1042),\n (('t', 'a'), 1027),\n (('&lt;START&gt;', 'z'), 929),\n (('v', 'i'), 911),\n (('k', 'e'), 895),\n (('s', 'e'), 884),\n (('&lt;START&gt;', 'h'), 874),\n (('r', 'o'), 869),\n (('e', 's'), 861),\n (('z', 'a'), 860),\n (('o', '&lt;END&gt;'), 855),\n (('i', 'r'), 849),\n (('b', 'r'), 842),\n (('a', 'v'), 834),\n (('m', 'e'), 818),\n (('e', 'i'), 818),\n (('c', 'a'), 815),\n (('i', 'y'), 779),\n (('r', 'y'), 773),\n (('e', 'm'), 769),\n (('s', 't'), 765),\n (('h', 'i'), 729),\n (('t', 'e'), 716),\n (('n', 'd'), 704),\n (('l', 'o'), 692),\n (('a', 'e'), 692),\n (('a', 't'), 687),\n (('s', 'i'), 684),\n (('e', 'a'), 679),\n (('d', 'i'), 674),\n (('h', 'e'), 674),\n (('&lt;START&gt;', 'g'), 669),\n (('t', 'o'), 667),\n (('c', 'h'), 664),\n (('b', 'e'), 655),\n (('t', 'h'), 647),\n (('v', 'a'), 642),\n (('o', 'l'), 619),\n (('&lt;START&gt;', 'i'), 591),\n (('i', 'o'), 588),\n (('e', 't'), 580),\n (('v', 'e'), 568),\n (('a', 'k'), 568),\n (('a', 'a'), 556),\n (('c', 'e'), 551),\n (('a', 'b'), 541),\n (('i', 't'), 541),\n (('&lt;START&gt;', 'y'), 535),\n (('t', 'i'), 532),\n (('s', 'o'), 531),\n (('m', '&lt;END&gt;'), 516),\n (('d', '&lt;END&gt;'), 516),\n (('&lt;START&gt;', 'p'), 515),\n (('i', 'c'), 509),\n (('k', 'i'), 509),\n (('o', 's'), 504),\n (('n', 'o'), 496),\n (('t', '&lt;END&gt;'), 483),\n (('j', 'o'), 479),\n (('u', 's'), 474),\n (('a', 'c'), 470),\n (('n', 'y'), 465),\n (('e', 'v'), 463),\n (('s', 's'), 461),\n (('m', 'o'), 452),\n (('i', 'k'), 445),\n (('n', 't'), 443),\n (('i', 'd'), 440),\n (('j', 'e'), 440),\n (('a', 'z'), 435),\n (('i', 'g'), 428),\n (('i', 'm'), 427),\n (('r', 'r'), 425),\n (('d', 'r'), 424),\n (('&lt;START&gt;', 'f'), 417),\n (('u', 'r'), 414),\n (('r', 'l'), 413),\n (('y', 's'), 401),\n (('&lt;START&gt;', 'o'), 394),\n (('e', 'd'), 384),\n (('a', 'u'), 381),\n (('c', 'o'), 380),\n (('k', 'y'), 379),\n (('d', 'o'), 378),\n (('&lt;START&gt;', 'v'), 376),\n (('t', 't'), 374),\n (('z', 'e'), 373),\n (('z', 'i'), 364),\n (('k', '&lt;END&gt;'), 363),\n (('g', 'h'), 360),\n (('t', 'r'), 352),\n (('k', 'o'), 344),\n (('t', 'y'), 341),\n (('g', 'e'), 334),\n (('g', 'a'), 330),\n (('l', 'u'), 324),\n (('b', 'a'), 321),\n (('d', 'y'), 317),\n (('c', 'k'), 316),\n (('&lt;START&gt;', 'w'), 307),\n (('k', 'h'), 307),\n (('u', 'l'), 301),\n (('y', 'e'), 301),\n (('y', 'r'), 291),\n (('m', 'y'), 287),\n (('h', 'o'), 287),\n (('w', 'a'), 280),\n (('s', 'l'), 279),\n (('n', 's'), 278),\n (('i', 'z'), 277),\n (('u', 'n'), 275),\n (('o', 'u'), 275),\n (('n', 'g'), 273),\n (('y', 'd'), 272),\n (('c', 'i'), 271),\n (('y', 'o'), 271),\n (('i', 'v'), 269),\n (('e', 'o'), 269),\n (('o', 'm'), 261),\n (('r', 'u'), 252),\n (('f', 'a'), 242),\n (('b', 'i'), 217),\n (('s', 'y'), 215),\n (('n', 'c'), 213),\n (('h', 'y'), 213),\n (('p', 'a'), 209),\n (('r', 't'), 208),\n (('q', 'u'), 206),\n (('p', 'h'), 204),\n (('h', 'r'), 204),\n (('j', 'u'), 202),\n (('g', 'r'), 201),\n (('p', 'e'), 197),\n (('n', 'l'), 195),\n (('y', 'i'), 192),\n (('g', 'i'), 190),\n (('o', 'd'), 190),\n (('r', 's'), 190),\n (('r', 'd'), 187),\n (('h', 'l'), 185),\n (('s', 'u'), 185),\n (('a', 'x'), 182),\n (('e', 'z'), 181),\n (('e', 'k'), 178),\n (('o', 'v'), 176),\n (('a', 'j'), 175),\n (('o', 'h'), 171),\n (('u', 'e'), 169),\n (('m', 'm'), 168),\n (('a', 'g'), 168),\n (('h', 'u'), 166),\n (('x', '&lt;END&gt;'), 164),\n (('u', 'a'), 163),\n (('r', 'm'), 162),\n (('a', 'w'), 161),\n (('f', 'i'), 160),\n (('z', '&lt;END&gt;'), 160),\n (('u', '&lt;END&gt;'), 155),\n (('u', 'm'), 154),\n (('e', 'c'), 153),\n (('v', 'o'), 153),\n (('e', 'h'), 152),\n (('p', 'r'), 151),\n (('d', 'd'), 149),\n (('o', 'a'), 149),\n (('w', 'e'), 149),\n (('w', 'i'), 148),\n (('y', 'm'), 148),\n (('z', 'y'), 147),\n (('n', 'z'), 145),\n (('y', 'u'), 141),\n (('r', 'n'), 140),\n (('o', 'b'), 140),\n (('k', 'l'), 139),\n (('m', 'u'), 139),\n (('l', 'd'), 138),\n (('h', 'n'), 138),\n (('u', 'd'), 136),\n (('&lt;START&gt;', 'x'), 134),\n (('t', 'l'), 134),\n (('a', 'f'), 134),\n (('o', 'e'), 132),\n (('e', 'x'), 132),\n (('e', 'g'), 125),\n (('f', 'e'), 123),\n (('z', 'l'), 123),\n (('u', 'i'), 121),\n (('v', 'y'), 121),\n (('e', 'b'), 121),\n (('r', 'h'), 121),\n (('j', 'i'), 119),\n (('o', 't'), 118),\n (('d', 'h'), 118),\n (('h', 'm'), 117),\n (('c', 'l'), 116),\n (('o', 'o'), 115),\n (('y', 'c'), 115),\n (('o', 'w'), 114),\n (('o', 'c'), 114),\n (('f', 'r'), 114),\n (('b', '&lt;END&gt;'), 114),\n (('m', 'b'), 112),\n (('z', 'o'), 110),\n (('i', 'b'), 110),\n (('i', 'u'), 109),\n (('k', 'r'), 109),\n (('g', '&lt;END&gt;'), 108),\n (('y', 'v'), 106),\n (('t', 'z'), 105),\n (('b', 'o'), 105),\n (('c', 'y'), 104),\n (('y', 't'), 104),\n (('u', 'b'), 103),\n (('u', 'c'), 103),\n (('x', 'a'), 103),\n (('b', 'l'), 103),\n (('o', 'y'), 103),\n (('x', 'i'), 102),\n (('i', 'f'), 101),\n (('r', 'c'), 99),\n (('c', '&lt;END&gt;'), 97),\n (('m', 'r'), 97),\n (('n', 'u'), 96),\n (('o', 'p'), 95),\n (('i', 'h'), 95),\n (('k', 's'), 95),\n (('l', 's'), 94),\n (('u', 'k'), 93),\n (('&lt;START&gt;', 'q'), 92),\n (('d', 'u'), 92),\n (('s', 'm'), 90),\n (('r', 'k'), 90),\n (('i', 'x'), 89),\n (('v', '&lt;END&gt;'), 88),\n (('y', 'k'), 86),\n (('u', 'w'), 86),\n (('g', 'u'), 85),\n (('b', 'y'), 83),\n (('e', 'p'), 83),\n (('g', 'o'), 83),\n (('s', 'k'), 82),\n (('u', 't'), 82),\n (('a', 'p'), 82),\n (('e', 'f'), 82),\n (('i', 'i'), 82),\n (('r', 'v'), 80),\n (('f', '&lt;END&gt;'), 80),\n (('t', 'u'), 78),\n (('y', 'z'), 78),\n (('&lt;START&gt;', 'u'), 78),\n (('l', 't'), 77),\n (('r', 'g'), 76),\n (('c', 'r'), 76),\n (('i', 'j'), 76),\n (('w', 'y'), 73),\n (('z', 'u'), 73),\n (('l', 'v'), 72),\n (('h', 't'), 71),\n (('j', '&lt;END&gt;'), 71),\n (('x', 't'), 70),\n (('o', 'i'), 69),\n (('e', 'u'), 69),\n (('o', 'k'), 68),\n (('b', 'd'), 65),\n (('a', 'o'), 63),\n (('p', 'i'), 61),\n (('s', 'c'), 60),\n (('d', 'l'), 60),\n (('l', 'm'), 60),\n (('a', 'q'), 60),\n (('f', 'o'), 60),\n (('p', 'o'), 59),\n (('n', 'k'), 58),\n (('w', 'n'), 58),\n (('u', 'h'), 58),\n (('e', 'j'), 55),\n (('n', 'v'), 55),\n (('s', 'r'), 55),\n (('o', 'z'), 54),\n (('i', 'p'), 53),\n (('l', 'b'), 52),\n (('i', 'q'), 52),\n (('w', '&lt;END&gt;'), 51),\n (('m', 'c'), 51),\n (('s', 'p'), 51),\n (('e', 'w'), 50),\n (('k', 'u'), 50),\n (('v', 'r'), 48),\n (('u', 'g'), 47),\n (('o', 'x'), 45),\n (('u', 'z'), 45),\n (('z', 'z'), 45),\n (('j', 'h'), 45),\n (('b', 'u'), 45),\n (('o', 'g'), 44),\n (('n', 'r'), 44),\n (('f', 'f'), 44),\n (('n', 'j'), 44),\n (('z', 'h'), 43),\n (('c', 'c'), 42),\n (('r', 'b'), 41),\n (('x', 'o'), 41),\n (('b', 'h'), 41),\n (('p', 'p'), 39),\n (('x', 'l'), 39),\n (('h', 'v'), 39),\n (('b', 'b'), 38),\n (('m', 'p'), 38),\n (('x', 'x'), 38),\n (('u', 'v'), 37),\n (('x', 'e'), 36),\n (('w', 'o'), 36),\n (('c', 't'), 35),\n (('z', 'm'), 35),\n (('t', 's'), 35),\n (('m', 's'), 35),\n (('c', 'u'), 35),\n (('o', 'f'), 34),\n (('u', 'x'), 34),\n (('k', 'w'), 34),\n (('p', '&lt;END&gt;'), 33),\n (('g', 'l'), 32),\n (('z', 'r'), 32),\n (('d', 'n'), 31),\n (('g', 't'), 31),\n (('g', 'y'), 31),\n (('h', 's'), 31),\n (('x', 's'), 31),\n (('g', 's'), 30),\n (('x', 'y'), 30),\n (('y', 'g'), 30),\n (('d', 'm'), 30),\n (('d', 's'), 29),\n (('h', 'k'), 29),\n (('y', 'x'), 28),\n (('q', '&lt;END&gt;'), 28),\n (('g', 'n'), 27),\n (('y', 'b'), 27),\n (('g', 'w'), 26),\n (('n', 'h'), 26),\n (('k', 'n'), 26),\n (('g', 'g'), 25),\n (('d', 'g'), 25),\n (('l', 'c'), 25),\n (('r', 'j'), 25),\n (('w', 'u'), 25),\n (('l', 'k'), 24),\n (('m', 'd'), 24),\n (('s', 'w'), 24),\n (('s', 'n'), 24),\n (('h', 'd'), 24),\n (('w', 'h'), 23),\n (('y', 'j'), 23),\n (('y', 'y'), 23),\n (('r', 'z'), 23),\n (('d', 'w'), 23),\n (('w', 'r'), 22),\n (('t', 'n'), 22),\n (('l', 'f'), 22),\n (('y', 'h'), 22),\n (('r', 'w'), 21),\n (('s', 'b'), 21),\n (('m', 'n'), 20),\n (('f', 'l'), 20),\n (('w', 's'), 20),\n (('k', 'k'), 20),\n (('h', 'z'), 20),\n (('g', 'd'), 19),\n (('l', 'h'), 19),\n (('n', 'm'), 19),\n (('x', 'z'), 19),\n (('u', 'f'), 19),\n (('f', 't'), 18),\n (('l', 'r'), 18),\n (('p', 't'), 17),\n (('t', 'c'), 17),\n (('k', 't'), 17),\n (('d', 'v'), 17),\n (('u', 'p'), 16),\n (('p', 'l'), 16),\n (('l', 'w'), 16),\n (('p', 's'), 16),\n (('o', 'j'), 16),\n (('r', 'q'), 16),\n (('y', 'p'), 15),\n (('l', 'p'), 15),\n (('t', 'v'), 15),\n (('r', 'p'), 14),\n (('l', 'n'), 14),\n (('e', 'q'), 14),\n (('f', 'y'), 14),\n (('s', 'v'), 14),\n (('u', 'j'), 14),\n (('v', 'l'), 14),\n (('q', 'a'), 13),\n (('u', 'y'), 13),\n (('q', 'i'), 13),\n (('w', 'l'), 13),\n (('p', 'y'), 12),\n (('y', 'f'), 12),\n (('c', 'q'), 11),\n (('j', 'r'), 11),\n (('n', 'w'), 11),\n (('n', 'f'), 11),\n (('t', 'w'), 11),\n (('m', 'z'), 11),\n (('u', 'o'), 10),\n (('f', 'u'), 10),\n (('l', 'z'), 10),\n (('h', 'w'), 10),\n (('u', 'q'), 10),\n (('j', 'y'), 10),\n (('s', 'z'), 10),\n (('s', 'd'), 9),\n (('j', 'l'), 9),\n (('d', 'j'), 9),\n (('k', 'm'), 9),\n (('r', 'f'), 9),\n (('h', 'j'), 9),\n (('v', 'n'), 8),\n (('n', 'b'), 8),\n (('i', 'w'), 8),\n (('h', 'b'), 8),\n (('b', 's'), 8),\n (('w', 't'), 8),\n (('w', 'd'), 8),\n (('v', 'v'), 7),\n (('v', 'u'), 7),\n (('j', 's'), 7),\n (('m', 'j'), 7),\n (('f', 's'), 6),\n (('l', 'g'), 6),\n (('l', 'j'), 6),\n (('j', 'w'), 6),\n (('n', 'x'), 6),\n (('y', 'q'), 6),\n (('w', 'k'), 6),\n (('g', 'm'), 6),\n (('x', 'u'), 5),\n (('m', 'h'), 5),\n (('m', 'l'), 5),\n (('j', 'm'), 5),\n (('c', 's'), 5),\n (('j', 'v'), 5),\n (('n', 'p'), 5),\n (('d', 'f'), 5),\n (('x', 'd'), 5),\n (('z', 'b'), 4),\n (('f', 'n'), 4),\n (('x', 'c'), 4),\n (('m', 't'), 4),\n (('t', 'm'), 4),\n (('z', 'n'), 4),\n (('z', 't'), 4),\n (('p', 'u'), 4),\n (('c', 'z'), 4),\n (('b', 'n'), 4),\n (('z', 's'), 4),\n (('f', 'w'), 4),\n (('d', 't'), 4),\n (('j', 'd'), 4),\n (('j', 'c'), 4),\n (('y', 'w'), 4),\n (('v', 'k'), 3),\n (('x', 'w'), 3),\n (('t', 'j'), 3),\n (('c', 'j'), 3),\n (('q', 'w'), 3),\n (('g', 'b'), 3),\n (('o', 'q'), 3),\n (('r', 'x'), 3),\n (('d', 'c'), 3),\n (('g', 'j'), 3),\n (('x', 'f'), 3),\n (('z', 'w'), 3),\n (('d', 'k'), 3),\n (('u', 'u'), 3),\n (('m', 'v'), 3),\n (('c', 'x'), 3),\n (('l', 'q'), 3),\n (('p', 'b'), 2),\n (('t', 'g'), 2),\n (('q', 's'), 2),\n (('t', 'x'), 2),\n (('f', 'k'), 2),\n (('b', 't'), 2),\n (('j', 'n'), 2),\n (('k', 'c'), 2),\n (('z', 'k'), 2),\n (('s', 'j'), 2),\n (('s', 'f'), 2),\n (('z', 'j'), 2),\n (('n', 'q'), 2),\n (('f', 'z'), 2),\n (('h', 'g'), 2),\n (('w', 'w'), 2),\n (('k', 'j'), 2),\n (('j', 'k'), 2),\n (('w', 'm'), 2),\n (('z', 'c'), 2),\n (('z', 'v'), 2),\n (('w', 'f'), 2),\n (('q', 'm'), 2),\n (('k', 'z'), 2),\n (('j', 'j'), 2),\n (('z', 'p'), 2),\n (('j', 't'), 2),\n (('k', 'b'), 2),\n (('m', 'w'), 2),\n (('h', 'f'), 2),\n (('c', 'g'), 2),\n (('t', 'f'), 2),\n (('h', 'c'), 2),\n (('q', 'o'), 2),\n (('k', 'd'), 2),\n (('k', 'v'), 2),\n (('s', 'g'), 2),\n (('z', 'd'), 2),\n (('q', 'r'), 1),\n (('d', 'z'), 1),\n (('p', 'j'), 1),\n (('q', 'l'), 1),\n (('p', 'f'), 1),\n (('q', 'e'), 1),\n (('b', 'c'), 1),\n (('c', 'd'), 1),\n (('m', 'f'), 1),\n (('p', 'n'), 1),\n (('w', 'b'), 1),\n (('p', 'c'), 1),\n (('h', 'p'), 1),\n (('f', 'h'), 1),\n (('b', 'j'), 1),\n (('f', 'g'), 1),\n (('z', 'g'), 1),\n (('c', 'p'), 1),\n (('p', 'k'), 1),\n (('p', 'm'), 1),\n (('x', 'n'), 1),\n (('s', 'q'), 1),\n (('k', 'f'), 1),\n (('m', 'k'), 1),\n (('x', 'h'), 1),\n (('g', 'f'), 1),\n (('v', 'b'), 1),\n (('j', 'p'), 1),\n (('g', 'z'), 1),\n (('v', 'd'), 1),\n (('d', 'b'), 1),\n (('v', 'h'), 1),\n (('h', 'h'), 1),\n (('g', 'v'), 1),\n (('d', 'q'), 1),\n (('x', 'b'), 1),\n (('w', 'z'), 1),\n (('h', 'q'), 1),\n (('j', 'b'), 1),\n (('x', 'm'), 1),\n (('w', 'g'), 1),\n (('t', 'b'), 1),\n (('z', 'x'), 1)]</pre> <p>We will store bigrams in a <code>PyTorch</code> tensor instead of a dictionary. Each character will be mapped to an id.</p> In\u00a0[7]: Copied! <pre>import string\n\n# character mapping from string to integer\nchars = list(string.ascii_lowercase)\nstoi = {ch: i for i, ch in enumerate(chars)}\nstoi['&lt;START&gt;'] = 26\nstoi['&lt;END&gt;'] = 27\nstoi\n</pre> import string  # character mapping from string to integer chars = list(string.ascii_lowercase) stoi = {ch: i for i, ch in enumerate(chars)} stoi[''] = 26 stoi[''] = 27 stoi Out[7]: <pre>{'a': 0,\n 'b': 1,\n 'c': 2,\n 'd': 3,\n 'e': 4,\n 'f': 5,\n 'g': 6,\n 'h': 7,\n 'i': 8,\n 'j': 9,\n 'k': 10,\n 'l': 11,\n 'm': 12,\n 'n': 13,\n 'o': 14,\n 'p': 15,\n 'q': 16,\n 'r': 17,\n 's': 18,\n 't': 19,\n 'u': 20,\n 'v': 21,\n 'w': 22,\n 'x': 23,\n 'y': 24,\n 'z': 25,\n '&lt;START&gt;': 26,\n '&lt;END&gt;': 27}</pre> In\u00a0[8]: Copied! <pre>import torch\n\nSIZE = len(stoi)\n\ndef get_bigrams(n):\n  bigrams = torch.zeros((SIZE, SIZE))\n  for w in words[:n]:\n    w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n    for ch1, ch2 in zip(w, w[1:]):\n      bigrams[stoi[ch1], stoi[ch2]] += 1\n  return bigrams\n</pre> import torch  SIZE = len(stoi)  def get_bigrams(n):   bigrams = torch.zeros((SIZE, SIZE))   for w in words[:n]:     w = [''] + list(w) + ['']     for ch1, ch2 in zip(w, w[1:]):       bigrams[stoi[ch1], stoi[ch2]] += 1   return bigrams In\u00a0[9]: Copied! <pre>bigrams = get_bigrams(len(words))\n</pre> bigrams = get_bigrams(len(words)) <p>We can refer to bigrams by indices and slicing. Modify <code>i</code> and <code>j</code> and run the code to get a sense of the table.</p> In\u00a0[10]: Copied! <pre>itos = {ch: i for i, ch in stoi.items()} # reverse stoi\n\ni, j = 0, 1\ncount = bigrams[i, j]\nprint(f'({itos[i]}, {itos[j]}): {count}')\n</pre> itos = {ch: i for i, ch in stoi.items()} # reverse stoi  i, j = 0, 1 count = bigrams[i, j] print(f'({itos[i]}, {itos[j]}): {count}') <pre>(a, b): 541.0\n</pre> <p>Exercise: Find the probability of a character (e.g. <code>b</code>) being the first character (hint: it will follow <code>&lt;START&gt;</code>).</p> In\u00a0[11]: Copied! <pre>counts = bigrams[stoi['&lt;START&gt;']]\nprobs = counts / counts.sum()\nprobs[stoi['b']]\n</pre> counts = bigrams[stoi['']] probs = counts / counts.sum() probs[stoi['b']] Out[11]: <pre>tensor(0.0408)</pre> <p>To generate some output we need to understand <code>torch.multinomial</code>. Let's have a simpler probability distribution for three classes (<code>0</code>, <code>1</code>, <code>2</code>). Our goal is to generate <code>n</code> samples according to the given probabilities. Setting <code>replacement=True</code> means the same class index can be picked multiple times. The higher a class' probability, the more often it is likely to appear in the samples. Rerun the cell below and notice how probabilities are related to the generated samples.</p> In\u00a0[12]: Copied! <pre>p = torch.rand(3)\np /= p.sum()\nsamples = torch.multinomial(p, num_samples=10, replacement=True)\n\nprint(p)\nprint(samples)\n</pre> p = torch.rand(3) p /= p.sum() samples = torch.multinomial(p, num_samples=10, replacement=True)  print(p) print(samples) <pre>tensor([0.5375, 0.1891, 0.2735])\ntensor([0, 0, 0, 2, 0, 1, 0, 0, 1, 0])\n</pre> <p>Once we understand the logic of <code>torch.multinomial</code>, we will randomly pick a next character based on our probability distribution. The higher is the frequency of the bigram, the more likely is that the random sampler will return us that character.</p> In\u00a0[13]: Copied! <pre>next_char = torch.multinomial(probs, num_samples=1, replacement=True)\nnext_char, itos[next_char.item()]\n</pre> next_char = torch.multinomial(probs, num_samples=1, replacement=True) next_char, itos[next_char.item()] Out[13]: <pre>(tensor([17]), 'r')</pre> <p>We will start with bigrams of <code>&lt;START&gt;</code>. Once we randomly generate the next character based on its probability distribution, we will start looking for bigrams starting with that generated character. This process will continue until we our sampling returns <code>&lt;END&gt;</code>.</p> <p>We will work with the probability matrix from now on, instead of the frequency matrix. Below, <code>dim=1</code> ensures that we sum along the row of the matrix, when <code>keepdim=True</code> keeps the extra dimension. Refer to the <code>PyTorch</code> documentation and test out different parameters.</p> In\u00a0[14]: Copied! <pre>probs = bigrams/bigrams.sum(dim=1, keepdim=True)\nprobs.shape\n</pre> probs = bigrams/bigrams.sum(dim=1, keepdim=True) probs.shape Out[14]: <pre>torch.Size([28, 28])</pre> In\u00a0[15]: Copied! <pre>def sample_names(n=10):\n  names = ''\n  for i in range(n):\n    id = stoi['&lt;START&gt;']\n    while id != stoi['&lt;END&gt;']:\n      p = probs[id]\n      next_char = torch.multinomial(p, 1, replacement=True)\n      id = next_char.item()\n      names += itos[id]\n  return names.replace(\"&lt;END&gt;\", \"\\n\")\n</pre> def sample_names(n=10):   names = ''   for i in range(n):     id = stoi['']     while id != stoi['']:       p = probs[id]       next_char = torch.multinomial(p, 1, replacement=True)       id = next_char.item()       names += itos[id]   return names.replace(\"\", \"\\n\") In\u00a0[21]: Copied! <pre>print(sample_names())\n</pre> print(sample_names()) <pre>tarios\ncaly\nkelaherthrwa\ndyaronn\nzenel\nbrid\nc\nsh\nve\nminica\n\n</pre> <p>We can evaluate our model and determine the loss function with likelihood. Note that our prediction probabilities are generated by simply counting bigram frequencies.</p> In\u00a0[22]: Copied! <pre>n = 1\nfor w in words[:n]:\n  w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    print(f'{ch1, ch2}: {p.item():.4f}')\n</pre> n = 1 for w in words[:n]:   w = [''] + list(w) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     print(f'{ch1, ch2}: {p.item():.4f}') <pre>('&lt;START&gt;', 'e'): 0.0478\n('e', 'm'): 0.0377\n('m', 'm'): 0.0253\n('m', 'a'): 0.3899\n('a', '&lt;END&gt;'): 0.1960\n</pre> <p>The logic of bigram model is that the probability of rare characters coming together in names (e.g. <code>xy</code>) will be much smaller than the more common cases (e.g. <code>na</code>). A better training corpus captures more realistic character transitions and assigns higher probabilities to frequently seen patterns.</p> <p>Since the model assumes that each character depends only on the previous one (Markov assumption), the joint probability of a sequence is the product of all conditional probabilities:</p> <p>$P(c_1, c_2, \\ldots, c_n) = P(c_1) \\cdot P(c_2 \\mid c_1) \\cdot P(c_3 \\mid c_2) \\cdots P(c_n \\mid c_{n-1})$</p> <p>Likelihood estimates this quality for our model by multiplying all prediction probabilities. Higher is the joint probability, the better is model's prediction quality. However, direct multiplication may have the following issue:</p> In\u00a0[24]: Copied! <pre>n = 5\nfor word in words[:n]:\n  likelihood = 1.0\n  w = ['&lt;START&gt;'] + list(word) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    likelihood *= p\n  print(f'Model predicts {word} is {likelihood:.9f} likely')\n</pre> n = 5 for word in words[:n]:   likelihood = 1.0   w = [''] + list(word) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     likelihood *= p   print(f'Model predicts {word} is {likelihood:.9f} likely') <pre>Model predicts emma is 0.000003478 likely\nModel predicts olivia is 0.000000025 likely\nModel predicts ava is 0.000165674 likely\nModel predicts isabella is 0.000000000 likely\nModel predicts sophia is 0.000000026 likely\n</pre> <p>Question: How to fix the issue above?</p> <p>As can be seen, the result of chained multiplication is a very small number (somewhat resembling vanishing gradient problem). To resolve this issue, individual probabilities between <code>0</code> and <code>1</code> are mapped to a <code>log</code> function domain (-$\\infty$, 0]. Logarithm function is monotonic (preserves order): maximum probability is mapped to <code>0</code>, smaller probabilities are mapped to bigger negative values.</p> In\u00a0[25]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\np = np.linspace(0.001, 1, 200)\nlog_p = np.log(p)\n\nplt.figure(figsize=(6, 4))\nplt.plot(p, log_p)\nplt.title(\"Natural Log Function\")\nplt.grid(True)\n</pre> import numpy as np import matplotlib.pyplot as plt  p = np.linspace(0.001, 1, 200) log_p = np.log(p)  plt.figure(figsize=(6, 4)) plt.plot(p, log_p) plt.title(\"Natural Log Function\") plt.grid(True) In\u00a0[26]: Copied! <pre>n = 1\nfor w in words[:n]:\n  w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    log_p = torch.log(p)\n    print(f'{ch1, ch2}: {p.item():.4f} | {log_p.item():.4f}')\n</pre> n = 1 for w in words[:n]:   w = [''] + list(w) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     log_p = torch.log(p)     print(f'{ch1, ch2}: {p.item():.4f} | {log_p.item():.4f}') <pre>('&lt;START&gt;', 'e'): 0.0478 | -3.0408\n('e', 'm'): 0.0377 | -3.2793\n('m', 'm'): 0.0253 | -3.6772\n('m', 'a'): 0.3899 | -0.9418\n('a', '&lt;END&gt;'): 0.1960 | -1.6299\n</pre> <p>Log-likelihood also has an advantage of making calculations and hence optimization (calculation of gradients) faster due to the product rule:</p> <p>$\\log P(c_1, c_2, \\ldots, c_n) = \\log P(c_1) + \\log P(c_2 \\mid c_1) + \\log P(c_3 \\mid c_2) + \\cdots + \\log P(c_n \\mid c_{n-1})$</p> In\u00a0[27]: Copied! <pre>n = 5\nfor word in words[:n]:\n  log_likelihood = 0.0\n  w = ['&lt;START&gt;'] + list(word) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    log_p = torch.log(p)\n    log_likelihood += log_p\n  print(f'Model predicts {word} is {log_likelihood} likely')\n</pre> n = 5 for word in words[:n]:   log_likelihood = 0.0   w = [''] + list(word) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     log_p = torch.log(p)     log_likelihood += log_p   print(f'Model predicts {word} is {log_likelihood} likely') <pre>Model predicts emma is -12.568990707397461 likely\nModel predicts olivia is -17.511159896850586 likely\nModel predicts ava is -8.705486297607422 likely\nModel predicts isabella is -21.5141544342041 likely\nModel predicts sophia is -17.468196868896484 likely\n</pre> <p>As optimization algorithms usually strive for minimizing the loss, it makes sense to invert the negative values of the log-likelihood to be positive. We can generate a single loss value by averaging negative log-likelihoods across all the samples.</p> In\u00a0[30]: Copied! <pre>log_likelihood = 0.0\nfor word in words:\n  w = ['&lt;START&gt;'] + list(word) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    log_p = torch.log(p)\n    log_likelihood += log_p\nloss = -log_likelihood / len(words)\nprint(f'Loss: {loss}')\n</pre> log_likelihood = 0.0 for word in words:   w = [''] + list(word) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     log_p = torch.log(p)     log_likelihood += log_p loss = -log_likelihood / len(words) print(f'Loss: {loss}') <pre>Loss: 17.478591918945312\n</pre> <p>As we know, logarithmic function is undefined at <code>0</code>, which we need to take into consideration. Consider the case when a character combination has never occured in our training data.</p> In\u00a0[31]: Copied! <pre>for word in ['jq']:\n  log_likelihood = 0.0\n  w = ['&lt;START&gt;'] + list(word) + ['&lt;END&gt;']\n  for ch1, ch2 in zip(w, w[1:]):\n    p = probs[stoi[ch1], stoi[ch2]]\n    log_p = torch.log(p)\n    log_likelihood += log_p\n  print(f'Model predicts {word} is {log_likelihood} likely')\n</pre> for word in ['jq']:   log_likelihood = 0.0   w = [''] + list(word) + ['']   for ch1, ch2 in zip(w, w[1:]):     p = probs[stoi[ch1], stoi[ch2]]     log_p = torch.log(p)     log_likelihood += log_p   print(f'Model predicts {word} is {log_likelihood} likely') <pre>Model predicts jq is -inf likely\n</pre> <p>In case any character sequence in string will return infinite likelihood, it will lead to infinite loss as well, which is undesirable.</p> <p>Question: How to avoid infinite loss?</p> <p>Model-smoothing is a simple technique, which aims to assign a minimal non-zero probability to cases leading to infinite likelihood. Run the next cell and replicate the experiments above to see the outcome.</p> In\u00a0[32]: Copied! <pre>bigrams = bigrams + 1  # model smoothing avoids zero probabilities\nprobs = bigrams/bigrams.sum(dim=1, keepdim=True)\n</pre> bigrams = bigrams + 1  # model smoothing avoids zero probabilities probs = bigrams/bigrams.sum(dim=1, keepdim=True) <p>Our frequency-based bigram model didn't perform well due its simplicity. We will now build a neural network-based bigram model with the aim of increasing individual bigram prediction probabilities (recall that likelihood was calculated by multiplying conditional probabilities). Instead of counting bigrams in our training set, we will learn parameters leading to reduced loss. We will now rewrite our <code>get_bigrams()</code> function to suit the training of neural network model, where the label of each character will be the next character.</p> In\u00a0[33]: Copied! <pre>def get_bigrams(n):\n  X, Y = [], []\n  for w in words[:n]:\n    w = ['&lt;START&gt;'] + list(w) + ['&lt;END&gt;']\n    for ch1, ch2 in zip(w, w[1:]):\n      X.append(stoi[ch1])\n      Y.append(stoi[ch2])\n  return torch.tensor(X), torch.tensor(Y)\n</pre> def get_bigrams(n):   X, Y = [], []   for w in words[:n]:     w = [''] + list(w) + ['']     for ch1, ch2 in zip(w, w[1:]):       X.append(stoi[ch1])       Y.append(stoi[ch2])   return torch.tensor(X), torch.tensor(Y) In\u00a0[34]: Copied! <pre>X, Y = get_bigrams(1)\nX, Y\n</pre> X, Y = get_bigrams(1) X, Y Out[34]: <pre>(tensor([26,  4, 12, 12,  0]), tensor([ 4, 12, 12,  0, 27]))</pre> In\u00a0[35]: Copied! <pre>[itos[x.item()] for x in X], [itos[y.item()] for y in Y]\n</pre> [itos[x.item()] for x in X], [itos[y.item()] for y in Y] Out[35]: <pre>(['&lt;START&gt;', 'e', 'm', 'm', 'a'], ['e', 'm', 'm', 'a', '&lt;END&gt;'])</pre> <p>We will one-hot encode our data with the <code>torch.nn.functional</code> module function, in order to not inject unnecessary numerical pattern to our data.</p> In\u00a0[38]: Copied! <pre>import torch.nn.functional as F\n\nX_train = F.one_hot(X, num_classes=SIZE).float()\ny_train = F.one_hot(Y, num_classes=SIZE).float()\n</pre> import torch.nn.functional as F  X_train = F.one_hot(X, num_classes=SIZE).float() y_train = F.one_hot(Y, num_classes=SIZE).float() In\u00a0[39]: Copied! <pre>plt.imshow(X_train);\n</pre> plt.imshow(X_train); <p>We will now generate weights for each character and find their linear transformation.</p> In\u00a0[41]: Copied! <pre>W = torch.randn((SIZE, 1))\nX_train @ W\n</pre> W = torch.randn((SIZE, 1)) X_train @ W Out[41]: <pre>tensor([[ 0.5013],\n        [-0.8258],\n        [ 1.0634],\n        [ 1.0634],\n        [ 0.9002]])</pre> <p>For each input character, our goal is to predict not a single probability, but probabilities for all possible output characters. Hence, we will update our weight matrix to correspond to both input and output. We will set <code>requires_grad=True</code> for future gradient calculation.</p> In\u00a0[42]: Copied! <pre>W = torch.randn((SIZE, SIZE), requires_grad=True)\n(X_train @ W).shape\n</pre> W = torch.randn((SIZE, SIZE), requires_grad=True) (X_train @ W).shape Out[42]: <pre>torch.Size([5, 28])</pre> <p>Note that <code>torch.randn()</code> function is generating weight values corresponding to Guassian distribution. Our goal is to map these values to all be positive, so that we can interpret the ouput as probabilities later on. The idea is similar to <code>log()</code> function previously. Here, the output values lower than zero will be mapped to be below <code>1</code> approaching <code>0</code>, when positive values will grow towards infinity.</p> In\u00a0[43]: Copied! <pre>x = np.linspace(-4, 4, 200)\nexp_x = np.exp(x)\n\nplt.figure(figsize=(6, 4))\nplt.plot(x, exp_x)\nplt.title(\"Exponentiation Function\")\nplt.grid(True)\n</pre> x = np.linspace(-4, 4, 200) exp_x = np.exp(x)  plt.figure(figsize=(6, 4)) plt.plot(x, exp_x) plt.title(\"Exponentiation Function\") plt.grid(True) <p>We call the raw scores we obtain after a linear transformation logits (log-counts). These values are real numbers, not constrained to be positive or normalized. To interpret them as prediction probabilities, we apply the exponential function to map them to the positive domain. The output of exponentiation can be interpreted as unnormalized character frequencies or relative likelihoods. When we normalize these exponentials (i.e. divide by their total sum), we obtain values that sum to <code>1.0</code>, forming a proper probability distribution. This is exactly what we want. Note that all these functions are differentiable and applying exponentiation and normalization is exactly what softmax function does:</p> <p>$\\text{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)}$</p> In\u00a0[44]: Copied! <pre>logits = X_train @ W\n\n# softmax\ncounts = torch.exp(logits)\nprobs = counts / counts.sum(dim=1, keepdim=True)\n\nprobs[0], probs[0].sum()\n</pre> logits = X_train @ W  # softmax counts = torch.exp(logits) probs = counts / counts.sum(dim=1, keepdim=True)  probs[0], probs[0].sum() Out[44]: <pre>(tensor([0.0471, 0.0249, 0.0381, 0.0411, 0.0384, 0.0678, 0.0478, 0.0210, 0.0155,\n         0.0191, 0.0920, 0.0500, 0.0138, 0.0090, 0.0042, 0.0120, 0.1256, 0.0500,\n         0.0855, 0.0199, 0.0043, 0.0082, 0.0215, 0.0744, 0.0232, 0.0054, 0.0234,\n         0.0168], grad_fn=&lt;SelectBackward0&gt;),\n tensor(1.0000, grad_fn=&lt;SumBackward0&gt;))</pre> <p>Exercise: Calculate the average negative log-likelihood (loss) of the model.</p> In\u00a0[45]: Copied! <pre>pred_probs = probs[torch.arange(len(Y)), Y]\nloss = -pred_probs.log().mean()\nloss\n</pre> pred_probs = probs[torch.arange(len(Y)), Y] loss = -pred_probs.log().mean() loss Out[45]: <pre>tensor(3.5038, grad_fn=&lt;NegBackward0&gt;)</pre> <p>We know how to call backward pass and optimize our model from previous lectures. We will combine all the steps, train our model on the whole dataset, and backpropagate through our network.</p> In\u00a0[46]: Copied! <pre>X, Y = get_bigrams(len(words))\n\nX_train = F.one_hot(X, num_classes=SIZE).float()\ny_train = F.one_hot(Y, num_classes=SIZE).float()\n\nW = torch.randn((SIZE, SIZE), requires_grad=True)\n</pre> X, Y = get_bigrams(len(words))  X_train = F.one_hot(X, num_classes=SIZE).float() y_train = F.one_hot(Y, num_classes=SIZE).float()  W = torch.randn((SIZE, SIZE), requires_grad=True) In\u00a0[47]: Copied! <pre>num_epochs = 100\nlearning_rate = 1\nlambda_ = 0.01\n\nfor epoch in range(num_epochs):\n  # forward pass\n  logits = X_train @ W\n  counts = torch.exp(logits)\n  probs = counts / counts.sum(dim=1, keepdim=True)\n  pred_probs = probs[torch.arange(len(Y)), Y]\n\n  l2 = (W**2).sum() # regularization\n  loss = -pred_probs.log().mean() + lambda_ * l2.sum()\n\n  # backward pass\n  W.grad = None\n  loss.backward()\n\n  # optimization\n  W.data -= learning_rate * W.grad\n\n  if (epoch + 1) % 10 == 0:\n    print(f'{epoch+1}/{num_epochs}, loss: {loss}')\n</pre> num_epochs = 100 learning_rate = 1 lambda_ = 0.01  for epoch in range(num_epochs):   # forward pass   logits = X_train @ W   counts = torch.exp(logits)   probs = counts / counts.sum(dim=1, keepdim=True)   pred_probs = probs[torch.arange(len(Y)), Y]    l2 = (W**2).sum() # regularization   loss = -pred_probs.log().mean() + lambda_ * l2.sum()    # backward pass   W.grad = None   loss.backward()    # optimization   W.data -= learning_rate * W.grad    if (epoch + 1) % 10 == 0:     print(f'{epoch+1}/{num_epochs}, loss: {loss}') <pre>10/100, loss: 9.035313606262207\n20/100, loss: 7.005525588989258\n30/100, loss: 5.685214042663574\n40/100, loss: 4.8257904052734375\n50/100, loss: 4.2659807205200195\n60/100, loss: 3.901088237762451\n70/100, loss: 3.6630942821502686\n80/100, loss: 3.5077712535858154\n90/100, loss: 3.406341552734375\n100/100, loss: 3.3400678634643555\n</pre> <p>Follows the implementation of Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Janvin: A neural probabilistic language model, based on Andrej Karpathy's building makemore: part 2.</p> <p>We will modify the <code>get_bigrams()</code> function to include custom <code>block_size</code>, which simply implies <code>N-1</code> of <code>N-gram</code> (bigram has the block size of <code>1</code>).</p> In\u00a0[48]: Copied! <pre>def get_ngrams(end, start=0, block_size=3):\n  X, Y = [], []\n  for w in words[start:end]:\n    context = ['&lt;START&gt;'] * block_size\n    for ch in list(w) + ['&lt;END&gt;']:\n      X.append([stoi[c] for c in context])\n      Y.append(stoi[ch])\n      context = context[1:] + [ch]\n  return torch.tensor(X), torch.tensor(Y)\n</pre> def get_ngrams(end, start=0, block_size=3):   X, Y = [], []   for w in words[start:end]:     context = [''] * block_size     for ch in list(w) + ['']:       X.append([stoi[c] for c in context])       Y.append(stoi[ch])       context = context[1:] + [ch]   return torch.tensor(X), torch.tensor(Y) In\u00a0[49]: Copied! <pre>X, Y = get_ngrams(1, block_size=3) # try different block sizes\n</pre> X, Y = get_ngrams(1, block_size=3) # try different block sizes In\u00a0[50]: Copied! <pre>for x, y in zip(X, Y):\n  context = [itos[i.item()] for i in x]\n  target = itos[y.item()]\n  print(f\"{context}: {target}\")\n</pre> for x, y in zip(X, Y):   context = [itos[i.item()] for i in x]   target = itos[y.item()]   print(f\"{context}: {target}\") <pre>['&lt;START&gt;', '&lt;START&gt;', '&lt;START&gt;']: e\n['&lt;START&gt;', '&lt;START&gt;', 'e']: m\n['&lt;START&gt;', 'e', 'm']: m\n['e', 'm', 'm']: a\n['m', 'm', 'a']: &lt;END&gt;\n</pre> <p>It is managable to convert <code>28</code> character indices by one hot encoding them to suit a neural network. However, this approach becomes innefficient when the vocabulary size increases. What if we have <code>10,000</code> words and our goal is to predict the next word? We would have to create large vectors with lots of zeros, not only wasting resources, but also losing similarity information among tokens (dot product of any two vectors will always be <code>0</code> due to orthogonality).</p> <p>A different approach is to get some smaller dimensional embedding of an index. Initially, these embeddings (parameters) are random, but over the course of training, model updates them to reflect the actual usage of the words in a context. When vectors of two different characters are learned to be close to each other, then we can conclude that, in our data, these characters were in a similar context. If two characters often show up in the same positions relative to surrounding characters, their embeddings get pulled toward each other. Consider the simple case below:</p> <pre>context1 = ['b', 'a', 'd', 'a']\ncontext2 = ['m', 'a', 'd', 'a']\ntarget = 'm'\n</pre> <p>If the training data consists of these contexts, then, in order to predict <code>m</code> for both contexts, their learned embeddings should be similar in value.</p> <p>For our case, <code>2</code> dimensional embeddings will be enough. For small datasets, having high embedding dimensionality may cause overfitting. When dataset is bigger, increasing dimensions helps to learn more nuanced relationships in the data, albeit at a higher computational cost.</p> In\u00a0[51]: Copied! <pre>X\n</pre> X Out[51]: <pre>tensor([[26, 26, 26],\n        [26, 26,  4],\n        [26,  4, 12],\n        [ 4, 12, 12],\n        [12, 12,  0]])</pre> In\u00a0[52]: Copied! <pre>C = torch.randn((SIZE, 2))\nemb = C[X]\nemb\n</pre> C = torch.randn((SIZE, 2)) emb = C[X] emb Out[52]: <pre>tensor([[[-0.6723,  0.3140],\n         [-0.6723,  0.3140],\n         [-0.6723,  0.3140]],\n\n        [[-0.6723,  0.3140],\n         [-0.6723,  0.3140],\n         [-1.4631, -1.0324]],\n\n        [[-0.6723,  0.3140],\n         [-1.4631, -1.0324],\n         [ 0.6817,  1.5840]],\n\n        [[-1.4631, -1.0324],\n         [ 0.6817,  1.5840],\n         [ 0.6817,  1.5840]],\n\n        [[ 0.6817,  1.5840],\n         [ 0.6817,  1.5840],\n         [-0.8068,  0.7403]]])</pre> <p>Now we will initilaize weights and biases by considering correct shape. In order to be able to use matrix multiplication, we will have to flatten embeddings as well.</p> In\u00a0[53]: Copied! <pre>emb.shape\n</pre> emb.shape Out[53]: <pre>torch.Size([5, 3, 2])</pre> <p>Question: What do embedding dimensions correspond to?</p> <p>Exercise: Pass embeddings through a single neuron.</p> In\u00a0[57]: Copied! <pre>layer_size = 100\nin_features = emb.shape[1] * emb.shape[2]\n\nW1 = torch.randn((in_features, layer_size))\nb1 = torch.randn(layer_size)\n\nout = emb.view(-1, in_features) @ W1 + b1\nact = torch.tanh(out)\n\nW1.shape, b1.shape, out.shape\n</pre> layer_size = 100 in_features = emb.shape[1] * emb.shape[2]  W1 = torch.randn((in_features, layer_size)) b1 = torch.randn(layer_size)  out = emb.view(-1, in_features) @ W1 + b1 act = torch.tanh(out)  W1.shape, b1.shape, out.shape Out[57]: <pre>(torch.Size([6, 100]), torch.Size([100]), torch.Size([5, 100]))</pre> In\u00a0[58]: Copied! <pre>emb.view(-1, in_features)\n</pre> emb.view(-1, in_features) Out[58]: <pre>tensor([[-0.6723,  0.3140, -0.6723,  0.3140, -0.6723,  0.3140],\n        [-0.6723,  0.3140, -0.6723,  0.3140, -1.4631, -1.0324],\n        [-0.6723,  0.3140, -1.4631, -1.0324,  0.6817,  1.5840],\n        [-1.4631, -1.0324,  0.6817,  1.5840,  0.6817,  1.5840],\n        [ 0.6817,  1.5840,  0.6817,  1.5840, -0.8068,  0.7403]])</pre> <p>Exercise: Create the next (final) layer.</p> In\u00a0[59]: Copied! <pre>W2 = torch.randn((layer_size, SIZE))\nb2 = torch.randn(SIZE)\n\nlogits = act @ W2 + b2\nlogits.shape\n</pre> W2 = torch.randn((layer_size, SIZE)) b2 = torch.randn(SIZE)  logits = act @ W2 + b2 logits.shape Out[59]: <pre>torch.Size([5, 28])</pre> <p>Exercise: Calculate loss.</p> In\u00a0[60]: Copied! <pre>counts = logits.exp()\nprob = counts / counts.sum(dim=1, keepdim=True)\nprob.shape\n</pre> counts = logits.exp() prob = counts / counts.sum(dim=1, keepdim=True) prob.shape Out[60]: <pre>torch.Size([5, 28])</pre> In\u00a0[65]: Copied! <pre>loss = -prob[torch.arange(prob.shape[0]), Y].log().mean()\nloss\n</pre> loss = -prob[torch.arange(prob.shape[0]), Y].log().mean() loss Out[65]: <pre>tensor(16.2439)</pre> <p>Average negative log-likelihood loss can be calculated with built-in <code>PyTorch</code> functions as well. Cross-entropy is simply <code>Softmax + Negative Log-Likelihood</code>.</p> In\u00a0[66]: Copied! <pre># loss = F.nll_loss(torch.log(prob), Y)\nloss = F.cross_entropy(logits, Y)\nloss\n</pre> # loss = F.nll_loss(torch.log(prob), Y) loss = F.cross_entropy(logits, Y) loss Out[66]: <pre>tensor(16.2439)</pre> <p>Exercise: Let's combine everything together and train on whole data.</p> In\u00a0[96]: Copied! <pre>X, Y = get_ngrams(len(words))\n</pre> X, Y = get_ngrams(len(words)) In\u00a0[97]: Copied! <pre>C = torch.randn((SIZE, 2), requires_grad=True)\nemb = C[X]\n\nlayer_size = 100\nin_features = emb.shape[1] * emb.shape[2]\n\nW1 = torch.randn((in_features, layer_size), requires_grad=True)\nb1 = torch.randn(layer_size, requires_grad=True)\n\nW2 = torch.randn((layer_size, SIZE), requires_grad=True)\nb2 = torch.randn(SIZE, requires_grad=True)\n\nparams = [C, W1, W2, b1, b2]\n</pre> C = torch.randn((SIZE, 2), requires_grad=True) emb = C[X]  layer_size = 100 in_features = emb.shape[1] * emb.shape[2]  W1 = torch.randn((in_features, layer_size), requires_grad=True) b1 = torch.randn(layer_size, requires_grad=True)  W2 = torch.randn((layer_size, SIZE), requires_grad=True) b2 = torch.randn(SIZE, requires_grad=True)  params = [C, W1, W2, b1, b2] In\u00a0[100]: Copied! <pre>def train(X, Y, params, num_epochs=10, learning_rate=0.01):\n  C, W1, W2, b1, b2 = params\n  for epoch in range(num_epochs):\n    # forward pass\n    emb = C[X]\n    in_features = emb.shape[1] * emb.shape[2]\n    out = emb.view(-1, in_features) @ W1 + b1\n    act = torch.tanh(out)\n    logits = act @ W2 + b2\n    loss = F.cross_entropy(logits, Y)\n    print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss}')\n    # backward pass\n    for p in params:\n      p.grad = None\n    loss.backward()\n    # optimization\n    for p in params:\n      p.data -= learning_rate * p.grad\n</pre> def train(X, Y, params, num_epochs=10, learning_rate=0.01):   C, W1, W2, b1, b2 = params   for epoch in range(num_epochs):     # forward pass     emb = C[X]     in_features = emb.shape[1] * emb.shape[2]     out = emb.view(-1, in_features) @ W1 + b1     act = torch.tanh(out)     logits = act @ W2 + b2     loss = F.cross_entropy(logits, Y)     print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss}')     # backward pass     for p in params:       p.grad = None     loss.backward()     # optimization     for p in params:       p.data -= learning_rate * p.grad In\u00a0[101]: Copied! <pre>train(X, Y, params)\n</pre> train(X, Y, params) <pre>Epoch: 1/10, Loss: 14.651238441467285\nEpoch: 2/10, Loss: 14.548416137695312\nEpoch: 3/10, Loss: 14.447701454162598\nEpoch: 4/10, Loss: 14.349052429199219\nEpoch: 5/10, Loss: 14.252448081970215\nEpoch: 6/10, Loss: 14.157876968383789\nEpoch: 7/10, Loss: 14.065322875976562\nEpoch: 8/10, Loss: 13.974783897399902\nEpoch: 9/10, Loss: 13.886248588562012\nEpoch: 10/10, Loss: 13.799703598022461\n</pre> <p>Exercise: Notice how slow it is to train on the whole data. Find out why. Can you also train on mini-batches? See the hint below:</p> In\u00a0[102]: Copied! <pre>batch_size = 4\nidx = torch.randint(0, X.shape[0], (batch_size,))\nidx, X[idx]\n</pre> batch_size = 4 idx = torch.randint(0, X.shape[0], (batch_size,)) idx, X[idx] Out[102]: <pre>(tensor([114762,  40173,  66663,  87063]),\n tensor([[ 7, 11,  4],\n         [18,  4, 24],\n         [ 0, 13,  4],\n         [12,  8, 11]]))</pre> In\u00a0[103]: Copied! <pre>def run(X, Y, params, num_epochs, lr=0.1, batch_size=None):\n  C, W1, W2, b1, b2 = params\n\n  for epoch in range(1, num_epochs+1):\n    if batch_size:\n      idx = torch.randint(0, X.size(0), (batch_size,))\n      batch_X, batch_Y = X[idx], Y[idx]\n    else:\n      batch_X, batch_Y = X, Y\n\n    emb = C[batch_X]\n    in_features = emb.shape[1] * emb.shape[2]\n    out = emb.view(-1, in_features) @ W1 + b1\n    act = torch.tanh(out)\n    logits = act @ W2 + b2\n    loss = F.cross_entropy(logits, batch_Y)\n\n    if epoch % (100 if batch_size else 1) == 0:\n      print(f'Epoch {epoch}, Loss {loss.item()}')\n\n    for p in params:\n      p.grad = None\n    loss.backward()\n\n    with torch.no_grad():\n      for p in params:\n        p.data -= lr * p.grad\n</pre> def run(X, Y, params, num_epochs, lr=0.1, batch_size=None):   C, W1, W2, b1, b2 = params    for epoch in range(1, num_epochs+1):     if batch_size:       idx = torch.randint(0, X.size(0), (batch_size,))       batch_X, batch_Y = X[idx], Y[idx]     else:       batch_X, batch_Y = X, Y      emb = C[batch_X]     in_features = emb.shape[1] * emb.shape[2]     out = emb.view(-1, in_features) @ W1 + b1     act = torch.tanh(out)     logits = act @ W2 + b2     loss = F.cross_entropy(logits, batch_Y)      if epoch % (100 if batch_size else 1) == 0:       print(f'Epoch {epoch}, Loss {loss.item()}')      for p in params:       p.grad = None     loss.backward()      with torch.no_grad():       for p in params:         p.data -= lr * p.grad In\u00a0[104]: Copied! <pre>run(X, Y, params, num_epochs=10, batch_size=None)\n</pre> run(X, Y, params, num_epochs=10, batch_size=None) <pre>Epoch 1, Loss 13.715128898620605\nEpoch 2, Loss 12.94245719909668\nEpoch 3, Loss 12.326409339904785\nEpoch 4, Loss 11.779574394226074\nEpoch 5, Loss 11.269842147827148\nEpoch 6, Loss 10.805395126342773\nEpoch 7, Loss 10.449189186096191\nEpoch 8, Loss 10.27437686920166\nEpoch 9, Loss 9.558894157409668\nEpoch 10, Loss 9.190631866455078\n</pre> In\u00a0[105]: Copied! <pre>run(X, Y, params, num_epochs=1000, batch_size=64)\n</pre> run(X, Y, params, num_epochs=1000, batch_size=64) <pre>Epoch 100, Loss 2.8726632595062256\nEpoch 200, Loss 2.904355525970459\nEpoch 300, Loss 3.114861488342285\nEpoch 400, Loss 2.7828195095062256\nEpoch 500, Loss 2.580034017562866\nEpoch 600, Loss 2.742938995361328\nEpoch 700, Loss 2.582310676574707\nEpoch 800, Loss 2.6041131019592285\nEpoch 900, Loss 2.6011006832122803\nEpoch 1000, Loss 2.5459964275360107\n</pre>"},{"location":"notebooks/06_nn_ngram/#05-neural-network-n-gram-model","title":"05. Neural Network N-Gram Model\u00b6","text":"1 May 2025 \u00b7    <p>Important</p> <p>     The notebook is currently under revision.   </p> <p>Note</p> <p>The notebook is highly based on Andrej Karpathy's makemore See the original video.</p>"},{"location":"notebooks/06_nn_ngram/#bigram-model","title":"Bigram Model\u00b6","text":""},{"location":"notebooks/06_nn_ngram/#average-negative-log-likelihood","title":"Average Negative Log-Likelihood\u00b6","text":""},{"location":"notebooks/06_nn_ngram/#neural-network-bigram-model","title":"Neural Network Bigram Model\u00b6","text":""},{"location":"notebooks/06_nn_ngram/#neural-network-n-gram-model","title":"Neural Network N-gram Model\u00b6","text":""},{"location":"notebooks/06_nn_ngram/#training-on-mini-batches","title":"Training on Mini-Batches\u00b6","text":""},{"location":"notebooks/07_vae/","title":"07. Variational Autoencoders (VAE)","text":"<p>But the latent process of which we speak, is far from being obvious to men\u2019s minds, beset as they now are. For we mean not the measures, symptoms, or degrees of any process which can be exhibited in the bodies themselves, but simply a continued process, which, for the most part, escapes the observation of the senses. ~ Francis Bacon (Novum Organum)</p> <p>https://www.cs.toronto.edu/~rgrosse/courses/csc311_f20/readings/L07%20Probabilistic%20Models.pdf</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n</pre> import torch import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>class Decoder(nn.Module):\n  def __init__(self, latent_dim, out_dim=784):\n    super().__init__()\n    self.fc1 = nn.Linear(latent_dim, 512)\n    self.fc2 = nn.Linear(512, out_dim)\n\n  def forward(self, z):\n    z = z.view(z.size(0), -1)\n    logits = self.fc2(F.relu(self.fc1(z)))\n    return F.sigmoid(logits)\n</pre> class Decoder(nn.Module):   def __init__(self, latent_dim, out_dim=784):     super().__init__()     self.fc1 = nn.Linear(latent_dim, 512)     self.fc2 = nn.Linear(512, out_dim)    def forward(self, z):     z = z.view(z.size(0), -1)     logits = self.fc2(F.relu(self.fc1(z)))     return F.sigmoid(logits) In\u00a0[\u00a0]: Copied! <pre>LATENT_DIM = 20\nBATCH_SIZE = 32\n</pre> LATENT_DIM = 20 BATCH_SIZE = 32 <p>As mentioned above, we need to sample $z$ from a multivariate Gaussian distribution, where $\\mu \\in [-\\infty, \\infty]$ and $\\sigma^2 \\geq 0$. We will use <code>torch.randn()</code> function to simulate random generation. Exponential function <code>torch.exp()</code> will be needed for mapping infinite range to be non-negative for variance.</p> In\u00a0[\u00a0]: Copied! <pre>mean    = torch.randn(BATCH_SIZE, LATENT_DIM)\nlogvar  = torch.randn(BATCH_SIZE, LATENT_DIM)\nvar     = torch.exp(logvar) # var &gt;= 0\n</pre> mean    = torch.randn(BATCH_SIZE, LATENT_DIM) logvar  = torch.randn(BATCH_SIZE, LATENT_DIM) var     = torch.exp(logvar) # var &gt;= 0 <p>In practice, we will learn suitable values for mean and variance. We can now sample $z \\sim \\mathcal{N}(\\mu,\\; \\mathrm{diag}(\\sigma^2))$, noting that <code>torch.normal()</code> function accepts $\\sigma$ instead of $\\sigma^2$.</p> In\u00a0[\u00a0]: Copied! <pre>z = torch.normal(mean, torch.sqrt(var))\ndec = Decoder(LATENT_DIM)\np_x_given_z = dec.forward(z)\n</pre> z = torch.normal(mean, torch.sqrt(var)) dec = Decoder(LATENT_DIM) p_x_given_z = dec.forward(z) <p>Plotting the generated distribution $p_\\theta(x \\mid z)$ above will naturally produce noise, as the decoder parameters are random. We need to figure out a way to train our network for optimal parameters.</p> In\u00a0[\u00a0]: Copied! <pre>plt.imshow(p_x_given_z[0].detach().numpy().reshape(28,28), cmap='gray');\n</pre> plt.imshow(p_x_given_z[0].detach().numpy().reshape(28,28), cmap='gray'); In\u00a0[\u00a0]: Copied! <pre>class Encoder(nn.Module):\n  def __init__(self, latent_dim, out_dim=128):\n    super().__init__()\n    self.conv1   = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1)\n    self.conv2   = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1)\n    self.fc1     = nn.Linear(32 * 5 * 5, out_dim)\n    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n    self.mean    = nn.Linear(out_dim, latent_dim)\n    self.logvar  = nn.Linear(out_dim, latent_dim)\n\n  def forward(self, x):\n    out1 = self.maxpool(F.relu(self.conv1(x)))\n    out2 = self.maxpool(F.relu(self.conv2(out1)))\n    out2 = out2.view(out2.size(0), -1)\n    out3 = F.relu(self.fc1(out2))\n    mean = self.mean(out3)\n    logvar = self.logvar(out3)\n    return mean, logvar\n</pre> class Encoder(nn.Module):   def __init__(self, latent_dim, out_dim=128):     super().__init__()     self.conv1   = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1)     self.conv2   = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1)     self.fc1     = nn.Linear(32 * 5 * 5, out_dim)     self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)     self.mean    = nn.Linear(out_dim, latent_dim)     self.logvar  = nn.Linear(out_dim, latent_dim)    def forward(self, x):     out1 = self.maxpool(F.relu(self.conv1(x)))     out2 = self.maxpool(F.relu(self.conv2(out1)))     out2 = out2.view(out2.size(0), -1)     out3 = F.relu(self.fc1(out2))     mean = self.mean(out3)     logvar = self.logvar(out3)     return mean, logvar <p>Once we have our encoder, we can sample the latent variable $z$ conditioned on the input data. We will now load the MNIST dataset and demonstrate the process for a single batch of data.</p> In\u00a0[\u00a0]: Copied! <pre>DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nNUM_WORKERS = 2\nBATCH_SIZE = 64\nLATENT_DIM = 30\n</pre> DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' NUM_WORKERS = 2 BATCH_SIZE = 64 LATENT_DIM = 30 In\u00a0[\u00a0]: Copied! <pre>train_data   = torchvision.datasets.MNIST('root',train=True,transform=transforms.ToTensor(),download=True)\ntrain_loader = torch.utils.data.DataLoader(train_data,shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n</pre> train_data   = torchvision.datasets.MNIST('root',train=True,transform=transforms.ToTensor(),download=True) train_loader = torch.utils.data.DataLoader(train_data,shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.91M/9.91M [00:00&lt;00:00, 23.0MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.9k/28.9k [00:00&lt;00:00, 614kB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.65M/1.65M [00:00&lt;00:00, 5.67MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.54k/4.54k [00:00&lt;00:00, 8.95MB/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>enc = Encoder(LATENT_DIM).to(DEVICE)\ndec = Decoder(LATENT_DIM).to(DEVICE)\n</pre> enc = Encoder(LATENT_DIM).to(DEVICE) dec = Decoder(LATENT_DIM).to(DEVICE) In\u00a0[\u00a0]: Copied! <pre>imgs, _ = next(iter(train_loader))\nimgs = imgs.to(DEVICE)\n\nmean, logvar = enc.forward(imgs)\nvar = torch.exp(logvar)\n\nq_z_given_x = torch.normal(mean, torch.sqrt(var))\np_x_given_z = dec.forward(q_z_given_x)\n</pre> imgs, _ = next(iter(train_loader)) imgs = imgs.to(DEVICE)  mean, logvar = enc.forward(imgs) var = torch.exp(logvar)  q_z_given_x = torch.normal(mean, torch.sqrt(var)) p_x_given_z = dec.forward(q_z_given_x) <p>Note that the MNIST pixel intensities are continuous values in $[0,1]$, not binary values $0$ and $1$. We can still use a Bernoulli likelihood for each pixel because the Bernoulli log-likelihood is exactly the same expression used in binary cross-entropy. Thus,</p> <p>$$ p_\\theta(x \\mid z) = \\prod_{j=1}^{784} p_j^{\\,x_j} (1 - p_j)^{\\,1 - x_j}, $$</p> <p>and</p> <p>$$ \\log p_\\theta(x \\mid z) = \\sum_{j=1}^{784} x_j \\log p_j + (1 - x_j) \\log (1 - p_j) $$</p> <p>remain valid even when the observed pixel values $x_j$ are real numbers in $[0,1]$. Treating $x_j$ as a fractional target simply corresponds to using binary cross-entropy, which is well-defined for any $x_j \\in [0,1]$.</p> In\u00a0[\u00a0]: Copied! <pre>LL = -F.binary_cross_entropy(imgs.view(imgs.size(0),-1), p_x_given_z, reduction='sum')\nprint(f'Log-likelihood: {LL.item():.4f}')\n</pre> LL = -F.binary_cross_entropy(imgs.view(imgs.size(0),-1), p_x_given_z, reduction='sum') print(f'Log-likelihood: {LL.item():.4f}') <pre>Log-likelihood: -2033668.5000\n</pre> <p>The generated output will naturally again be noise, as neither encoder nor decoder parameters ($\\phi$ and $\\theta$) are optimal. The goal of VAE is to efficiently learn those parameters and maximize the likelihood.</p> In\u00a0[\u00a0]: Copied! <pre>def reparam(mean, var):\n  eps = torch.randn_like(mean)\n  std = torch.sqrt(var)\n  z = mean + std * eps\n  return z\n</pre> def reparam(mean, var):   eps = torch.randn_like(mean)   std = torch.sqrt(var)   z = mean + std * eps   return z In\u00a0[\u00a0]: Copied! <pre>LR = 0.001\nNUM_EPOCHS = 10\n</pre> LR = 0.001 NUM_EPOCHS = 10 In\u00a0[\u00a0]: Copied! <pre>optimizer = torch.optim.Adam(list(enc.parameters()) + list(dec.parameters()), lr=LR)\n</pre> optimizer = torch.optim.Adam(list(enc.parameters()) + list(dec.parameters()), lr=LR) Out[\u00a0]: <pre>Decoder(\n  (fc1): Linear(in_features=30, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=784, bias=True)\n)</pre> In\u00a0[\u00a0]: Copied! <pre>dec.train()\nfor e in range(NUM_EPOCHS):\n  loss = 0\n  for X, _ in train_loader:\n    X = X.to(DEVICE)\n    optimizer.zero_grad()\n\n    mean, logvar = enc.forward(X)\n    var = torch.exp(logvar)\n\n    q_z_given_x = reparam(mean, var)\n    p_x_given_z = dec.forward(q_z_given_x)\n\n    LL = -F.binary_cross_entropy(p_x_given_z, X.view(X.size(0), -1), reduction='sum')\n    KL = 0.5 * torch.sum(mean**2 + var - 1 - logvar)\n    ELBO = LL - KL\n    batch_loss = -ELBO\n\n    batch_loss.backward()\n    optimizer.step()\n    loss += batch_loss.item() / (BATCH_SIZE * 784)\n  print(f\"Epoch {e+1}/{NUM_EPOCHS}, Loss: {loss:.4f}\")\n</pre> dec.train() for e in range(NUM_EPOCHS):   loss = 0   for X, _ in train_loader:     X = X.to(DEVICE)     optimizer.zero_grad()      mean, logvar = enc.forward(X)     var = torch.exp(logvar)      q_z_given_x = reparam(mean, var)     p_x_given_z = dec.forward(q_z_given_x)      LL = -F.binary_cross_entropy(p_x_given_z, X.view(X.size(0), -1), reduction='sum')     KL = 0.5 * torch.sum(mean**2 + var - 1 - logvar)     ELBO = LL - KL     batch_loss = -ELBO      batch_loss.backward()     optimizer.step()     loss += batch_loss.item() / (BATCH_SIZE * 784)   print(f\"Epoch {e+1}/{NUM_EPOCHS}, Loss: {loss:.4f}\") <pre>Epoch 1/10, Loss: 121.4904\nEpoch 2/10, Loss: 121.4055\nEpoch 3/10, Loss: 121.3329\nEpoch 4/10, Loss: 121.2401\nEpoch 5/10, Loss: 121.1607\nEpoch 6/10, Loss: 121.0556\nEpoch 7/10, Loss: 121.0213\nEpoch 8/10, Loss: 120.9725\nEpoch 9/10, Loss: 120.8978\nEpoch 10/10, Loss: 120.7906\n</pre> In\u00a0[\u00a0]: Copied! <pre>dec.eval()\n\nwith torch.no_grad():\n  z = torch.randn(BATCH_SIZE, LATENT_DIM, device=DEVICE)\n  imgs = dec.forward(z)\n\nimgs = imgs.detach().cpu().numpy()\n</pre> dec.eval()  with torch.no_grad():   z = torch.randn(BATCH_SIZE, LATENT_DIM, device=DEVICE)   imgs = dec.forward(z)  imgs = imgs.detach().cpu().numpy() In\u00a0[\u00a0]: Copied! <pre>num_imgs = imgs.shape[0]\nimgs_per_row = 8\nnum_rows = (num_imgs + imgs_per_row - 1) // imgs_per_row\n\nplt.figure(figsize=(12, num_rows * 2))\n\nfor i in range(num_imgs):\n    plt.subplot(num_rows, imgs_per_row, i + 1)\n    plt.imshow(imgs[i].reshape(28, 28), cmap='gray')\n    plt.axis('off')\n\nplt.show()\n</pre> num_imgs = imgs.shape[0] imgs_per_row = 8 num_rows = (num_imgs + imgs_per_row - 1) // imgs_per_row  plt.figure(figsize=(12, num_rows * 2))  for i in range(num_imgs):     plt.subplot(num_rows, imgs_per_row, i + 1)     plt.imshow(imgs[i].reshape(28, 28), cmap='gray')     plt.axis('off')  plt.show() In\u00a0[\u00a0]: Copied! <pre>train_data = torchvision.datasets.CIFAR10('root',train=True,transform=transforms.ToTensor(),download=True)\ntrain_loader = torch.utils.data.DataLoader(train_data,shuffle=True, batch_size=32, num_workers=2)\n</pre> train_data = torchvision.datasets.CIFAR10('root',train=True,transform=transforms.ToTensor(),download=True) train_loader = torch.utils.data.DataLoader(train_data,shuffle=True, batch_size=32, num_workers=2) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 170M/170M [00:03&lt;00:00, 43.6MB/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>batch = iter(train_loader)\nimgs, labels = next(batch)\nplt.imshow(imgs[0].reshape(3, 32, 32).permute(2, 1, 0));\n</pre> batch = iter(train_loader) imgs, labels = next(batch) plt.imshow(imgs[0].reshape(3, 32, 32).permute(2, 1, 0)); In\u00a0[\u00a0]: Copied! <pre>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Encoder(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1)\n    self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1)\n    self.fc1 = nn.Linear(32 * 5 * 5, 128)\n    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n    self.mean = nn.Linear(128, 20)\n    self.log_sigma = nn.Linear(128, 20)\n\n  def forward(self, x):\n    out1 = self.maxpool(F.relu(self.conv1(x)))\n    out2 = self.maxpool(F.relu(self.conv2(out1)))\n    out2 = out2.view(out2.size(0), -1)\n    out3 = self.fc1(out2)\n    mean = self.mean(out3)\n    log_sigma = self.log_sigma(out3)\n    return mean, log_sigma\n\n  def rsample(self, mean, log_sigma):\n    eps = torch.randn_like(mean)\n    std = torch.exp(0.5 * log_sigma)\n    z = mean + std * eps\n    return z\n\nclass Decoder(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.fc1 = nn.Linear(20, 128)\n    self.fc2 = nn.Linear(128, 784)\n\n  def forward(self, x):\n    out1 = F.relu(self.fc1(x))\n    out2 = torch.sigmoid(self.fc2(out1))\n    return out2.view(out2.size(0), 1, 28, 28)\n</pre> import torch.nn as nn import torch.nn.functional as F  class Encoder(nn.Module):   def __init__(self):     super().__init__()     self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1)     self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1)     self.fc1 = nn.Linear(32 * 5 * 5, 128)     self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)     self.mean = nn.Linear(128, 20)     self.log_sigma = nn.Linear(128, 20)    def forward(self, x):     out1 = self.maxpool(F.relu(self.conv1(x)))     out2 = self.maxpool(F.relu(self.conv2(out1)))     out2 = out2.view(out2.size(0), -1)     out3 = self.fc1(out2)     mean = self.mean(out3)     log_sigma = self.log_sigma(out3)     return mean, log_sigma    def rsample(self, mean, log_sigma):     eps = torch.randn_like(mean)     std = torch.exp(0.5 * log_sigma)     z = mean + std * eps     return z  class Decoder(nn.Module):   def __init__(self):     super().__init__()     self.fc1 = nn.Linear(20, 128)     self.fc2 = nn.Linear(128, 784)    def forward(self, x):     out1 = F.relu(self.fc1(x))     out2 = torch.sigmoid(self.fc2(out1))     return out2.view(out2.size(0), 1, 28, 28) In\u00a0[\u00a0]: Copied! <pre>epochs = 50\nfor e in range(epochs):\n  loss = 0.0\n  for img, labels in train_loader:\n    img, labels = img.to(device=device), labels.to(device=device)\n    optimizer.zero_grad()\n    mean, log_sigma = enc.forward(img)\n    z = enc.rsample(mean, log_sigma)\n    out = dec.forward(z)\n    LL = -F.binary_cross_entropy(out, img, reduction='sum')\n    sigma = torch.exp(0.5 * log_sigma)\n    KL = 0.5 * torch.sum(mean*mean + torch.exp(log_sigma) - log_sigma - 1)\n    ELBO = LL - KL\n    batch_loss = -ELBO\n    batch_loss.backward()\n    optimizer.step()\n    loss += batch_loss\n  print(f'Epoch {e} loss is {loss/len(train_loader)}')\n</pre> epochs = 50 for e in range(epochs):   loss = 0.0   for img, labels in train_loader:     img, labels = img.to(device=device), labels.to(device=device)     optimizer.zero_grad()     mean, log_sigma = enc.forward(img)     z = enc.rsample(mean, log_sigma)     out = dec.forward(z)     LL = -F.binary_cross_entropy(out, img, reduction='sum')     sigma = torch.exp(0.5 * log_sigma)     KL = 0.5 * torch.sum(mean*mean + torch.exp(log_sigma) - log_sigma - 1)     ELBO = LL - KL     batch_loss = -ELBO     batch_loss.backward()     optimizer.step()     loss += batch_loss   print(f'Epoch {e} loss is {loss/len(train_loader)}')"},{"location":"notebooks/07_vae/#07-variational-autoencoders-vae","title":"07. Variational Autoencoders (VAE)\u00b6","text":"25 Nov 2025 \u00b7    <p>Important</p> <p>     The notebook is currently under revision.   </p>"},{"location":"notebooks/07_vae/#decoder-network","title":"Decoder Network\u00b6","text":"<p>Let's say our latent variable $z$ is sampled from a multivariate Gaussian distribution: $z \\sim \\mathcal{N}(\\mu,\\; \\mathrm{diag}(\\sigma^2))$. For example, drawing each variable from a normal distribution $\\mathcal{N}(0, 1)$ may look like: $ z = [-1.2,\\; 0.4,\\; 2.1,\\; -0.7,\\; \\dots].$</p> <p>We can pass an $L$-dimensional input $z$ to our neural network (decoder) with parameters $\\theta$. The decoder should produce a conditional distribution $p_\\theta(x \\mid z)$, which tells how likely an output $x$ is for a given latent $z$. By adjusting $\\theta$, we want this conditional distribution to produce samples whose overall distribution approximates the true data distribution $p^\\ast(x)$. In other words, when $z$ is drawn from some latent distribution, the resulting samples $x \\sim p_\\theta(x \\mid z)$ should resemble real observable data. The true distribution $p^\\ast(x)$ is some unknown underlying distribution of universe the data lives in.</p> <p>Let's consider MNIST dataset, where each data point is a <code>28x28</code> grayscale image. The decoder, then, should generate <code>784</code> probabilities (one for each pixel):</p> <p>$$ p_\\theta(x = 1 \\mid z) = (p_1, p_2, \\dots, p_{784}). $$</p> <p>Here, each $p_j \\in [0,1]$ is the model's predicted probability that pixel $j$ is \"on\". With these probabilities, the conditional likelihood of the image $x$ given the latent variable $z$, also denoted by $p_\\theta(x \\mid z)$ (now a scalar) is:</p> <p>$$ p_\\theta(x \\mid z) = \\prod_{j=1}^{784} \\text{Bernoulli}(x_j \\mid p_j), $$</p> <p>where $x_j$ is the observed pixel value in the data. This expression defines a Bernoulli distribution for each pixel, and the full likelihood is the product over all pixels, assuming independence. It tells us how likely a particular image $x$ is, given a latent vector $z$.</p>"},{"location":"notebooks/07_vae/#intractable-integral","title":"Intractable Integral\u00b6","text":"<p>As we have already introduced the latent variable $z$ to our probabilistic model, we can suggest that our objective is to learn the joint distribution $p(x, z)$, which has a well-known formula:</p> <p>$$ p_\\theta(x, z) = p_\\theta(x \\mid z)\\, p(z). $$</p> <p>Both terms on the right are tractable. The decoder will produce the conditional likelihood $p_\\theta(x \\mid z)$ via a forward pass. The prior $p(z)$ we can choose to be a simple Gaussian, which has a closed-form density function we can easily compute. The difficulty appears only when we try to compute the marginal likelihood (evidence) of an observed image $x$, which is our approximation of $p^\\ast(x)$, given by another well-known formula:</p> <p>$$ p_\\theta(x) = \\int p_\\theta(x, z)\\, dz. $$</p> <p>Suppose a simple example of a 1-dimensional latent variable, where $z \\in \\mathbb{R}$ is a single real number. Even in this 1D case, computing the integral exactly requires evaluating the network $p_\\theta(x \\mid z)$ at every possible real value of $z$ from $ -\\infty$ to $ +\\infty$. Even if each dimension were sampled at only $100$ points, this would require $100^{20}$ evaluations for a 20-dimensional latent, which is a number larger than the number of atoms in the universe.</p> <p>Hence, for a continuous high-dimensional latent vector, evaluating the integral above is computationally impossible due to integral not having a closed-form solution. Integration requires evaluating the decoder network at every point in an $L$-dimensional space which is intractable.</p> <p>If we could somehow calculate this integral exactly, training the model would be straightforward. For each data point, we would compute log-likelihood $\\log p_\\theta(x)$ and optimize it with respect to $\\theta$ using ordinary gradient descent. Alas, the integral is intractable.</p>"},{"location":"notebooks/07_vae/#encoder-network","title":"Encoder Network\u00b6","text":"<p>Let's introduce the posterior distribution $p_\\theta(z \\mid x)$ to our probabilistic model. This distribution tells us how likely each latent vector $z$ is after we observe a particular data point $x$. We can define the posterior through Bayes' rule:</p> <p>$$ p_\\theta(z \\mid x) = \\frac{p_\\theta(x \\mid z)\\, p(z)}{p_\\theta(x)} = \\frac{p_\\theta(x, z)}{\\int p_\\theta(x, z)\\, dz}. $$</p> <p>However, the posterior is also intractable, as the denominator is intractable. How to tackle this problem? Authors of the Variational Autoencoder (VAE) paper introduce the encoder (recognition) model which aims at optimizing variational parameters $\\phi$ such that the generated posterior $q_\\phi(z \\mid x)$ is as close as possible to the intractable true posterior $p_\\theta(z \\mid x)$.</p> <p>In simple terms, the encoder is a neural network with learnable parameters $\\phi$ that outputs an approximate mean and variance of the required Gaussian distribution for each input $x$. Instead of sampling $z$ randomly out of blue, we will sample our latent variable from the learned distribution $q_\\phi(z \\mid x)$ which is dependent on the input data.</p>"},{"location":"notebooks/07_vae/#evidence-lower-bound-elbo","title":"Evidence Lower Bound (ELBO)\u00b6","text":"<p>Recall that we couldn't maximize the marginal likelihood $\\log p_\\theta(x)$ due to intractability. It turns out, with a nice trick up our sleeves, we can rewrite the marginal likelihood as expectation. The trick is that integrating any probability density over its entire domain is equal to $1$.</p> <p>$$ p_\\theta(x) = p_\\theta(x)\\int q_\\phi(z \\mid x)\\,dz  = \\mathbb{E}_{q_\\phi(z \\mid x)}[\\,p_\\theta(x)\\,]. $$</p> <p>Our likelihood function $p_\\theta(x)$ above returns a constant for each observed data point which does not depend on $z$. A constant function means that every possible value of the variable, no matter its probability, produces exactly the same output, so the weighted average (expectation) cannot change that value.</p> <p>Once we rewrite our likelihood as expectation, we can now make use of our variational posterior and Bayes rule to reach the following equation for log-likelihood:</p> <p>$$ \\begin{aligned} \\log p_\\theta(x) &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)}[\\, \\log p_\\theta(x)\\,] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)}\\!\\left[\\, \\log \\left[ \\frac{p_\\theta(x,z)}{p_\\theta(z \\mid x)} \\right]\\right] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)}\\!\\left[\\, \\log \\left[\\frac{p_\\theta(x,z)}{q_\\phi(z \\mid x)} \\cdot \\frac{q_\\phi(z \\mid x)}{p_\\theta(z \\mid x)}\\right] \\right] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)}\\!\\left[\\log \\frac{p_\\theta(x,z)}{q_\\phi(z \\mid x)}\\right] + \\mathbb{E}_{q_\\phi(z \\mid x)}\\!\\left[\\log \\frac{q_\\phi(z \\mid x)}{p_\\theta(z \\mid x)}\\right]. \\end{aligned} $$</p> <p>Here, the equation on the right corresponds to the Kullback-Leibler divergence formula $\\mathrm{KL}\\!\\left[q_\\phi(z \\mid x)\\,\\|\\, p_\\theta(z \\mid x)\\right]$ between variational and true posteriors. As KL divergence is positive or zero (in case our encoder somehow generates the true posterior), and $p_\\theta(z \\mid x)$ is unknown, we can ignore the KL divergence and concentrate on the first equation, which we call Evidence Lower Bound (ELBO). We can safely claim that our equation on the left will return a lower-bound of the log-likelihood for observed data:</p> <p>$$\\mathcal{L}_{\\theta,\\phi}(x) \\leq \\log p_\\theta(x).$$</p> <p>We can rewrite ELBO equation further:</p> <p>$$ \\begin{aligned} \\mathcal{L}_{\\theta,\\phi}(x) &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)}\\!\\left[\\log \\frac{p_\\theta(x,z)}{q_\\phi(z \\mid x)}\\right] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)} \\big[\\log p_\\theta(x,z) - \\log q_\\phi(z \\mid x)\\big] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)} \\big[\\log p_\\theta(x \\mid z) + \\log p_\\theta(z) - \\log q_\\phi(z \\mid x)\\big] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)}\\big[\\log p_\\theta(x \\mid z)\\big] - \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\, p_\\theta(z)\\right). \\end{aligned} $$</p> <p>It turns out, instead of maximizing the log-likelihood of our evidence (a single data point) which is intractable, we can aim at maximizing ELBO, the lower bound of our evidence likelihood, which is fully tractable. We can simply plug in and calculate the solution for the equation above and optimize our network for both $\\phi$ and $\\theta$ within a single backward pass.</p>"},{"location":"notebooks/07_vae/#unbiased-gradient-estimation","title":"Unbiased Gradient Estimation\u00b6","text":"<p>We should now calculate gradients but we have a problem again. A single data point ELBO gradient is intractable as the expected value of the likelihood requires computing integral over all possible latent variables. Luckily, we can find an unbiased estimator of the gradient w.r.t. $\\theta$ with the help of Monte Carlo sampling.</p> <p>$$ \\begin{aligned} \\nabla_{\\theta} \\mathcal{L}_{\\theta,\\phi}(x) &amp;= \\nabla_{\\theta}\\, \\mathbb{E}_{q_{\\phi}(z \\mid x)}     \\left[ \\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x) \\right] \\\\ &amp;= \\mathbb{E}_{q_{\\phi}(z \\mid x)}     \\left[ \\nabla_{\\theta}\\left( \\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x) \\right) \\right] \\\\ &amp;\\simeq \\nabla_{\\theta}\\left( \\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x) \\right) \\\\ &amp;= \\nabla_{\\theta}\\log p_{\\theta}(x, z). \\end{aligned} $$</p> <p>Monte Carlo estimator allows us to draw a few samples from $q_{\\phi}(z \\mid x)$ and approximate the expectation by averaging the function values at those samples. This works because the average of randomly drawn samples converges to the true expectation as $K$ increases. Using this approximation inside the gradient gives</p> <p>$$ \\nabla_{\\theta}\\, \\mathbb{E}_{q_{\\phi}(z \\mid x)}[f(z)] \\approx \\frac{1}{K} \\sum_{k=1}^{K} \\nabla_{\\theta} f\\!\\left(z^{(k)}\\right), \\qquad z^{(k)} \\sim q_{\\phi}(z \\mid x). $$</p> <p>which is an unbiased estimate of the true gradient. During neural network training, a single sample already provides an unbiased Monte Carlo estimate, as averaging comes from the minibatch: if a batch contains $B$ data points, the stochastic gradient $\\frac{1}{B} \\sum_{i=1}^{B} \\nabla f\\!\\left(z^{(1)}_i\\right)$ acts as the Monte Carlo average. Hence, we can use $K = 1$ and rely on minibatches to reduce variance.</p>"},{"location":"notebooks/07_vae/#reparametrization-trick","title":"Reparametrization Trick\u00b6","text":"<p>Estimating gradients w.r.t variational parameters $\\phi$ is more complicated. The parameter $\\phi$ appears inside the distribution $q_{\\phi}(z \\mid x)$, which means that when we take a gradient of the ELBO, we must also differentiate through the sampling operation itself:</p> <p>$$ \\begin{aligned} \\nabla_{\\phi} \\mathcal{L}_{\\theta,\\phi}(x) &amp;= \\nabla_{\\phi}\\, \\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left[ \\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x) \\right]. \\\\ &amp;\\neq \\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left[\\nabla_{\\phi}\\, (\\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x) \\right)]. \\end{aligned} $$</p> <p>To obtain a usable gradient estimator, we rewrite the sampling process in a differentiable form. Instead of sampling $z$ directly from $q_{\\phi}(z \\mid x)$, we introduce a noise variable $\\epsilon$ that does not depend on $\\phi$:</p> <p>$$ z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x)\\,\\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0, I). $$</p> <p>This is the reparameterization trick. Now the randomness comes only from $\\epsilon$, while $z$ becomes a deterministic and differentiable function of $\\phi$. Using this transformation, the expectation can be rewritten as</p> <p>$$ \\mathbb{E}_{q_{\\phi}(z \\mid x)}[f(z)] = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[ f\\!\\left(\\mu_{\\phi}(x) + \\sigma_{\\phi}(x)\\epsilon\\right) \\right], $$</p> <p>allowing gradients to pass through $z$ via standard backpropagation. The resulting gradient becomes</p> <p>$$ \\begin{aligned} \\nabla_{\\phi} \\mathcal{L}_{\\theta,\\phi}(x) &amp;= \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[ \\nabla_{\\phi} \\left( \\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x) \\right) \\right]. \\end{aligned} $$</p> <p>where $z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x)\\, \\epsilon$ which provides a low-variance and unbiased estimator that makes VAE optimization possible. As a consequence, a single data point Monte Carlo estimator becomes</p> <p>$$ \\tilde{\\mathcal{L}}_{\\theta,\\phi}(x) = \\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x). $$</p> <p>Applying reparametrization trick allows PyTorch smoothly use autograd engine when calculating gradients.</p>"},{"location":"notebooks/07_vae/#vae-training","title":"VAE Training\u00b6","text":"<p>We now have built all the tools required for coding the training logic of VAE. Almost all the tools. As the derivation is complicated and distractive for the main text, I will note the formula for the closed-form ELBO KL-divergence term in the appendix.</p>"},{"location":"notebooks/07_vae/#vae-generation","title":"VAE Generation\u00b6","text":"<p>After we train VAE network, all we need to do is sample our $z$ from a Guassian prior and pass it to our decoder for generation. If we have done everything right, we may even see some MNIST-like numbers on the screen.</p>"},{"location":"notebooks/07_vae/#closed-form-gaussian-kl-divergence","title":"Closed-Form Gaussian KL-divergence\u00b6","text":"<p>We already know that $\\log p_\\theta(x \\mid z)$ can be treated as a BCE loss. However, we still need to figure out closed-form solutions for finding the right-hand side of ELBO, which has the following log form:</p> <p>$$ \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right) = \\mathbb{E}_{q_\\phi(z\\mid x)} \\Big[ \\log q_\\phi(z\\mid x) - \\log p(z) \\Big]. $$</p> <p>Gaussian family is one of the few distributions where the KL-divergence has a simple analytic form. For example, in 1D, approximate posterior distribution $q_\\phi(z \\mid x)$ and prior could be sampled from $\\mathcal{N}(\\mu, \\sigma^2)$ and $\\mathcal{N}(0, 1)$ respectively. Knowing that standard normal is a special case of normal distribution, we have:</p> <p>\\begin{aligned} \\log q_\\phi(z \\mid x) &amp;= \\log\\!\\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\!\\left(-\\frac{(z-\\mu)^{2}}{2\\sigma^{2}}\\right) \\right] \\\\ &amp;= -\\frac{1}{2}\\log(2\\pi\\sigma^{2}) - \\frac{(z-\\mu)^{2}}{2\\sigma^{2}}. \\end{aligned}</p> <p>Therefore, subtracting Gaussians has the following form:</p> <p>\\begin{aligned} \\log q_\\phi(z \\mid x) - \\log p(z) &amp;= \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma^{2}) - \\frac{(z-\\mu)^{2}}{2\\sigma^{2}} \\right] - \\left[ -\\frac{1}{2}\\log(2\\pi) - \\frac{z^{2}}{2} \\right] \\\\ &amp;= -\\frac{1}{2}\\log\\sigma^{2} - \\frac{(z-\\mu)^{2}}{2\\sigma^{2}} + \\frac{z^{2}}{2}. \\end{aligned}</p> <p>Substituting these moments into the KL expression with its expectation form, we obtain: $$ \\begin{aligned} \\mathbb{E}_{q_\\phi(z\\mid x)} \\Big[ \\log q_\\phi(z\\mid x) - \\log p(z) \\Big] &amp;= \\mathbb{E}_{q_\\phi(z \\mid x)} \\Big[ -\\tfrac{1}{2}\\log\\sigma^{2} - \\tfrac{(z-\\mu)^{2}}{2\\sigma^{2}} + \\tfrac{z^{2}}{2} \\Big] \\\\[6pt] &amp;= -\\tfrac{1}{2}\\log\\sigma^{2} - \\tfrac{1}{2\\sigma^{2}} \\mathbb{E}_{q_\\phi(z \\mid x)}[(z-\\mu)^{2}] + \\tfrac{1}{2} \\mathbb{E}_{q_\\phi(z \\mid x)}[z^{2}]. \\end{aligned} $$</p> <p>which becomes, after substituting the known Gaussian moments $\\sigma^{2}$ and $\\mu^{2} + \\sigma^{2}$:</p> <p>$$ \\begin{aligned} \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right) &amp;= -\\tfrac{1}{2}\\log\\sigma^{2} - \\tfrac{1}{2} + \\tfrac{1}{2}\\mu^{2} + \\tfrac{1}{2}\\sigma^{2} \\\\[6pt] &amp;= \\tfrac{1}{2} \\left( \\mu^{2} + \\sigma^{2} - 1 - \\log\\sigma^{2} \\right). \\end{aligned} $$</p> <p>For $L$-dimensional latent variable, approximate posterior we choose to be a multivariate Gaussian with a diagonal covariance: $q_\\phi(z \\mid x)=\\mathcal{N}\\!\\big(\\mu, \\mathrm{diag}(\\sigma^2)\\big),$ and the prior a standard multivariate normal: $p(z) = \\mathcal{N}(0, I).$ Because the covariance of $q_\\phi(z \\mid x)$ is diagonal, both $q_\\phi(z \\mid x)$ and $p(z)$ factorize over dimensions: $$ q_\\phi(z \\mid x) = \\prod_{j=1}^L q_{\\phi,j}(z_j \\mid x), \\qquad p(z) = \\prod_{j=1}^L p_j(z_j). $$</p> <p>From here, we can derive the final closed-form equation for the KL-divergence between approximate posterior and prior:</p> <p>$$ \\begin{aligned} \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right) &amp;= \\sum_{j=1}^L \\mathrm{KL}\\!\\left(q_{\\phi,j}(z_j \\mid x)\\,\\|\\,p_j(z_j)\\right) \\\\ &amp;= \\tfrac{1}{2} \\sum_{j=1}^L \\left( \\mu_j^{2} + \\sigma_j^{2} - 1 - \\log\\sigma_j^{2} \\right). \\end{aligned} $$</p>"},{"location":"notebooks/07_vae/#cifar-10","title":"CIFAR-10\u00b6","text":""},{"location":"supplementary/","title":"Suppementary Material","text":"<p>Important</p> <p>     The page is currently under development.   </p> <p>This page contains optional supplementary material intended to support the main course content. It provides additional background, intuition, or alternative explanations but is not required for assessments.</p>"},{"location":"supplementary/pca/","title":"Principal Component Analysis","text":"<p>Important</p> <p>     The page is currently under development.   </p>"},{"location":"supplementary/svd/","title":"Singular Value Decomposition","text":"16 Oct 2025 <p>Presented below is an important concept of the introductory linear algebra called singular value decomposition, frequently implemented in machine/deep learning frameworks for optimizing calculations. It is the engine of Principal Component Analysis and is used for obtaining pseudoinverse. Below, I tried to build the understanding gradually by providing necessary background and quick proofs. The text presumes prior knowledge on eigenvalues and eigenvectors.  </p>"},{"location":"supplementary/svd/#eigendecomposition","title":"Eigendecomposition","text":"<p>By definition, multiplying a matrix \\(A\\) by one of its eigenvectors scales that eigenvector by the corresponding eigenvalue. Hence, \\(AQ = Q\\Lambda\\), where \\(Q\\) is the matrix whose columns are the linearly independent eigenvectors of \\(A\\) (if such a full set exists, \\(Q\\) forms an eigenbasis), and \\(\\Lambda\\) is a diagonal matrix whose diagonal entries are the eigenvalues of \\(A\\). If \\(Q\\) is invertible, we can write:  $$ A Q Q^{-1} = Q \\Lambda Q^{-1} \\quad \\Rightarrow \\quad A = Q \\Lambda Q^{-1}. $$ As a result, applying \\(A\\) to any vector \\(v\\) is equivalent to applying \\(Q \\Lambda Q^{-1}\\) to \\(v\\), since both represent the same linear transformation. Geometrically, the coordinates of \\(v\\) are first expressed in the eigenbasis via \\(Q^{-1}\\), then scaled by the eigenvalues via \\(\\Lambda\\), and finally transformed back to the original standard basis (e.g. \\(\\hat{\\imath}, \\hat{\\jmath}\\)) via \\(Q\\).</p> <p>For a symmetric matrix, eigenvectors corresponding to distinct eigenvalues are orthogonal. If \\(A\\mathbf{v}_1 = \\lambda_1 \\mathbf{v}_1\\) and \\(A\\mathbf{v}_2 = \\lambda_2 \\mathbf{v}_2\\), then $ \\mathbf{v}_1^T A \\mathbf{v}_2 = \\lambda_2 \\mathbf{v}_1^T \\mathbf{v}_2 $. Because \\(A^T = A\\) and \\((XY)^T = Y^T X^T\\), we can write $ \\mathbf{v}_1^T A \\mathbf{v}_2 = (A^T \\mathbf{v}_1)^T \\mathbf{v}_2 = (A \\mathbf{v}_1)^T \\mathbf{v}_2 = \\lambda_1 \\mathbf{v}_1^T \\mathbf{v}_2$. Combining both gives \\((\\lambda_1 - \\lambda_2)\\mathbf{v}_1^T \\mathbf{v}_2 = 0\\), and since \\(\\lambda_1 \\neq \\lambda_2\\), it follows that $\\mathbf{v}_1^T \\mathbf{v}_2 = 0 $.</p> <p>Thus, the eigenvectors can be chosen orthonormal, and \\(Q\\) is orthogonal with \\(Q^{-1} = Q^T\\). The diagonalization becomes  $$ A = Q \\Lambda Q^T. $$ Geometrically, orthogonal \\(Q\\) represents a rotation or reflection, which preserves inner products and thus lengths and angles. This means orthogonal transformations preserve the geometric structure of space:  $$  | Q\\mathbf{v} |^2 = (Q\\mathbf{v})^T (Q\\mathbf{v}) = \\mathbf{v}^T Q^T Q \\mathbf{v} = \\mathbf{v}^T \\mathbf{v} = | \\mathbf{v} |^2, $$  showing that the squared length remains unchanged. Consequently, a symmetric matrix scales space only along mutually perpendicular eigenvector directions without introducing shear or non-uniform angular distortion.</p>"},{"location":"supplementary/svd/#singular-value-decomposition_1","title":"Singular Value Decomposition","text":"<p>Eigendecomposition described above is a powerful tool, but it has two major limitations: it only applies to square matrices, and even then, not every square matrix is diagonalizable (some lack a full set of linearly independent eigenvectors). In practical applications we usually deal with rectangular matrices or non-diagonalizable ones.</p> <p>Quadratic form is an expression of the type $ \\mathbf{x}^T A \\mathbf{x} $, where \\(A\\) is a square matrix and \\(\\mathbf{x} \\in \\mathbb{R}^n\\). It arises naturally when measuring the squared length of a vector after a linear transformation. If we first transform \\(\\mathbf{x}\\) by a matrix \\(A\\), the squared length becomes $$ |A\\mathbf{x}|^2 = (A\\mathbf{x})^T (A\\mathbf{x}) = \\mathbf{x}^T A^T A \\mathbf{x}. $$ Quadratic form shows how \\(A\\) scales or distorts space along different directions. Let $ \\mathbf{y} = A \\mathbf{x} $, then \\(\\mathbf{x}^T A \\mathbf{x} = \\mathbf{x}^T (A \\mathbf{x}) = \\mathbf{x}^T \\mathbf{y}\\) is the dot product between the original vector $ \\mathbf{x} $ and its transformed version $ A\\mathbf{x} $. If $ \\mathbf{x}^T A \\mathbf{x} $ is large and positive, then $ A $ stretches $ \\mathbf{x} $ strongly in roughly the same direction. If $ \\mathbf{x}^T A \\mathbf{x} = 0 $, then $ A $ sends $ \\mathbf{x} $ to a vector orthogonal to itself -- a pure rotation-like behavior. If $ \\mathbf{x}^T A \\mathbf{x} $ is negative, then $ A $ flips $ \\mathbf{x} $ in the opposite direction to its original orientation.</p> <p>Both \\(A^T A\\) (size \\(n \\times n\\)) and \\(A A^T\\) (size \\(m \\times m\\)) are always symmetric. This is because \\((A^T A)^T = A^T (A^T)^T = A^T A\\), and similarly \\((A A^T)^T = (A^T)^T A^T = A A^T\\). They are also positive semidefinite: for any nonzero vector \\(\\mathbf{x}\\), we have quadratic form $ \\mathbf{x}^T (A^T A) \\mathbf{x} = (A \\mathbf{x})^T (A \\mathbf{x}) = | A \\mathbf{x} |^2 \\geq 0 $, and  $ \\mathbf{y}^T (A A^T) \\mathbf{y} = (A^T \\mathbf{y})^T (A^T \\mathbf{y}) = | A^T \\mathbf{y} |^2 \\geq 0 $. Because they are symmetric, both matrices are diagonalizable with orthogonal eigenvectors. </p> <p>Let \\(A^T A \\mathbf{v}_i = \\sigma_i^2 \\mathbf{v}_i\\), where \\(\\mathbf{v}_i\\) are orthonormal eigenvectors and \\(\\sigma_i^2\\) are the eigenvalues of \\(A^T A\\). Then: $$ |A \\mathbf{v}_i|^2 = \\mathbf{v}_i^T A^T A \\mathbf{v}_i = \\sigma_i^2 \\mathbf{v}_i^T \\mathbf{v}_i = \\sigma_i^2 $$ implies that \\(\\sigma_i^2\\) is the squared length of the transformed unit vector. Because \\(A^T A\\) is positive semidefinite, we define \\(\\sigma_i = \\sqrt{\\sigma_i^2} \\ge 0\\) as the singular values of \\(A\\). The new vector \\(A\\mathbf{v}_i\\) has length \\(\\sigma_i\\), hence the matrix \\(A\\) transforms the eigenvectors of \\(A^T A\\) by scaling their lengths by the square roots of the eigenvalues (singular values) of \\(A^T A\\). The eigenvectors themselves indicate the directions along which \\(A\\) changes length by exactly \\(\\sigma_i\\).</p> <p>We define the Singular Value Decomposition (SVD) of \\(A\\) as: $$ A = U \\Sigma V^T. $$ Here \\(V\\) is an \\(n \\times n\\) orthogonal matrix whose columns \\(\\mathbf{v}_i\\) are eigenvectors of \\(A^T A\\) (right singular vectors), \\(\\Sigma\\) is an \\(m \\times n\\) diagonal matrix whose diagonal entries \\(\\sigma_i\\) are the singular values of \\(A\\), and \\(U\\) is an \\(m \\times m\\) orthogonal matrix whose columns \\(\\mathbf{u}_i\\) are eigenvectors of \\(A A^T\\) (left singular vectors).</p> <p>The columns of \\(U\\) arise similarly from the eigendecomposition of \\(A A^T\\). If \\(A A^T \\mathbf{u}_i = \\sigma_i^2 \\mathbf{u}_i,\\) then \\(\\mathbf{u}_i\\) is a unit eigenvector of \\(A A^T\\), and \\(\\sigma_i^2\\) is the same eigenvalue as before. Since \\(A A^T\\) is \\(m \\times m\\) and symmetric, it is diagonalizable with an orthonormal set of eigenvectors forming the columns of \\(U\\).</p> <p>Once we have computed the eigenvalues and right singular vectors of \\(A^T A\\), we can obtain the left singular vectors directly from them. Multiplying both sides of \\(A^T A \\mathbf{v}_i = \\sigma_i^2 \\mathbf{v}_i\\) by \\(A\\) gives $ (A A^T)(A \\mathbf{v}_i) = \\sigma_i^2 (A \\mathbf{v}_i) $,  so \\(A \\mathbf{v}_i\\) is an eigenvector of \\(A A^T\\) with eigenvalue \\(\\sigma_i^2\\). To achieve orthonormal basis, we normalize \\(A \\mathbf{v}_i\\) to unit length. Having \\(\\|A \\mathbf{v}_i\\| = \\sigma_i\\) leads to \\(\\mathbf{u}_i = \\sigma_i^{-1} A \\mathbf{v}_i.\\) Multiplying $ A \\mathbf{v}_i = \\sigma_i \\mathbf{u}_i $ by \\(A^T\\) on the left yields $ A^T \\mathbf{u}_i = \\sigma_i \\mathbf{v}_i $. These equations guarantee that SVD formula is consistent: multiplying both sides on the right by \\(V\\) gives \\(A V = U \\Sigma\\), and multiplying on the left by \\(U^T\\) gives \\(U^T A = \\Sigma V^T\\).</p> <p>Geometrically, \\(A\\) takes the unit vector in the direction of $ \\mathbf{v}_i $ in the input space, rotates (or reflects) it into the new orthogonal direction $ \\mathbf{u}_i $ in the output space, and stretches it by a factor of $ \\sigma_i $. Similarly, \\(A^T\\) takes the unit vector in the direction of $ \\mathbf{u}_i $ in the output space, maps it back into the corresponding direction $ \\mathbf{v}_i $ in the input space, and scales it by the same factor $ \\sigma_i $. Hence, \\(A\\) can be understood as a composition of three transformations: a rotation or reflection by \\(V^T\\), a scaling along orthogonal axes by \\(\\Sigma\\), and a rotation or reflection by \\(U\\). For any vector \\(\\mathbf{v}\\), $ A\\mathbf{v} = U \\Sigma V^T \\mathbf{v} $,  so \\(\\mathbf{v}\\) is first expressed in the basis of right singular vectors by \\(V^T\\), then scaled by singular values by \\(\\Sigma\\), and finally mapped into the new space in the basis of left singular vectors by \\(U\\). Thus, we extend the concept of eigendecomposition to all real matrices.</p>"}]}